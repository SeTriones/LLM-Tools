[
  {
    "id": "gOG9Zoyn4R",
    "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
    "abstract": "Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.",
    "authors": [
      "~Seo_Hyun_Kim1",
      "~Sunwoo_Hong1",
      "~Hojung_Jung1",
      "~Youngrok_Park1",
      "~Se-Young_Yun1"
    ],
    "pdf": "/pdf/c9e51b3a7fd8e3054c62f2565b0c9f7dc9ec8cb8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference optimization, achieving significant speedups (2.78x) without additional training. It focuses on improving GPU utilization and throughput during the inference process of language generation models. The approach uses KL divergence to optimize the sampling process, which is a clever inference optimization technique that reduces latency while maintaining output quality.",
      "Irrelevant Aspects": "The research focuses on masked diffusion models rather than the transformer-based LLMs that are central to my work. It doesn't address distributed training strategies, model parallelism techniques, or memory optimization methods that are crucial for large-scale LLM deployment. The paper doesn't discuss quantization, pruning, or other compression techniques that are vital for LLM inference optimization.",
      "Summary": "KLASS presents a novel inference optimization technique for masked diffusion models that uses KL divergence to accelerate sampling. While it achieves impressive speedups and demonstrates broad applicability across domains, its focus on diffusion models rather than transformer architectures limits its direct relevance to my core research interests in LLM optimization."
    }
  },
  {
    "id": "JeP0lpusYw",
    "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
    "abstract": "Model merging is a technique that combines multiple large pretrained models into a single model, enhancing performance and broadening task adaptability without original data or additional training. However, most existing model merging methods focus primarily on exploring the parameter space, merging models with identical architectures. Despite its potential, merging in the architecture space remains in its early stages due to the vast search space and challenges related to layer compatibility. This paper designs a hierarchical model merging framework named HM3, formulating a bilevel multi-objective model merging problem across both parameter and architecture spaces. At the parameter level, HM3 integrates existing merging methods to quickly identify optimal parameters. Based on these, an actor-critic strategy with efficient policy discretization is employed at the architecture level to explore inference paths with Markov property in the layer-granularity search space for reconstructing these optimal models. By training reusable policy and value networks, HM3 learns Pareto optimal models to provide customized solutions for various tasks. Experimental results on language and vision tasks demonstrate that HM3 outperforms methods focusing solely on the parameter or architecture space.",
    "authors": [
      "~Yu_Zhou25",
      "~Xingyu_Wu3",
      "~Jibin_Wu1",
      "~Liang_Feng3",
      "~KC_Tan1"
    ],
    "pdf": "/pdf/0c8cef2f64069880e00648aa0b4deae962c9b746.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Model merging for large pretrained models, which can optimize deployment and resource usage. Addressing both parameter and architecture spaces. Enhancing performance without additional training. Exploring inference paths relevant to inference optimization. Efficient policy discretization suggesting computational efficiency. Pareto optimal models potentially balancing performance and resource usage. Experiments on language tasks including LLMs.",
      "Irrelevant Aspects": "Limited focus on explicit GPU utilization optimization. No direct mention of training optimization. Doesn't appear to address scalability in distributed systems or cluster management. Limited discussion on throughput and latency metrics.",
      "Summary": "This paper presents HM3, a hierarchical framework for merging multiple large pretrained models across both parameter and architecture spaces. It formulates a bilevel multi-objective optimization problem, using existing merging methods at the parameter level and an actor-critic strategy at the architecture level to explore efficient inference paths. While the work is relevant to inference optimization and resource efficiency through model merging, it doesn't directly address training optimization or explicit GPU utilization improvements, which are key aspects of my research focus."
    }
  },
  {
    "id": "l4F50jpiVH",
    "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
    "abstract": "We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.",
    "authors": [
      "~Deokjae_Lee1",
      "~Hyun_Oh_Song1"
    ],
    "pdf": "/pdf/518ecfba9ed223f4e43c3783e924fd758b01eb9e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on weight-only post-training quantization (PTQ) for LLMs, which directly addresses inference optimization. It specifically targets memory footprint and latency reduction, which are key for efficient LLM deployment. The introduction of Q-Palette, a collection of fractional-bit quantizers with optimized CUDA kernels, directly impacts GPU utilization. The paper proposes a mixed-scheme quantization framework that jointly optimizes quantizer choices and layer fusion decisions given resource constraints, which is highly relevant for improving throughput and reducing latency. It addresses memory-bound, small-batch inference scenarios, which are common bottlenecks in LLM deployment.",
      "Irrelevant Aspects": "The paper focuses exclusively on post-training quantization and doesn't address training optimization. There's limited discussion of scalability across multiple GPUs or distributed systems. The focus appears to be more on edge device deployment rather than large-scale data center deployments. The paper doesn't seem to address other inference optimization techniques beyond quantization.",
      "Summary": "This paper introduces Q-Palette, a framework for weight-only post-training quantization of LLMs that uses information-theoretic analysis to develop fractional-bit quantizers. It provides both theoretical insights (optimal bit allocation for Gaussianized weights) and practical implementations (optimized CUDA kernels). The work directly addresses key challenges in LLM inference optimization including memory footprint reduction and latency improvements, particularly in memory-bound scenarios. While highly relevant for inference optimization and GPU utilization, it doesn't cover training optimization or distributed scalability aspects of ML systems."
    }
  },
  {
    "id": "Ve693NkzcU",
    "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$  Pruning",
    "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been of great importance recently. However, most existing sparse attention algorithms use a fixed budget of how many tokens to use in their computations. This simple static decision raises critical issues in real-world deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. \nIn this paper, we reveal a key insight that leveraging the idea of top-$p$ sampling (a.k.a., nucleus sampling) in sparse attention could enable efficient and adaptive budget decisions. Based on this, we propose Twilight, a framework that enhances any existing sparse attention algorithm with adaptive budget decision capabilities without sacrificing accuracy. \nEmpirical results show that Twilight can adaptively prune up to 98% tokens with nearly no accuracy loss in both mid- and long-context scenarios, leading to a $1.4\\times$ speedup over state-of-the-art sparse attention mechanisms.",
    "authors": [
      "~Chaofan_Lin1",
      "~Jiaming_Tang1",
      "~Shuo_Yang22",
      "~Hanshuo_Wang1",
      "~Tian_Tang1",
      "~Boyu_Tian1",
      "~Ion_Stoica1",
      "~Song_Han5",
      "~Mingyu_Gao1"
    ],
    "pdf": "/pdf/690f5f6390bd74e24fcac72c3bcf39e1a480e996.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on attention sparsity for long-context LLMs, which directly addresses inference optimization - a core interest. It proposes adaptive budget decisions using top-p sampling ideas, which can improve GPU utilization by reducing unnecessary computations. The reported 98% token pruning with minimal accuracy loss and 1.4x speedup over existing methods directly contributes to higher throughput and lower latency goals. The hierarchical approach and adaptiveness are relevant to scalable system design.",
      "Irrelevant Aspects": "The paper appears to focus primarily on inference optimization rather than training optimization. There's limited mention of hardware-specific implementations or multi-GPU scaling strategies. The abstract doesn't indicate detailed memory optimization techniques, which are often crucial for LLM deployment. There may be insufficient discussion on integration with existing ML frameworks.",
      "Summary": "Twilight presents an adaptive attention sparsity framework using hierarchical top-p pruning to accelerate long-context LLMs. The approach enables dynamic token pruning decisions (up to 98%) based on input characteristics rather than fixed budgets, achieving significant speedups while maintaining accuracy. This work is highly relevant to inference optimization, GPU utilization, and throughput improvement goals in large language model systems."
    }
  },
  {
    "id": "cCefuzQrjK",
    "title": "A geometric framework for momentum-based optimizers for low-rank training",
    "abstract": "Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we identify and analyze potential difficulties that these training methods encounter when used to train low-rank parameterizations of weights. In particular, we show that classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape. To address this, we introduce novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space. We validate our methods through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.",
    "authors": [
      "~Steffen_Schotthöfer1",
      "~Timon_Klein1",
      "~Jonas_Kusch1"
    ],
    "pdf": "/pdf/4a097e0e25ed0747750b7c27b5d9664c5d4df566.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization for large neural networks through low-rank parameterizations, which directly relates to reducing computational costs and improving efficiency in training large language models. The focus on faster convergence aligns with the goal of better GPU utilization and higher throughput. The techniques for reducing computational and storage costs could potentially enable training larger models with limited resources.",
      "Irrelevant Aspects": "The paper doesn't explicitly address inference optimization, which is a key part of my research focus. There's limited discussion on distributed training scalability or specific GPU utilization strategies. The work appears more theoretical and mathematical, focusing on optimization geometry rather than practical implementation details for low-latency inference or specific GPU architectural optimizations.",
      "Summary": "This paper introduces geometrically-aware optimizers for low-rank neural network training, addressing convergence issues in standard momentum methods. By incorporating dynamical low-rank approximation principles, the approach shows faster convergence and improved performance within parameter budgets. While valuable for training efficiency optimization, it lacks direct relevance to inference optimization and specific GPU utilization strategies."
    }
  },
  {
    "id": "OM0Qkq9xtY",
    "title": "Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected",
    "abstract": "This study aims to enlarge our current knowledge on the application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) emulates the synaptic turnover of real brain networks, reducing the computational demands of training and inference in ANNs. However, existing DST methods face difficulties in maintaining peak performance at high connectivity sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method that is used in DST for growing synaptic connectivity in sparse neural networks. CHT leverages a gradient-free, topology-driven link regrowth mechanism, which has been shown to achieve ultra-sparse (1\\% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $\\mathcal{O}(N\\cdot d^3)$- N node network size, d node degree - hence it can be efficiently applied only to ultra-sparse networks. (ii) it rigidly selects top link prediction scores, which is inappropriate for the early training epochs, when the network topology presents many unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. Then, we propose a matrix multiplication GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to $\\mathcal{O}(N^3)$, enabling a fast implementation of link prediction in large-scale models. Moreover, we introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we propose a sigmoid-based gradual density decay strategy, leading to an advanced framework referred to as CHTss. Empirical results show that BRF offers performance advantages over previous network science models. Using 1\\% of connections, CHTs outperforms fully connected networks in MLP architectures on visual classification tasks, compressing some networks to less than 30\\% of the nodes. Using 5\\% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, with only 30\\% of the connections, both CHTs and CHTss achieve superior performance over other dynamic sparse training methods, and perform on par with—or even surpass—their fully connected counterparts in language modeling across various sparsity levels within the LLaMA model family. The code is available at: https://github.com/biomedical-cybernetics/Cannistraci-Hebb-training.",
    "authors": [
      "~Yingtao_Zhang3",
      "~Diego_Cerretti1",
      "~Jialin_Zhao1",
      "~Wenjing_Wu1",
      "~Ziheng_Liao1",
      "~Umberto_Michieli1",
      "~Carlo_Vittorio_Cannistraci1"
    ],
    "pdf": "/pdf/a117458f401ed7fbd494ab0f2eb44f9fba605ea8.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses sparse connectivity in Transformers and LLaMA models, which is crucial for GPU optimization. It presents methods to maintain performance with only 1-30% of connections, directly impacting memory usage and computation. The GPU-friendly matrix multiplication approximation reduces computational complexity from O(N·d³) to O(N³), which is significant for large models. The results showing superior performance on machine translation and language modeling tasks with sparse connectivity are directly relevant to LLM optimization goals.",
      "Irrelevant Aspects": "The biological motivation and neuroscience terminology add complexity without directly contributing to optimization insights. The focus on visual classification tasks with MLP architectures is less relevant to LLM optimization. The detailed explanation of link prediction mechanisms based on brain network science could be streamlined to focus on their computational impact rather than biological inspiration.",
      "Summary": "This paper presents highly relevant research on sparse connectivity for LLMs, introducing techniques that enable Transformers and LLaMA models to perform comparably to fully connected networks while using only 1-30% of connections. The methods directly address computational efficiency through GPU-friendly implementations and reduced complexity. The demonstrated performance on machine translation and language modeling with significant sparsity makes this work valuable for achieving better GPU utilization, scalability, higher throughput and lower latency in LLM systems."
    }
  },
  {
    "id": "9PL1DIIB7e",
    "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genetics presents significant challenges. Capturing complex genomic interactions requires modeling long-range global dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene. This poses substantial computational demands under conventional model architectures and training paradigms. Additionally, traditional LLM training approaches are suboptimal for DNA sequences: autoregressive training, while efficient for training, only supports unidirectional sequence understanding. However, DNA is inherently bidirectional. For instance, bidirectional promoters regulate gene expression in both directions and govern approximately 11% of human gene expression. Masked language models (MLMs) enable bidirectional understanding. However, they are inefficient since only masked tokens contribute to loss calculations at each training step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm, integrating the optimization efficiency of autoregressive modeling with the bidirectional comprehension capability of masked modeling. JanusDNA's architecture leverages a Mamba-Attention Mixture-of-Experts (MoE) design, combining the global, high-resolution context awareness of attention mechanisms with the efficient sequential representation learning capabilities of Mamba. The MoE layers further enhance the model's capacity through sparse parameter scaling, while maintaining manageable computational costs. Notably, JanusDNA can process up to 1 million base pairs at single-nucleotide resolution on a single 80GB GPU using its hybrid architecture. Extensive experiments and ablation studies demonstrate that JanusDNA achieves new state-of-the-art performance on three genomic representation benchmarks. Remarkably, JanusDNA surpasses models with 250x more activated parameters, underscoring its efficiency and effectiveness. Code available at https://anonymous.4open.science/r/JanusDNA/.",
    "authors": [
      "~Qihao_Duan1",
      "~Bingding_Huang1",
      "~Zhenqiao_Song1",
      "~Irina_Lehmann1",
      "~Lei_Gu3",
      "~Roland_Eils1",
      "~Benjamin_Wild1"
    ],
    "pdf": "/pdf/e0890fe573f380ab7dfa6c28d2d868aa21121b43.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Novel pretraining paradigm combining autoregressive efficiency with bidirectional comprehension; Mamba-Attention MoE hybrid architecture for computational efficiency; Sparse parameter scaling through MoE layers; Memory efficiency enabling processing of 1 million base pairs on single 80GB GPU; Parameter efficiency claims of surpassing models with 250x more activated parameters",
      "Irrelevant Aspects": "Domain-specific focus on DNA/genomic sequences rather than general NLP; Evaluation on genomic benchmarks rather than ML system benchmarks; Biological motivations and explanations that don't directly inform general optimizations",
      "Summary": "JanusDNA presents architectural innovations relevant to ML system optimization despite its genomic domain focus. The paper combines Mamba-Attention MoE architecture, introduces an efficient pretraining paradigm, and achieves impressive memory and parameter efficiency. These technical contributions address fundamental challenges in GPU utilization, scalability, and efficiency for large models, though the specific application is to DNA sequences rather than general language understanding."
    }
  },
  {
    "id": "XxRKqFsvoK",
    "title": "Exploring Landscapes for Better Minima along Valleys",
    "abstract": "Finding lower and better-generalizing minima is crucial for deep learning. However, most existing optimizers stop searching the parameter space once they reach a local minimum. Given the complex geometric properties of the loss landscape, it is difficult to guarantee that such a point is the lowest or provides the best generalization. To address this, we propose an adaptor \"E\" for gradient-based optimizers. The adapted optimizer tends to continue exploring along landscape valleys (areas with low and nearly identical losses) in order to search for potentially better local minima even after reaching a local minimum. This approach increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization. We also provide a proof of convergence for the adapted optimizers in both convex and non-convex scenarios for completeness. Finally, we demonstrate their effectiveness in an important but notoriously difficult training scenario, large-minibatch training, where Lamb is the benchmark optimizer. Our testing results show that the adapted Lamb, ALTO, increases the test accuracy (generalization) of the current state-of-the-art optimizer by an average of 2.5\\% across a variety of large-batch training tasks. This work potentially opens a new research direction in the design of optimization algorithms.",
    "authors": [
      "~Tong_Zhao5",
      "~Jiacheng_Li10",
      "~Yuanchang_Zhou1",
      "~Guangming_Tan1",
      "~Weile_Jia1"
    ],
    "pdf": "/pdf/7eb65210d89188df610837e0c1783a538b2ef8d1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimization algorithms for deep learning, specifically addressing large-minibatch training which is crucial for GPU utilization and scalability. It improves upon the Lamb optimizer which is designed for large batch training in language models. The technique could lead to more efficient training, potentially reducing training time and resource usage, and may reduce the number of training epochs needed, affecting throughput.",
      "Irrelevant Aspects": "The paper focuses more on generalization than computational efficiency. There's no direct mention of inference optimization, which is half of my research focus. The paper doesn't address specific techniques for memory optimization during training or distributed training challenges that are important for scalability.",
      "Summary": "This paper introduces an adaptor for gradient-based optimizers that explores along loss landscape valleys to find better minima, particularly applied to large-minibatch training. While relevant to training optimization for large language models and GPU utilization, it doesn't cover inference optimization or specific system efficiency techniques central to my research focus."
    }
  },
  {
    "id": "Mc0eJHZhW5",
    "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
    "abstract": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long‐context inference on resource‐constrained devices.\n  Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs.\n  We introduce Low Rank Query and Key attention (LRQK), a two‐stage framework that jointly decomposes the full‐precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step.\n  By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU–CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU–GPU data movement.\n  Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at \\url{https://github.com/tenghuilee/LRQK}.",
    "authors": [
      "~Li_Tenghui1",
      "~Guoxu_Zhou1",
      "~Xuyang_ZHAO5",
      "~Yuning_Qiu1",
      "~Qibin_Zhao1"
    ],
    "pdf": "/pdf/6f4d30e09c90aa1b1db7f9a4f4507b0af52f9cc0.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs, addressing GPU memory constraints and computational efficiency during long-context processing. It introduces a low-rank decomposition technique for attention mechanisms, which directly relates to improving GPU utilization and reducing memory costs. The method reduces computational complexity from O(n^2) to O(lr) while maintaining accuracy. The mixed GPU-CPU cache with selective data transfer optimizes memory bandwidth usage, which is crucial for better throughput and lower latency. The approach has been validated on current models (LLaMA-3-8B and Qwen2.5-7B) showing practical applicability.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization rather than training optimization, which is another key aspect of my research interests. While it mentions quantization as related work, the core approach doesn't rely on quantization techniques. The paper doesn't address model architecture changes but focuses on computational optimizations for existing architectures.",
      "Summary": "LRQK introduces a two-stage framework that decomposes full-precision query and key matrices into compact low-rank factors during prefill, then uses these projections for efficient attention computation during decoding. By selecting only top-k tokens and recent tokens with a mixed GPU-CPU cache system, it maintains exact attention outputs while significantly reducing memory usage and data transfer between GPU and CPU. The method demonstrates competitive performance with leading sparse-attention approaches while offering substantial memory savings with minimal accuracy loss on long-context benchmarks."
    }
  },
  {
    "id": "XsNi2STaj0",
    "title": "Overcoming Long Context Limitations of State Space Models via Context Dependent Sparse Attention",
    "abstract": "Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, joint recall, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA). Our code is available at: https://github.com/DeepGraphLearning/HAX.",
    "authors": [
      "~Zhihao_Zhan1",
      "~Jianan_Zhao2",
      "~Zhaocheng_Zhu1",
      "~Jian_Tang1"
    ],
    "pdf": "/pdf/513de78913093bd84e5f5969eab361e2ce29d0fe.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Addresses quadratic scaling challenge of Transformer architecture with sequence length, Focuses on State Space Models (SSMs) with sub-quadratic time complexity, Introduces sparse attention mechanisms (CDSA) to improve efficiency, Aims to bridge theoretical analysis with practical applications for NLP, Proposes HAX implementation that's tailored to natural language domains, Testing on real-world benchmarks provides practical insights",
      "Irrelevant Aspects": "Heavy focus on theoretical proofs rather than implementation details, The synthetic tasks analysis may have limited direct relevance to practical optimization, Limited discussion of specific GPU utilization metrics or throughput/latency measurements",
      "Summary": "This paper addresses the challenge of long-context modeling in NLP by improving State Space Models through Context-Dependent Sparse Attention. The authors identify limitations in existing SSMs for handling complex contextual recall tasks and propose HAX, a solution that combines hashing-based attention with sparse key selection. The work demonstrates performance improvements on both synthetic and real-world benchmarks while maintaining sub-quadratic computational complexity, making it highly relevant to optimizing language models for better efficiency and scalability."
    }
  },
  {
    "id": "sH0ZwzDJZn",
    "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection",
    "abstract": "A primary impediment to scaling reinforcement learning (RL) for large language model (LLM) training is the substantial computational cost, predominantly arising from the necessity of multi-sampling for policy optimization and evaluation. This underscores the critical yet challenging nature of efficient training data selection. Drawing inspiration from the Zone of Proximal Development (ZPD) theory, which posits that learners acquire knowledge more effectively from tasks of intermediate difficulty, we hypothesize that LLMs exhibit optimal learning from data they have not yet mastered but demonstrate the potential to comprehend. Conventional methodologies for assessing data difficulty or informativeness typically rely on computationally intensive multi-sampling or iterative procedures. To address this limitation, we introduce UFO-RL (**U**ncertainty-**F**ocused **O**ptimization for **R**einforcement **L**earning), a novel framework that employs a computationally efficient single-pass uncertainty estimation technique to identify informative training instances. This method, requiring only a single forward pass and obviating the need for iterative next-token computation, achieves a significant acceleration (up to 185$\\times$) in data evaluation compared to multi-sampling approaches. UFO-RL leverages this efficient metric to select data within the model's estimated ZPD for training. Extensive experimentation across diverse LLMs and mathematical benchmarks demonstrates that training with a mere 10\\% of the data, carefully selected by UFO-RL, yields performance comparable to or even surpassing that of full-data training. Furthermore, this targeted data selection results in up to a 16$\\times$ reduction in overall training time, concurrently enhancing training stability and improving generalization capabilities. Thus, UFO-RL presents a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning efforts on the most informative and valuable data, thereby mitigating the computational bottlenecks associated with traditional RL training.",
    "authors": [
      "~Yang_Zhao25",
      "~Kai_Xiong2",
      "~Xiao_Ding1",
      "~Li_Du3",
      "~YangouOuyang1",
      "~Zhouhao_Sun1",
      "~Jiannan_Guan1",
      "~Wenbin_Zhang5",
      "~Bin_Liu26",
      "~Dong_Hu1",
      "~Bing_Qin2",
      "~Ting_Liu2"
    ],
    "pdf": "/pdf/70cc22bfcccd26d6359b9f517b233ce2dfc7f828.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "Training optimization for LLMs using reinforcement learning",
        "Computational efficiency improvements with 185× acceleration in data evaluation",
        "GPU utilization enhancement through single-pass uncertainty estimation",
        "Scalability solutions for RL fine-tuning of LLMs",
        "Throughput improvement with up to 16× reduction in training time",
        "Latency reduction by eliminating multi-sampling requirements",
        "Training stability improvements during optimization",
        "Resource efficiency using only 10% of data while maintaining performance"
      ],
      "Irrelevant Aspects": [
        "Zone of Proximal Development pedagogical theory (conceptual foundation rather than technical optimization)",
        "Specific mathematical benchmarks used for evaluation",
        "Detailed comparison to existing data assessment methodologies",
        "Generalization capability improvements (secondary benefit)"
      ],
      "Summary": "UFO-RL presents a highly relevant framework for optimizing LLM training by introducing an uncertainty-focused approach that eliminates computationally expensive multi-sampling in RL training. The method achieves significant improvements in GPU utilization, scalability, throughput, and latency through efficient data selection based on single-pass uncertainty estimation. With impressive results including 185× faster data evaluation and 16× reduction in overall training time while using only 10% of data, this work directly addresses key optimization challenges in LLM training systems."
    }
  },
  {
    "id": "QwY1vk67T3",
    "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
    "abstract": "Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models have be released at https://github.com/MiliLab/RoMA.",
    "authors": [
      "~Fengxiang_Wang1",
      "~Yulin_Wang1",
      "~Mingshuo_Chen1",
      "~Haotian_Wang2",
      "~Hongzhen_Wang2",
      "~Haiyan_Zhao2",
      "~Yangang_Sun1",
      "~Shuo_Wang13",
      "~Di_Wang13",
      "~Long_Lan2",
      "~Wenjing_Yang1",
      "~Jing_Zhang17"
    ],
    "pdf": "/pdf/354d480d06d86efbfb86026118fc75cd6bab16b8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses scalability challenges in large models by replacing quadratic-complexity self-attention with linear-complexity Mamba architecture, which is directly relevant to GPU utilization and scalability concerns. It presents techniques for efficient handling of high-resolution data through auto-regressive learning strategies and demonstrates scaling laws for model and data size increases. The framework enables scalable self-supervised pretraining using large-scale data, and empirically shows improved computational efficiency over Transformer-based approaches.",
      "Irrelevant Aspects": "The application domain is remote sensing vision tasks rather than language processing, which differs from my focus on large language models. The evaluation focuses on computer vision tasks like scene classification, object detection, and semantic segmentation rather than NLP tasks.",
      "Summary": "RoMA presents a framework for scaling Mamba-based foundation models for remote sensing applications, addressing quadratic complexity limitations of ViTs through linear-complexity Mamba architecture. The paper introduces rotation-aware pretraining and multi-scale token prediction to handle challenges specific to remote sensing imagery. While domain-specific to vision tasks, the core contributions on model scaling, complexity reduction, and computational efficiency provide valuable insights relevant to large model optimization across domains."
    }
  },
  {
    "id": "Dkgx2pS4Ww",
    "title": "Quantifying Elicitation of Latent Capabilities in Language Models",
    "abstract": "Large language models often possess latent capabilities that lie dormant unless explicitly elicited, or surfaced, through fine-tuning or prompt engineering. Predicting, assessing, and understanding these latent capabilities pose significant challenges in the development of effective, safe AI systems. In this work, we recast elicitation as an information-constrained fine-tuning problem and empirically characterize upper bounds on the minimal number of parameters needed to achieve specific task performances. We find that training as few as 10–100 randomly chosen parameters—several orders of magnitude fewer than state-of-the-art parameter-efficient methods—can recover up to 50\\% of the performance gap between pretrained-only and full fine-tuned models, and 1,000s to 10,000s of parameters can recover 95\\% of this performance gap. We show that a logistic curve fits the relationship between the number of trained parameters and model performance gap recovery. This scaling generalizes across task formats and domains, as well as model sizes and families, extending to reasoning models and remaining robust to increases in inference compute. To help explain this behavior, we consider a simplified picture of elicitation via fine-tuning where each trainable parameter serves as an encoding mechanism for accessing task-specific knowledge. We observe a relationship between the number of trained parameters and how efficiently relevant model capabilities can be accessed and elicited, offering a potential route to distinguish elicitation from teaching.",
    "authors": [
      "~Elizabeth_Donoway1",
      "~Hailey_Joren1",
      "~Arushi_Somani1",
      "~Henry_Sleight1",
      "~Julian_Michael1",
      "~Michael_R_DeWeese1",
      "~John_Schulman1",
      "~Ethan_Perez1",
      "~Fabien_Roger1",
      "~Jan_Leike1"
    ],
    "pdf": "/pdf/b55bdc2ef06849cb1dfc21176b236e646456e87f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates parameter-efficient fine-tuning methods that can significantly reduce the number of parameters needed to unlock model capabilities, with findings that just 10-100 parameters can recover up to 50% of performance gap. This has direct implications for training optimization, GPU utilization efficiency, and resource allocation. The scaling laws identified across model sizes and families could inform resource-efficient training strategies.",
      "Irrelevant Aspects": "The paper focuses more on quantifying and explaining the phenomenon of capability elicitation rather than providing specific implementation techniques for GPU optimization, throughput improvement, or latency reduction. It doesn't directly address inference optimization techniques or hardware-specific optimizations.",
      "Summary": "This paper recasts elicitation of latent capabilities as an information-constrained fine-tuning problem, finding that surprisingly few parameters (10-100) can recover significant performance. The identified scaling laws could inform parameter-efficient training approaches, but the paper is more theoretical than focused on practical optimization techniques for better GPU utilization and lower latency."
    }
  },
  {
    "id": "Tdl89SZItB",
    "title": "Accurate KV Cache Eviction via Anchor Direction Projection for Efficient LLM Inference",
    "abstract": "Key-Value (KV) cache eviction---which retains the KV pairs of the most important tokens while discarding less important ones---is a critical technique for optimizing both memory usage and inference latency in large language models (LLMs).\nHowever, existing approaches often rely on simple heuristics---such as attention weights---to measure token importance, overlooking the spatial relationships between token value states in the vector space.\nThis often leads to suboptimal token selections and thus performance degradation.\nTo tackle this problem, we propose a novel method, namely **AnDPro** (**An**chor **D**irection **Pro**jection), which introduces a projection-based scoring function to more accurately measure token importance.\nSpecifically, AnDPro operates in the space of value vectors and leverages the projections of these vectors onto an *``Anchor Direction''*---the direction of the pre-eviction output---to measure token importance and guide more accurate token selection.\nExperiments on $16$ datasets from the LongBench benchmark demonstrate that AnDPro can maintain $96.07\\\\%$ of the full cache accuracy using only $3.44\\\\%$ KV cache budget, reducing KV cache budget size by $46.0\\\\%$ without compromising quality compared to previous state-of-the-arts.",
    "authors": [
      "~Zijie_Geng1",
      "~Jie_Wang1",
      "~Ziqi_Liu9",
      "~Feng_Ju5",
      "~Yiming_Li21",
      "~Xing_Li6",
      "~Mingxuan_Yuan1",
      "~Jianye_HAO1",
      "~Defu_Lian1",
      "~Enhong_Chen1",
      "~Feng_Wu1"
    ],
    "pdf": "/pdf/27e5165f6a6821bbd2a496ddaa625c6b9de5e57e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs through KV cache eviction, which is critical for memory optimization, reducing latency, and improving throughput. It introduces a novel approach to efficiently manage GPU memory during inference, enabling better utilization and scalability. The method's ability to maintain high accuracy with significantly reduced cache budget directly contributes to more efficient LLM deployment.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization and does not address training optimization aspects. It does not cover broader GPU utilization strategies beyond memory management. The technique is specialized for cache management rather than providing a general optimization framework.",
      "Summary": "This paper introduces AnDPro, a novel KV cache eviction method for LLM inference that leverages anchor direction projection to more accurately determine token importance. The technique achieves significant memory reduction while maintaining performance, maintaining 96.07% of full cache accuracy using only 3.44% of the KV cache budget. This represents a 46% reduction in cache size compared to previous state-of-the-arts, which directly impacts inference efficiency, memory usage, and latency - all critical aspects for optimized LLM deployment."
    }
  },
  {
    "id": "3CbwwCpsSk",
    "title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms",
    "abstract": "*Can algebraic geometry enhance the sharpness, robustness, and interpretability of modern neural reasoning models  by equipping them with a mathematically grounded inductive bias?* \nTo answer this, we introduce Tropical Attention, an attention mechanism grounded in tropical geometry that lifts the attention kernel into tropical projective space, where reasoning is piecewise-linear and 1-Lipschitz, thus preserving the polyhedral decision structure inherent to combinatorial reasoning. We prove that multi-head Tropical Attention (MHTA) stacks universally approximate tropical circuits and realize tropical transitive closure through composition, achieving polynomial resource bounds without invoking recurrent mechanisms. These guarantees explain why the induced polyhedral decision boundaries remain sharp and scale-invariant, rather than smoothed by Softmax. Empirically, we show that Tropical Attention delivers stronger out-of-distribution generalization in both length and value, with high robustness against perturbative noise, and substantially faster inference with fewer parameters compared to Softmax-based and recurrent attention baselines, respectively. For the first time, we push the domain of neural algorithmic reasoning beyond **PTIME** problems to **NP-hard/complete** problems, paving the way toward  sharper and more expressive Large Reasoning Models (LRMs) capable of tackling complex combinatorial challenges in Phylogenetics, Cryptography, Particle Physics, and Mathematical Discovery. The code is available at https://github.com/Baran-phys/Tropical-Attention/.",
    "authors": [
      "~Baran_Hashemi1",
      "~Kurt_Pasque1",
      "~Chris_Teska1",
      "~Ruriko_Yoshida2"
    ],
    "pdf": "/pdf/b54c5433902864b654a2b785b3efaa62f97e8cc5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Introduction of Tropical Attention as an alternative to Softmax-based attention, claims of substantially faster inference with fewer parameters, polynomial resource bounds without recurrent mechanisms, fundamental architectural innovation in attention mechanisms, potential scalability improvements for complex combinatorial problems",
      "Irrelevant Aspects": "Lack of focus on training optimization techniques, no mention of specific GPU utilization strategies, absence of distributed training discussions, no details on quantization or model compression, limited system-level implementation details",
      "Summary": "The paper introduces Tropical Attention, a mathematically-grounded alternative to Softmax attention that claims to provide faster inference with fewer parameters while maintaining sharp decision boundaries for combinatorial reasoning. While highly relevant to inference optimization through its novel attention mechanism, the paper focuses more on mathematical properties than system-level implementation details for GPU utilization and scalability."
    }
  },
  {
    "id": "xwqTt26NJf",
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "abstract": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
    "authors": [
      "~Daniel_Mingyi_Israel1",
      "~Guy_Van_den_Broeck1",
      "~Aditya_Grover1"
    ],
    "pdf": "/pdf/a8cc0f5a2c3b0eefea1b471fff7e088d2ed095b8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization for large language models, focusing on improving throughput through parallel token generation. It introduces Adaptive Parallel Decoding (APD) to overcome autoregressive decoding bottlenecks. The method optimizes KV caching and discusses the tradeoff between throughput and quality, directly aligning with research interests in better GPU utilization, scalability, and higher throughput.",
      "Irrelevant Aspects": "The focus on diffusion LLMs (dLLMs) rather than traditional autoregressive models makes it somewhat specialized. The multiplicative mixture between dLLM marginal probabilities and joint probability of sequences under a small auxiliary autoregressive model is a highly specific technique that may not generalize to all LLM architectures.",
      "Summary": "This paper introduces Adaptive Parallel Decoding (APD) to accelerate diffusion large language models by dynamically adjusting the number of tokens sampled in parallel. The method inverts the standard speculative decoding approach and incorporates optimizations like KV caching. It achieves significantly higher throughput with minimal quality degradation by providing tunable parameters to flexibly tradeoff performance and quality."
    }
  },
  {
    "id": "m0bR0sxhfL",
    "title": "CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs",
    "abstract": "Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs).\nWhile on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training.\nCascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application.\nIn this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization.\nFurthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods.\nWe introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction.\nOur CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\\times$ to $2.3\\times$ over autoregressive decoding across various LLMs and datasets.\nDyTC improves the average speedup by $47$\\% and $48$\\% over cascade-based baseline and tree-based baseline algorithms, respectively.\nCAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve.",
    "authors": [
      "~Zhiyuan_Ning2",
      "~Jiawei_Shao1",
      "~Ruge_Xu1",
      "~Xinfei_Guo1",
      "~Jun_Zhang25",
      "~Chi_Zhang14",
      "~Xuelong_Li2"
    ],
    "pdf": "/pdf/7be7febdbc687ff1d863bbeaf1f37fb1b683f4bc.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on lossless inference acceleration for LLMs through speculative decoding, which directly relates to GPU utilization, throughput optimization, and latency reduction. The CAS-Spec method uses dynamic self-speculative decoding without requiring specialized training, making it practical for existing systems. The Dynamic Tree Cascade (DyTC) algorithm adaptively routes draft models and assigns draft lengths, showing sophisticated runtime optimization. With reported speedups of 1.1× to 2.3× over autoregressive decoding and improvements of 47-48% over baseline methods, this work represents significant advances in inference efficiency. The technique leverages layer sparsity and activation quantization, which are key optimization strategies for LLM deployment.",
      "Irrelevant Aspects": "The paper does not address training optimization techniques, which is part of my expertise. It provides limited discussion on memory optimization beyond quantization. The work doesn't explore distributed inference strategies across multiple nodes or clusters. There's no detailed analysis of how the method performs under different resource constraints or with different GPU architectures.",
      "Summary": "CAS-Spec presents a novel cascade adaptive self-speculative decoding method that accelerates LLM inference without specialized training. By using dynamically switchable acceleration strategies (layer sparsity and quantization) and introducing a Dynamic Tree Cascade algorithm for adaptive routing, it achieves state-of-the-art speedups among on-the-fly speculative methods. The technique is practical, easily integratable, and addresses key performance metrics of interest in LLM deployment, making it highly relevant to inference optimization research."
    }
  },
  {
    "id": "1SCMFCGliM",
    "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
    "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT  removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75\\% and sampling cost by 99\\%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model’s structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.",
    "authors": [
      "~Ke_Ji1",
      "~Jiahao_Xu1",
      "~Tian_Liang4",
      "~Qiuzhi_Liu1",
      "~Zhiwei_He1",
      "~Xiaoyuan_Liu6",
      "~Xingyu_Chen5",
      "~Junying_Chen2",
      "~Benyou_Wang2",
      "~Zhaopeng_Tu1",
      "~Haitao_Mi1",
      "~Dong_Yu2"
    ],
    "pdf": "/pdf/b261818533917595d3cba6ac4e58c4a6d02409ce.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces a computationally efficient fine-tuning method (UPFT) that reduces training time by 75% and sampling costs by 99%, which directly relates to my interests in training optimization and GPU utilization. The method's focus on minimal data usage (as few as 8 tokens) and resource efficiency aligns well with scalability and throughput optimization goals. The paper demonstrates how to achieve substantial performance gains without expensive sampling procedures, which is central to efficient resource utilization.",
      "Irrelevant Aspects": "The paper's focus on reasoning enhancement per se is less central to my systems optimization interests. The detailed analysis of where errors occur in reasoning chains and the comparison with specific reasoning fine-tuning methods like Rejection Sampling Fine-Tuning are more about model capabilities than system performance. The preservation of structural knowledge aspect, while important for model behavior, is less relevant to my GPU optimization focus.",
      "Summary": "This paper presents Unsupervised Prefix Fine-Tuning (UPFT), an efficient method to improve LLM reasoning capabilities by training only on initial prefixes (as few as 8 tokens). It eliminates the need for labeled data and expensive sampling, matching supervised method performance while reducing training time by 75% and sampling costs by 99%. The work offers a scalable, resource-efficient alternative to conventional fine-tuning approaches, making it highly relevant to my research on optimizing LLM training and inference systems."
    }
  },
  {
    "id": "Nf8yfPDFTl",
    "title": "SeerAttention: Self-distilled Attention Gating for Efficient Long-context Prefilling",
    "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity hinders efficiency and scalability, especially for long-context processing. A promising approach is to leverage sparsity in attention.  However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics at the attention head level, struggling to adapt dynamically to different contexts efficiently. We propose SeerAttention, a simple yet effective attention mechanism that directly learns the block-level attention sparsity from the LLM itself. Inspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention augments the conventional attention with a **learnable gate** that **selectively activates important blocks** within the attention map. Specifically, the gate first pools the query (Q) and key (K) tensors along the sequence dimension and processes them through learnable linear layers. The resulting matrices are then multiplied together to produce the gating scores, which are used to predict block-level attention sparsity. Combined with our block-sparse FlashAttention kernel, SeerAttention can achieve significant speedup on GPUs. When applied to pre-trained LLMs, SeerAttention only requires training the gate parameters in a lightweight self-distillation manner, allowing rapid convergence. Our evaluation results demonstrate that SeerAttention achieves better model accuracy and lower latency for long-context pre-filling compared to prior methods. Code is available at: https://github.com/microsoft/SeerAttention.",
    "authors": [
      "~Yizhao_Gao2",
      "~Zhichen_Zeng2",
      "~DaYou_Du1",
      "~Shijie_Cao1",
      "~Peiyuan_Zhou2",
      "~Jiaxing_Qi2",
      "~Junjie_Lai1",
      "~Hayden_Kwok-Hay_So1",
      "~Ting_Cao1",
      "~Fan_Yang28",
      "~Mao_Yang1"
    ],
    "pdf": "/pdf/3fed457e271e8a060d1210ad2068e08fd91505c3.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Optimization of attention mechanism for better GPU utilization, addressing the quadratic complexity bottleneck in LLMs, proposed learnable approach to attention sparsity rather than fixed patterns, development of block-sparse FlashAttention kernel for better GPU performance, self-distillation training approach for minimal adaptation overhead, focus on long-context processing which is increasingly important in modern LLMs, claims of better accuracy and lower latency compared to prior methods.",
      "Irrelevant Aspects": "Primary focus on attention mechanism rather than system-level optimizations, emphasis on pre-filling phase rather than complete inference pipeline optimization, may not extensively cover distributed training or multi-GPU scaling strategies, limited discussion on memory optimization beyond attention computation.",
      "Summary": "SeerAttention proposes a learnable gating mechanism to dynamically determine block-level attention sparsity in LLMs, addressing the quadratic complexity of attention for long contexts. The method draws inspiration from MoE gating, processes Q and K tensors to generate gating scores for selective block activation, and combines this with a custom block-sparse FlashAttention kernel. The approach requires minimal additional training through self-distillation of gate parameters, enabling efficient adaptation of pre-trained models. The paper reports improved accuracy and reduced latency for long-context pre-filling compared to existing methods, with open-source implementation available."
    }
  },
  {
    "id": "ZtzWvNKOCr",
    "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training",
    "abstract": "Parameter-efficient training, based on low-rank optimization, has become a highly successful tool for fine-tuning large deep-learning models. However, these methods fail at low-rank pre-training tasks where maintaining the low-rank structure and the objective remains a challenging task. We propose the Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel low-rank inducing training\nstrategy inspired by the iteratively reweighted least squares (IRLS) framework. Q3R is based on a quadratic regularizer term which majorizes a smoothed log determinant serving as rank surrogate objective. Unlike other low-rank training techniques, Q3R is able to train weight matrices with prescribed, low target ranks of models that achieve comparable predictive performance as dense models, with\nsmall computational overhead, while remaining fully compatible with existing architectures. In experiments, we are able to truncate 60% of the parameters of a ViT-Tiny parameters with marginal loss in CIFAR-10 performance and up to 80%\nwith only 4% accuracy drop. The efficacy of Q3R is confirmed on Transformers\nacross both image and language tasks, including for low-rank fine-tuning.",
    "authors": [
      "~Ipsita_Ghosh1",
      "~Ethan_Nguyen1",
      "~Christian_Kümmerle1"
    ],
    "pdf": "/pdf/c2a2fb00f465cfd835062b355f4867f27f84204d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parameter-efficient training through low-rank optimization directly addresses training optimization for large models. Working with Transformers for language tasks is highly relevant to LLM optimization. The demonstrated parameter reduction (60-80%) with minimal performance loss could significantly improve GPU utilization and memory efficiency during training. The small computational overhead and compatibility with existing architectures make it practical for current systems. Its application to both pre-training and fine-tuning covers the complete lifecycle of LLM development.",
      "Irrelevant Aspects": "The focus on image tasks with ViT is less directly relevant to language model optimization. The mathematical details of the quadratic reweighted approach and the log determinant rank surrogate are more algorithmic details than system-level concerns for GPU optimization.",
      "Summary": "Q3R presents a low-rank inducing training strategy that reduces model parameters by 60-80% while maintaining performance, with direct applications to Transformer models used in language tasks. The technique has small computational overhead and is compatible with existing architectures, making it relevant for improving GPU utilization and scalability in LLM training systems."
    }
  },
  {
    "id": "Ms6IXbfzzX",
    "title": "ZeroS: Zero‑Sum Linear Attention for Efficient Transformers",
    "abstract": "Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.",
    "authors": [
      "~Jiecheng_Lu1",
      "~Xu_Han14",
      "~Yan_Sun5",
      "~Viresh_Pati1",
      "~Yubin_Kim4",
      "~Siddhartha_Somani1",
      "~Shihao_Yang1"
    ],
    "pdf": "/pdf/53b99866f7a487c410012b2077f3c4dc78c72742.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces ZeroS, a linear attention method that reduces complexity from O(N²) to O(N) while maintaining performance comparable to standard softmax attention. This directly impacts computational efficiency, which is crucial for GPU utilization and scalability in large language models. The O(N) complexity enables processing longer sequences with the same hardware resources, potentially improving throughput and reducing latency during both training and inference.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU utilization measurements, scalability experiments across different GPU configurations, or specific throughput/latency benchmarks. It appears more focused on the theoretical and algorithmic aspects of attention rather than implementation details for maximizing hardware efficiency or training optimization techniques specific to LLMs.",
      "Summary": "ZeroS presents a novel linear attention mechanism that maintains O(N) complexity while matching or exceeding standard softmax attention performance. By removing the constant zero-order term and reweighting zero-sum softmax residuals, it enables contrastive operations within a single attention layer. This work is highly relevant for optimizing transformer models, as the reduced complexity directly impacts scalability and computational efficiency, though it doesn't explicitly focus on hardware-specific optimization strategies."
    }
  },
  {
    "id": "zPKeJAEo27",
    "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
    "abstract": "Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. In this work, we focus on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. We then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, we lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In our data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset.",
    "authors": [
      "~Sang_Keun_Choe1",
      "~Hwijeen_Ahn1",
      "~Juhan_Bae2",
      "~Kewen_Zhao2",
      "~Youngseog_Chung1",
      "~Adithya_Pratapa1",
      "~Willie_Neiswanger2",
      "~Emma_Strubell1",
      "~Teruko_Mitamura1",
      "~Jeff_Schneider1",
      "~Eduard_Hovy1",
      "~Roger_Baker_Grosse1",
      "~Eric_Xing1"
    ],
    "pdf": "/pdf/11e62620a0483e1bd44bc46243a02f05213a57db.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses scalability challenges in applying data valuation to LLMs with significant improvements in throughput (6,500x) and GPU memory reduction (5x). LoGra's efficient gradient projection strategy demonstrates impressive GPU utilization optimization for Llama3-8B-Instruct with a 1B-token dataset. The LogIX software package that transforms existing training code represents valuable system implementation work. These aspects align with my expertise in LLM optimization and GPU utilization.",
      "Irrelevant Aspects": "The primary focus is on data valuation rather than direct training or inference optimization. The paper aims to attribute value to data sources rather than optimize model performance. While it addresses GPU utilization, it's specifically in the context of data valuation rather than general LLM training or inference pipelines.",
      "Summary": "This paper presents LoGra, an efficient gradient projection strategy for applying influence functions to LLM-scale data valuation, achieving 6,500x throughput improvement and 5x GPU memory reduction. While its primary focus is data attribution rather than direct performance optimization, the significant GPU utilization gains and scalability contributions make it relevant to LLM system optimization."
    }
  },
  {
    "id": "EZfDHprhZM",
    "title": "FlashMoE: Fast Distributed MoE in a Single Kernel",
    "abstract": "The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashMoE obviates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thus unlocking payload efficiency, where we eliminate bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashMoE achieves up to 9× higher GPU utilization, 6× lower latency, 5.7× higher throughput, and 4× better overlap efficiency compared to state-of-the-art baselines—despite using FP32 while baselines use FP16. FlashMoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. We provide code at https://github.com/osayamenja/FlashMoE.",
    "authors": [
      "~Osayamen_Jonathan_Aimuyo1",
      "~Byungsoo_Oh1",
      "~Rachee_Singh1"
    ],
    "pdf": "/pdf/2c2490612c920437b49ef30e3b50bce49f86cee5.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "GPU utilization optimization, throughput and latency improvements, kernel fusion techniques, distributed training optimization, MoE models scaling, performance evaluation on H100 GPUs, RDMA-based communication, device-side operations to eliminate CPU bottlenecks",
      "Irrelevant Aspects": "Limited focus on inference optimization, specifically targets MoE architectures rather than general LLM optimization",
      "Summary": "FlashMoE presents a system for optimizing Mixture-of-Experts model training by creating a single GPU kernel that fuses expert computation and inter-GPU communication, achieving up to 9× higher GPU utilization, 6× lower latency, and 5.7× higher throughput compared to existing approaches. The work demonstrates principled GPU kernel-hardware co-design to unlock performance in distributed ML systems."
    }
  },
  {
    "id": "5jneOToPou",
    "title": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
    "abstract": "Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for **E**fficient **E**ncoder-**D**ecoder **D**iffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks.",
    "authors": [
      "~Marianne_Arriola1",
      "~Yair_Schiff1",
      "~Hao_Phung1",
      "~Aaron_Gokaslan1",
      "~Volodymyr_Kuleshov1"
    ],
    "pdf": "/pdf/3d753fd65082c632fc592f9aeba5b84e35fb1823.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference acceleration through an encoder-decoder architecture for discrete diffusion models, which aligns with goals of higher throughput and lower latency. It discusses training efficiency improvements for block diffusion models, relevant to training optimization. The architectural separation between token representation and denoising likely improves GPU utilization through optimized computational workflow. The paper explicitly claims superior trade-offs between generation quality and inference throughput, which is central to the research interest. The parallel token sampling approach contrasts with autoregressive methods, offering potential performance benefits. Testing on practical tasks (summarization, translation, mathematical reasoning) demonstrates real-world applicability.",
      "Irrelevant Aspects": "The focus on discrete diffusion models rather than conventional transformer architectures may limit applicability to some existing systems. No explicit discussion of specific GPU utilization metrics or detailed hardware optimizations. Limited information about distributed training or multi-GPU/multi-node scaling for large-scale systems. The paper doesn't appear to address memory optimization techniques beyond the architectural changes.",
      "Summary": "This paper introduces an encoder-decoder architecture for discrete diffusion language models (E2D2) that improves both training and inference efficiency. The key innovation is separating token representation (encoder) from denoising (lightweight decoder), enabling faster inference by avoiding full network invocation at every denoising step. The approach shows superior trade-offs between generation quality and inference throughput on various tasks. The architectural design has strong relevance to GPU utilization and scalability research, though its focus on diffusion models rather than standard transformers may limit broader applicability to existing LLM systems."
    }
  },
  {
    "id": "5Qe7AGO3Eq",
    "title": "Multipole Attention for Efficient Long Context Reasoning",
    "abstract": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens.\nWhile sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process.\nOur work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. \nOur method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy.\nAdditionally, in order to accelerate long generation tasks, we design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for  accelerating attention to the previous output tokens.\nWe evaluate our method using emerging LRMs such as Qwen-8B and Deepseek-R1-Distil-Qwen2.5-14B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings.\nWe also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications.",
    "authors": [
      "~Coleman_Richard_Charles_Hooper1",
      "~Sebastian_Zhao1",
      "~Luca_Manolache1",
      "~Sehoon_Kim1",
      "~Michael_W._Mahoney1",
      "~Sophia_Shao1",
      "~Kurt_Keutzer1",
      "~Amir_Gholami2"
    ],
    "pdf": "/pdf/4a59f2871b0a32683974881a2aae4cc7fdc39173.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Directly addresses LLM inference optimization through a novel attention mechanism that reduces computation while maintaining accuracy. Focuses on GPU utilization improvements with custom kernel implementations and 4.5x speedup claims. Addresses KV cache pressure which is critical for memory efficiency in long-context scenarios. Provides practical performance gains for throughput and latency in generative models.",
      "Irrelevant Aspects": "Primarily focused on reasoning models rather than general LLMs. Does not address training optimization techniques. Limited discussion of multi-GPU scalability. Does not cover hardware-specific optimizations beyond kernel implementations.",
      "Summary": "This paper presents Multipole Attention, an innovation that selectively computes exact attention only for important tokens while using cluster-based approximations for others. This approach directly tackles the inference bottleneck in long-context reasoning by reducing KV cache pressure and computational requirements. The 4.5x speedup claims with custom kernel implementations demonstrate significant potential for improving GPU utilization and reducing latency in LLM inference systems."
    }
  },
  {
    "id": "LTgUInLTbP",
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce **SuperRS-VQA** (avg. 8,376$\\times$8,376) and **HighRS-VQA** (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: *Background Token Pruning* and *Anchored Token Selection*, to reduce the memory footprint while preserving key semantics. Integrating these techniques, we introduce **GeoLLaVA-8K**, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench. Datasets and code were released at https://github.com/MiliLab/GeoLLaVA-8K.",
    "authors": [
      "~Fengxiang_Wang1",
      "~Mingshuo_Chen1",
      "~Yueying_Li6",
      "~Di_Wang13",
      "~Haotian_Wang2",
      "~Zonghao_Guo1",
      "~Zefan_Wang2",
      "~Shan_Boqi1",
      "~Long_Lan2",
      "~Yulin_Wang1",
      "~Hongzhen_Wang2",
      "~Wenjing_Yang1",
      "~Bo_Du3",
      "~Jing_Zhang17"
    ],
    "pdf": "/pdf/d8a7185abcf4204532a860bdabb45a2c9bb10919.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses token explosion challenges in processing ultra-high-resolution images, proposing Background Token Pruning and Anchored Token Selection techniques to reduce memory footprint while preserving semantics. These optimizations directly relate to GPU memory utilization and processing efficiency. The model architecture modifications to handle 8K×8K resolution inputs demonstrate scalability solutions for large multimodal models. The finding that pruning background tokens can improve performance suggests potential inference speed optimizations.",
      "Irrelevant Aspects": "The strong focus on remote sensing imagery makes the techniques domain-specific rather than general-purpose LLM optimizations. Introduction of remote sensing datasets (SuperRS-VQA and HighRS-VQA) is more about data availability than optimization techniques. Performance benchmarking is conducted on domain-specific metrics (XLRS-Bench) rather than general efficiency metrics. The paper doesn't address traditional text-only LLM optimization challenges.",
      "Summary": "GeoLLaVA-8K introduces optimization techniques for processing ultra-high-resolution (8K×8K) images in multimodal models, specifically addressing memory challenges through token pruning strategies. While domain-focused on remote sensing, the technical approaches for managing large-scale multimodal inputs and reducing memory footprint provide valuable insights for GPU utilization and scalability in multimodal systems. The paper demonstrates how strategic token reduction can maintain or even improve model performance while reducing computational requirements."
    }
  },
  {
    "id": "KaYMGsnZ4R",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering substantial potential for improving runtime efficiency. However, existing diffusion models fail to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models, which generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, designed to enforce constraints with sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model’s predicted distribution while strictly adhering to any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a $68$\\% points of improvement over unconstrained inference.  The code is available at [**DINGO**](https://github.com/uiuc-focal-lab/DINGO).",
    "authors": [
      "~Tarun_Suresh1",
      "~Debangshu_Banerjee2",
      "~Shubham_Ugare1",
      "~Sasa_Misailovic1",
      "~Gagandeep_Singh1"
    ],
    "pdf": "/pdf/431affcf58926eb368d1a07a66bab58c196789ea.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on inference optimization for diffusion LLMs which offers potential for improving runtime efficiency; addresses parallel token prediction which can enhance GPU utilization; proposes an efficient dynamic programming-based constrained decoding strategy; demonstrates significant performance improvements (68%); relevant to throughput and efficiency improvements in LLM systems",
      "Irrelevant Aspects": "Primarily focuses on constrained decoding rather than overall system optimization; emphasis on output structure adherence rather than GPU utilization or scalability metrics; limited discussion of hardware-specific optimizations; doesn't address training optimization which is part of my research interest",
      "Summary": "DINGO presents a constrained decoding strategy for diffusion LLMs that enforces formal constraints while maintaining the true output distribution. While the paper offers valuable insights into inference efficiency and parallel processing of diffusion LLMs, its primary focus on output structure constraints rather than comprehensive system optimization makes it moderately relevant to my research interests in LLM training and inference optimization for GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "GuvQJGgbLm",
    "title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones",
    "abstract": "Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.",
    "authors": [
      "~Parsa_Mirtaheri1",
      "~Ezra_Edelman1",
      "~Samy_Jelassi1",
      "~Eran_Malach3",
      "~Enric_Boix-Adserà1"
    ],
    "pdf": "/pdf/3522982c33f5d5199bd558f48909b1bccbf81615.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization strategies, test-time computation scaling, sequential vs parallel processing trade-offs, reasoning capability improvements, resource allocation for LLM inference",
      "Irrelevant Aspects": "Training optimization techniques, specific GPU utilization methods, explicit latency and throughput measurements, hardware-specific implementation details",
      "Summary": "This paper explores the critical question of optimal inference-time computation allocation in large language models, specifically comparing sequential scaling (longer chains of thought) against parallel scaling (multiple short chains of thought). The authors demonstrate that sequential scaling can provide exponential advantages over parallel scaling for certain reasoning tasks, particularly graph connectivity problems. They validate their theoretical findings through comprehensive experiments with various language models, including both custom-trained models and large reasoning models."
    }
  },
  {
    "id": "Guar1tumDr",
    "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
    "abstract": "With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation.\nTo compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the *temporal patterns* in attention scores, resulting in a noticeable degradation in LLM performance. \nTo address this challenge, we propose **AttentionPredictor**, which is the **first learning-based method to directly predict attention patterns for KV cache compression and critical token identification**. \nSpecifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves **13$\\times$** KV cache compression and **5.6$\\times$** speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
    "authors": [
      "~Qingyue_Yang1",
      "~Jie_Wang1",
      "~Xing_Li6",
      "~Zhihai_Wang1",
      "~Chen_Chen34",
      "~Lei_Chen26",
      "~Xianzhi_Yu1",
      "~Wulong_Liu1",
      "~Jianye_HAO1",
      "~Mingxuan_Yuan1",
      "~Bin_Li8"
    ],
    "pdf": "/pdf/9866d9cc5e4e40d9680092c9701baa94a07c6fce.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on inference optimization for LLMs through KV cache compression, introduces a learning-based method for attention pattern prediction, claims significant speedup (5.6×) and compression (13×) improvements, addresses memory efficiency which impacts GPU utilization, and provides a practical solution for reducing inference latency and improving throughput.",
      "Irrelevant Aspects": "Does not address training optimization, limited discussion on scalability across distributed systems, no explicit mention of GPU-specific utilization techniques beyond cache considerations, and focuses primarily on a single aspect of inference optimization rather than a comprehensive approach.",
      "Summary": "AttentionPredictor presents a novel learning-based approach for KV cache compression in LLM inference by predicting attention patterns rather than relying on static modeling. The method claims to achieve 13× KV cache compression and 5.6× speedup in cache offloading scenarios while maintaining comparable LLM performance. The paper introduces a lightweight unified convolution model to capture spatiotemporal patterns and a cross-token critical cache prefetching framework. While highly relevant to inference optimization, it focuses specifically on cache management rather than broader GPU utilization or scalability aspects."
    }
  },
  {
    "id": "tirl2l9oKg",
    "title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
    "abstract": "Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. \nRAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. \nThe graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference.\nExtensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}.",
    "authors": [
      "~Xingliang_Wang1",
      "~Zemin_Liu1",
      "~Junxiao_Han1",
      "~Shuiguang_Deng1"
    ],
    "pdf": "/pdf/2c14726b32b7480217772d637c5094332ff58091.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper's core contribution is applying Retrieval-Augmented Generation (RAG) to a new modality, Graph Foundation Models (GFMs). RAG is a key inference optimization paradigm that avoids costly retraining and reduces the parameter burden on the main model, directly impacting scalability and memory usage. The explicit focus on designing an 'efficient logarithmic-time retrieval' system and a 'hierarchical graph indexing architecture' is highly relevant to my interest in reducing latency and improving system scalability. The entire framework is an end-to-end system for efficient inference, which aligns well with a systems-level perspective.",
      "Irrelevant Aspects": "The primary domain is Graph Foundation Models (GFMs), not Large Language Models (LLMs) specifically. While the underlying principles are transferable, the graph-specific neural architectures and challenges are a distinct subfield. Furthermore, a significant stated goal is improving 'reasoning faithfulness,' which is a model accuracy and quality metric, not a direct systems performance metric like throughput, latency, or GPU utilization. The paper does not appear to focus on low-level GPU kernel optimization or novel model parallelization techniques.",
      "Summary": "This paper introduces RAG4GFM, a framework that leverages Retrieval-Augmented Generation to improve Graph Foundation Models. From a systems optimization perspective, its relevance is high because RAG is a key strategy for making model inference more efficient, scalable, and adaptable. The paper's strong focus on building an efficient, logarithmic-time retrieval and indexing system directly addresses core challenges of latency and scalability in large-scale ML systems. However, its application is specific to GFMs rather than LLMs, and it also prioritizes model quality ('reasoning faithfulness') alongside system efficiency."
    }
  },
  {
    "id": "KI8qan2EA7",
    "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs",
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often *hierarchical*—higher-order interactions are accompanied by their lower-order subsets—which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions.   Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20\\% over marginal attribution approaches while using *$10\\times$ fewer inferences* than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. *Data attribution*, where we identify interactions among CIFAR-10 training samples that influence test predictions, and *mechanistic interpretability*, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. The ProxySPEX algorithm is available at <https://github.com/mmschlk/shapiq>.",
    "authors": [
      "~Landon_Butler1",
      "~Abhineet_Agarwal1",
      "~Justin_Singh_Kang1",
      "~Yigit_Efe_Erginbas1",
      "~Bin_Yu5",
      "~Kannan_Ramchandran1"
    ],
    "pdf": "/pdf/08d7fc56afca28ad2ef1c0a22c6d0340dc3240c8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference efficiency in LLMs, specifically reducing the number of model inferences needed for interpretability analysis. It claims to use 10× fewer inferences than SPEX, which directly impacts GPU utilization and computational costs. The method addresses scalability issues with large models and provides a practical solution for applying interpretability methods to LLMs. The approach uses gradient boosted trees as a proxy to reduce inference requirements, which is an interesting optimization strategy. The paper is available as open-source implementation.",
      "Irrelevant Aspects": "The paper primarily focuses on interpretability rather than direct performance optimization of LLMs. It doesn't address training optimization or explicitly discuss throughput and latency metrics. The method is for post-hoc analysis rather than runtime optimization of model serving. The paper doesn't directly discuss GPU memory utilization or model parallelization strategies.",
      "Summary": "ProxySPEX presents a method to make LLM interpretability more inference-efficient by using gradient boosted trees as a proxy to extract feature interactions. While primarily focused on interpretability, the paper's approach to reducing inference requirements has relevance to LLM system optimization, particularly in making certain analyses practical for large models without prohibitive computational costs. The 10× reduction in inferences compared to SPEX represents a significant efficiency gain that could enable better resource utilization in LLM systems, even if not directly addressing serving throughput or latency."
    }
  },
  {
    "id": "lVV7F0piDK",
    "title": "Gated Integration of Low-Rank Adaptation for Continual Learning  of Large Language Models",
    "abstract": "Continual learning (CL), which requires the model to learn multiple tasks sequentially, is crucial for large language models (LLMs). Recently, low-rank adaptation (LoRA), one of the most representative parameter-efficient fine-tuning (PEFT) methods, has gained increasing attention in CL of LLMs. However, most existing CL methods based on LoRA typically expand a new LoRA branch to learn each new task and force the new and old LoRA branches to influence old tasks equally, potentially leading to forgetting. In this work, we propose a new method, called gated integration of low-rank adaptation (GainLoRA), for CL of LLMs. GainLoRA expands a new LoRA branch for each new task and introduces gating modules to integrate the new and old LoRA branches. Furthermore, GainLoRA leverages the new gating module to minimize the influence from the new LoRA branch to old tasks, effectively mitigating forgetting and improving the model's overall performance. Experimental results on CL benchmarks demonstrate that GainLoRA outperforms existing state-of-the-art methods.",
    "authors": [
      "~Yan-Shuo_Liang1",
      "~Jiarui_Chen5",
      "~Wu-Jun_Li1"
    ],
    "pdf": "/pdf/341aef9c1489a0b08162c50602a544cfd509cdbd.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimizing LoRA (Low-Rank Adaptation), which is a parameter-efficient fine-tuning method for LLMs. This relates to training optimization as it proposes a more effective way to adapt models to new tasks without extensive retraining. LoRA methods in general are memory-efficient, which can contribute to better GPU utilization.",
      "Irrelevant Aspects": "The paper doesn't directly address GPU utilization, scalability, throughput, or latency optimization. The focus is primarily on continual learning methodology and preventing catastrophic forgetting rather than performance optimization during training or inference.",
      "Summary": "The paper introduces GainLoRA, a method for improving continual learning in LLMs by using gated integration of LoRA modules. While relevant to LLM optimization, its focus is on learning efficiency rather than computational performance metrics."
    }
  },
  {
    "id": "MHGViOjZ27",
    "title": "Generative Caching for Structurally Similar Prompts and Responses",
    "abstract": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce GenCache, a generative cache that produces variation-aware responses for structurally similar prompts. GenCache identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that GenCache achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.",
    "authors": [
      "~Sarthak_Chakraborty1",
      "~Suman_Nath1",
      "~Xuchao_Zhang1",
      "~Chetan_Bansal1",
      "~Indranil_Gupta1"
    ],
    "pdf": "/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM inference optimization through an innovative caching mechanism that reduces latency by ~34%, increases cache hit rates to 83%, and handles structurally similar prompts efficiently. This directly addresses my interest in higher throughput and lower latency for LLM systems. The approach improves GPU utilization by avoiding redundant computations for similar prompts, which is crucial for scalable systems.",
      "Irrelevant Aspects": "The paper does not address training optimization aspects, which is part of my research focus. It also lacks discussion of specific GPU utilization techniques and doesn't cover model parallelization or distributed computing strategies. The focus is solely on caching rather than broader system architecture optimizations.",
      "Summary": "GenCache introduces a generative caching approach for structurally similar prompts in LLM inference, achieving significant latency reduction (~34%) and high cache hit rates (83%). While highly relevant to inference optimization goals, it doesn't cover training optimization or comprehensive GPU utilization strategies."
    }
  },
  {
    "id": "jFaFCc5978",
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present **AnytimeReasoner**, a novel framework for optimizing reasoning performance under varying thinking budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, **Budget Relative Policy Optimization (BRPO)**, to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
    "authors": [
      "~Penghui_Qi1",
      "~Zichen_Liu1",
      "~Tianyu_Pang1",
      "~Chao_Du1",
      "~Wee_Sun_Lee1",
      "~Min_Lin1"
    ],
    "pdf": "/pdf/8136e4668a09f8c47a2454d9e72728d4fdea055e.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Optimization of test-time computation for LLMs, token budget management and efficiency, RL optimization for inference efficiency, new optimization algorithm (BRPO) for more efficient training and deployment, enhancement of token efficiency for better resource utilization",
      "Irrelevant Aspects": "Focus primarily on mathematical reasoning tasks rather than general applications, limited discussion of hardware-specific optimizations or distributed training/inference approaches",
      "Summary": "This paper introduces AnytimeReasoner, a framework for optimizing LLM reasoning under varying token budget constraints. The approach introduces verifiable dense rewards into the reasoning process and uses a new optimization technique called Budget Relative Policy Optimization (BRPO) to enhance training and deployment efficiency. The method improves token efficiency and outperforms existing approaches across various thinking budgets, directly addressing concerns about GPU utilization, throughput, and resource efficiency in LLM inference."
    }
  },
  {
    "id": "8C8F4NmHfz",
    "title": "Tail-Optimized Caching for LLM Inference",
    "abstract": "Prompt caching is critical for reducing latency and cost in LLM inference---OpenAI and Anthropic report up to 50–90\\% cost savings through prompt reuse. Despite its widespread success, little is known about what constitutes an optimal prompt caching policy, particularly when optimizing tail latency—a metric of central importance to practitioners. The widely used Least Recently Used (LRU) policy can perform arbitrarily poor on this metric, as it is oblivious to the heterogeneity of conversation lengths. To address this gap, we propose Tail-Optimized LRU, a simple two-line modification that reallocates KV cache capacity to prioritize high-latency conversations by evicting cache entries that are unlikely to affect future turns. Though the implementation is simple, we prove its optimality under a natural stochastic model of conversation dynamics, providing the first theoretical justification for LRU in this setting---a result that may be of independent interest to the caching community. \nExperimentally, on real conversation data WildChat~\\citep{zhao2024wildchat}, Tail-Optimized LRU achieves up to 27.5\\% reduction in P90 tail Time to First Token latency and 23.9\\% in P95 tail latency compared to LRU, along with up to 38.9\\% decrease in SLO violations of 200ms. \nWe believe this provides a practical and theoretically grounded option for practitioners seeking to optimize tail latency in real-world LLM deployments.",
    "authors": [
      "~Wenxin_Zhang8",
      "~Yueying_Li1",
      "~Ciamac_C._Moallemi1",
      "~Tianyi_Peng1"
    ],
    "pdf": "/pdf/e1de153a325de078751ea89518138e4071143fd6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on LLM inference optimization, specifically addressing latency reduction through improved caching strategies. It presents Tail-Optimized LRU, a modification to standard LRU caching that optimizes for tail latency, which is critical for real-world LLM deployments. The paper demonstrates practical improvements in Time to First Token latency and reduction in SLO violations. The work is theoretically grounded and provides empirical validation on real conversation data, making it relevant to inference optimization efforts.",
      "Irrelevant Aspects": "The paper does not address training optimization, focusing solely on inference aspects. There is no discussion of GPU utilization optimization or multi-GPU/distributed system scalability. The work concentrates specifically on prompt caching policies rather than a comprehensive approach to LLM system optimization.",
      "Summary": "Tail-Optimized Caching for LLM Inference presents a modified LRU caching policy designed to optimize tail latency in LLM inference systems. The paper demonstrates that standard LRU can perform poorly when optimizing for tail latency due to its obliviousness to conversation length heterogeneity. The proposed Tail-Optimized LRU addresses this by reallocating KV cache capacity to prioritize high-latency conversations. The authors provide theoretical justification for their approach and demonstrate experimental results showing up to 27.5% reduction in P90 tail latency and 38.9% decrease in SLO violations on real conversation data."
    }
  },
  {
    "id": "o4zN34ahEK",
    "title": "Spark Transformer: Reactivating Sparsity in Transformer FFN and Attention",
    "abstract": "The discovery of the *lazy neuron phenomenon* (Li et al., 2022), where fewer than 10% of the feedforward networks (FFN) parameters in trained Transformers are activated per token, has spurred significant interests in *activation sparsity* for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits across CPUs, GPUs, and TPUs, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity, e.g., by reverting to ReLU or applying top-k masking, often degrade model quality, increase parameter count, or complicate training. Sparse attention, the application of sparse activation to the attention mechanism, often face similar challenges.\n    \nThis paper introduces the Spark Transformer, a novel architecture that achieves high activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. Our method realizes sparsity via top-$k$ masking for explicit control over sparsity level. Crucially, we introduce *statistical top-k*, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-k operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8\\% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40xon GPU.",
    "authors": [
      "~Chong_You1",
      "~Kan_Wu2",
      "~Zhipeng_Jia1",
      "~Lin_Chen14",
      "~Srinadh_Bhojanapalli1",
      "~Jiaxian_Guo2",
      "~Utku_Evci1",
      "~Jan_Wassenberg1",
      "~Praneeth_Netrapalli1",
      "~Jeremiah_J._Willcock1",
      "~Suvinay_Subramanian1",
      "~Felix_Chern1",
      "~Alek_Andreev1",
      "~Shreya_Pathak1",
      "~Felix_X._Yu2",
      "~Prateek_Jain1",
      "~David_E_Culler1",
      "~Henry_Levy1",
      "~Sanjiv_Kumar1"
    ],
    "pdf": "/pdf/83a01256d6f2b9860a6ad66ea9ffdc712111468f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses GPU utilization and inference optimization, key areas of my research interest. It introduces hardware-aware optimization techniques through the statistical top-k algorithm that avoids costly sorting. The reported speedups of 1.40x on GPU and 1.79x on CPU are directly relevant to improving throughput and reducing latency. The approach of reducing FLOPs by 2.5x while maintaining model quality addresses the challenge of making large models more efficient. The focus on activation sparsity in both FFN and attention mechanisms provides a comprehensive optimization approach that could significantly improve resource utilization during inference.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization with limited discussion of training optimization techniques. There's minimal exploration of distributed training or multi-GPU scaling strategies, which are important aspects of my research. Memory optimization techniques, which are crucial for both training and inference efficiency, receive little attention. The paper doesn't address system-level optimizations or scheduling strategies that could further enhance GPU utilization.",
      "Summary": "Spark Transformer introduces a novel approach to reactivating sparsity in Transformer models, addressing both FFN and attention mechanisms. The method achieves significant activation sparsity (only 8% FFN neurons activated) while maintaining model quality through parameter reallocation and a hardware-friendly statistical top-k algorithm. The paper reports substantial FLOPs reduction (2.5x) and meaningful speedups during decoding on both CPU (1.79x) and GPU (1.40x), making it highly relevant to my research on improving GPU utilization, throughput, and reducing latency in large language models."
    }
  },
  {
    "id": "7AGXSlXcK6",
    "title": "Predictability Enables Parallelization of Nonlinear State Space Models",
    "abstract": "The rise of parallel computing hardware has made it increasingly important to understand which nonlinear state space models can be efficiently parallelized. Recent advances have shown that evaluating a state space model can be recast as solving a parallelizable optimization problem, and sometimes this approach yields dramatic speed-ups in evaluation time. However, the factors that govern the difficulty of these optimization problems remain unclear, limiting the larger adoption of the technique. In this work, we establish a precise relationship between the dynamics of a nonlinear system and the conditioning of its corresponding optimization formulation. We show that the predictability of a system, defined as the degree to which small perturbations in state influence future behavior, directly governs the number of optimization steps required for evaluation. In predictable systems, the state trajectory can be computed in $\\mathcal{O}((\\log T)^2)$ time, where $T$ is the sequence length, a major improvement over the conventional sequential approach. In contrast, chaotic or unpredictable systems exhibit poor conditioning, with the consequence that parallel evaluation converges too slowly to be useful. Importantly, our theoretical analysis demonstrates that for predictable systems, the optimization problem is always well-conditioned, whereas for unpredictable systems, the conditioning degrades exponentially as a function of the sequence length. We validate our claims through extensive experiments, providing practical guidance on when nonlinear dynamical systems can be efficiently parallelized, and highlighting predictability as a key design principle for parallelizable models.",
    "authors": [
      "~Xavier_Gonzalez1",
      "~Leo_Kozachkov1",
      "~David_M._Zoltowski1",
      "~Kenneth_L._Clarkson1",
      "~Scott_Linderman1"
    ],
    "pdf": "/pdf/ce3962ea4a5fbcc9516e7750a99733855fb35d9d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parallelization of state space models (increasingly relevant for LLMs), speed-up in evaluation time, efficiency analysis of parallelization, O((log T)²) computational complexity for predictable systems, predictability as a design principle for parallelizable models",
      "Irrelevant Aspects": "Heavy theoretical analysis with less practical implementation details, limited discussion of training optimization, no explicit GPU discussion",
      "Summary": "This paper presents theoretical insights on parallelizing nonlinear state space models by recasting evaluation as an optimization problem. Given the growing importance of state space models in LLM architectures (like Mamba), these insights are highly relevant to my research on inference optimization. The paper establishes a relationship between system predictability and optimization conditioning, claiming O((log T)²) evaluation time for predictable systems through parallelization. While theoretical in nature, if these insights can be translated to practical implementations for state space model-based LLMs, they could significantly impact GPU utilization, throughput, and latency optimization."
    }
  },
  {
    "id": "T1V8BJO0iG",
    "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
    "abstract": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A^\\star$-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V^\\star$, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A^\\star$-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30\\% compared to PPO, GRPO, and REBEL. Implementation of $A^\\star$-PO can be found at https://github.com/ZhaolinGao/A-PO.",
    "authors": [
      "~Kianté_Brantley2",
      "~Mingyu_Chen2",
      "~Zhaolin_Gao1",
      "~Jason_D._Lee1",
      "~Wen_Sun1",
      "~Wenhao_Zhan1",
      "~Xuezhou_Zhang2"
    ],
    "pdf": "/pdf/b1758af58d36e9832bc3a336c75ffb891ed2aa37.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimizing RL training for LLMs, directly addressing my interest in training optimization. It specifically targets computational efficiency, reducing training time by up to 2× and memory usage by over 30%, which aligns with my focus on GPU utilization and throughput. The A*-PO method eliminates costly online value estimation and requires only single generation per prompt, potentially improving training efficiency. The paper addresses memory consumption issues, which is crucial for GPU utilization.",
      "Irrelevant Aspects": "The paper is narrowly focused on reasoning tasks rather than general LLM optimization. It only addresses training optimization, not inference optimization, which is half of my research interest. The method is specifically for RL fine-tuning rather than the complete training pipeline. It doesn't appear to discuss distributed training or scalability across multiple GPUs, which are important aspects of my research focus.",
      "Summary": "This paper presents A*-PO, a two-stage policy optimization framework for RL fine-tuning of LLMs on reasoning tasks. It aims to reduce computational overhead and memory consumption during training by approximating the optimal advantage function. The method shows competitive performance on reasoning benchmarks while significantly improving training efficiency. The relevance to my research interests is primarily in the training optimization domain, particularly for reducing computational costs and memory usage during LLM fine-tuning, though it doesn't address inference optimization or general LLM training beyond reasoning tasks."
    }
  },
  {
    "id": "eV2Y8Gt6JY",
    "title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models",
    "abstract": "Vision–language models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce \\textbf{S}pectrum-Aware \\textbf{T}est-Time \\textbf{S}teering (\\textbf{STS}), a \\textit{lightweight adaptation framework} that extracts a spectral subspace from the textual embeddings to define principal semantic directions, and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8× faster with a 12× smaller memory footprint than conventional test-time prompt tuning. The code is available at \\url{https://github.com/kdafnis/STS}.",
    "authors": [
      "~Konstantinos_M._Dafnis1",
      "~Dimitris_N._Metaxas1"
    ],
    "pdf": "/pdf/a95fd3b6a4361dc9c82788104eb7ec089edac683.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Lightweight inference optimization framework that operates in latent space without backpropagation through encoders. Claims 8× faster inference speed and 12× smaller memory footprint compared to conventional methods. Minimizes computational overhead by adapting only a small number of per-sample parameters. Directly addresses throughput and memory efficiency concerns during inference.",
      "Irrelevant Aspects": "Focuses on Vision-Language Models rather than pure language models. Addresses domain shift adaptation rather than general inference optimization. The spectral subspace extraction is a specialized technique for handling test-time distribution changes rather than a general systems optimization approach.",
      "Summary": "This paper presents Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework for improving Vision-Language Model efficiency during inference. While the focus is on VLMs and handling domain shifts, its approach to minimizing computational overhead, reducing memory footprint, and accelerating inference directly aligns with optimization goals for large models, making it moderately relevant to my research interests in inference optimization."
    }
  },
  {
    "id": "uC3DI4YPRv",
    "title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM",
    "abstract": "Recently, test-time scaling of Large Language Models (LLMs) has emerged as a practical alternative to parameter and data scaling. Reasoning tasks often require large-scale, RLVR-based LLMs, while more economical LLMs can handle simpler tasks. Routing an LLM tailored to *suitability* (*i.e.*, capability and cost) ensures usability and efficiency. We introduce LLMRec, which routes the most suitable LLM to the user query without pre-inference on the candidate LLM zoo. It pioneeringly reframes the LLM routing problem as a comprehensive recommendation system (RecSys) task. Our core insight is that an LLM's suitability for a query is a complex, latent signal equal to user-item preference. LLMRec systematically engineers features for candidate LLMs (intrinsic attributes and capability distributions), queries (general semantics and meta-dimensional info), and context (inference type, cost budgets). It also incorporates behavioral features to learn high-order interactions. LLMRec is designed to generalize to out-of-domain datasets and adapt to new LLMs as the model zoo evolves. We define the metric with the Pareto frontier under user-specified cost budgets. Across six datasets, LLMRec achieves an average cost reduction of over 38% while maintaining accuracy and consistently outperforming baselines in converging toward the Pareto frontier.",
    "authors": [
      "~Yi-Kai_Zhang2",
      "~Shiyin_Lu1",
      "~Qing-Guo_Chen1",
      "~Weihua_Luo2",
      "~De-Chuan_Zhan1",
      "~Han-Jia_Ye1"
    ],
    "pdf": "/pdf/f8ce3f48c335fa784e0e6aef63e636d13c30d93d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization through intelligent routing to cost-effective models, focuses on 38% cost reduction which implies better resource utilization, emphasizes scalability through adaptability to evolving model zoos, and potentially improves throughput by directing simpler queries to faster models",
      "Irrelevant Aspects": "Does not address training optimization techniques, lacks explicit focus on GPU utilization optimization, does not directly discuss latency and throughput metrics, and centers on test-time scaling rather than training optimization",
      "Summary": "LLMRec introduces a routing system that selects the most suitable LLM for each query based on capability and cost requirements. By reframing LLM routing as a recommendation system task, it achieves significant cost reduction while maintaining accuracy. While this addresses important aspects of inference efficiency and resource utilization, it doesn't directly address training optimization or specific GPU utilization techniques that are central to my research interests."
    }
  },
  {
    "id": "tKPqbamNb9",
    "title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models",
    "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like “Wait” or “Let me rethink” can improve performance.  This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance—creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.",
    "authors": [
      "~Soumya_Suvra_Ghosal2",
      "~Souradip_Chakraborty1",
      "~Avinash_Reddy1",
      "~Yifu_Lu1",
      "~Mengdi_Wang1",
      "~Dinesh_Manocha3",
      "~Furong_Huang1",
      "~Mohammad_Ghavamzadeh2",
      "~Amrit_Singh_Bedi1"
    ],
    "pdf": "/pdf/9a4ddca48299d0fb623da4e9a0093d29392e48a2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses test-time scaling of reasoning models, which is a key aspect of inference optimization. It proposes 'parallel thinking' as an alternative approach that makes better use of inference budget, which relates to GPU utilization and efficiency. The research evaluates performance metrics relevant to throughput considerations and discusses inference budgeting, which is directly related to efficient resource utilization.",
      "Irrelevant Aspects": "The paper doesn't discuss training optimization strategies. There's no discussion about specific hardware implementation or low-level GPU optimization. The paper focuses more on the cognitive aspects of reasoning rather than system implementation aspects. No clear analysis of latency or throughput in concrete terms.",
      "Summary": "The paper investigates test-time scaling in reasoning models, finding that extended thinking traces lead to initial improvements followed by performance decline due to 'overthinking.' It proposes an alternative 'parallel thinking' approach that generates multiple reasoning paths within the same inference budget, achieving up to 20% higher accuracy. This research provides insights into efficient inference budget utilization, though it doesn't deeply address the technical implementation aspects of GPU optimization."
    }
  },
  {
    "id": "UJSaY7p53L",
    "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers",
    "abstract": "As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model’s pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on ∞-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.",
    "authors": [
      "~Woomin_Song1",
      "~Sai_Muralidhar_Jayanthi1",
      "~Srikanth_Ronanki1",
      "~Kanthashree_Mysore_Sathyendra1",
      "~Jinwoo_Shin1",
      "~Aram_Galstyan1",
      "~Shubham_Katiyar1",
      "~Sravan_Babu_Bodapati1"
    ],
    "pdf": "/pdf/c4638f02f2deda43c7fb1d4692e6ac8518809a8b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization for long-context processing, memory efficiency through compressed KV cache management, latency reduction (30% faster inference), scalability improvements for handling 1M context length, selective recompute techniques for resource efficiency",
      "Irrelevant Aspects": "Limited focus on training optimization, no explicit discussion of GPU utilization improvements, doesn't address throughput metrics specifically, specialized for long-context scenarios rather than general LLM optimization",
      "Summary": "REFORM is an inference framework that optimizes long-context processing through a two-phase approach involving compressed KV cache maintenance and selective recompute. It achieves significant performance gains (50% on RULER, 27% on BABILLong) while reducing inference time by 30% and memory usage by 5%. The paper is directly relevant to inference optimization research but lacks focus on training optimization and explicit GPU utilization improvements."
    }
  },
  {
    "id": "2D4TuZyNnr",
    "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
    "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.",
    "authors": [
      "~Annabelle_Sujun_Tang1",
      "~Christopher_Priebe1",
      "~Rohan_Mahapatra1",
      "~Lianhui_Qin1",
      "~Hadi_Esmaeilzadeh1"
    ],
    "pdf": "/pdf/f12352eb5f36069adc288b069f2053c6afba76c8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization and model serving efficiency, which are core areas of my expertise. It introduces a novel approach using LLMs themselves to guide compiler optimizations, focusing on hardware-informed transformations that could significantly improve GPU utilization. The combination of LLM reasoning with Monte Carlo Tree Search for exploring the optimization space is particularly relevant to my interest in optimization algorithms for ML systems. The paper's focus on achieving substantial speedups with fewer samples directly aligns with my goals of improving throughput and reducing latency in LLM systems.",
      "Irrelevant Aspects": "The paper appears to focus primarily on inference/serving optimization rather than training optimization, which is only part of my expertise area. Additionally, the approach may introduce additional computational overhead from the LLM-guided optimization process itself, which isn't fully addressed in the abstract. The paper also doesn't explicitly discuss scalability across multiple GPUs or distributed systems, which is an important aspect of my research interests.",
      "Summary": "The Reasoning Compiler paper introduces a novel framework that leverages LLMs to guide compiler optimizations for efficient model serving. By formulating optimization as a sequential decision process with LLM proposals and MCTS exploration, it aims to navigate the vast optimization space more efficiently than existing approaches. The paper directly addresses the high costs of serving large-scale models by achieving substantial speedups with fewer samples, making it highly relevant to my research focus on LLM inference optimization, GPU utilization, and performance improvement. While primarily focused on inference rather than training, the innovative use of LLM reasoning for optimization decisions represents an important advancement in the field of ML systems optimization."
    }
  },
  {
    "id": "kePsKwxvaV",
    "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner",
    "abstract": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's *eigenvalues* and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent *eigenbasis* computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms.",
    "authors": [
      "~Runa_Eschenhagen1",
      "~Aaron_Defazio1",
      "~Tsung-Hsien_Lee1",
      "~Richard_E_Turner1",
      "~Hao-Jun_Michael_Shi1"
    ],
    "pdf": "/pdf/dedb2a84aa0eafb0711b26c62e30474f3459d82a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses optimization algorithms for training neural networks, specifically focusing on Shampoo which has shown success in large-scale training. It investigates and aims to improve heuristics in the algorithm, which could enhance training efficiency for large language models. The work addresses scaling challenges and proposes methods to reduce computational complexity, which relates to GPU utilization and throughput.",
      "Irrelevant Aspects": "The paper primarily focuses on training rather than inference optimization, so its direct relevance to inference optimization is limited. It doesn't explicitly address latency concerns in inference. The focus is specifically on the Shampoo optimizer rather than a broader range of optimization techniques.",
      "Summary": "This paper investigates the Shampoo optimizer, focusing on its heuristics and proposing improvements to make it more principled. It decomposes the preconditioner to understand eigenvalue and eigenbasis updates separately, proposing an adaptive criterion for eigenbasis computation. While primarily focused on training optimization rather than inference, the work has strong relevance to large-scale model training efficiency and could contribute to improved GPU utilization in the training process."
    }
  },
  {
    "id": "bk1IlSAwxR",
    "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling",
    "abstract": "Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a $7\\times$ improvement in training speed for 100K sequence length and $9\\times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning (SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at \\url{https://github.com/CLAIRE-Labo/RAT}.",
    "authors": [
      "~Xiuying_Wei1",
      "~Anunay_Yadav1",
      "~Razvan_Pascanu1",
      "~Caglar_Gulcehre1"
    ],
    "pdf": "/pdf/82fc9d73686d395607b5076ccaba141fb70c41b0.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses computational bottlenecks in transformers, particularly the softmax attention mechanism affecting both training and inference efficiency. It proposes RAT architecture that achieves significant improvements in training speed (7x for 100K sequence length) and generation speed (9x at 4K position), directly relating to throughput optimization. The design reduces cache memory usage, important for GPU utilization. The paper addresses efficient handling of long sequences, a key challenge in scaling language models. The proposed method balances efficiency and performance, central to optimization research. The hybrid architecture interleaving RAT with local attention shows improvements in both speed and performance. The paper includes actual implementation and experimental results with 1.3B parameter models, demonstrating practical applicability.",
      "Irrelevant Aspects": "The paper doesn't focus specifically on GPU utilization metrics beyond general efficiency. There isn't detailed discussion of latency measurements, only generation speed improvements. The paper appears to be more about architectural innovation than system-level optimizations. No specific mention of distributed training or inference optimizations across multiple GPUs.",
      "Summary": "The RAT paper introduces an intermediate design between RNNs and attention mechanisms that partitions inputs into chunks, using recurrence within chunks and attention across them. This approach achieves significant training (7x) and generation (9x) speed improvements while maintaining performance comparable to standard attention. A hybrid architecture combining RAT with local attention further enhances both efficiency and performance. While highly relevant to optimization goals, the paper focuses more on architectural innovation than system-level optimizations or detailed GPU utilization metrics."
    }
  },
  {
    "id": "fm14gUThwh",
    "title": "Multi-head Temporal Latent Attention",
    "abstract": "While Transformer self-attention offers strong parallelism, the  Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3$\\times$ speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.",
    "authors": [
      "~Keqi_Deng1",
      "~Phil_Woodland1"
    ],
    "pdf": "/pdf/2a3fde81e485e82d650781455b129ce0e60f4d3e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses a critical bottleneck in Transformer inference - the KV cache that grows linearly with sequence length. MTLA's approach of compressing the KV cache along both latent and temporal dimensions is highly relevant to LLM inference optimization. The reported 5.3x speedup and 8.3x GPU memory reduction are substantial improvements that directly align with goals of better GPU utilization, higher throughput, and lower latency. The stride-aware causal mask mechanism is an elegant solution to maintain consistency between training and inference.",
      "Irrelevant Aspects": "The paper focuses primarily on speech-related tasks and text summarization rather than general LLM applications. It doesn't address training optimization techniques, which is part of my expertise. There's limited discussion about integration with other inference optimization strategies like quantization or knowledge distillation. The paper doesn't explore distributed inference scenarios for extremely large models, which is increasingly important in the LLM landscape.",
      "Summary": "MTLA presents a significant advancement in Transformer inference efficiency by introducing temporal compression to the KV cache on top of existing latent compression techniques. The hyper-network approach for dynamically merging temporally adjacent KV vectors and the stride-aware causal mask for maintaining training-inference consistency are technically sound. While the evaluation is limited to speech and summarization tasks, the approach appears broadly applicable to Transformer architectures. The reported improvements in memory usage and inference speed make this a highly relevant contribution to LLM optimization research, particularly for applications processing long sequences where KV cache memory is a critical constraint."
    }
  },
  {
    "id": "H918WyPf0s",
    "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking",
    "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications.\nDue to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results.\nThis fixed computation disregards query difficulty and document distribution, leading to inefficiencies. \nWe propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. \nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions.\nResults on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy–efficiency trade-off and scales better with compute than fixed-computation baselines.\nThese results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models.",
    "authors": [
      "~Soyoung_Yoon1",
      "~Gyuwan_Kim1",
      "~GYU-HWUNG_CHO2",
      "~seung-won_hwang2"
    ],
    "pdf": "/pdf/94b4a6ec40bae1b4a0a48f35ba8b051259a24c1b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference optimization through adaptive computation based on uncertainty estimates, which aligns with my interest in reducing inference costs and improving efficiency. It focuses on improving the accuracy-efficiency trade-off and scalability with compute resources for LLM-based reranking systems, which is directly relevant to GPU utilization optimization and throughput improvement. The uncertainty-aware approach allows for dynamic resource allocation, potentially reducing latency for certain queries while maintaining accuracy. The method avoids unnecessary computations when confidence levels are reached, which could significantly improve throughput in practical deployment scenarios.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization and does not address training optimization techniques for LLMs. It does not cover low-level GPU utilization strategies, kernel optimizations, or memory management during training. The work is application-specific to reranking tasks rather than addressing general LLM architecture optimization. There's no discussion of distributed training systems or frameworks for scaling training across multiple GPUs. The paper doesn't address quantization, distillation, or other model compression techniques that are relevant to my broader research interests.",
      "Summary": "AcuRank presents an uncertainty-aware adaptive computation framework for LLM-based reranking that dynamically adjusts computation based on confidence levels. While this has significant relevance to inference optimization, scalability, and throughput improvement for reranking applications, it is limited in scope compared to my broader research interests in both training and inference optimization for LLMs. The adaptive computation approach is interesting and applicable to my focus on resource-efficient AI systems, but the paper doesn't address the full range of optimization techniques I'm interested in."
    }
  },
  {
    "id": "Ei6IsmxYrb",
    "title": "How to Scale Second-Order Optimization",
    "abstract": "Several recently introduced deep learning optimizers inspired by second-order methods have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results, with some finding quickly diminishing advantage over AdamW with scale. In this work, we investigate \\emph{how to scale} second-order optimizers to achieve optimal performance at scale. Through theoretical and empirical analysis, we derive scaling rules for hyperparameters such as learning rate and weight decay as we scale up model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. For compute-optimal scaling, we find scaling independent weight decay as $1/\\mathrm{width}$ is nearly optimal across optimizers, and that second-order optimizers have a substantially larger optimal model size compared to AdamW for a fixed compute budget. Applying these scaling rules, we show Muon achieves close to $1.4\\times$ or higher speedup over AdamW in training transformer language models, while incorrect scaling can decrease the speedup from $1.4\\times$ to below $1.1\\times$ from $190$M to $640$M parameter models.",
    "authors": [
      "~Zixi_Chen4",
      "~Shikai_Qiu1",
      "~Hoang_Phan1",
      "~Qi_Lei1",
      "~Andrew_Gordon_Wilson1"
    ],
    "pdf": "/pdf/d9ff9b9df54dd1e155b0d792f9a86d879a81a53c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization for large language models, scalability of optimization methods, speedup over AdamW (1.4×), hyperparameter scaling rules for model width and depth, transformer language models training efficiency",
      "Irrelevant Aspects": "Does not cover inference optimization, latency considerations, or explicit GPU utilization techniques beyond what's implied by the speedup",
      "Summary": "The paper investigates scaling second-order optimization methods (Shampoo, SOAP, Muon) for large language model training, deriving hyperparameter scaling rules and demonstrating 1.4× speedup over AdamW for transformer language models when properly scaled"
    }
  },
  {
    "id": "byNNv5Et10",
    "title": "3BASiL: An Algorithmic Framework for Sparse plus Low-Rank Compression of LLMs",
    "abstract": "Sparse plus Low-Rank $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of Large Language Models (LLMs) has emerged as a promising direction in $\\textit{model compression}$, aiming to decompose pre-trained model weights into a sum of sparse and low-rank matrices $\\mathbf{W} \\approx \\mathbf{S} + \\mathbf{LR}$. Despite recent progress, existing methods often suffer from substantial performance degradation compared to dense models. In this work, we introduce $\\texttt{3BASiL-TM}$, an efficient one-shot post-training method for $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of LLMs that addresses this gap. Our approach first introduces a novel 3-Block Alternating Direction Method of Multipliers (ADMM) method, termed $\\texttt{3BASiL}$, to minimize the layer-wise reconstruction error with convergence guarantees. \nWe then design a transformer-matching ($\\texttt{TM}$) refinement step that jointly optimizes the sparse and low-rank components across transformer layers. This step minimizes a novel memory-efficient  loss that aligns outputs at the transformer level.\nNotably, the $\\texttt{TM}$ procedure is universal as it can enhance any $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition, including pure sparsity. Our numerical experiments show that $\\texttt{3BASiL-TM}$ reduces the WikiText2 perplexity gap to dense LLaMA-8B model by over 30% under a (2:4 Sparse + 64 LR) configuration, compared to prior methods. Moreover, our method achieves over 2.5x faster compression runtime on an A100 GPU compared to SOTA $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ method.\nOur code is available at https://github.com/mazumder-lab/3BASiL.",
    "authors": [
      "~Mehdi_Makni1",
      "~Xiang_Meng1",
      "~Rahul_Mazumder1"
    ],
    "pdf": "/pdf/73d6c16c58d988d2aa7bd2cbb3a3fea83bf60c5b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization through model compression, GPU performance optimization (2.5x faster compression on A100), scalability to large models (LLaMA-8B), maintaining model quality (reduced perplexity gap), potential for improved inference throughput and latency, sparse plus low-rank decomposition as an advanced compression technique",
      "Irrelevant Aspects": "Focuses on post-training compression rather than training optimization, doesn't explicitly measure inference throughput or latency improvements",
      "Summary": "This paper presents a sparse plus low-rank decomposition method for LLM compression that is highly relevant to my research interests in inference optimization and GPU utilization. The approach addresses key challenges in making large models more efficient for inference, which directly impacts throughput and latency. While it doesn't explicitly measure these metrics, the compression technique itself is fundamental to achieving better GPU utilization and scalability for LLMs in production environments."
    }
  },
  {
    "id": "Pvoy6fWaRJ",
    "title": "KOALA++: Efficient Kalman-Based Optimization with Gradient-Covariance Products",
    "abstract": "We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix by recursively updating compact gradient covariance products. This design improves upon the original KOALA framework that assumed diagonal covariance by implicitly capturing richer uncertainty structure without storing the full covariance matrix and avoiding large matrix inversions. Across diverse tasks, including image classification and language modeling, KOALA++ achieves accuracy on par or better than state-of-the-art second-order optimizers while maintaining the efficiency of first-order methods.",
    "authors": [
      "~Zixuan_Xia2",
      "~Aram_Davtyan1",
      "~Paolo_Favaro1"
    ],
    "pdf": "/pdf/7f8d482a5a2d3895f7869bac6d5581964d478a22.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on efficient optimization for neural network training, which directly relates to my interest in LLM training optimization. It claims to achieve accuracy comparable to second-order optimizers while maintaining first-order efficiency, which could improve GPU utilization and throughput. The method avoids expensive second-order gradient calculations and large matrix inversions, potentially enhancing scalability. The paper mentions experiments on language modeling tasks, which is directly relevant to LLM training optimization.",
      "Irrelevant Aspects": "The paper's focus on Kalman filtering is a relatively niche optimization approach compared to more mainstream methods used in LLM training. The specific technique of gradient covariance products might be too narrow for my broader interests in LLM optimization. The mention of image classification as an application is less relevant to my LLM focus.",
      "Summary": "KOALA++ introduces a scalable Kalman-based optimization algorithm for neural networks that models gradient uncertainty efficiently. It improves upon the original KOALA by capturing richer uncertainty structure without storing full covariance matrices. The method claims to achieve accuracy comparable to second-order optimizers while maintaining first-order efficiency, with applications in both image classification and language modeling."
    }
  },
  {
    "id": "52Ehpe0Lu5",
    "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation is Wasteful",
    "abstract": "Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes.  In particular, rather than holding the decay rate of the second moment fixed across batch sizes, we propose to hold its half-life fixed in terms of tokens. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas. Finally, we show that a small batch size combined with an optimizer with a small state size can provide the performance benefits of full fine-tuning while maintaining a similar memory footprint to LoRA.",
    "authors": [
      "~Martin_Marek1",
      "~Sanae_Lotfi1",
      "~Aditya_Somasundaram1",
      "~Andrew_Gordon_Wilson1",
      "~Micah_Goldblum1"
    ],
    "pdf": "/pdf/8bf869b4e2a6ac77e7599efe5ce6e569b2d14ec4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses language model training optimization through small batch sizes, which impacts GPU utilization and throughput. It challenges conventional wisdom about gradient accumulation and proposes scaling rules for Adam hyperparameters to small batch sizes. The finding that small batch sizes achieve equal or better per-FLOP performance is highly relevant. The recommendation against gradient accumulation unless training on multiple devices is pertinent to distributed training optimization. The discovery that small batch sizes with small-state optimizers can provide full fine-tuning benefits while maintaining memory similar to LoRA is valuable for memory optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on training optimization rather than inference optimization. There's no explicit discussion of latency improvements, which is a key research interest. The paper doesn't appear to address specific hardware optimizations beyond general GPU utilization.",
      "Summary": "This paper is highly relevant to my research interests in LLM training optimization. It provides insights into small batch size training that could fundamentally improve GPU utilization and training throughput. The challenges to conventional gradient accumulation practices and the proposed hyperparameter scaling rules offer practical optimization approaches. However, the lack of focus on inference optimization and latency concerns prevents it from being maximally relevant."
    }
  },
  {
    "id": "4jFSekBaDT",
    "title": "The Best Instruction-Tuning Data are Those That Fit",
    "abstract": "High-quality supervised finetuning (SFT) data are essential for unlocking pretrained LLMs’ capabilities. Typically, instructions are paired with responses from various sources—by human annotators or other LMs—which are often out of the distribution of the target model to be finetuned. At scale, this mismatch can lead to diminishing returns and even hurt model performance and robustness. We hypothesize that SFT is most effective when the data is aligned with the model’s pretrained distribution, and propose **GRAPE**—a novel SFT framework that tailors supervision to the target model. For each instruction, it **g**athers **r**esponses from various sources and selects the one that **a**ligns most closely to the model’s **pre**trained distribution, as measured by the normalized probability. Standard SFT is then performed on these selected responses.\n\nWe first evaluate GRAPE in a controlled experiment, sampling multiple responses per question in the UltraInteract dataset from diverse models. We finetune using GRAPE-selected data on LMs from different families, including LLaMA-1-8B, Mistral-7B, and Qwen2.5-7B. GRAPE significantly outperforms strong baselines—including distilling from the strongest model—with absolute gains up to **13.8%** averaged across benchmarks, and outperforms a 3× larger data baseline with improvements up to **17.3%**.\n\nGRAPE's benefits generalize to off-the-shelf SFT data. When used to subsample from the post-training data of Tulu3 and Olmo-2, GRAPE surpasses strong baselines trained on 4.5× more data by **6.1%**, and outperforms state-of-the-art selection methods by **3.9%** on average. Notably, with only **1/3 the data** and **half the training epochs**, GRAPE enables LLaMA-1-8B to **exceed Tulu3-SFT performance by 3.5%**.\n\nOur findings highlight that aligning supervision with the pretrained distribution provides a simple yet powerful strategy to improve both the **efficiency** and **effectiveness** of SFT.",
    "authors": [
      "~Dylan_Zhang2",
      "~Qirun_Dai1",
      "~Hao_Peng4"
    ],
    "pdf": "/pdf/0170afc74e4e6ca42fe02ec7208322ee146122e9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on improving training efficiency in supervised fine-tuning, enabling models to achieve better performance with significantly less data and fewer training epochs. Demonstrates resource optimization by allowing smaller models to match or exceed the performance of models trained on much larger datasets. Addresses model-size efficiency which has implications for inference systems. The approach can potentially reduce overall training time and computational costs, contributing to better resource utilization in the training pipeline.",
      "Irrelevant Aspects": "Does not directly address GPU utilization techniques or specific hardware optimization strategies. Limited discussion of inference optimization, latency improvements, or throughput enhancements. Lacks detailed analysis of computational overhead introduced by the GRAPE selection process. Does not explore distributed training across multiple GPUs/nodes in depth. No examination of memory optimization or inference serving optimizations.",
      "Summary": "This paper introduces GRAPE, a framework that improves supervised fine-tuning efficiency by selecting training data that aligns with the target model's pretrained distribution. While not directly addressing GPU utilization or inference optimization, it offers significant improvements in training efficiency, enabling better performance with less data and fewer training epochs. The resource optimization aspects are relevant to the broader goal of making LLM systems more efficient, though the paper focuses on data efficiency rather than computational infrastructure optimization."
    }
  },
  {
    "id": "XIeE8jbM4K",
    "title": "Sketched Adaptive Distributed Deep Learning: A Sharp Convergence Analysis",
    "abstract": "Combining gradient compression with adaptive optimizers is a highly desirable goal in distributed learning, with potential benefits in both fewer communication rounds and less per-round communication. In spite of preliminary empirical promise, certain major challenges in the convergence analysis of such methods have stayed open: handling compression based approximation of both first and second moments (pre-conditioner) which appear as a ratio; avoiding dependence on the number of parameters, which is extremely large in modern deep models; and providing high-probability guarantees instead of in-expectation, which can hide high variance behavior.\n\nIn this work, we introduce a family of Sketched Adaptive Distributed Learning (SADL) algorithms which can use suitable unbiased gradient sketching for compression with suitable adaptive optimization algorithms. As our main contribution, we provide theoretical convergence guarantees of SADL algorithms which addresses all of the existing challenges. In particular, our guarantees hold with high probability, picks up only a logarithmic dependence on the number of parameters, and the first and second moment approximation is handled precisely yielding a dependence on the intrinsic dimension of the loss Hessian, which is significantly smaller than the full dimensionality of deep learning models.  Empirically, the SADL algorithms are shown to be competitive with and often outperform baselines on both vision and language tasks, in both supervised fine-tuning and training-from-scratch regimes. Further, the SADL algorithms are also competitive with the state-of-the-art communication-efficient distributed learning algorithms based on error feedback.",
    "authors": [
      "~Zhijie_Chen2",
      "~Qiaobo_Li1",
      "~Arindam_Banerjee4"
    ],
    "pdf": "/pdf/5b6e349d9e3da14661cfe84df20b6852a51fdfcc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Distributed learning optimization, gradient compression for communication efficiency, adaptive optimizers commonly used in large language model training, evaluation on language tasks, convergence guarantees with only logarithmic dependence on parameters, high-probability guarantees for training reliability, communication-efficient distributed learning algorithms that could improve GPU utilization and scalability",
      "Irrelevant Aspects": "Focus on vision tasks (less directly relevant to language models), heavy emphasis on theoretical aspects rather than practical implementation details, no explicit mention of inference optimization techniques",
      "Summary": "The paper introduces Sketched Adaptive Distributed Learning (SADL) algorithms that combine gradient compression with adaptive optimizers for distributed training. It provides strong theoretical guarantees with high-probability convergence bounds and only logarithmic dependence on model parameters. The methods are evaluated on both vision and language tasks, showing competitive performance with state-of-the-art communication-efficient approaches. This work is highly relevant to large language model training optimization, addressing key challenges in communication efficiency and distributed learning scalability, though it lacks direct focus on inference optimization."
    }
  },
  {
    "id": "h5YGwnMTke",
    "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models",
    "abstract": "Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we demonstrate that models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to 39% fewer sequential tokens on problems requiring more than 8,000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks, namely GPQA and Countdown, with up to 45% and 65% reduction in average sequential tokens respectively for longer reasoning trajectories, while matching the performance of the fine-tuned reasoning model.",
    "authors": [
      "~Emil_Biju1",
      "~Shayan_Talaei1",
      "~Zhemin_Huang1",
      "~Mohammadreza_Pourreza1",
      "~Azalia_Mirhoseini3",
      "~Amin_Saberi1"
    ],
    "pdf": "/pdf/2ac09a3b4dda169b90bb239fc45cb980a8bc3efd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Inference optimization through parallelization of reasoning processes, reduction in sequential token generation leading to lower latency and higher throughput, GPU utilization improvements via parallel execution, post-training optimization techniques, framework for dynamic identification of parallelizable subtasks",
      "Irrelevant Aspects": "Focus on reasoning model capabilities rather than system performance, limited discussion of hardware-level optimizations, primary emphasis on reasoning accuracy over system metrics",
      "Summary": "This paper presents SPRINT, a framework that optimizes inference for large reasoning models by enabling parallel execution of independent reasoning subtasks. The approach reduces sequential token generation by 39-65% while maintaining performance, directly addressing throughput and latency optimization goals. The method involves post-training fine-tuning on curated data to teach models dynamic parallelization, which aligns well with inference optimization objectives for better GPU utilization and scalability."
    }
  },
  {
    "id": "zj45hoQhjD",
    "title": "Scalable In-context Ranking with Generative Models",
    "abstract": "In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that BlockRank Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.",
    "authors": [
      "~Nilesh_Gupta1",
      "~Chong_You1",
      "~Srinadh_Bhojanapalli1",
      "~Sanjiv_Kumar1",
      "~Inderjit_S_Dhillon1",
      "~Felix_X._Yu2"
    ],
    "pdf": "/pdf/2bde23a7feb803c4553d37803139b782512dcc0f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization by reducing attention complexity from quadratic to linear, which is a critical bottleneck in LLM deployment. It achieves significant speedups (4.7x) while maintaining or improving performance, directly contributing to higher throughput and lower latency. The approach scales to handle large contexts (100K tokens) efficiently, addressing a key challenge in LLM deployment. The focus on optimizing attention mechanisms and reducing computational requirements aligns perfectly with GPU utilization optimization goals.",
      "Irrelevant Aspects": "The paper is specifically focused on Information Retrieval and ranking tasks, rather than general LLM optimization. Some optimizations are tailored specifically for the In-context Ranking paradigm rather than being broadly applicable to all LLM use cases. The evaluation is limited to ranking benchmarks rather than broader LLM performance metrics.",
      "Summary": "This paper presents BlockRank, a method to optimize LLM attention for In-context Ranking tasks. It identifies and exploits inherent attention structures in LLMs fine-tuned for ranking - specifically inter-document block sparsity and query-document block relevance patterns. By enforcing these patterns during both architecture design and fine-tuning, the method reduces computational complexity from quadratic to linear. The approach achieves 4.7x speedup for 100 documents and can handle 500 documents (100K context) within a second while matching or exceeding SOTA ranking performance. This work directly contributes to LLM inference optimization, scalability, and efficiency - core aspects of my research focus on machine learning systems."
    }
  },
  {
    "id": "JV6ZOUb7BD",
    "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage",
    "abstract": "We present the design and implementation of a new lifetime-aware tensor offloading\nframework for GPU memory expansion using low-cost PCIe-based solid-state\ndrives (SSDs). Our framework, TERAIO, is developed explicitly for large language\nmodel (LLM) training with multiple GPUs and multiple SSDs. Its design is driven\nby our observation that the active tensors take only a small fraction (1.7% on\naverage) of allocated GPU memory in each LLM training iteration, the inactive\ntensors are usually large and will not be used for a long period of time, creating\nample opportunities for offloading/prefetching tensors to/from slow SSDs without\nstalling the GPU training process. TERAIO accurately estimates the lifetime (active\nperiod of time in GPU memory) of each tensor with the profiling of the first few\niterations in the training process. With the tensor lifetime analysis, TERAIO will\ngenerate an optimized tensor offloading/prefetching plan and integrate it into the\ncompiled LLM program via PyTorch. TERAIO has a runtime tensor migration\nengine to execute the offloading/prefetching plan via GPUDirect storage, which\nallows direct tensor migration between GPUs and SSDs for alleviating the CPU\nbottleneck and maximizing the SSD bandwidth utilization. In comparison with\nstate-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that\nTERAIO improves the training performance of various LLMs by 1.47× on average,\nand achieves 80.7% of the ideal performance assuming unlimited GPU memory.",
    "authors": [
      "~Ziqi_Yuan2",
      "~Haoyang_Zhang9",
      "~Yirui_Eric_Zhou1",
      "~Apoorve_Mohan1",
      "~I-Hsin_Chung1",
      "~Seetharami_Seelam1",
      "~Jian_Huang2"
    ],
    "pdf": "/pdf/80825867b457e7c96427848264d5acee0f767fc6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM training optimization with a novel tensor offloading approach using SSDs, addressing GPU memory constraints which is a key bottleneck in LLM training. It utilizes GPUDirect storage for efficient data transfer, works with multiple GPUs for scalability, implements in PyTorch, compares with state-of-the-art solutions like ZeRO-Offload and ZeRO-Infinity, and reports significant performance improvements (1.47× on average). The approach optimizes GPU memory and SSD bandwidth utilization through lifetime-aware tensor management.",
      "Irrelevant Aspects": "The paper doesn't focus on inference optimization, doesn't explicitly address latency improvements, doesn't directly discuss throughput improvements, doesn't cover quantization or model compression techniques, and doesn't mention model parallelism strategies beyond what's related to memory management.",
      "Summary": "TERAIO presents a lifetime-aware tensor offloading framework that uses SSDs to expand GPU memory for LLM training. By accurately estimating tensor lifetimes and implementing optimized offloading/prefetching plans via GPUDirect storage, it achieves 1.47× performance improvement compared to state-of-the-art solutions while reaching 80.7% of ideal performance assuming unlimited GPU memory."
    }
  },
  {
    "id": "Ar62cqTduE",
    "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
    "abstract": "Linear Transformers and State Space Models have emerged as efficient alternatives\nto softmax Transformers for causal sequence modeling, enabling parallel training\nvia matrix multiplication and efficient RNN-style inference. However, despite their\nsuccess in causal tasks, no unified framework exists for applying Linear Transformers to bidirectional sequence modeling. We introduce LION, the first framework to\nsystematically extend Linear Transformers to the bidirectional setting. LION generalizes three core representations commonly used in the causal case—full Linear\nAttention , bidirectional RNN, and chunkwise parallel form—to the bidirectional\nsetting. These forms are theoretically equivalent and enable models to exploit the\nstrengths of each during training and inference. We prove that a broad class of\nLinear Transformers can be extended using LION and validate our framework via\nthree core examples based on the choice of decay type: LION-LIT, the bidirectional\nextension of [25]; LION-D, based on [44]; and LION-S, a variant using selective\ndecay [34, 13]. Across standard bidirectional tasks, LION enables models to match\nor exceed the performance of softmax Transformers, while offering significantly\nfaster training and more efficient inference than existing State Space Models.",
    "authors": [
      "~Arshia_Afzal1",
      "~Elias_Abad_Rocamora1",
      "~Leyla_Naz_Candogan1",
      "~Pol_Puigdemont1",
      "~Francesco_Tonin1",
      "~Yongtao_Wu1",
      "~Mahsa_Shoaran1",
      "~Volkan_Cevher1"
    ],
    "pdf": "/pdf/10f9b20338892c423f6c1f43e0a16edd0b4f2665.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Faster training via matrix multiplication, efficient RNN-style inference, improved GPU utilization through parallel processing, higher throughput and lower latency claims, better scalability than softmax Transformers",
      "Irrelevant Aspects": "Primarily focused on architectural extension rather than system-level optimization, limited discussion of specific GPU optimization techniques, lacks detailed implementation for production environments",
      "Summary": "This paper introduces LION, a framework extending Linear Transformers to bidirectional sequence modeling with significant efficiency improvements. It directly addresses my research interests by claiming faster training through parallel matrix multiplication and efficient inference via RNN-style processing. The focus on speed and efficiency makes it relevant for GPU utilization and scalability, though it emphasizes architectural innovation over specific optimization techniques."
    }
  },
  {
    "id": "e4IlBqhbTO",
    "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning",
    "abstract": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost.\nWe introduce **C3PO** (*Cost Controlled Cascaded Prediction Optimization*), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget.\nWe provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.",
    "authors": [
      "~Antonios_Valkanas1",
      "~Soumyasundar_Pal1",
      "~Pavel_Rumiantsev1",
      "~Yingxue_Zhang1",
      "~Mark_Coates1"
    ],
    "pdf": "/pdf/7e9f932dcbfb5b1eae4eba320757830a89785dc4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference cost optimization for LLMs through cascaded inference approaches, where smaller models handle easy queries and larger models handle difficult ones. This directly impacts GPU utilization by efficiently allocating computational resources. The self-supervised framework C3PO provides cost control guarantees, which contributes to better scalability for LLM deployment. The focus on cost-efficiency improvements aligns with my interest in optimizing resource utilization, and the approach would naturally improve throughput by reducing unnecessary computation on expensive models.",
      "Irrelevant Aspects": "The paper doesn't explicitly focus on GPU-specific optimization techniques or parallelization strategies. There's limited discussion of latency reduction methods specifically. The research is primarily focused on reasoning benchmarks rather than general optimization across all LLM tasks. The approach doesn't address batch processing optimizations or memory management techniques that could further improve GPU utilization.",
      "Summary": "C3PO introduces a self-supervised framework for optimizing LLM cascades with probabilistic cost constraints. By minimizing regret with respect to the most powerful model and using conformal prediction for cost bounding, it achieves state-of-the-art performance on reasoning benchmarks with improved accuracy and cost-efficiency. While the paper addresses important aspects of inference optimization that impact GPU utilization and scalability, it focuses more on accuracy-cost tradeoffs than on specific GPU utilization or latency optimization techniques."
    }
  },
  {
    "id": "IJryQAOy0p",
    "title": "Remasking Discrete Diffusion Models with Inference-Time Scaling",
    "abstract": "Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. When applied to large pretrained diffusion language models, ReMDM boosts the model’s performance on downstream tasks requiring factual knowledge grasp and reasoning ability.",
    "authors": [
      "~Guanghan_Wang1",
      "~Yair_Schiff1",
      "~Subham_Sekhar_Sahoo1",
      "~Volodymyr_Kuleshov1"
    ],
    "pdf": "/pdf/0c977a863006164f57b33a6e638f6d83d52bf588.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces ReMDM, a method that enables inference-time compute scaling for masked discrete diffusion models. This is highly relevant to my research interests in inference optimization, as it provides a mechanism to adjust computation based on available resources, directly impacting throughput and latency optimization. The approach allows for flexible resource allocation during inference by increasing or decreasing the number of sampling steps based on computational budget. This has significant implications for optimizing inference processes on GPUs, particularly for large pretrained models where efficient resource utilization is critical for achieving better scalability and performance.",
      "Irrelevant Aspects": "The paper focuses more on the algorithmic improvements and output quality rather than explicit GPU utilization techniques or hardware-level optimizations. There's limited discussion on training optimization aspects, which is part of my research interest. The paper doesn't address low-level memory management strategies or specific kernel optimizations that would be directly applicable to maximizing GPU efficiency. While it discusses computation scaling, it doesn't provide detailed analysis of how this translates to actual GPU utilization patterns or throughput measurements.",
      "Summary": "This paper presents ReMDM, a novel sampler for masked discrete diffusion models that enables iterative refinement and inference-time compute scaling. The approach introduces an interesting method for flexible resource allocation during inference, allowing for trade-offs between computational resources and output quality. While it doesn't focus specifically on GPU utilization techniques, the concept of inference-time scaling is valuable for optimizing inference processes and could have meaningful implications for achieving better throughput and latency in large language model deployment."
    }
  },
  {
    "id": "67xkPEM3bZ",
    "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
    "abstract": "Large language model (LLM) agents for web interfaces have advanced rapidly, yet open-source systems still lag behind proprietary agents. Bridging this gap is key to enabling customizable, efficient, and privacy-preserving agents. Two challenges hinder progress: the reproducibility issues in RL and LLM agent training, where results often depend on sensitive factors like seeds and decoding parameters, and the focus of prior work on single-step tasks, overlooking the complexities of web-based, multi-step decision-making.\n\nWe address these gaps by providing a statistically driven study of training LLM agents for web tasks. Our two-stage pipeline combines imitation learning from a Llama 3.3 70B teacher with on-policy fine-tuning via Group Relative Policy Optimization (GRPO) on a Llama 3.1 8B student. Through 240 configuration sweeps and rigorous bootstrapping, we chart the first compute allocation curve for open-source LLM web agents. Our findings show that dedicating one-third of compute to teacher traces and the rest to RL improves MiniWoB++ success by 6 points and closes 60\\% of the gap to GPT-4o on WorkArena, while cutting GPU costs by 45\\%. We introduce a principled hyperparameter sensitivity analysis, offering actionable guidelines for robust and cost-effective agent training.",
    "authors": [
      "~Dheeraj_Vattikonda1",
      "~Santhoshi_Ravichandran1",
      "~Emiliano_Penaloza1",
      "~Hadi_Nekoei1",
      "~Thibault_Le_Sellier_de_Chezelles1",
      "~Megh_Thakkar1",
      "~Nicolas_Gontier1",
      "~Miguel_Muñoz-Mármol1",
      "~Sahar_Omidi_Shayegan1",
      "~Stefania_Raimondo1",
      "~Xue_Liu1",
      "~Alexandre_Drouin2",
      "~Alexandre_Piché1",
      "~Alexandre_Lacoste1",
      "~Massimo_Caccia1"
    ],
    "pdf": "/pdf/0252f1f5697c0153740a7438e473e45964e85102.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization for LLMs using imitation learning and GRPO, GPU cost reduction by 45%, compute allocation optimization, statistical analysis of training configurations, efficient resource allocation strategies",
      "Irrelevant Aspects": "Focus limited to web agents rather than general LLM optimization, minimal discussion of inference optimization, no explicit coverage of scalability challenges",
      "Summary": "This paper presents a statistically-driven approach to training LLM web agents, combining imitation learning with GRPO reinforcement learning. It establishes compute allocation curves that improve task success rates while significantly reducing GPU costs. The work offers valuable insights into training optimization and resource efficiency, though it's specifically focused on web-based multi-step decision-making tasks rather than general LLM optimization."
    }
  },
  {
    "id": "DIjRvEKOeG",
    "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training",
    "abstract": "Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20\\% compared to state-of-the-art methods.",
    "authors": [
      "~Yehonathan_Refael1",
      "~Guy_Smorodinsky1",
      "~Tom_Tirer1",
      "~Ofir_Lindenbaum1"
    ],
    "pdf": "/pdf/ca161a298797bc19d3bdae9ebd503ac9a27dc1e4.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses memory-efficient LLM training using low-rank gradient optimization, which is crucial for GPU utilization. It introduces SUMO, a novel optimizer that not only reduces memory requirements by up to 20% but also accelerates convergence, which directly impacts training throughput. The method uses exact SVD for moment orthogonalization, improving optimization in anisotropic loss landscapes typical of LLMs. The paper demonstrates improvements in stability, performance, and convergence rates, all of which are key metrics for efficient training optimization.",
      "Irrelevant Aspects": "The paper doesn't appear to address inference optimization specifically, which is part of my research focus. There's no mention of latency reduction during inference or model serving optimizations. The abstract also doesn't discuss distributed training aspects or multi-GPU scaling strategies, which would be relevant for understanding broader scalability implications.",
      "Summary": "SUMO introduces a novel optimization approach for memory-efficient LLM training that combines subspace-aware moment orthogonalization with exact SVD to accelerate convergence while reducing memory usage. The method addresses limitations of existing low-rank gradient methods by aligning optimization steps with the spectral characteristics of the loss landscape. With demonstrated memory reduction of up to 20% and improved convergence rates, this work is highly relevant for training optimization and resource utilization in LLM development."
    }
  },
  {
    "id": "0Y7AxxNCYh",
    "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities",
    "abstract": "The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called *token premiums*. Having high token premiums leads to less throughput during training and increases costs at inference. \nIn this paper, we show that even after controlling for dataset size, vocabulary size, and data content, monolingual tokenizers exhibit a wide range of token premiums across languages. To understand the cross-linguistic differences that cause these token premiums,\nwe train a suite of approximately 7,000 comparable monolingual tokenizers for 97 languages, manipulating tokenization algorithm vocabulary size, and dataset size. We measure token premiums and test for a relationship between factors such as data similarity (between tokenizer training and evaluation), vocabulary size, and pre-tokenization. We also investigate the role of language-specific features such as writing system and word length. We find that similarity between training and test data does not impact token premiums, but vocabulary size and pre-tokenization do. While simply increasing vocabulary size does not lead to reduced token premium effects, we can determine an \"optimal\" vocabulary size for each language to achieve significantly reduced token premium effects. We also train superword tokenizers which allow merges over whitespaces, and we find that they both reduce token premium effects and improve compression overall. Thus, intervening on the vocabulary size or the pre-tokenizer significantly reduces crosslingual token premium effects.",
    "authors": [
      "~Catherine_Arnett1",
      "~Tyler_A._Chang1",
      "~Stella_Biderman1",
      "~Ben_Bergen1"
    ],
    "pdf": "/pdf/ad885c101250e1c7b0540da17ae173dc4ed67e7c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses tokenization efficiency which impacts training throughput and inference costs. It discusses methods to reduce token premiums through vocabulary size optimization and pre-tokenization approaches. These techniques directly improve GPU utilization, reduce latency, and enhance computational efficiency in LLM systems. The research provides practical interventions that can be implemented to optimize the tokenization stage of LLM pipelines, which is fundamental to overall system performance.",
      "Irrelevant Aspects": "The paper has a specialized focus on cross-lingual disparities rather than general optimization techniques for LLMs. The linguistic analysis of writing systems and language-specific features, while important for the specific research question, has limited direct relevance to broader LLM optimization challenges. Some approaches like 'superword tokenizers' are specialized solutions that may not have widespread adoption yet in mainstream optimization pipelines.",
      "Summary": "This paper explores and mitigates cross-lingual tokenizer inequities that cause different languages to require varying numbers of tokens for the same content. These 'token premiums' impact training throughput and inference costs. Through training 7,000 tokenizers across 97 languages, the researchers find that vocabulary size and pre-tokenization significantly affect token premiums. They provide methods to optimize tokenization efficiency through determining optimal vocabulary sizes per language and implementing superword tokenizers, which can reduce computational requirements and improve resource utilization in multilingual LLM systems."
    }
  },
  {
    "id": "2VX79YLT9s",
    "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration",
    "abstract": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise to reduce the global communication need. Local SGD consists of three parts: a local optimization processes, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer learning in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to apply to when we use momentum in the outer optimizer, and also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning.  We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.",
    "authors": [
      "~Ahmed_Khaled1",
      "~Satyen_Kale2",
      "~Arthur_Douillard1",
      "~Chi_Jin1",
      "~Rob_Fergus1",
      "~Manzil_Zaheer1"
    ],
    "pdf": "/pdf/0e5076fb8001aa7354dab0c67538d9d2ddc24119.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Distributed training optimization, communication bottleneck reduction in Local SGD, outer optimizer hyperparameter tuning, scalability for large models, learning rate optimization strategies, experimental validation with language models",
      "Irrelevant Aspects": "Inference optimization techniques, GPU utilization specifics, memory optimization strategies, hardware-specific optimizations, model architecture changes, quantization or compression methods",
      "Summary": "This paper is highly relevant to the training optimization aspect of my research interests, particularly for distributed training of large language models. It addresses communication bottlenecks in distributed training, a critical challenge when training LLMs at scale. The focus on outer optimizers in Local SGD provides insights that could improve training efficiency and reduce communication overhead, potentially leading to higher throughput. The paper validates findings with standard language models, making it directly applicable to my work on LLMs. However, it doesn't address inference optimization, GPU utilization specifics, or memory optimization techniques, which are also important aspects of my research interests."
    }
  },
  {
    "id": "PNU2mPauvd",
    "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability",
    "abstract": "Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup $(1.70-2.10\\times)$ with a minimal performance drop $(<2)$\\% as compared to full model performance.",
    "authors": [
      "~Divya_Jyoti_Bajpai1",
      "~Manjesh_Kumar_Hanawal1"
    ],
    "pdf": "/pdf/bfb6a2b676bcd7281c7070b8d1f05e2b91649f3d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization through early exit strategies in deep neural networks, addressing computational efficiency and latency reduction. It proposes an adaptive approach to threshold adjustment for exit decisions, claiming improvements in speedup (1.70-2.10×) with minimal performance drop (<2%). The approach is validated on diverse tasks including text generation, which is relevant to LLMs.",
      "Irrelevant Aspects": "The paper seems more focused on decision-making aspects of early exits rather than the technical details of GPU utilization. It doesn't explicitly mention GPU utilization or scalability aspects that would be central to my research interests. The paper uses a Multi-Armed Bandit framework, which is more of a decision-making approach than a system optimization technique. The focus is on risk control and reliability rather than pure performance optimization.",
      "Summary": "The paper addresses inference optimization through early exit strategies in deep neural networks, which is relevant to my interest in inference optimization. It proposes a method called UAT that adapts exit thresholds using a Multi-Armed Bandit framework, balancing computational efficiency and prediction quality. The paper demonstrates significant speedups (1.70-2.10×) with minimal performance drop, which aligns with goals of improving throughput and reducing latency. However, it doesn't specifically focus on GPU utilization or scalability aspects that are central to my research interests."
    }
  },
  {
    "id": "QuqsEIVWIG",
    "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation",
    "abstract": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines.",
    "authors": [
      "~Sangmin_Bae1",
      "~Yujin_Kim2",
      "~Reza_Bayat1",
      "~Sungnyun_Kim1",
      "~Jiyoun_Ha1",
      "~Tal_Schuster1",
      "~Adam_Fisch2",
      "~Hrayr_Harutyunyan1",
      "~Ziwei_Ji1",
      "~Aaron_Courville3",
      "~Se-Young_Yun1"
    ],
    "pdf": "/pdf/00603a51485cef43d2cf84d3c59cec31d4b8d23d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training optimization through parameter sharing, Inference optimization through selective computation and caching, Memory efficiency improvements, Throughput enhancements, Adaptive computation at the token level, KV sharing variant for reduced memory footprint, Evaluation across model scales",
      "Irrelevant Aspects": "Limited focus on GPU-specific optimizations, Minimal discussion on distributed training/inference, Not much emphasis on specialized kernel implementations",
      "Summary": "Mixture-of-Recursions (MoR) introduces a unified framework that combines parameter sharing and adaptive computation in Recursive Transformers. By reusing layers across recursion steps and dynamically assigning different recursion depths to tokens, MoR reduces computational and memory demands during both training and inference. The approach achieves better performance with higher throughput and smaller model sizes compared to baseline methods, forming a new Pareto frontier across models from 135M to 1.7B parameters."
    }
  },
  {
    "id": "JCTTLKEBza",
    "title": "PolarQuant: Leveraging Polar Transformation for Key Cache Quantization and Decoding Acceleration",
    "abstract": "The increasing demand for long-context generation has made the KV cache in large language models a bottleneck in memory consumption. Quantizing the cache to lower bit widths is an effective way to reduce memory costs; however, previous methods struggle with key cache quantization due to outliers, resulting in suboptimal performance. We propose a novel quantization approach PolarQuant, which provides a new perspective for key cache quantization and efficiently addresses the outlier dilemma. We observe that the distribution of the key states reveals well-structured patterns under polar transformation. Outliers generally appear in only one of the two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied. When represented as two-dimensional vectors, these dimensions exhibit well-organized patterns, with radii and angles smoothly distributed in polar space. This alleviates the channel-wise outliers, making them well-suited for key cache quantization. PolarQuant divides key vectors into groups of two-dimensional sub-vectors, encoding them as the quantized radius and the polar angle, rather than quantizing original key vectors directly. PolarQuant achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models.\nOur code is available at https://github.com/ericshwu/PolarQuant.",
    "authors": [
      "~Songhao_Wu1",
      "~Ang_Lv1",
      "~xiao_feng3",
      "~Yufei_zhang10",
      "~Xun_Zhang4",
      "~Guojun_Yin1",
      "~Wei_Lin7",
      "~Rui_Yan2"
    ],
    "pdf": "/pdf/f871dc73d430806b70e1ed32e0172b4f8e785c4e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization through KV cache quantization, memory optimization for better GPU utilization, decoding acceleration for improved throughput and latency, novel polar transformation approach for handling outliers, table lookup method for computational efficiency",
      "Irrelevant Aspects": "Does not address training optimization, lacks discussion on distributed strategies for scalability across multiple GPUs, no detailed GPU utilization metrics or comprehensive throughput/latency benchmarks, focuses specifically on KV cache without addressing other LLM system components",
      "Summary": "PolarQuant presents a novel quantization approach for LLM key caches using polar transformation, addressing the outlier challenge in existing methods. By transforming key vectors into polar coordinates and quantizing radius and angle separately, it reduces memory consumption while maintaining model performance. The method also accelerates decoding through table lookup for query-key inner products. While focused specifically on inference optimization rather than training, it directly addresses critical aspects of GPU efficiency, memory utilization, and decoding speed - all core to my research interests in LLM systems optimization."
    }
  },
  {
    "id": "z1wIUZtBmK",
    "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation",
    "abstract": "Recent studies suggest that context-aware low-rank approximation is a useful tool for compression and fine-tuning of modern large-scale neural networks.\nIn this type of approximation, a norm is weighted by a matrix of input activations, significantly improving metrics over the unweighted case. \nNevertheless, existing methods for neural networks suffer from numerical instabilities due to their reliance on classical formulas involving explicit Gram matrix computation and their subsequent inversion. \nWe demonstrate that this can degrade the approximation quality or cause numerically singular matrices.\n\nTo address these limitations, we propose a novel _inversion-free regularized framework_ that is based entirely on stable decompositions and overcomes the numerical pitfalls of prior art. \nOur method can handle all possible challenging scenarios: (1)&nbsp;when calibration matrices exceed GPU memory capacity, (2)&nbsp;when input activation matrices are nearly singular, and even (3)&nbsp;when insufficient data prevents unique approximation.\nFor the latter, we prove that our solution converges to a desired approximation and derive explicit error bounds.",
    "authors": [
      "~Uliana_Parkina1",
      "~Maxim_Rakhuba1"
    ],
    "pdf": "/pdf/bf0b06f8993d1fffb06d15a64dd86680ac6258bb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses context-aware low-rank approximation for compression and fine-tuning of large-scale neural networks, which is directly applicable to optimizing LLMs. It proposes solutions for handling GPU memory limitations and numerical instabilities in existing methods. The inversion-free regularized framework can improve model efficiency during training and inference, potentially reducing computational overhead. The focus on handling scenarios with nearly singular matrices and insufficient data makes it robust for real-world applications.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss throughput optimization, latency reduction, or distributed computing across multiple GPUs. It appears more focused on mathematical aspects of approximation rather than practical deployment strategies. There's no specific mention of how the technique affects LLM inference speed or training scalability across nodes. The abstract doesn't directly address parallel computing strategies or efficient batch processing that are crucial for LLM optimization.",
      "Summary": "COALA presents an inversion-free regularized framework for context-aware low-rank approximation to address numerical instabilities in existing methods for neural network compression and fine-tuning. While relevant to LLM optimization through efficient model compression and handling GPU memory constraints, it doesn't directly address throughput, latency, or distributed scalability aspects that are central to LLM systems optimization."
    }
  },
  {
    "id": "Hmepi1Fm2g",
    "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
    "abstract": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized on general-purpose corpora. These tokenizers’ fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a novel method for achieving\ncontext-adaptive tokenization in LLMs at inference time. Leveraging an online data compression algorithm (Lempel–Ziv–Welch), zip2zip dynamically expands its active vocabulary at inference time by continuously replacing fragmented token sequences with more compact hypertokens, which it can immediately output during generation.  In doing so, the model refines its internal tokenization scheme to match\nthe token distribution of the current context, reducing redundancy and improving representational efficiency. zip2zip consists of three key components: (1) a tokenizer based on Lempel–Ziv–Welch compression that incrementally merges co-occurring tokens into reusable hypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that computes embeddings for newly formed hypertokens at runtime; and (3) a variant of autoregressive language modeling that pretrains the model to handle hypertokenized, compressed text sequences as inputs and outputs. We show that an existing LLM can be uptrained for zip2zip in 10 GPU-hours via parameter-efficient finetuning. The resulting LLM performs test-time adaptation, learning to use hypertokens in unseen contexts and reducing input and output tokens by 15–40%.\nCode and models are released at https://github.com/epfl-dlab/zip2zip.",
    "authors": [
      "~Saibo_Geng1",
      "~Nathan_Ranchin1",
      "~Yunzhen_Yao1",
      "~Maxime_Peyrard2",
      "~Chris_Wendler1",
      "~Michael_Gastpar1",
      "~Robert_West1"
    ],
    "pdf": "/pdf/0f10815a6be342f3a8137fe6510088e422bd3a6b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference-time adaptation without requiring full retraining, Parameter-efficient finetuning (10 GPU-hours), Dynamic vocabulary expansion during inference, Token reduction (15-40%), Online compression algorithm applied to tokenization, Runtime computation of embeddings for new hypertokens",
      "Irrelevant Aspects": "Limited focus on GPU implementation details, Minimal discussion of parallelization strategies, Lack of coverage on distributed training/inference techniques, Insufficient depth on memory optimization",
      "Summary": "This paper introduces zip2zip, an inference-time adaptive tokenization method that uses online compression to dynamically expand vocabulary with hypertokens. It reduces token count by 15-40% through context-specific optimization, requiring only 10 GPU-hours of parameter-efficient finetuning. The approach directly impacts GPU utilization, throughput, and latency by processing fewer tokens, making it highly relevant to LLM optimization research."
    }
  },
  {
    "id": "2jwAjomEDB",
    "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
    "abstract": "Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 38% of the KV cache. This KV-cache reduction also leads to a 50% memory saving and a 2x speedup over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.",
    "authors": [
      "~Zefan_Cai1",
      "~Wen_Xiao2",
      "~Hanshi_Sun1",
      "~Cheng_Luo5",
      "~Yikai_Zhang2",
      "~Ke_Wan3",
      "~Yucheng_Li5",
      "~Yeyang_Zhou1",
      "~Li-Wen_Chang1",
      "~Jiuxiang_Gu2",
      "~Zhen_Dong3",
      "~Anima_Anandkumar1",
      "~Abedelkadir_Asi1",
      "~Junjie_Hu2"
    ],
    "pdf": "/pdf/ac69d3edde70382472407199920e54d347f5f466.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models, specifically focusing on KV cache compression which is critical for memory efficiency and throughput. It provides concrete performance improvements including 50% memory savings and 2x speedup in chain-of-thought reasoning inference, directly impacting GPU utilization. The method specifically targets transformer architecture optimization during inference, which is core to my research interest in inference optimization for large language models.",
      "Irrelevant Aspects": "The paper focuses solely on inference optimization and does not address training optimization aspects. It appears limited to reasoning models rather than general LLM optimization techniques. There's no mention of distributed systems optimization or multi-GPU scalability strategies. The approach is specific to KV cache compression without addressing other potential bottlenecks in the inference pipeline.",
      "Summary": "R-KV presents a KV cache compression technique specifically designed for reasoning models with long outputs. It achieves impressive performance by preserving nearly 100% of full KV cache performance while using only 10% of the cache, resulting in significant memory savings and speedup. While highly relevant to inference optimization, it doesn't address training optimization or broader scalability concerns."
    }
  },
  {
    "id": "KYTFXxTJ12",
    "title": "Towards Fully FP8 GEMM LLM Training at Scale",
    "abstract": "Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains.\nWe introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. Additionally, we identify key metrics for monitoring low-precision training and predicting potential future divergences.",
    "authors": [
      "~Alejandro_Hernández-Cano1",
      "~Dhia_Garbaya1",
      "~Imanol_Schlag3",
      "~Martin_Jaggi1"
    ],
    "pdf": "/pdf/d2ce27cb3c31933fde4f429ba5637ab55f006352.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "FP8 data format optimization for LLM pre-training, addressing stability challenges in low-precision training, GEMM optimization for improved throughput, architecture design for FP8 computation in both forward and backward passes, reducing outlier activations for stable training, monitoring metrics for low-precision training, achieving scale without compromising model performance",
      "Irrelevant Aspects": "The abstract doesn't contain significant aspects that are irrelevant to my research focus on LLM training optimization",
      "Summary": "This paper introduces novel LLM architectures enabling fully FP8 GEMM computations throughout transformer blocks during training, addressing previous stability limitations. The approach reduces outlier activations and provides monitoring metrics, achieving significant throughput improvements at scale while maintaining BF16-level model performance. This represents a significant advancement in numerical precision optimization for LLM training, directly impacting GPU utilization and training efficiency."
    }
  },
  {
    "id": "S3GhJooWIC",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space.  Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. \nWe train a proof-of-concept model from scratch with 3.5 billion parameters and 800 billion tokens. We show that this model can effortlessly use varying levels of compute, significantly improving with additional compute especially on reasoning tasks, such as math and coding. Further, this architecture naturally reduces compute costs via zero-shot per-token adaptive compute, KV-cache sharing and speculative decoding.",
    "authors": [
      "~Jonas_Geiping1",
      "~Sean_Michael_McLeish1",
      "~Neel_Jain1",
      "~John_Kirchenbauer1",
      "~Siddharth_Singh7",
      "~Brian_R._Bartoldson1",
      "~Bhavya_Kailkhura1",
      "~Abhinav_Bhatele1",
      "~Tom_Goldstein1"
    ],
    "pdf": "/pdf/14c70cb9442ba3067a5abf112e08a003028eec21.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Test-time computation scaling, implicit reasoning in latent space, recurrent block iteration architecture, zero-shot per-token adaptive compute, KV-cache sharing, speculative decoding, GPU utilization implications, inference optimization techniques, scalability approaches, memory efficiency during inference",
      "Irrelevant Aspects": "Specific focus on reasoning tasks like math and coding, comparison with chain-of-thought approaches, specialized training data requirements, proof-of-concept model specifications (3.5B parameters), token count details (800B tokens)",
      "Summary": "This paper presents a novel language model architecture that scales test-time computation through recurrent blocks that unroll to arbitrary depth, rather than generating more tokens. The approach is highly relevant to inference optimization as it introduces techniques like zero-shot adaptive compute, KV-cache sharing, and speculative decoding to reduce costs. These methods directly impact GPU utilization, throughput, and latency - key areas in my research. While the paper includes some application-specific details about reasoning tasks, its core contributions about efficient inference architecture make it valuable for understanding new approaches to scaling LLM computation efficiently."
    }
  },
  {
    "id": "boNYskaXnO",
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache.\nVector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce $\\textbf{NSNQuant}$, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. \nBy applying a three-step transformation—$\\textbf{1)}$ a token-wise normalization ($\\textbf{N}$ormalize), $\\textbf{2)}$ a channel-wise centering ($\\textbf{S}$hift), and $\\textbf{3)}$ a second token-wise normalization ($\\textbf{N}$ormalize)—with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. \nThis alignment enables robust, calibration-free vector quantization using a single reusable codebook.\nExtensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gains over full-precision baselines.",
    "authors": [
      "~Donghyun_Son3",
      "~Euntae_Choi1",
      "~Sungjoo_Yoo1"
    ],
    "pdf": "/pdf/ee69cf99788d4c221668ea3edddbc059ac8203d3.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization by reducing KV cache memory usage, which is crucial for better GPU utilization. It focuses on low-bit quantization (1-bit and 2-bit) techniques, a cutting-edge approach for inference acceleration. The method reportedly provides up to 3× throughput gains over full-precision baselines. It also addresses scalability concerns for processing large batch sizes and long sequences, which are key challenges in deploying LLMs efficiently.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization. It lacks detailed analysis of latency improvements, only mentioning throughput gains. The approach is a specific technique rather than a comprehensive framework for inference optimization. It doesn't appear to discuss hardware-specific optimizations or performance across different GPU architectures.",
      "Summary": "NSNQuant introduces a calibration-free vector quantization technique for compressing the KV cache in LLMs during inference. The method uses a three-step transformation (Normalize, Shift, Normalize) with Hadamard transform to align token distributions with standard normal distribution, enabling robust quantization without requiring calibration datasets. The technique reportedly outperforms existing methods in both 1-bit and 2-bit settings, providing strong generalization and up to 3× throughput gains over full-precision baselines."
    }
  },
  {
    "id": "M8zmlixh9y",
    "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining",
    "abstract": "Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are \"in-batch\" examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose *Breaking the Batch Barrier* (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4–16× smaller than that required by other methods. Moreover, experiments show that B3 generalizes well across domains and tasks, maintaining strong performance even when trained with considerably weaker teachers.",
    "authors": [
      "~Raghuveer_Thirukovalluru1",
      "~Rui_Meng1",
      "~Ye_Liu4",
      "~Karthikeyan_K1",
      "~Mingyi_Su1",
      "~Ping_Nie1",
      "~Semih_Yavuz1",
      "~Yingbo_Zhou1",
      "~Wenhu_Chen3",
      "~Bhuwan_Dhingra1"
    ],
    "pdf": "/pdf/d6234b19645ddc5e0f1f1ec9f13d0c8c44f1e8d7.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization through smart batch mining, GPU utilization improvement by enabling smaller batch sizes (4-16× reduction), training scalability enhancement, efficient use of computational resources, model scale considerations (7B and 2B models), and potential throughput improvements during training",
      "Irrelevant Aspects": "No focus on inference optimization or latency reduction, specialized for contrastive learning rather than general LLM training, multimodal focus may limit applicability to text-only models, lacks specific GPU-level optimization techniques or kernel improvements",
      "Summary": "B3 introduces a batch construction strategy for contrastive learning that enables high-quality training with significantly smaller batch sizes (as small as 64), improving GPU utilization and training scalability. While valuable for training optimization and potentially throughput, it doesn't address inference optimization or latency reduction, limiting its comprehensive relevance to the specified research interests."
    }
  },
  {
    "id": "JbJVWljk7r",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
    "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new $\\texttt{FP4}$ Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves $\\textbf{1038}$ $\\texttt{TOPS}$ on $\\texttt{RTX5090}$, which is a $\\textbf{5}\\times$ speedup over the fastest FlashAttention on $\\texttt{RTX5090}$. Experiments show that our $\\texttt{FP4}$ attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient $\\texttt{8-bit}$ attention for both forward and backward propagation. Experiments indicate that $\\texttt{8-bit}$ attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention.",
    "authors": [
      "~Jintao_Zhang2",
      "~Jia_wei6",
      "~Haoxu_Wang1",
      "~Pengle_Zhang1",
      "~Xiaoming_Xu3",
      "~Haofeng_Huang3",
      "~Kai_Jiang9",
      "~Jun_Zhu2",
      "~Jianfei_Chen1"
    ],
    "pdf": "/pdf/6781aa0caeaa24f549d8b48d6e740ea7b0680baf.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses both inference and training optimization of attention mechanisms, which are critical in LLMs. It achieves 5× speedup over FlashAttention using FP4 Tensor Cores on Blackwell GPUs, significantly improving GPU utilization and throughput. The dual focus on inference (FP4) and training (8-bit) optimization makes it highly relevant. It explores low-bit computations which are crucial for efficient LLM systems.",
      "Irrelevant Aspects": "The paper doesn't specifically address latency improvements, though throughput improvements often correlate with latency benefits. There's no mention of distributed systems for very large-scale deployment. The 8-bit training optimization is still exploratory with some limitations.",
      "Summary": "SageAttention3 presents a highly relevant optimization technique for attention mechanisms in LLMs, using FP4 for inference (5× speedup) and exploring 8-bit for training. It directly targets GPU utilization and throughput improvements through low-bit computations and hardware-specific optimizations, aligning strongly with my research interests in LLM training and inference optimization."
    }
  },
  {
    "id": "X4SCxcgb3O",
    "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
    "abstract": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
    "authors": [
      "~Zachary_Charles1",
      "~Gabriel_Teston1",
      "~Lucio_M._Dery1",
      "~J_Keith_Rush1",
      "~Nova_Fallen1",
      "~Zachary_Garrett1",
      "~Arthur_Szlam3",
      "~Arthur_Douillard1"
    ],
    "pdf": "/pdf/6c0133c70ac56661096c7b4c03807bee67175bcf.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses communication bottlenecks in large language model training, which directly impacts GPU utilization and scalability. It analyzes DiLoCo, a method that reduces synchronization demands in distributed training, potentially improving throughput. The study of scaling laws for LLMs under compute constraints, examination of optimal batch sizes, and performance comparisons with data-parallel training are highly relevant to optimizing training systems. The focus on how algorithmic factors affect training efficiency and the claim that DiLoCo scales better than conventional approaches with model size align with expertise in training optimization and scalability.",
      "Irrelevant Aspects": "The paper appears to focus primarily on training optimization rather than inference optimization. There's limited discussion of specific GPU utilization techniques at the hardware level. The research doesn't seem to address latency concerns beyond synchronization issues. The paper may not cover memory optimization strategies or specific hardware implementation details that would be relevant to comprehensive system optimization.",
      "Summary": "This paper investigates DiLoCo, a communication-efficient approach for training large language models that reduces synchronization overhead. The authors analyze scaling laws for DiLoCo when training LLMs under fixed compute budgets, finding that it scales predictably and robustly with model size. When properly tuned, DiLoCo can outperform traditional data-parallel training even at smaller model sizes, offering benefits including increased optimal batch sizes and better generalization. The research provides insights into how factors like model replicas and token budget affect training efficiency, offering a path to more scalable LLM training systems."
    }
  },
  {
    "id": "J4w4RtwLyB",
    "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
    "abstract": "Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model’s ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. \nWe introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, ''target'' segments selectively attend only to the KV-caches of their designated ''source'' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. \nWe evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network.\nBy effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings.",
    "authors": [
      "~Haoyu_Peter_Wang1",
      "~Peihao_Wang1",
      "~Mufei_Li2",
      "~Shikun_Liu3",
      "~Siqi_Miao1",
      "~Zhangyang_Wang1",
      "~Pan_Li2"
    ],
    "pdf": "/pdf/baee695648d2c8263751d5b496fc8dce1e50d3ab.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through sparse attention patterns, improving GPU utilization and scalability. It introduces a KV-cache optimization that reduces computational complexity from O(n²) to selective attention based on graph structure. This approach could increase throughput and reduce latency, particularly for tasks with inherent structural dependencies. The method also optimizes context window usage and reduces positional bias, all of which are critical for efficient LLM deployment.",
      "Irrelevant Aspects": "The paper primarily focuses on inference optimization rather than training optimization, which is only part of my research interest. The evaluation on specific benchmarks like RAG and Arxiv-QA, while necessary to validate the approach, is less central to my systems-level optimization focus. The paper appears to emphasize architectural changes over detailed system implementation aspects.",
      "Summary": "Graph-KV introduces an inference optimization technique that breaks from standard sequential processing by implementing sparse attention patterns based on structural dependencies. It leverages KV-caches of text segments as condensed representations and enables selective attention between segments rather than full sequential attention. This graph-structured approach sparsifies attention, reduces computational complexity, and improves context window utilization. The method shows significant performance improvements across various benchmarks involving structured data and reasoning tasks, making it a valuable contribution to LLM inference optimization research."
    }
  },
  {
    "id": "0T8i3uXq3O",
    "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
    "abstract": "In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are cached for long periods. We introduce an alternate bounded update combining a historical eigenbasis with instantaneous normalization, resulting in across-the-board stability and significantly lower computational requirements. Second, we adapt a shape-aware scaling to enable learning rate transfer across network width. Third, we find that high learning rates result in large parameter noise, and propose a simple iterate-averaging scheme which unblocks faster learning. To properly confirm these findings, we introduce a pointed Transformer training benchmark, considering three objectives (language modelling, image classification, and diffusion modelling) across different stages of training. On average, SPlus is able to reach the validation performance of Adam within 44-58% of the gradient steps and 62-83% of the wallclock time.",
    "authors": [
      "~Kevin_Frans1",
      "~Sergey_Levine1",
      "~Pieter_Abbeel2"
    ],
    "pdf": "/pdf/30b99b0c687d52d55ac60c852edf8e596d9f64d0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on neural network optimization with proposed SPlus method improving Shampoo optimizer; addresses stability and computational efficiency for training; claims faster convergence (44-58% of gradient steps and 62-83% of wallclock time compared to Adam); includes Transformer training benchmark across different objectives; solves matrix-inverse caching stability issues; proposes iterate-averaging scheme enabling faster learning",
      "Irrelevant Aspects": "Does not address inference optimization; lacks discussion of memory optimization techniques; no coverage of distributed training strategies or multi-GPU scalability; no mention of quantization or inference-specific techniques; includes non-LLM applications (image classification, diffusion modeling)",
      "Summary": "The paper presents SPlus, an improved optimization algorithm based on Shampoo that enhances stability and computational efficiency for neural network training. While highly relevant to training optimization aspects of large language models with its focus on faster convergence and stability, it doesn't cover inference optimization or distributed training scalability, limiting its complete alignment with my expertise."
    }
  },
  {
    "id": "2Gnp8sdwVe",
    "title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs",
    "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation.\nTo bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., \\Chinchilla's law). \nOur methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, outperforming Chinchilla's law, whose extrapolation error is 433\\% higher. \nThis allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. \nFurthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. To foster further research, we are comprehensively open-sourcing all code, data, results (https://github.com/Farseer-Scaling-Law/Farseer), all training logs (https://wandb.ai/billzid/Farseer?nw=nwuserbillzid), all models used in scaling law fitting (https://huggingface.co/Farseer-Scaling-Law).",
    "authors": [
      "~Houyi_Li1",
      "~Wenzhen_Zheng1",
      "~Qiufeng_Wang3",
      "~Zhenyu_Ding1",
      "~Haoying_Wang1",
      "~Zili_Wang1",
      "~Shijie_Xuyang1",
      "~Ning_Ding6",
      "~Shuigeng_Zhou1",
      "~Xiangyu_Zhang1",
      "~Daxin_Jiang2"
    ],
    "pdf": "/pdf/b2d2dae1fc87cdd4510e8c2672fcf585d1c653f2.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces Farseer, a refined scaling law for LLMs that improves predictive accuracy across scales, which is directly relevant to training optimization. The research addresses the critical scaling gap between small-scale experiments and production systems, enabling more efficient resource utilization. With 3 million NVIDIA H100 GPU hours spent on training approximately 1,000 LLMs across diverse configurations, this work provides substantial empirical data on scaling behavior that can inform better GPU utilization strategies. The improved compute allocation insights could enhance training efficiency and resource planning in large ML systems.",
      "Irrelevant Aspects": "The paper focuses primarily on scaling laws rather than specific training/inference implementation techniques. There's minimal discussion of latency reduction strategies or throughput optimization methods. The work appears to be more theoretical and predictive rather than offering concrete implementation improvements for GPU utilization. The paper does not seem to address inference optimization directly, which is one of my core research interests.",
      "Summary": "Farseer presents a refined scaling law for LLMs that offers better predictive accuracy across different scales compared to previous approaches like Chinchilla's law. By creating a model loss surface, it enables more accurate extrapolation from small-scale experiments to large production systems. The paper validates its approach with extensive empirical data from training 1,000 models using 3 million H100 GPU hours. While primarily focused on predictive scaling laws rather than specific optimization techniques, this work has significant implications for resource planning and efficient GPU utilization in training large language models."
    }
  },
  {
    "id": "ktNJgpmjjP",
    "title": "AutoJudge: Judge Decoding Without Manual Annotation",
    "abstract": "We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. \nInstead of matching the original model output distribution token-by-token, we identify the generated tokens that affect the downstream quality of the response, relaxing the distribution match guarantee so that the \"unimportant\" tokens can be generated faster.\nOur approach relies on a semi‑greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped.\nWe then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality.\nWe evaluate AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. \nNotably, on GSM8K with the Llama 3.1 70B target model, our approach achieves up to ${\\approx}2{\\times}$ speedup \\textit{over speculative decoding} at the cost of a ${\\le} 1\\%$ drop in accuracy.\nWhen applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting ${\\ge}25$ tokens per speculation cycle at a$~ 2\\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.",
    "authors": [
      "~Roman_Garipov1",
      "~Fedor_Velikonivtsev1",
      "~Ivan_Ermakov1",
      "~Ruslan_Svirschevski1",
      "~Vage_Egiazarian1",
      "~Max_Ryabinin1"
    ],
    "pdf": "/pdf/808eba07561d6583ed7fcabb5891309dd6f38bd2.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through speculative decoding techniques, which is central to my research focus. It introduces a method to accelerate LLM inference by identifying important versus unimportant tokens, allowing faster generation while maintaining quality. The approach achieves significant speedups (up to 2x) with minimal accuracy reduction, directly impacting GPU utilization, throughput, and latency - all key metrics in my research. The method's compatibility with modern LLM inference frameworks and its task-specific optimization approach are particularly valuable aspects. The paper's quantitative results on mathematical reasoning and programming benchmarks provide concrete evidence of performance improvements.",
      "Irrelevant Aspects": "The paper focuses specifically on speculative decoding optimization rather than exploring broader inference optimization techniques. It concentrates on task-specific applications rather than general-purpose acceleration methods. The research doesn't address training optimization, which is another aspect of my expertise. The evaluation is limited to specific benchmarks (GSM8K, LiveCodeBench) rather than comprehensive across diverse tasks. The approach requires training a lightweight classifier, which adds computational overhead not discussed in detail.",
      "Summary": "AutoJudge introduces a task-specific lossy speculative decoding method that accelerates LLM inference by identifying and selectively processing tokens based on their importance to downstream quality. The approach uses a semi-greedy search algorithm to determine which mismatches between target and draft models require correction, then trains a lightweight classifier to predict these at inference time. The method achieves up to 2x speedup over standard speculative decoding with minimal accuracy reduction (≤1% on GSM8K), and can automatically accept ≥25 tokens per speculation cycle with only 2% accuracy drop on programming benchmarks. The technique requires no human annotation and integrates easily with existing inference frameworks, making it a practical contribution to LLM inference optimization."
    }
  },
  {
    "id": "z1Cvcovlms",
    "title": "Sim-LLM: Optimizing LLM Inference at the Edge through Inter-Task KV Reuse",
    "abstract": "KV cache technology, by storing key-value pairs, helps reduce the computational overhead incurred by *large language models* (LLMs). It facilitates their deployment on resource-constrained edge computing nodes like edge servers. However, as the complexity and size of tasks increase, KV cache usage leads to substantial GPU memory consumption. Existing research has focused on mitigating KV cache memory usage through sequence length reduction, task-specific compression, and dynamic eviction policies. However, these methods are computationally expensive for resource-constrained edge computing nodes. To tackle this challenge, this paper presents Sim-LLM, a novel inference optimization mechanism that leverages task similarity to reduce KV cache memory consumption for LLMs. By caching KVs from processed tasks and reusing them for subsequent similar tasks during inference, Sim-LLM significantly reduces memory consumption while boosting system throughput and increasing maximum batch size, all with minimal accuracy degradation. Evaluated on both A40 and A100 GPUs, Sim-LLM achieves a system throughput improvement of up to 39.40\\% and a memory reduction of up to 34.65%, compared to state-of-the-art approaches. Our source code is available at https://github.com/CGCL-codes/SimLLM.",
    "authors": [
      "~Ruikun_Luo1",
      "~Changwei_Gu1",
      "~Qiang_He2",
      "~Feifei_Chen1",
      "~Song_Wu5",
      "~Hai_Jin1",
      "~Yun_Yang1"
    ],
    "pdf": "/pdf/e8355810346fa0731f27685d0e9743b63680e658.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on LLM inference optimization, addresses GPU memory consumption, claims to improve system throughput by up to 39.40%, reduces memory consumption by up to 34.65%, increases maximum batch size for better scalability, proposes a novel KV reuse mechanism, evaluates on A40 and A100 GPUs which are common in LLM workloads",
      "Irrelevant Aspects": "Specific focus on edge computing rather than general datacenter deployments, doesn't address training optimization only inference, approach is limited to KV cache optimization rather than comprehensive inference optimization, minimal discussion on latency improvements",
      "Summary": "Sim-LLM presents a novel inference optimization mechanism that leverages task similarity to reduce KV cache memory consumption for LLMs. The paper demonstrates significant improvements in throughput (up to 39.40%) and memory reduction (up to 34.65%) on A40 and A100 GPUs. While focused on edge computing, the techniques presented are relevant to general LLM inference optimization, particularly for GPU utilization and scalability, though it doesn't cover training optimization or comprehensive latency improvements."
    }
  },
  {
    "id": "W9Y0jtf45v",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling",
    "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving all progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints. Code and dataset are available at the [LIMOPro GitHub repository.](https://github.com/GAIR-NLP/LIMOPro)",
    "authors": [
      "~Yang_Xiao6",
      "~Jessie_Wang1",
      "~Ruifeng_Yuan1",
      "~Chunpu_Xu2",
      "~Kaishuai_Xu1",
      "~Wenjie_Li1",
      "~Pengfei_Liu1"
    ],
    "pdf": "/pdf/0c3ecaba9b2c2e94d46866f4f888a2d4ba4787dd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization and test-time scaling for LLMs, which aligns perfectly with research on reducing latency and increasing throughput. It introduces a method (PIR) that optimizes training data to improve inference efficiency, connecting training optimization with inference performance. The approach reduces token usage by 3-41%, which directly impacts GPU utilization and throughput. It focuses on computational efficiency where response time is valuable, demonstrates results across different model sizes indicating work on scalability, and produces more concise reasoning chains to reduce computational burden during inference.",
      "Irrelevant Aspects": "The specific focus on mathematical reasoning benchmarks (AIME, AMC, GPQA Diamond) is somewhat domain-specific, though the principles could apply more broadly. The paper emphasizes reasoning capabilities more than general optimization techniques, though there's significant overlap.",
      "Summary": "LIMOPro presents PIR, a framework that optimizes reasoning chains in LLM training data by identifying and pruning low-importance steps while preserving essential solution paths. This approach improves inference efficiency by generating more concise reasoning chains, resulting in better accuracy (+0.9% to +6.6%) with significantly reduced token usage (-3% to -41%) across reasoning benchmarks. The work demonstrates how training data optimization can lead to better test-time scaling properties, addressing both training and inference optimization with clear benefits for computational efficiency, throughput, and response time."
    }
  },
  {
    "id": "NligLHO7yG",
    "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm.  In this paper, we propose **HybridNorm**, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.",
    "authors": [
      "~Zhijian_Zhuo1",
      "~Yutao_Zeng1",
      "~Ya_Wang2",
      "~Sijun_Zhang1",
      "~Xiaoqing_Li4",
      "~Jian_Yang10",
      "~zhou_Xun2",
      "~Jinwen_Ma1"
    ],
    "pdf": "/pdf/393bca38af6b4a1b851f9fbf476c1171a6251f89.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses transformer training stability and efficiency through a novel normalization approach, which directly relates to training optimization for large language models. The focus on deep transformer networks and large-scale models aligns well with LLM training optimization. The proposed HybridNorm method improves gradient flow and model robustness, which can enhance training efficiency and potentially GPU utilization. The experimental validation on both dense and sparse transformer variants demonstrates broad applicability to different model architectures.",
      "Irrelevant Aspects": "The paper appears to focus primarily on training optimization rather than inference optimization. There is no explicit discussion of latency, throughput, or specific GPU utilization metrics in the abstract. The method's impact on inference performance is not clearly addressed.",
      "Summary": "HybridNorm introduces a hybrid normalization strategy that combines Pre-Norm and Post-Norm approaches by using QKV normalization within attention mechanisms and Post-Norm in feed-forward networks. This approach aims to improve training stability and efficiency for deep transformer models. The method is validated on large-scale transformer models across multiple benchmarks, showing consistent improvements over existing normalization approaches. While highly relevant to training optimization for LLMs, the paper doesn't appear to address inference optimization aspects in detail."
    }
  },
  {
    "id": "uX4dyc7Z5Z",
    "title": "Group-Level Data Selection for Efficient Pretraining",
    "abstract": "The efficiency and quality of language model pretraining are largely determined by the way pretraining data are selected. In this paper, we introduce *Group-MATES*, an efficient group-level data selection approach to optimize the speed-quality frontier of language model pretraining. Specifically, Group-MATES parameterizes costly group-level selection with a relational data influence model. To train this model, we sample training trajectories of the language model and collect oracle data influences alongside. The relational data influence model approximates the oracle data influence by weighting individual influence with relationships among training data. To enable efficient selection with our relational data influence model, we partition the dataset into small clusters using relationship weights and select data within each cluster independently. Experiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves 3.5\\%-9.4\\% relative performance gains over random selection across 22 downstream tasks, nearly doubling the improvements achieved by state-of-the-art individual data selection baselines. Furthermore, Group-MATES reduces the number of tokens required to reach a certain downstream performance by up to 1.75x, substantially elevating the speed-quality frontier. Further analyses highlight the critical role of relationship weights in the relational data influence model and the effectiveness of our cluster-based inference. Our code is open-sourced at https://github.com/facebookresearch/Group-MATES.",
    "authors": [
      "~Zichun_Yu1",
      "~Fei_Peng5",
      "~Jie_Lei3",
      "~Arnold_Overwijk1",
      "~Wen-tau_Yih1",
      "~Chenyan_Xiong1"
    ],
    "pdf": "/pdf/402496f1e4c1a0cddc82fae91b7f1f998b7c83c6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on optimizing the efficiency and quality of language model pretraining through data selection, which relates to training optimization. Claims to reduce token requirements by up to 1.75x, improving training efficiency. Partitions datasets into clusters for independent data selection, which could improve computational efficiency and scalability.",
      "Irrelevant Aspects": "Does not directly address GPU utilization, which is a key aspect of the research interest. Focuses on pretraining data selection rather than inference optimization. Does not appear to address latency concerns. The method of sampling training trajectories and collecting oracle data influences might add computational overhead.",
      "Summary": "The paper presents Group-MATES, a group-level data selection approach for language model pretraining that aims to optimize the speed-quality frontier. It uses a relational data influence model and clustering to select data more effectively, claiming performance improvements over random selection and reduced token requirements. While relevant to training optimization and efficiency, it doesn't directly focus on GPU utilization, inference optimization, or latency concerns."
    }
  },
  {
    "id": "o9iReV4FGm",
    "title": "Fast attention mechanisms: a tale of parallelism",
    "abstract": "Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the expressive power previously established for standard attention in terms of matching the capabilities of MPC algorithms, and (2) can solve key reasoning tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC framework, we further prove that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers, thereby providing a unified way to reason about a broad class of efficient attention approximations.",
    "authors": [
      "~Jingwen_Liu1",
      "~Hantao_Yu1",
      "~Clayton_Sanford1",
      "~Alexandr_Andoni1",
      "~Daniel_Hsu1"
    ],
    "pdf": "/pdf/137d718710777d0ff3de0dd20ab299216adc53db.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses the quadratic time complexity of transformers, introduces an efficient attention mechanism with sub-quadratic complexity, focuses on parallelism, and maintains expressive power while optimizing efficiency - all directly relevant to LLM training and inference optimization.",
      "Irrelevant Aspects": "The paper seems to focus more on theoretical aspects rather than practical implementation details, doesn't appear to include empirical GPU utilization or latency measurements, doesn't explicitly address memory optimization, and doesn't seem to cover distributed training aspects.",
      "Summary": "This paper introduces ANNA, an efficient attention mechanism with sub-quadratic complexity for transformers, addressing a key scalability challenge for LLMs. While theoretically valuable and directly relevant to improving transformer efficiency, the paper appears to focus more on theoretical aspects rather than practical implementation details and empirical performance measurements."
    }
  },
  {
    "id": "VBKgukQlRG",
    "title": "Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling",
    "abstract": "Multi-bit quantization networks enable flexible deployment of deep neural networks by supporting multiple precision levels within a single model. However, existing approaches suffer from significant training overhead as full-dataset updates are repeated for each supported bit-width, resulting in a cost that scales linearly with the number of precisions. Additionally, extra fine-tuning stages are often required to support additional or intermediate precision options, further compounding the overall training burden. To address this issue, we propose two techniques that greatly reduce the training overhead without compromising model utility: (i) Weight bias correction enables shared batch normalization and eliminates the need for fine-tuning by neutralizing quantization-induced bias across bit-widths and aligning activation distributions; and (ii) Bit-wise coreset sampling strategy allows each child model to train on a compact, informative subset selected via gradient-based importance scores by exploiting the implicit knowledge transfer phenomenon. Experiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and ViT architectures demonstrate that our method achieves competitive or superior accuracy while reducing training time up to 7.88×.",
    "authors": [
      "~Jinhee_Kim2",
      "~Jae_Jun_An1",
      "~Kang_Eun_Jeon1",
      "~Jong_Hwan_Ko2"
    ],
    "pdf": "/pdf/907386da52cb249a8a89bbf52d67f7fa00776703.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Multi-bit quantization techniques for flexible model deployment, Weight bias correction methodology, Coreset sampling for efficient training on informative subsets, Significant reduction in training time (up to 7.88×), Training optimization methodologies, Potential for improved GPU utilization through quantization",
      "Irrelevant Aspects": "Focus on computer vision tasks and datasets rather than NLP/LLMs, Testing primarily on ResNet and ViT architectures rather than language models, Limited discussion of inference-specific optimizations (latency, throughput), No specific mention of LLM-scale model challenges",
      "Summary": "This paper presents efficient multi-bit quantization techniques for neural network training, introducing weight bias correction and bit-wise coreset sampling to reduce training overhead while supporting multiple precision levels. The methods achieve significant training time reductions (up to 7.88×) while maintaining competitive accuracy on vision tasks with ResNet and ViT architectures. While focused on computer vision applications, the quantization and training efficiency techniques could potentially be adapted to LLM optimization contexts."
    }
  },
  {
    "id": "8z3cOVER4z",
    "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
    "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly important. However, scaling LLMs to longer contexts incurs slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper presents RetrievalAttention, a training-free approach to both accelerate the decoding phase and reduce GPU memory consumption by pre-building KV vector indexes for fixed contexts and maintaining them in CPU memory for efficient retrieval. Unlike conventional KV cache methods, RetrievalAttention integrate approximate nearest neighbor search (ANNS) indexes into attention computation. We observe that off-the-shelf ANNS techniques often fail due to the out-of-distribution (OOD) nature of query and key vectors in attention mechanisms. RetrievalAttention overcomes this with an attention-aware vector index. Our evaluation shows RetrievalAttention achieves near full attention accuracy while accessing only 1-3\\% of the data, significantly reducing inference costs. Remarkably, RetrievalAttention enables LLMs with 8B parameters to handle 128K tokens on a single NVIDIA RTX4090 (24GB), achieving a decoding speed of 0.107 seconds per token.",
    "authors": [
      "~Di_Liu7",
      "~Meng_Chen6",
      "~Baotong_Lu1",
      "~Huiqiang_Jiang2",
      "~Zhenhua_Han1",
      "~Qianxi_Zhang1",
      "~Qi_Chen2",
      "~Chengruidong_Zhang1",
      "~Bailu_Ding1",
      "~Kai_Zhang25",
      "~Chen_Chen63",
      "~Fan_Yang28",
      "~Yuqing_Yang1",
      "~Lili_Qiu3"
    ],
    "pdf": "/pdf/07fc974752fbdfffc98d100f757b7d299a810126.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization by proposing RetrievalAttention to accelerate decoding and reduce GPU memory consumption. It focuses on improving GPU utilization by maintaining KV vectors in CPU memory and using vector retrieval to access only 1-3% of data. The approach enables 8B parameter models to handle 128K tokens on a single RTX4090, significantly improving resource utilization. It addresses key challenges in long-context LLM deployment including memory constraints and inference speed, demonstrating substantial throughput improvements (0.107 seconds per token).",
      "Irrelevant Aspects": "The paper focuses solely on inference optimization and doesn't address training optimization aspects. It doesn't appear to discuss distributed inference across multiple GPUs, concentrating on single-GPU scenarios. The approach might have limitations that aren't fully explored in the abstract, particularly regarding accuracy trade-offs in different application domains.",
      "Summary": "RetrievalAttention presents a training-free method to accelerate LLM inference with long contexts by integrating vector retrieval into attention computation. It addresses critical challenges in GPU memory utilization and decoding speed by maintaining KV vector indexes in CPU memory and using attention-aware vector indexing to efficiently retrieve relevant data. The approach enables running 8B parameter models with 128K token context on a single consumer GPU (RTX4090) while maintaining high accuracy, representing a significant advance in LLM inference optimization and resource efficiency."
    }
  },
  {
    "id": "jOsuKwiCL0",
    "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
    "abstract": "In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of ``step,'' which is difficult to define for long-context reasoning models.\nBy collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (\\texttt{VGS}) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-$n$.\nMoreover, \\texttt{VGS} significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced at \\codeurl.",
    "authors": [
      "~Kaiwen_Wang1",
      "~Jin_Peng_Zhou1",
      "~Jonathan_Daniel_Chang1",
      "~Zhaolin_Gao1",
      "~Nathan_Kallus1",
      "~Kianté_Brantley2",
      "~Wen_Sun1"
    ],
    "pdf": "/pdf/c86c1e84e64f2b6fbcb7805989fee4d841d252bf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper proposes a method for value model training that significantly reduces inference FLOPs required to achieve the same performance as majority voting, which directly relates to inference optimization. It discusses test-time compute scaling, block-wise value-guided search, and compares with standard methods in terms of efficiency, all of which are relevant to computational efficiency and inference optimization for large language models.",
      "Irrelevant Aspects": "The focus on long-context reasoning traces and the collection of 2.5 million reasoning traces is more about the application domain and data acquisition than system optimization. The comparison with process reward models (PRMs) is primarily about model architectures rather than system efficiency. The paper doesn't specifically address GPU utilization, scalability, or latency measurements in detail.",
      "Summary": "This paper presents a value-guided search method that improves inference efficiency by reducing FLOPs needed for chain-of-thought reasoning. While it has relevant aspects for inference optimization, its primary focus is on reasoning methodology rather than system-level optimizations for GPU utilization, scalability, or latency, resulting in moderate relevance to my research interests."
    }
  },
  {
    "id": "Xeb2EYBKkr",
    "title": "Two Heads are Better than One: Simulating Large Transformers with Small Ones",
    "abstract": "The quadratic complexity of self‑attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences?\n\nIn this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice.",
    "authors": [
      "~Hantao_Yu1",
      "~Josh_Alman1"
    ],
    "pdf": "/pdf/502b801592db1a1218e7b18c685370c9556ad136.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses the quadratic complexity of self-attention, which is a fundamental bottleneck in transformer models, especially for long sequences. It proposes a method to simulate large transformers with multiple small transformers, which could potentially improve GPU utilization since GPUs are optimized for processing smaller sequences. The approach could enhance scalability for handling long input sequences without requiring hardware changes. The method has potential implications for both training and inference efficiency. The paper discusses theoretical bounds on computational complexity, which is relevant to optimization. It considers practical scenarios like sliding window masking and attention sinks, which are common in real-world applications. The approach could lead to better throughput and potentially lower latency for long sequence processing.",
      "Irrelevant Aspects": "The paper appears to be primarily theoretical, focusing on proofs and bounds rather than practical implementation details. There's no explicit mention of GPU utilization metrics, memory optimization techniques, or parallelization strategies that would be central to my interest in hardware optimization. The paper doesn't appear to discuss quantization, pruning, or other model compression techniques relevant to optimization. There's no indication of discussing distributed training or serving infrastructure. The paper doesn't seem to address specialized hardware accelerators beyond a brief mention. It doesn't discuss batch processing optimization, which is crucial for throughput. There's no mention of specific inference serving systems or techniques to reduce latency.",
      "Summary": "This paper proposes simulating large transformers with multiple small transformers to address the quadratic complexity of self-attention in long sequences. While it presents theoretical bounds and considers practical scenarios, it's primarily theoretical and lacks implementation details. The approach has potential relevance to GPU utilization, scalability, throughput, and latency optimization, but the paper doesn't focus on these aspects directly. The concept of using multiple smaller models instead of one large one could be relevant to my research interests in optimizing LLM systems, but the theoretical nature of the paper limits its immediate applicability."
    }
  },
  {
    "id": "uvyr9bYwL6",
    "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings",
    "abstract": "Large Reasoning Models (LRMs) achieve superior performance by extending the thought length. However, a lengthy thinking trajectory leads to reduced efficiency. Most of the existing methods are stuck in the assumption of overthinking and attempt to reason efficiently by compressing the Chain-of-Thought, but this often leads to performance degradation. To address this problem, we introduce A*-Thought, an efficient tree search-based unified framework designed to identify and isolate the most essential thoughts from the extensive reasoning chains produced by these models. It formulates the reasoning process of LRMs as a search tree, where each node represents a reasoning span in the giant reasoning space. By combining the A* search algorithm with a cost function specific to the reasoning path, it can efficiently compress the chain of thought and determine a reasoning path with high information density and low cost. In addition, we also propose a bidirectional importance estimation mechanism, which further refines this search process and enhances its efficiency beyond uniform sampling. Extensive experiments on several advanced math tasks show that A*-Thought effectively balances performance and efficiency over a huge search space. Specifically, A*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with low-budget and reduce the length of the output token by nearly 50\\% with high-budget. The proposed method is also compatible with several other LRMs, demonstrating its generalization capability. The code can be accessed at: https://github.com/AI9Stars/AStar-Thought.",
    "authors": [
      "~Xiaoang_Xu1",
      "~Shuo_Wang13",
      "~Xu_Han2",
      "~Zhenghao_Liu2",
      "~Huijia_Wu1",
      "~Pei_Pei_Li2",
      "~Zhiyuan_Liu1",
      "~Maosong_Sun1",
      "~Zhaofeng_He1"
    ],
    "pdf": "/pdf/38488a17c621dbf0003d6f92b1c2a48bce677a54.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on inference efficiency for LLMs, compresses reasoning chains to reduce computational overhead, addresses low-resource settings, reduces output token length by nearly 50% (impacting throughput and latency), improves performance with low-budget (relates to GPU utilization), compatible with multiple LRMs (suggesting scalability)",
      "Irrelevant Aspects": "Focuses more on algorithmic aspects than system-level optimizations, doesn't address training optimization, no explicit mention of GPU utilization or hardware-specific optimizations, no discussion on parallelization strategies or distributed computing, limited discussion on scaling with model size or hardware resources",
      "Summary": "A*-Thought is a tree search-based framework for identifying essential thoughts from extensive reasoning chains in Large Reasoning Models. It uses A* search algorithm with a cost function and bidirectional importance estimation to compress reasoning chains while maintaining performance. The method improves performance of QwQ-32B by 2.39× with low-budget and reduces output token length by nearly 50% with high-budget. While highly relevant to inference optimization for LLMs, it focuses more on algorithmic efficiency than system-level or hardware-specific optimizations."
    }
  },
  {
    "id": "AwsiYZ2ets",
    "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models",
    "abstract": "Layer pruning has emerged as a widely used technique for compressing large language models (LLMs). However, existing layer pruning approaches often incur substantial performance degradation. We identify the majority of this degradation to a single yet previously overlooked issue: \\textit{the mismatch of activation magnitudes at the pruning interface}. \nThe pre-interface activations exhibit significantly different scales from the post-interface ones, causing the distributional shift as it propagates through the remaining layers. \nTo address this issue, we introduce \\textsc{LinearPatch}, a lightweight and plug-and-play technique that fuses two operations into one matrix multiply at the pruning interface: (i) a Hadamard transformation that suppresses massive outliers at particular tokens and (ii) a channel-wise scaling that aligns activation statistics. \nOn LLaMA-3-8B, \\textsc{LinearPatch} preserves up to \\textbf{94.15\\%} of the original model's performance when pruning 5 out of 32 layers, outperforming the previous state of the art by \\textbf{4\\%}. \nThe patch can be further refined with 5K unlabeled samples via memory-efficient offline distillation, pushing the retention to 95.16\\% within only 30 minutes on a single GPU.\nCode is available at \\url{https://github.com/chenxinrui-tsinghua/LinearPatch}.",
    "authors": [
      "~Xinrui_Chen1",
      "~Haoli_Bai2",
      "~Tao_Yuan4",
      "~Ruikang_Liu1",
      "~Kang_Zhao5",
      "~Xianzhi_Yu1",
      "~Lu_Hou2",
      "~Tian_Guan1",
      "~Yonghong_He1",
      "~Chun_Yuan1"
    ],
    "pdf": "/pdf/ad273c5c0dd5a36d07f308b403483e10bb3e3b6d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Model compression through layer pruning directly impacts inference optimization and reduces computational requirements. The lightweight, plug-and-play nature of LinearPatch suggests efficiency benefits. The demonstrated effectiveness on LLaMA-3-8B shows applicability to large models. The memory-efficient offline distillation that refines the model in 30 minutes on a single GPU is relevant to GPU utilization. Preserving high performance while reducing model size could improve inference latency and throughput.",
      "Irrelevant Aspects": "The paper doesn't directly address training optimization techniques. Limited discussion of actual throughput and latency measurements. Focus is on maintaining accuracy rather than explicitly optimizing for GPU utilization during inference. No detailed analysis of how pruning affects memory bandwidth or compute efficiency.",
      "Summary": "This paper presents LinearPatch, a technique to improve layer pruning in large language models by addressing activation magnitude mismatches at pruning interfaces. The approach combines a Hadamard transformation and channel-wise scaling into a single matrix multiplication, showing significant performance preservation when pruning LLaMA-3-8B. The method can be further refined with offline distillation in a computationally efficient manner."
    }
  },
  {
    "id": "E6ZdfjtoiX",
    "title": "Efficient Data Selection at Scale via Influence Distillation",
    "abstract": "Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\\textit{landmark-based approximation}$: influence is precisely computed for a small subset of \"landmark\" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\\times$ faster selection.",
    "authors": [
      "~Mahdi_Nikdan1",
      "~Vincent_Cohen-Addad1",
      "~Dan_Alistarh7",
      "~Vahab_Mirrokni2"
    ],
    "pdf": "/pdf/6ceb9ccc843dc4f4c42f1de2c9fd1a83da8149bf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimizing LLM training through efficient data selection, which is directly relevant to training optimization. It introduces Influence Distillation, a method that uses second-order information to weight training samples optimally. This can improve GPU utilization during training by reducing the amount of data needed while maintaining performance. The landmark-based approximation addresses scalability concerns, and the demonstrated 3.5x faster selection contributes to overall training efficiency. The method applies to various LLM families (Llama and Qwen), showing its broad applicability for training optimization.",
      "Irrelevant Aspects": "The paper does not address inference optimization, which is another key research interest. There's no discussion about optimizing model inference for lower latency or higher throughput. The focus is entirely on data selection for training rather than computational or architectural optimizations for inference. The paper doesn't explore techniques for improving GPU utilization during the inference phase.",
      "Summary": "Influence Distillation introduces a novel framework for data selection in LLM training that uses second-order information to optimally weight training samples. While highly relevant to training optimization and resource efficiency during the training phase, the paper doesn't address inference optimization aspects. The method shows promising results for making LLM training more efficient through intelligent data selection, achieving up to 3.5x faster selection while maintaining performance across various models and tasks."
    }
  },
  {
    "id": "CH72XyZs4y",
    "title": "In Search of Adam’s Secret Sauce",
    "abstract": "Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, $\\beta_1=\\beta_2$. Beyond robust performance, this choice affords new theoretical insights, highlights the \"secret sauce\" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective.",
    "authors": [
      "~Antonio_Orvieto3",
      "~Robert_M._Gower1"
    ],
    "pdf": "/pdf/d2de68be318d2ac7661579409536262dccab57c3.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimization of transformer-based language models, specifically investigating Adam optimizer and its variants. It explores optimizations that could improve training efficiency, with extensive empirical studies across different scales. The research into constraining Adam momentum parameters could potentially lead to more efficient training processes.",
      "Irrelevant Aspects": "The paper appears more focused on theoretical understanding rather than practical GPU utilization. There's no explicit mention of inference optimization, GPU memory efficiency, or detailed discussion on how proposed optimizations affect throughput and latency in practical implementations.",
      "Summary": "This paper investigates Adam optimizer and its variants for training transformer-based language models through extensive empirical studies. While highly relevant to understanding training optimization algorithms, it appears more theoretical than practical for my focus on GPU utilization, scalability, throughput, and latency. The constraint of equal momentum parameters (β₁=β₂) presents an interesting approach but needs further evaluation for practical efficiency gains."
    }
  },
  {
    "id": "1C4mXyh31p",
    "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression",
    "abstract": "Large language models are increasingly capable of handling long-context inputs, but the memory overhead of KV cache remains a major bottleneck for general-purpose deployment. While many compression strategies have been explored, sequence-level compression is particularly challenging due to its tendency to lose important details. We present UniGist, a gist token-based long context compression framework that removes the need for chunk-wise training, enabling the model to learn how to compress and utilize long-range context during training. To fully exploit the sparsity, we introduce a gist shift trick that transforms the attention layout into a right-aligned block structure and develop a block-table-free sparse attention kernel based on it. UniGist further supports one-pass training and flexible chunk sizes during inference, allowing efficient and adaptive context processing. Experiments across multiple long-context tasks show that UniGist significantly improves compression quality, with especially strong performance in recalling details and long-range dependency modeling.",
    "authors": [
      "~Chenlong_Deng1",
      "~Zhisong_Zhang1",
      "~Kelong_Mao1",
      "~Shuaiyi_Li1",
      "~Tianqing_Fang1",
      "~Hongming_Zhang2",
      "~Haitao_Mi1",
      "~Dong_Yu2",
      "~Zhicheng_Dou1"
    ],
    "pdf": "/pdf/8757228d10c75444816024ec46d5a907c5baeebc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory optimization for KV cache, hardware-aligned sequence compression, sparse attention kernel development, one-pass training optimization, flexible inference processing, block-table-free approach",
      "Irrelevant Aspects": "Focus on compression quality over direct performance metrics, limited explicit discussion of GPU utilization metrics",
      "Summary": "UniGist presents a framework for long context compression in LLMs that directly addresses memory overhead issues and provides hardware-aligned solutions through sparse attention mechanisms. The approach optimizes both training and inference processes with its one-pass training capability and flexible chunk sizes. The development of a block-table-free sparse attention kernel suggests computational efficiency improvements, making this work highly relevant to LLM optimization for better performance and scalability."
    }
  },
  {
    "id": "ifsyZYYDNs",
    "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
    "abstract": "Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. \nHowever, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data.\nSynthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training.\nIn this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. \nBuilding on these insights, we propose {\\bf S}elf-Denoising Monte {\\bf C}arlo {\\bf An}notation (\\textsc{Scan}), an efficient data synthesis and noise-tolerant learning framework.\nOur key findings indicate that:\n(1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through self-denoising strategy, enabling PRMs to achieve superior performance with only 6\\% the inference cost required by vanilla MC estimation.\n(2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench.\nDespite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K.\nFurthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of \\textsc{Scan} for scalable, cost-efficient, and robust PRM training.",
    "authors": [
      "~Yuyang_Ding1",
      "~Xinyu_Shi4",
      "~Juntao_Li2",
      "~xiaobo_liang1",
      "~Zhaopeng_Tu1",
      "~Min_Zhang9"
    ],
    "pdf": "/pdf/90e683c8f88b8fe14fd41acb175f70e1cbcdd305.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses important optimization challenges in LLM training, particularly for Process Reward Models. It introduces a self-denoising Monte Carlo method that reduces inference costs to just 6% of vanilla MC estimation, which is highly relevant to cost-efficient training. The approach demonstrates that lightweight models (1.5B parameters) can produce high-quality annotations, enabling more efficient resource utilization. The scalability aspect of the method aligns with research interests in large-scale ML systems optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on a specific application (mathematical reasoning PRMs) rather than general system optimizations. It lacks detailed discussion of GPU utilization, distributed computing strategies, or infrastructure-level optimizations. While it mentions cost reduction, it doesn't extensively address throughput and latency optimizations that are central to LLM inference systems research.",
      "Summary": "SCAN presents a method to optimize Process Reward Model training through self-denoising Monte Carlo annotation, achieving significant cost reductions (6% of vanilla MC estimation) and performance improvements. While relevant to training optimization and resource efficiency, it focuses on a specific application rather than the broader system-level optimizations of GPU utilization and scalability at the infrastructure level."
    }
  },
  {
    "id": "tXxsCbKdQv",
    "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples",
    "abstract": "Recently, Sharma et al. (2024) suggested a method called LAyer- SElective-Rank reduction (LASER) which demonstrated that pruning high‑order components of carefully chosen LLM’s weight matrices can boost downstream accuracy—without any gradient‑based fine‑tuning. Yet LASER’s exhaustive, per‑matrix search (each requiring full‑dataset forward passes) makes it impractical for rapid deployment.  We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected—eliminating the layer‑by‑layer sweep, (ii) The gradient of each matrix’s singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data—both for computing the indicative gradients and for measuring the final accuracy—suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size.  As a results, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets—entirely without fine‑tuning.",
    "authors": [
      "~Shiva_Sreeram1",
      "~Alaa_Maalouf1",
      "~Pratyusha_Sharma1",
      "~Daniela_Rus1"
    ],
    "pdf": "/pdf/9685026dd634e9aeea13578f8edb6871e651cc9e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through efficient weight matrix rank reduction, making models more computationally efficient. The approach requires minimal computation (single gradient step) and data (100 samples) for adaptation, which is highly relevant to scalable deployment. The method improves accuracy while reducing computational overhead, aligning with research on efficient model systems. The focus on removing exhaustive searches and reducing the number of inspected matrices contributes to better GPU utilization and system efficiency.",
      "Irrelevant Aspects": "The paper doesn't directly address training optimization techniques or pre-training efficiency. It lacks specific discussion of GPU utilization strategies, throughput measurements, or latency optimizations. The method focuses on adaptation rather than end-to-end training or inference pipeline optimization. The paper doesn't explore hardware-specific optimizations or distributed computing aspects that might be relevant to large-scale deployment.",
      "Summary": "This paper proposes an efficient LLM adaptation method that builds on LASER, enabling downstream task adaptation with just a single gradient step on 100 samples and without fine-tuning. The approach reduces computational requirements by selectively reducing the rank of specific weight matrices identified through gradient analysis. The method demonstrates significant accuracy improvements while being substantially faster than previous approaches, making it relevant for LLM inference optimization and efficient deployment scenarios."
    }
  },
  {
    "id": "klmc4fwPLd",
    "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
    "abstract": "Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output $\\mathrm{softmax}(QK^\\top)V$, ensuring that the retained tokens best preserve the model’s predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to $9.6$\\% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40\\% at high compression, offering a practical speed-accuracy tradeoff.",
    "authors": [
      "~Ayan_Sengupta1",
      "~Siddhant_Chaudhary1",
      "~Tanmoy_Chakraborty2"
    ],
    "pdf": "/pdf/f12828f6d272d5491dbcabbe3074297e131f7056.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs through KV cache compression, directly addressing memory and latency reduction which are critical for performance optimization. It claims to reduce generation latency by up to 40% at high compression, which aligns with the goal of lower latency. The technique is compatible with FlashAttention and Grouped Query Attention, which are important optimization technologies for LLMs. The compression approach addresses scalability concerns, potentially enabling models to run on hardware with limited memory.",
      "Irrelevant Aspects": "The paper does not discuss training optimization, focusing only on inference. There's no explicit discussion of GPU utilization metrics or how the method improves GPU efficiency. While it mentions latency reduction, it doesn't provide detailed analysis of throughput improvements. The paper concentrates on a specific mathematical approach (CUR decomposition) rather than broader system optimization strategies.",
      "Summary": "CurDKV introduces a value-centric approach to KV cache compression that uses CUR matrix decomposition to select important tokens during inference. The method theoretically minimizes attention reconstruction loss and empirically demonstrates higher accuracy than existing methods under aggressive compression. It offers significant latency reductions while maintaining compatibility with existing attention optimizations like FlashAttention. The paper makes a valuable contribution to inference optimization for LLMs, though it doesn't address training optimization or provide detailed GPU utilization analysis."
    }
  },
  {
    "id": "zSrb8rtH9M",
    "title": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks",
    "abstract": "Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood.\nIn this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity.\nFor shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality.\nFor deep MoEs, we show that $\\mathcal{O}(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks.\nOur analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants.",
    "authors": [
      "~Mingze_Wang2",
      "~Weinan_E2"
    ],
    "pdf": "/pdf/beeb23d8411e1d63a630db039329180c7e0ffeeb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on Mixture-of-Experts (MoE) networks which are fundamental to modern LLMs, analysis of architectural components like gating mechanisms and expert networks, discussion on efficiency implications, insights on scalability through exponential number of structured tasks, understanding of hyperparameter effects that can inform optimization decisions",
      "Irrelevant Aspects": "Primarily theoretical rather than practical implementation focus, limited discussion of GPU utilization specifically, doesn't directly address training or inference optimization techniques, no mention of throughput or latency optimization, focuses on expressive power rather than computational efficiency",
      "Summary": "The paper provides valuable theoretical insights into the expressive power of Mixture-of-Experts networks, which are crucial components in many large language model architectures. While it doesn't directly address GPU utilization, scalability, throughput, or latency optimization, understanding the theoretical foundations and architectural components of MoEs can inform better design and optimization decisions for LLMs. The paper's analysis of how different MoE components contribute to modeling complex tasks provides context that could be useful for optimization strategies, though practical implementation details are outside its scope."
    }
  },
  {
    "id": "eX0m4qMYVN",
    "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive",
    "abstract": "Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. \nThis execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead.\nWhile modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication.\nTo bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory.\nBuilding on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels.\nEvaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by $1.61\\times$ on average in end-to-end latency across different models and configurations.",
    "authors": [
      "~Xinhao_Luo2",
      "~Zihan_Liu12",
      "~Yangjie_Zhou1",
      "~Shihan_Fang1",
      "~Ziyu_Huang3",
      "~Yu_Feng12",
      "~Chen_Zhang11",
      "~Shixuan_Sun1",
      "~Zhenzhe_Zheng1",
      "~Jingwen_Leng1",
      "~Minyi_Guo1"
    ],
    "pdf": "/pdf/5de12982204c2d61c6ccd843e55fba210ab885df.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on reducing latency, improving GPU utilization, and increasing throughput. It introduces innovative cluster-level communication primitives (ClusterReduce and ClusterGather) that enable on-chip data exchange without involving off-chip memory. The approach expands operator fusion scope by composing multiple decoding stages into single fused kernels, which directly aligns with my interest in GPU optimization techniques. The reported 1.61× improvement in end-to-end latency demonstrates significant performance gains.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization rather than training optimization, which is only part of my research interests. Additionally, the solution appears to be tailored specifically to NVIDIA Hopper architecture, which may limit its broader applicability across different GPU platforms.",
      "Summary": "ClusterFusion introduces a novel approach to optimize LLM inference by leveraging cluster-level communication primitives to expand operator fusion scope. By keeping intermediate results on-chip and reducing kernel launch overhead, the framework achieves 1.61× latency improvement over state-of-the-art inference frameworks. The work represents a significant advancement in LLM inference optimization through better utilization of modern GPU architectures and collective communication patterns."
    }
  },
  {
    "id": "p3HBEtNDRY",
    "title": "Streaming Attention Approximation via Discrepancy Theory",
    "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. \n  \nOur main contribution is BalanceKV, a streaming algorithm for $\\epsilon$-approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks.",
    "authors": [
      "~Ekaterina_Kochetkova1",
      "~Kshiteej_Sheth1",
      "~Insu_Han1",
      "~Amir_Zandieh2",
      "~Michael_Kapralov1"
    ],
    "pdf": "/pdf/23692c44fc255241c700b7ce8ba92d7657eb56ad.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Addresses memory challenges for long-context token generation in LLMs; Focuses on attention approximation, a critical component in transformer-based models; Develops streaming algorithms which can be beneficial for efficient inference; Claims empirical performance improvements on benchmarks; Directly related to computational efficiency for LLMs",
      "Irrelevant Aspects": "The paper doesn't explicitly mention GPU utilization, though attention approximation would indirectly affect it; No explicit discussion of training optimization; The theoretical foundation (discrepancy theory, Banaszczyk's vector balancing theory) may be more mathematical than systems-oriented",
      "Summary": "This paper presents BalanceKV, a streaming algorithm for approximating attention computations in LLMs to address memory challenges in long-context token generation. It applies geometric processes and discrepancy theory to select balanced Key and Value tokens, claiming both theoretical guarantees and empirical performance improvements. The work is highly relevant to LLM inference optimization, focusing on computational efficiency and memory usage, which are critical for improving GPU utilization, throughput, and reducing latency in LLM inference systems."
    }
  },
  {
    "id": "6SI1pvb5xl",
    "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few",
    "abstract": "Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \\emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms. Code is available at \\href{https://github.com/QishuaiWen/CBSA}{this https URL}.",
    "authors": [
      "~Qishuai_Wen1",
      "~Zhiyuan_Huang5",
      "~Chun-Guang_Li3"
    ],
    "pdf": "/pdf/a6248dab5d754965c693b21c16f96b7a1c6231ad.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses the quadratic complexity of self-attention, which is a major bottleneck in transformer models, especially for large language models. The proposed CBSA approach claims to scale linearly, which would significantly improve efficiency for both training and inference. The focus on attention mechanisms, which are central to LLMs, and the token compression approach are directly relevant to optimization goals.",
      "Irrelevant Aspects": "The strong emphasis on interpretability is not directly related to my core expertise in GPU utilization and scalability. The mention of experiments on 'visual tasks' suggests the method might not be primarily tested on language tasks. The algorithm unrolling approach appears to focus more on mathematical formulation than practical implementation optimizations.",
      "Summary": "This paper proposes Contract-and-Broadcast Self-Attention (CBSA), a novel attention mechanism that addresses the quadratic complexity issue in standard attention by compressing tokens through representative selection. While it offers promising linear scaling for attention computation, the paper's focus on visual tasks and interpretability somewhat limits its direct relevance to LLM-specific optimization challenges. Nevertheless, the core approach of attention efficiency improvement is valuable to my research interests."
    }
  },
  {
    "id": "tAq9Gxdhr0",
    "title": "Disentangling Latent Shifts of In-Context Learning with Weak Supervision",
    "abstract": "In-context learning (ICL) enables large language models to perform few-shot learning by conditioning on labeled examples in the prompt. Despite its flexibility, ICL suffers from instability -- especially as prompt length increases with more demonstrations. To address this, we treat ICL as a source of weak supervision and propose a parameter-efficient method that disentangles demonstration-induced latent shifts from those of the query. An ICL-based teacher generates pseudo-labels on unlabeled queries, while a student predicts them using only the query input, updating a lightweight adapter. This captures demonstration effects in a compact, reusable form, enabling efficient inference while remaining composable with new demonstrations. Although trained on noisy teacher outputs, the student often outperforms its teacher through pseudo-label correction and coverage expansion, consistent with the weak-to-strong generalization effect. Empirically, our method improves generalization, stability, and efficiency across both in-domain and out-of-domain tasks, surpassing standard ICL and prior disentanglement methods.",
    "authors": [
      "~Josip_Jukić1",
      "~Jan_Šnajder1"
    ],
    "pdf": "/pdf/180e11f7cd7da27e9226e119f7d2df62db54dc88.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses efficiency of in-context learning (ICL), which relates to inference optimization. It proposes a parameter-efficient method using lightweight adapters and a teacher-student approach that enables more efficient inference with compact, reusable representations of demonstration effects. This could reduce computational requirements during inference and improve stability as prompt length increases, which impacts memory usage and processing time. The method claims to improve efficiency across tasks, which relates to throughput.",
      "Irrelevant Aspects": "The paper focuses more on the quality and stability of ICL rather than specific hardware optimizations. It doesn't explicitly address GPU utilization, parallelization techniques, or specific architectural changes to improve hardware utilization. The emphasis is on representation learning rather than computational efficiency optimizations at the hardware level.",
      "Summary": "This paper proposes a parameter-efficient method to improve in-context learning by treating ICL as weak supervision and using a teacher-student approach. While not directly addressing GPU utilization, the method improves inference efficiency through compact representation of demonstration effects and lightweight adapters, which is relevant to optimization goals but not specifically focused on hardware-level performance improvements."
    }
  },
  {
    "id": "r5tbCL9vAZ",
    "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
    "abstract": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.",
    "authors": [
      "~Tianhao_Chen3",
      "~Xin_Xu13",
      "~Zijing_Liu1",
      "~Pengxiang_Li2",
      "~Xinyuan_Song2",
      "~AJAY_KUMAR_JAISWAL1",
      "~Fan_Zhang41",
      "~Jishan_Hu1",
      "~Yang_Wang25",
      "~Hao_Chen1",
      "~Shizhe_Diao2",
      "~Shiwei_Liu2",
      "~Yu_Li4",
      "~Lu_Yin1",
      "~Can_Yang2"
    ],
    "pdf": "/pdf/86ac8dbe9f13c0ff02d636f7741a152447094fe9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on training optimization for LLMs, specifically improving convergence of pretraining for Pre-LN Transformer architectures. This aligns with my interest in training optimization that can lead to better GPU utilization and higher throughput. The approach works by scaling activations while preserving gradients, which can improve training dynamics and potentially reduce training time. The technique is tested across various model sizes (71M to 1B), demonstrating scalability, and can be combined with existing architectures, making it practical to implement.",
      "Irrelevant Aspects": "The paper does not directly address inference optimization, which is part of my research interest. There's no explicit mention of GPU utilization metrics, throughput measurements, or latency benchmarks. The paper doesn't discuss distributed training or other scalability concerns beyond the model size range. The focus is primarily on training dynamics rather than hardware efficiency or inference performance.",
      "Summary": "GPAS introduces a technique to improve the convergence of LLM pretraining by addressing activation variance growth in Pre-LN Transformers. While this has clear relevance to training optimization and could indirectly improve GPU utilization through faster convergence, the paper doesn't directly address inference optimization, throughput, or latency metrics that are central to my research interests. The approach shows promise for improving training dynamics across various architectures and model sizes, making it moderately relevant (7/10) to my overall research focus."
    }
  },
  {
    "id": "NcxmgX95ue",
    "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training",
    "abstract": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text without utilizing contextual metadata such as source, quality, or topic, leading to a context-free learning paradigm. While recent studies suggest that adding metadata like URL information as context (i.e., auxiliary inputs not used in the loss calculation) can improve training efficiency and downstream performance, they offer limited understanding of which types of metadata are truly effective and under what conditions. In this work, we conduct a systematic evaluation and  find that not all metadata types contribute equally. Only URL context speeds up training, whereas quality scores and topic/format domain information offer no clear benefit. Furthermore, the improved downstream performances of URL conditioning emerge only when longer prompts are used at inference time. In addition, we demonstrate that context-aware pretraining enables more controllable generation than context-free pretraining, in a classifier-free guidance fashion. Although topic and format metadata do not accelerate training, they are effective for steering outputs, offering human-interpretable control over generation.",
    "authors": [
      "~Dongyang_Fan2",
      "~Vinko_Sabolčec1",
      "~Martin_Jaggi1"
    ],
    "pdf": "/pdf/0204b8a4f4af262a098c69cc46a486c59b64efcf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Training efficiency improvements through URL metadata",
        "Performance benefits with different metadata types",
        "Context-aware pretraining for controllable generation",
        "Metadata effects on model behavior with different input lengths during inference",
        "Auxiliary input methodology affecting computational requirements",
        "Training optimization approaches using metadata",
        "Connection between metadata and controllable generation for efficient inference"
      ],
      "Irrelevant Aspects": [
        "No explicit analysis of GPU utilization metrics",
        "Limited discussion of scalability considerations",
        "No throughput or latency measurements",
        "Focus on qualitative metadata effects rather than quantitative system performance",
        "Lacks system implementation details",
        "No distributed training techniques discussed",
        "Limited inference optimization techniques beyond metadata effects"
      ],
      "Summary": "This paper investigates how different types of metadata (URLs, quality scores, topic/format) affect LLM training efficiency and downstream performance. It finds that only URL context speeds up training, while other metadata types are ineffective for training acceleration but useful for output steering. The findings on training acceleration directly relate to training optimization interests, and the methodology of using metadata as context could represent an approach to optimize training without significantly increasing computational costs. However, the paper lacks system performance metrics and implementation details valuable for GPU utilization, scalability, throughput, and latency optimization research."
    }
  },
  {
    "id": "ZDpPfg9pDc",
    "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding",
    "abstract": "The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs.\n    Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference.\n    Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers.\n    Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths.\n    To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment.\n    SubSpec achieves a high average acceptance length, delivering 9.1$\\times$ speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5$\\times$ speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
    "authors": [
      "~Pei-Shuo_Wang1",
      "~Jian-Jia_Chen2",
      "~Chun-Che_Yang1",
      "~Chi-Chih_Chang1",
      "~Ning-Chi_Huang1",
      "~Mohamed_S._Abdelfattah1",
      "~Kai-Chiang_Wu1"
    ],
    "pdf": "/pdf/98342d215ec3c042e7d15c061d02b4086e670e2b.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper addresses memory limitations for LLM deployment on consumer GPUs, focusing on inference acceleration through speculative decoding. SubSpec is a training-free, lossless method that constructs highly aligned draft models from quantized substitute layers. It achieves impressive speedups (9.1× for 7B model, 12.5× for 32B model) while sharing GPU-resident layers and KV-Cache to optimize memory usage. This directly targets my research interests in GPU utilization, scalability, throughput improvement, and latency reduction.",
      "Irrelevant Aspects": "The paper does not address training optimization, which is part of my research focus. It specifically targets offloading scenarios rather than general inference optimization approaches.",
      "Summary": "This paper introduces SubSpec, a training-free method that accelerates inference of offloaded LLMs through speculative decoding. By creating highly aligned draft models from quantized substitute layers of the target model, it eliminates the need for additional training while maintaining quality. The approach demonstrates significant speedups and memory efficiency, making it highly relevant to my research in LLM inference optimization for better GPU utilization and reduced latency."
    }
  },
  {
    "id": "zJSZupQ889",
    "title": "SALS: Sparse Attention in Latent Space for KV Cache Compression",
    "abstract": "Large Language Models (LLMs) capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value (KV) cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden dimension, suggesting the potential for effective compression. However, due to the widely adopted Rotary Position Embedding (RoPE) mechanism in modern LLMs, naive low‑-rank compression suffers severe accuracy degradation or creates a new speed bottleneck, as the low-rank cache must first be reconstructed in order to apply RoPE. In this paper, we introduce two key insights: first, the application of RoPE to the key vectors increases their variance, which in turn results in a higher rank; second, after the key vectors are transformed into the latent space, they largely maintain their representation across most layers. Based on these insights, we propose the Sparse Attention in Latent Space (SALS) framework. SALS projects the KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query--key interactions in this space. By reconstructing only a small subset of important tokens, it avoids the overhead of full KV cache reconstruction. We comprehensively evaluate SALS on various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and additionally verify its scalability on the RULER-128k benchmark with LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA performance by maintaining competitive accuracy. Under different settings, SALS achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention operator compared to FlashAttention2 on the 4K sequence. For the end-to-end throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared to GPT-fast on 4k and 32K sequences, respectively. The source code will be publicly available in the future.",
    "authors": [
      "~Junlin_Mu1",
      "~Hantao_Huang1",
      "~Jihang_Zhang1",
      "~Minghui_Yu3",
      "~Tao_Wang1",
      "~Yidong_Li1"
    ],
    "pdf": "/pdf/0aadb1ecaa69a337eba64e609d96a965bac5571e.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "Focuses on LLM inference optimization through KV cache compression",
        "Addresses GPU utilization improvements through memory reduction",
        "Demonstrates scalability to larger models and longer contexts",
        "Claims significant throughput improvements (1.4x to 4.5x)",
        "Reduces computational requirements which should lower latency",
        "Provides empirical results on multiple large models (LLaMA2, Mistral, LLaMA3.1)",
        "Directly addresses memory bandwidth constraints which impact GPU performance"
      ],
      "Irrelevant Aspects": [
        "Does not address training optimization, only inference",
        "Limited discussion of distributed systems or multi-GPU scenarios",
        "Focuses on a specific technique rather than broader optimization principles",
        "No direct comparison to other KV cache compression techniques in terms of hardware efficiency"
      ],
      "Summary": "This paper presents SALS, a novel approach for KV cache compression that projects caches into latent space and performs sparse token selection. It directly addresses key aspects of my research interests by optimizing LLM inference, improving GPU utilization through memory reduction, and demonstrating scalability with significant throughput improvements. The technique achieves 6.4x KV cache compression and up to 5.7x attention speedup, with end-to-end throughput improvements of 1.4x to 4.5x on different sequence lengths. While it doesn't address training optimization, its focus on inference efficiency, memory usage, and computational reduction makes it highly relevant to my work on LLM optimization."
    }
  },
  {
    "id": "Kz6eUL86XP",
    "title": "Do Language Models Use Their Depth Efficiently?",
    "abstract": "Modern LLMs are increasingly deep, and depth correlates with performance, albeit with diminishing returns. However, do these models use their depth efficiently? Do they compose more features to create higher-order computations that are impossible in shallow models, or do they merely spread the same kinds of computation out over more layers? To address these questions, we analyze the residual stream of the Llama 3.1, Qwen 3, and OLMo 2 family of models. We find: First, comparing the output of the sublayers to the residual stream reveals that layers in the second half contribute much less than those in the first half, with a clear phase transition between the two halves. Second, skipping layers in the second half has a much smaller effect on future computations and output predictions. Third, for multihop tasks, we are unable to find evidence that models are using increased depth to compose subresults in examples involving many hops. Fourth, we seek to directly address whether deeper models are using their additional layers to perform new kinds of computation. To do this, we train linear maps from the residual stream of a shallow model to a deeper one. We find that layers with the same relative depth map best to each other, suggesting that the larger model simply spreads the same computations out over its many layers. All this evidence suggests that deeper models are not using their depth to learn new kinds of computation, but only using the greater depth to perform more fine-grained adjustments to the residual. This may help explain why increasing scale leads to diminishing returns for stacked Transformer architectures.",
    "authors": [
      "~Róbert_Csordás1",
      "~Christopher_D_Manning1",
      "~Christopher_Potts1"
    ],
    "pdf": "/pdf/0691d8df4fdd96ae898d5845e2735b7f050f858f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper's analysis of depth efficiency in LLMs is highly relevant to optimization concerns. Findings that later layers contribute less suggest potential for computational savings and layer skipping during inference. The insights about inefficient depth usage could inform more efficient model architectures, reducing unnecessary computations. The paper addresses fundamental questions about model scalability that impact training resource allocation and inference performance. Understanding how depth relates to computational capability helps optimize resource utilization in both training and serving systems.",
      "Irrelevant Aspects": "The paper focuses primarily on mechanistic interpretability rather than system-level optimization. It analyzes feature composition and residual stream behaviors rather than directly addressing GPU utilization, throughput, or latency metrics. The research doesn't propose concrete implementation optimizations or system designs for improved efficiency. There's no discussion of hardware-specific optimizations or deployment strategies that would directly impact inference systems.",
      "Summary": "This paper investigates whether deep LLMs effectively utilize their depth or merely spread computation across additional layers without adding new capabilities. The authors analyze residual streams in popular model families and find that layers in the second half contribute significantly less than those in the first half, with a clear phase transition between them. Skipping layers in the second half has minimal impact on future computations and outputs. The evidence suggests that deeper models aren't performing fundamentally new computations but rather making more fine-grained adjustments across additional layers, potentially explaining diminishing returns from increased scale in stacked Transformer architectures."
    }
  },
  {
    "id": "fyeSq3m8CY",
    "title": "Tensor-Parallelism with Partially Synchronized Activations",
    "abstract": "Training and inference of Large Language Models (LLMs) with tensor-parallelism\nrequires substantial communication to synchronize activations. Our findings suggest\nthat with a few minor adjustments to current practices, LLMs can be trained\nwithout fully synchronizing activations, reducing bandwidth demands. We name\nthis “Communication-Aware Architecture for Tensor-parallelism” (CAAT-Net).\nWe train a 7B parameter CAAT-Net model and show that tensor-parallel communication\ncan be reduced by up to 50% with no significant drop in pretraining accuracy\nacross nearly all evaluated benchmarks. We also experiment with smaller 130M\nand 1.1B models to show the robustness and scalability of our method. We find that,\nin some scenarios, validation loss can even improve when reducing communication.\nFinally, we demonstrate how CAAT-Net accelerates both training and inference\nworkloads across various settings and model sizes.",
    "authors": [
      "~Itay_Lamprecht1",
      "~Asaf_Karnieli1",
      "~Yair_Hanani1",
      "~Niv_Giladi1",
      "~Daniel_Soudry1"
    ],
    "pdf": "/pdf/656611ece927bf48e77761369375f75907caacd7.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses tensor parallelism optimization for LLMs, focusing on communication reduction which significantly impacts GPU utilization and scalability. It demonstrates improvements in both training and inference workloads, with experiments across different model sizes (130M to 7B parameters) showing the method's robustness. The approach reduces bandwidth demands and synchronization overhead, directly contributing to higher throughput and better resource utilization.",
      "Irrelevant Aspects": "The paper doesn't explicitly address latency metrics for inference scenarios, and it doesn't cover memory optimization techniques. There's also limited discussion on hardware-specific optimizations beyond communication patterns.",
      "Summary": "The paper introduces CAAT-Net, a communication-aware architecture for tensor-parallelism that reduces activation synchronization requirements in LLM training and inference. By making minor adjustments to current practices, they achieve up to 50% reduction in tensor-parallel communication with minimal impact on pretraining accuracy across various benchmarks and model sizes. This approach accelerates both training and inference workloads while maintaining or sometimes improving validation loss."
    }
  },
  {
    "id": "16kX08MCav",
    "title": "Limitations of Normalization in Attention",
    "abstract": "This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures.",
    "authors": [
      "~Timur_Mudarisov1",
      "~Mikhail_Burtsev1",
      "~Tatiana_Petrova1",
      "~Radu_State1"
    ],
    "pdf": "/pdf/527ae5ee1958b1b41bfb0e6faf4f0dda8ca133f0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates normalization in attention mechanisms, which is fundamental to LLM efficiency. It analyzes token selection processes that directly impact computational efficiency during training and inference. The research on GPT-2 specifically addresses large language models. The gradient sensitivity challenges identified could affect training stability and convergence speed. Understanding these limitations could lead to more efficient attention implementations that improve GPU utilization.",
      "Irrelevant Aspects": "The paper appears more theoretical than applied, focusing on mathematical bounds and geometric properties rather than practical optimization techniques. It doesn't directly address GPU utilization, memory optimization, quantization, or other common inference optimization strategies. The theoretical framework may not translate immediately to throughput or latency improvements.",
      "Summary": "This paper provides theoretical insights into limitations of normalization in attention mechanisms, particularly how token selection efficiency degrades as more tokens are selected. While it offers valuable understanding of attention mechanisms that could inform future optimization strategies, it's primarily theoretical in nature and doesn't directly address practical optimization techniques for improving GPU utilization or reducing latency."
    }
  },
  {
    "id": "jMhRbV47pS",
    "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
    "abstract": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
    "authors": [
      "~Nicolas_Zucchet1",
      "~Francesco_D'Angelo1",
      "~Andrew_Kyle_Lampinen1",
      "~Stephanie_C.Y._Chan1"
    ],
    "pdf": "/pdf/eff77c0e34c1afa98f3048d1ce66080f4a7fe493.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly studies sparse attention patterns, which are critical for optimizing LLM computation and memory efficiency. Understanding how these patterns emerge can inform more efficient model architectures and training strategies. The findings about repetition accelerating emergence could lead to training optimization techniques. The theoretical framework for understanding attention pattern formation could help design models that naturally develop efficient sparse representations, reducing computational requirements and improving throughput.",
      "Irrelevant Aspects": "The research appears primarily focused on theoretical understanding rather than practical engineering solutions for GPU utilization. The studies seem to use smaller models rather than large-scale LLMs. The associative recall task might not be representative of real-world LLM workloads where optimization is most critical. There's limited emphasis on direct measurement or improvement of throughput, latency, or scalability metrics.",
      "Summary": "This paper investigates the emergence of sparse attention patterns in Transformers, providing theoretical and empirical analysis of when and how these efficiency-enhancing patterns develop during training. While primarily theoretical, the insights into sparse attention emergence are valuable for LLM optimization research, as sparse attention directly impacts computational efficiency and memory usage. The finding that repetition accelerates emergence could potentially inform training optimization strategies, though the paper doesn't directly address implementation aspects of GPU utilization or scalability."
    }
  },
  {
    "id": "z9oeQrcNh9",
    "title": "ARM: Adaptive Reasoning Model",
    "abstract": "While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the \"overthinking\" problem—excessive and unnecessary reasoning—which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones—Direct Answer, Short CoT, and Code—as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of $\\sim$30%, and up to $\\sim$70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a $\\sim$2$\\times$ speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens—ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage. All the resources will be released.",
    "authors": [
      "~Siye_Wu1",
      "~Jian_Xie3",
      "~Yikai_Zhang6",
      "~Aili_Chen1",
      "~Kai_Zhang10",
      "~Yu_Su2",
      "~Yanghua_Xiao1"
    ],
    "pdf": "/pdf/8f8dd782fedb7e07d7ecadae58b8443dca04c72f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization through adaptive token reduction (30% average, up to 70%), which improves latency and throughput. It introduces Ada-GRPO for training optimization with 2× speedup. The adaptive reasoning approach enhances resource efficiency and scalability by avoiding unnecessary computation. The multiple operational modes (Adaptive, Instruction-Guided, Consensus-Guided) provide system-level flexibility for optimizing performance based on different requirements.",
      "Irrelevant Aspects": "The paper focuses primarily on model architecture and reasoning formats rather than detailed hardware-specific optimizations. It doesn't delve deep into GPU utilization techniques or distributed training strategies. The emphasis is more on algorithmic efficiency than low-level system implementation details.",
      "Summary": "ARM: Adaptive Reasoning Model presents a system for optimizing LLM inference and training efficiency through adaptive reasoning formats. It reduces token usage significantly while maintaining performance, resulting in improved inference speed and reduced computational requirements. The approach includes multiple reasoning modes and a novel training algorithm (Ada-GRPO), contributing to both training and inference optimizations relevant to efficient ML system design."
    }
  },
  {
    "id": "Ynwl0V1YH0",
    "title": "Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints. Our source code is available on GitHub repository: https://github.com/xzhang9308/GLVQ.",
    "authors": [
      "~Xi_Zhang7",
      "~Xiaolin_Wu2",
      "~Jiamang_Wang1",
      "~Weisi_Lin1"
    ],
    "pdf": "/pdf/5bd34d855bbe3528830f57f72f2bbc0d5b34835d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Post-training quantization for LLM inference, memory reduction, lattice vector quantization with learnable parameters, efficient decoding through matrix-vector multiplication, improved model size and accuracy trade-offs",
      "Irrelevant Aspects": "Limited focus on training optimization, minimal discussion of GPU-specific optimizations or kernel implementations, primarily focused on compression rather than end-to-end system performance",
      "Summary": "The paper introduces Grouped Lattice Vector Quantization (GLVQ) for compressing LLM weights to lower bit-width formats, using learnable generation matrices and Babai rounding for optimization. It achieves a better trade-off between model size and accuracy compared to existing PTQ methods, enabling deployment of large models under resource constraints. The approach is relevant to inference optimization and resource utilization but focuses primarily on compression rather than comprehensive system performance optimizations."
    }
  },
  {
    "id": "JDKYmIh4rO",
    "title": "Noise Consistency Training: A Native Approach for One-step Generator in Learning Additional Controls",
    "abstract": "The pursuit of efficient and controllable high-quality content generation stands as a pivotal challenge in artificial intelligence-generated content (AIGC).\nWhile one-step generators, refined through diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions—such as structural constraints, semantic guidelines, or external inputs—poses a significant challenge. \nConventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. \nThis paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. \nNCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. \nThis loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. \nThis training objective can be interpreted as aligning the adapted generator with the intractable conditional distribution defined by a discriminative model and the one-step generator from moment-matching perspectives. \nNCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency.",
    "authors": [
      "~Yihong_Luo1",
      "~Shuchen_Xue1",
      "~Tianyang_Hu1",
      "~Jing_Tang5"
    ],
    "pdf": "/pdf/b8657b0c3ca4cfcc6830dd4037c1efa3f116000d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on efficient inference through one-step generators, which relates to reducing latency and improving throughput. It presents a lightweight approach (NCT) for adapting pre-trained models without expensive modifications, which aligns with optimization goals. The emphasis on computational efficiency, data efficiency, and modularity are directly relevant to GPU utilization and scalability concerns. The approach achieves state-of-the-art performance in a single forward pass, addressing inference optimization.",
      "Irrelevant Aspects": "The paper specifically addresses diffusion models for image generation rather than large language models. The technical details about noise consistency in diffusion spaces may not directly transfer to language model optimization. The focus on control signals for visual content generation (structural constraints, semantic guidelines) is more specific to image generation than text.",
      "Summary": "Noise Consistency Training (NCT) is presented as a lightweight method to enhance pre-trained one-step generators with new control capabilities. While the paper focuses on diffusion models for image generation rather than LLMs, its approach to efficient inference, modular adaptation without retraining, and computational optimization offers valuable insights for model optimization. The single-forward-pass generation directly addresses latency concerns, making it moderately relevant to LLM optimization research."
    }
  },
  {
    "id": "XMzxZ6h68o",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "abstract": "Training large language models (LLMs) models directly in low-precision  offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we  investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an \"optimal\" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet .",
    "authors": [
      "~Roberto_L._Castro1",
      "~Andrei_Panferov1",
      "~Soroush_Tabesh1",
      "~Oliver_Sieberling1",
      "~Jiale_Chen13",
      "~Mahdi_Nikdan1",
      "~Saleh_Ashkboos1",
      "~Dan_Alistarh7"
    ],
    "pdf": "/pdf/0b37d1feeecbf591f8585e7672ee93dcb3d87a7d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training optimization for LLMs, low-precision training techniques, GPU utilization improvements, throughput enhancement, CUDA kernel optimization, computational efficiency, resource efficiency, Blackwell architecture optimization",
      "Irrelevant Aspects": "Limited focus on inference optimization (though many techniques may apply), specific focus on FP4 rather than broader precision landscape, hardware-specific optimizations that may not generalize across all GPU architectures",
      "Summary": "This paper is highly relevant as it directly addresses LLM training optimization through native FP4 precision, demonstrating competitive performance against FP16 and FP8. It introduces Quartet, an end-to-end low-precision training approach with optimized CUDA kernels for Blackwell architecture, focusing on improving throughput and energy efficiency while maintaining accuracy. The work establishes new low-precision scaling laws that could advance our understanding of computational trade-offs in LLM training."
    }
  },
  {
    "id": "0RF80tUWuv",
    "title": "RidgeLoRA: Matrix Ridge Enhanced Low-Rank Adaptation of Large Language Models",
    "abstract": "As one of the state-of-the-art parameter-efficient fine-tuning~(PEFT) methods, Low-Rank Adaptation (LoRA) enables model optimization with reduced computational cost through trainable low-rank matrix. However, the low-rank nature makes it prone to produce a decrease in the representation ability, leading to suboptimal performance. In order to break this limitation, we propose RidgeLoRA, a lightweight architecture like LoRA that incorporates novel architecture and matrix ridge enhanced full-rank approximation, to match the performance of full-rank training, while eliminating the need for high memory and a large number of parameters to restore the rank of matrices. We provide a rigorous mathematical derivation to prove that RidgeLoRA has a better upper bound on the representations than vanilla LoRA. Furthermore, extensive experiments across multiple domains demonstrate that RidgeLoRA achieves better performance than other LoRA variants, and can even match or surpass full-rank training.",
    "authors": [
      "~Junda_Zhu1",
      "~Jun_Ai1",
      "~Yujun_Li1",
      "~Yichun_Yin2",
      "~Yasheng_Wang1",
      "~Lifeng_Shang1",
      "~Qun_Liu1"
    ],
    "pdf": "/pdf/0b2334140c8abbe8933679a2a8140b73e18eff45.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on parameter-efficient fine-tuning (PEFT) methods for large language models, directly addressing training optimization. RidgeLoRA aims to match full-rank training performance while reducing computational costs and memory requirements, which could improve GPU utilization and throughput. The enhancement to LoRA addresses representation ability limitations in low-rank adaptations, potentially offering better model performance with fewer resources.",
      "Irrelevant Aspects": "The paper appears to focus more on mathematical derivations and theoretical bounds rather than practical implementation details for GPU optimization. There's limited mention of inference optimization strategies, distributed training across multiple GPUs, or system-level resource scheduling. Quantization and other inference acceleration techniques are not explicitly addressed in the abstract.",
      "Summary": "RidgeLoRA presents an enhanced parameter-efficient fine-tuning method for large language models that improves upon LoRA by incorporating matrix ridge enhancement. The approach aims to achieve full-rank training performance while maintaining low memory and parameter requirements. While highly relevant to LLM training optimization, the paper appears to focus more on theoretical aspects rather than practical implementation details for maximizing GPU utilization and minimizing latency."
    }
  },
  {
    "id": "MzN2Xi7EMa",
    "title": "Attribution-Driven Adaptive Token Pruning for Transformers",
    "abstract": "Transformers have been widely adopted in natural language processing, computer vision, and other domains due to their exceptional performance across a variety of tasks. However, the computational cost of Transformers is prohibitively high, particularly when handling long input sequences, significantly increasing both training and inference time. Although various token pruning methods have been proposed to reduce the computational burden of Transformers, most approaches overlook critical differences in sequences in terms of length and complexity, leading to suboptimal compression efficiency.\n\nIn this paper, we propose AD-TP, an Attribution-Driven Adaptive Token Pruning method designed to retain only the most informative tokens. We analyze the performance of using accumulated attention values to measure token importance and find that attention values do not accurately reflect the actual contribution of each token to text understanding. Additionally, we observe significant variations in the length and complexity of different sequences within the dataset. Based on these insights, we adopt Integrated Gradients to evaluate token importance and introduce a lightweight adaptive token retainer module that dynamically generates pruning configurations for each input sequence. In addition, we incorporate both teacher supervision and self-supervised learning objectives to enhance the training efficiency, accuracy, and robustness of the model.\n\nExperiments conducted on GLUE, SQuAD, and 20News demonstrate that AD-TP outperforms state-of-the-art token pruning and model compression methods in both accuracy and computational efficiency. On GLUE, AD-TP reduces FLOPs by an average of 7.8× while improving performance by 0.6%.",
    "authors": [
      "~Yaoyao_Yan1",
      "~Hui_Yu11",
      "~Weizhi_Xu2"
    ],
    "pdf": "/pdf/8c4fe5f80603e41ea0682defcdd4d273128bc76e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization through token pruning, 7.8× FLOPs reduction while improving accuracy, adaptive pruning configuration per input sequence, Transformer focus for LLMs, training efficiency improvements, state-of-the-art compression method",
      "Irrelevant Aspects": "Focus on attribution methods rather than system-level optimizations, task-specific evaluations rather than general performance, missing implementation details for GPU utilization, lack of scalability discussion for larger models",
      "Summary": "This paper presents AD-TP, an adaptive token pruning method for Transformers that reduces computational cost by dynamically retaining only informative tokens. It achieves 7.8× FLOPs reduction while improving accuracy, which is highly relevant for optimizing large language models. The adaptive approach is promising, but the focus is more on token importance determination than system-level optimizations and scalability."
    }
  },
  {
    "id": "zZecO3RZ7Z",
    "title": "Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality",
    "abstract": "Data filtering has become a powerful tool for improving model performance while reducing computational cost. However, as large language model compute budgets continue to grow, the limited data volume provided by heavily filtered and deduplicated datasets will become a practical constraint. In efforts to better understand how to proceed, we study model performance at various compute budgets and across multiple pre-training datasets created through data filtering and deduplication. We find that, given appropriate modifications to the training recipe, repeating existing aggressively filtered datasets for up to ten epochs can outperform training on the ten times larger superset for a single epoch across multiple compute budget orders of magnitude. While this finding relies on repeating the dataset for many epochs, we also investigate repeats within these datasets at the document level. We find that not all documents within a dataset are equal, and we can create better datasets relative to a token budget by explicitly manipulating the counts of individual documents. We conclude by arguing that even as large language models scale, data filtering remains an important direction of research.",
    "authors": [
      "~Alex_Fang1",
      "~Hadi_Pouransari1",
      "~Matt_Jordan1",
      "~Alexander_T_Toshev1",
      "~Vaishaal_Shankar1",
      "~Ludwig_Schmidt1",
      "~Tom_Gunter1"
    ],
    "pdf": "/pdf/5dda0ba4fbf67c1b65e7147ccec64df745e7d1a6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models, examining how data filtering and repetition strategies impact computational efficiency. It explores the relationship between data quality, compute budgets, and model performance, which is directly relevant to optimizing GPU utilization and throughput during training. The findings about repeating filtered datasets versus using larger unfiltered datasets provide practical insights for maximizing training efficiency within constrained compute budgets.",
      "Irrelevant Aspects": "The paper does not specifically address inference optimization techniques or latency reduction strategies. There's no discussion of hardware-specific GPU optimization approaches or low-level implementation details that would maximize GPU utilization. The focus remains on data composition rather than computational optimization techniques or scaling infrastructure considerations.",
      "Summary": "This paper investigates data filtering and repetition strategies for large language model training, finding that repeating aggressively filtered datasets can outperform training on larger but unfiltered datasets. While highly relevant to training optimization and resource utilization, it lacks direct discussion of inference optimization, latency concerns, or specific GPU utilization techniques. The research provides valuable insights for optimizing training efficiency within compute constraints."
    }
  },
  {
    "id": "DpOSndSOZz",
    "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
    "abstract": "The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics—InfoBias and InfoGain—to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.",
    "authors": [
      "~Xixian_Yong1",
      "~Xiao_Zhou7",
      "~Yingying_Zhang5",
      "~Jinlin_Li5",
      "~Yefeng_Zheng3",
      "~Xian_Wu1"
    ],
    "pdf": "/pdf/72b099c2f4257233c570fee63033ba4ef835dec3.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents an entropy-based Adaptive Think strategy that optimizes inference by dynamically halting reasoning, achieving 50.80% reduction in token usage which directly impacts GPU utilization and latency. It addresses cost-efficiency in LLM deployment, introduces quantitative metrics for reasoning efficiency, and demonstrates improvements in resource utilization during inference.",
      "Irrelevant Aspects": "The paper focuses primarily on reasoning quality and information theory rather than system performance optimization. It lacks GPU-specific optimizations, has limited discussion of scalability challenges, and the modest 1.10% accuracy improvement is not directly relevant to performance optimization goals.",
      "Summary": "This paper explores reasoning efficiency in Large Reasoning Models through an information-theoretic approach, proposing metrics to quantify reasoning divergence and information contribution. The Adaptive Think strategy dynamically halts reasoning based on confidence, significantly reducing token usage while maintaining accuracy. While relevant to inference optimization and resource efficiency, it doesn't deeply address system-level performance, GPU-specific optimizations, or scalability aspects of large language model deployment."
    }
  },
  {
    "id": "hFxOZjHyTg",
    "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding",
    "abstract": "Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key–value (KV) cache grows linearly with time—quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for \\textit{streaming} video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94\\%, sustains real-time generation, and matches or surpasses full-cache accuracy—even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.",
    "authors": [
      "~Minsoo_Kim2",
      "~Kyuhong_Shim1",
      "~Jungwook_Choi1",
      "~Simyung_Chang1"
    ],
    "pdf": "/pdf/0273b0b3585a11ccb6d2f72afa3845c00339a8ee.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "KV cache compression, memory optimization, training-free inference optimization, real-time generation capabilities, throughput improvement, GPU memory reduction (up to 94%), handling long sequences, model performance evaluation across multiple architectures, balancing memory efficiency with accuracy preservation",
      "Irrelevant Aspects": "Primarily focuses on video understanding rather than text-only LLMs, emphasizes edge/on-device deployment over large-scale distributed systems, specific video-oriented metrics like Temporal-axis Redundancy (TaR)",
      "Summary": "InfiniPot-V introduces a training-free, query-agnostic framework for compressing KV caches in multimodal LLMs during streaming video understanding. The method enforces a hard memory cap by removing temporally redundant tokens and preserving semantically significant ones, reducing GPU memory usage by up to 94% while maintaining accuracy. Though focused on video applications, the KV cache optimization techniques are highly relevant to LLM inference efficiency and memory management."
    }
  },
  {
    "id": "3FsM6wWQL4",
    "title": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs",
    "abstract": "We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$  on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code  available at: https://github.com/Yeyke/HBLLM.",
    "authors": [
      "~Ningning_CHEN3",
      "~Weicai_Ye4",
      "~Ying_Jiang1"
    ],
    "pdf": "/pdf/ad1ca788c7639132b5606dd67c6d3fdd3349d441.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "1-bit post-training quantization for LLMs directly impacts inference optimization, memory usage reduction, and potentially improves GPU utilization. The method achieves state-of-the-art performance in extreme quantization, applying to popular models like OPT and LLaMA. The approach focuses on maintaining performance while minimizing storage overhead to 1.08 bits, which is crucial for efficient LLM deployment.",
      "Irrelevant Aspects": "Focuses solely on post-training quantization rather than training optimization. Uses a specialized approach (Haar wavelet transforms) that may have limited general applicability. Doesn't address multi-GPU scalability directly. Lacks discussion of latency improvements, which is a key optimization metric. The extreme 1-bit quantization focus might not represent practical quantization scenarios.",
      "Summary": "HBLLM introduces a wavelet-enhanced 1-bit post-training quantization method for LLMs using Haar wavelet transforms and structure-aware grouping strategies. The approach demonstrates state-of-the-art performance in extreme quantization, maintaining good model performance on LLaMA2-13B (perplexity of 6.71) while using only 1.08 bits for weight storage. This contributes to memory-efficient inference but doesn't address broader optimization concerns like training efficiency or multi-GPU scalability."
    }
  },
  {
    "id": "LSCAG7LPz4",
    "title": "Flexible Realignment of Language Models",
    "abstract": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates **Training-time Realignment (TrRa)**, which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by **54.63%** on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B’s **33.86%**. \nTo complement TrRa during inference, we introduce a layer adapter that enables **smooth Inference-time Realignment (InRa)**. This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even **during inference**. By encouraging deeper reasoning, it even surpassed its original performance.",
    "authors": [
      "~Wenhong_Zhu1",
      "~Ruobing_Xie2",
      "~Weinan_Zhang1",
      "~Rui_Wang10"
    ],
    "pdf": "/pdf/c517131ed7fac07b99d3c96c7a1d4a4646230c50.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Addresses both training and inference optimization for language models",
        "Achieves 54.63% reduction in token usage without performance degradation, directly impacting computational efficiency",
        "Enables flexible alignment control during inference with InRa",
        "Allows models to support both fast and slow thinking, with direct implications for latency optimization",
        "Describes efficient methods for processing embeddings during inference",
        "Maintains performance while improving efficiency"
      ],
      "Irrelevant Aspects": [
        "Focuses primarily on alignment optimization rather than computational optimization",
        "Does not explicitly focus on GPU utilization or scalability",
        "Lacks detailed analysis of throughput metrics",
        "No explicit discussion of distributed training or inference"
      ],
      "Summary": "This paper introduces a flexible realignment framework for language models with both training-time and inference-time components. While its primary focus is on alignment optimization, it achieves significant computational efficiency improvements, including 54.63% reduction in token usage and support for both fast and slow thinking during inference. These efficiency gains are relevant to the research interest on training and inference optimization, though the paper doesn't explicitly focus on GPU utilization, scalability, or throughput metrics."
    }
  },
  {
    "id": "tiGFiCrmKm",
    "title": "Finding Low-Rank Matrix Weights in DNNs via Riemannian Optimization: RAdaGrad and RAdamW",
    "abstract": "Finding low-rank matrix weights is a key technique for addressing the high memory usage and computational demands of large models. Most existing algorithms rely on the factorization of the low-rank matrix weights, which is non-unique and redundant. Their convergence is slow especially when the target low-rank matrices are ill-conditioned, because the convergence rate depends on the condition number of the Jacobian operator for the factorization and the Hessian of the loss function with respect to the weight matrix. To address this challenge, we adopt the Riemannian gradient descent (RGD) algorithm on the Riemannian manifold of fixed-rank matrices to update the entire low-rank weight matrix. This algorithm completely avoids the factorization, thereby eliminating the negative impact of the Jacobian condition number. Furthermore, by leveraging the geometric structure of the Riemannian manifold and selecting an appropriate metric, it mitigates the negative impact of the Hessian condition number. Ultimately, this results in our two plug-and-play optimizers: RAdaGrad and RAdamW, which are RGD with metrics adapted from AdaGrad and AdamW and restricted to the manifold. Our algorithms can be seamlessly integrated with various deep neural network architectures without any modifications. We evaluate the effectiveness of our algorithms through fine-tuning experiments on large language models and diffusion models. Experimental results consistently demonstrate that our algorithms provide superior performance compared to state-of-the-art methods. Additionally, our algorithm is not only effective for fine-tuning large models but is also applicable to deep neural network (DNN) compression.",
    "authors": [
      "~Fengmiao_Bian1",
      "~Jinyang_ZHENG1",
      "~Ziyun_Liu2",
      "~Jianzhou_Luo1",
      "~Jian-Feng_Cai4"
    ],
    "pdf": "/pdf/0d8a29160fba56f7293ff03be7d7b5756c1d2f26.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses optimization of large language models through low-rank matrix weights, which reduces memory usage and computational demands. The methods are evaluated on LLM fine-tuning and provide parameter efficiency. The algorithms are described as plug-and-play and applicable to various DNN architectures. The approach of avoiding factorization could lead to better computational efficiency. The paper addresses key challenges in scaling large models by reducing their parameter footprint.",
      "Irrelevant Aspects": "The paper focuses heavily on Riemannian optimization mathematics rather than practical GPU utilization techniques. It doesn't explicitly address inference optimization, latency measurements, or throughput improvements. The inclusion of diffusion models is outside the LLM focus area. The approach emphasizes parameter efficiency over computational optimization techniques like quantization or distillation that directly impact GPU performance. No explicit discussion of hardware-specific optimizations.",
      "Summary": "This paper introduces RAdaGrad and RAdamW, Riemannian optimization algorithms that directly update low-rank weight matrices without factorization, improving convergence especially for ill-conditioned matrices. The methods show superior performance in fine-tuning large language models and can be applied without architecture modifications. While relevant to LLM optimization through parameter efficiency, the paper's theoretical focus on Riemannian manifolds and lack of explicit GPU utilization or throughput analysis make it moderately relevant to practical inference and training acceleration research."
    }
  },
  {
    "id": "WDAKFpWftI",
    "title": "NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs",
    "abstract": "Meeting service-level objectives (SLOs) in Large Language Models (LLMs) serving is critical, but managing the high variability in load presents a significant challenge. Recent advancements in FP8 inference, backed by native hardware support, offer a potential solution: executing FP16 models by default, while switching to FP8 models during sudden load surges to achieve higher throughput at the cost of a slight quality degradation. Although this approach facilitates effective SLO management, it introduces additional memory overhead due to storing two versions of the same model. In response, this paper proposes NestedFP, an LLM serving technique that supports both FP16 and FP8 models in a memoryefficient manner by overlaying FP8 parameters onto FP16 parameters, allowing both models to share the same FP16 memory footprint. By leveraging a compact data format for the overlay and a specialized GEMM kernel optimized for this format, NestedFP ensures minimal degradation in both model quality and inference throughput across both FP8 and FP16 modes. NestedFP provides a flexible platform for dynamic, SLO-aware precision selection. The code is available at https://github.com/SNU-ARC/NestedFP.",
    "authors": [
      "~Haeun_Lee1",
      "~Omin_Kwon1",
      "~Yeonhong_Park1",
      "~Jae_W._Lee1"
    ],
    "pdf": "/pdf/8912e212d6374b90cdce4e6533f0d29c0d046ff9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory-efficient multi-precision inference, Specialized GEMM kernels, Dynamic precision selection based on load, SLO-aware serving strategies, FP8 inference optimization, GPU utilization improvements",
      "Irrelevant Aspects": "Doesn't address training optimization, Limited focus on distributed systems scaling, Doesn't discuss model architecture optimizations, Limited to precision-based optimizations",
      "Summary": "The paper presents NestedFP, a technique for memory-efficient dual-precision (FP16/FP8) LLM inference that overlays FP8 parameters onto FP16 memory, enabling dynamic precision switching without memory overhead. This approach helps manage SLOs during load variations by allowing higher throughput FP8 inference during surges while maintaining quality through FP16 during normal loads."
    }
  },
  {
    "id": "XUmGMBRv4M",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "abstract": "We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71$\\times$ speedup in inference latency and 35$\\times$ lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.",
    "authors": [
      "~Akhiad_Bercovich1",
      "~Mohammed_Dabbah1",
      "~Omri_Puny1",
      "~Ido_Galil1",
      "~Amnon_Geifman1",
      "~Yonatan_Geifman1",
      "~Izhak_Golan1",
      "~Ehud_Dov_Karpas1",
      "~Itay_Levy1",
      "~Zach_Moshe1",
      "~Najeeb_Nabwani1",
      "~Tomer_Ronen1",
      "~Itamar_Schen1",
      "~Ido_Shahaf1",
      "~Oren_Tropp1",
      "~Ran_Zilberstein1",
      "~Ran_El-Yaniv1"
    ],
    "pdf": "/pdf/241a1c5017cb5e9aa367d720bfee6b65a69053e8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference latency reduction, which is a key concern for my research interest. It focuses on GPU utilization through parallelization of FFN layers. The paper presents empirical results showing 1.71× speedup in inference latency and 35× lower per-token cost, which are significant performance improvements. It discusses scalability by applying the technique to a very large model (Llama-3.1-405B). The approach involves architectural optimization, which is central to inference optimization. It mentions maintaining model behavior while improving performance, which is crucial for practical deployment.",
      "Irrelevant Aspects": "The paper appears to focus primarily on inference optimization rather than training optimization, though my interests include both. There doesn't seem to be direct discussion of GPU utilization metrics, though the speedup implies improved utilization. The methodology section is not detailed in the abstract, so it's unclear how principled their approach is.",
      "Summary": "This paper is highly relevant to my research interests as it directly addresses inference optimization in large language models through architectural changes. It presents significant latency improvements (1.71× speedup) and cost reductions (35× lower per-token cost) while maintaining model performance. The approach involves parallelizing sequential FFN layers, which directly impacts GPU utilization and throughput. The application to a 405B parameter model demonstrates scalability of the approach."
    }
  },
  {
    "id": "phWwTrrJFv",
    "title": "Kinetics: Rethinking Test-Time Scaling Law",
    "abstract": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. The Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold (14B) than on smaller ones. A key reason is that in test-time scaling, attention—rather than parameter count—emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60-point gains in low-cost regimes and over 5-point gains in high-cost regimes for problem-solving accuracy on AIME and LiveCodeBench. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training where parameter scaling saturates, test-time accuracy continues to improve through increased generation.",
    "authors": [
      "~Ranajoy_Sadhukhan1",
      "~Zhuoming_Chen1",
      "~Haizhong_Zheng1",
      "~Beidi_Chen1"
    ],
    "pdf": "/pdf/94d7c92831c0cda98f8a44aa8d337af43e865c73.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Test-time scaling optimization, compute and memory efficiency analysis, model size considerations across 0.6B to 32B parameters, sparse attention implementation for efficiency, resource allocation guidance, attention as dominant cost factor in inference",
      "Irrelevant Aspects": "Limited GPU-specific implementation details, primary focus on accuracy rather than throughput/latency metrics, insufficient architectural details of sparse attention from systems perspective",
      "Summary": "The paper presents a new Kinetics Scaling Law for test-time computation that incorporates both compute and memory access costs, revealing that test-time scaling is more effective on models above 14B parameters. The key insight is that attention becomes the dominant cost factor during inference rather than parameter count. The authors propose sparse attention as a solution to enable more efficient resource utilization, demonstrating significant accuracy improvements on problem-solving benchmarks. This work directly informs efficient inference system design but lacks detailed hardware-specific optimizations and comprehensive throughput/latency benchmarks."
    }
  },
  {
    "id": "LZrRvYBqsJ",
    "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction",
    "abstract": "The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36\\%pt performance increase, recovering 88\\% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5\\% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.",
    "authors": [
      "~Jeffrey_Willette1",
      "~Heejun_Lee1",
      "~Sung_Ju_Hwang1"
    ],
    "pdf": "/pdf/6fbc5290ef9f74929813563e512225b32f29ffe5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses quadratic complexity in transformer attention mechanisms, directly impacting inference efficiency. It focuses on sparse attention inference methods which reduce computational burden and improve speed. The method achieves 32x speedup over Flash Attention 2 for 1M token sequences, demonstrating significant performance improvements. It maintains 98.5% sparsity while addressing performance degradation typical of sparse attention methods. The delta correction technique solves distributional shift issues that previously limited practical adoption of sparse attention.",
      "Irrelevant Aspects": "The paper focuses primarily on algorithmic improvements rather than system-level optimizations. Limited discussion of GPU-specific optimizations or implementation details for different hardware. No coverage of training optimizations (only inference). No mention of scaling to multiple GPUs or distributed inference systems. Limited discussion of memory optimization strategies beyond attention computation.",
      "Summary": "This paper introduces Delta Attention, a novel method to correct distributional shifts in sparse attention inference. It addresses the quadratic complexity of standard attention mechanisms while maintaining accuracy, achieving 32x speedup over Flash Attention 2 for long sequences. The approach can be applied on top of any sparse attention method, recovering 88% of quadratic attention accuracy while maintaining high sparsity. This work is highly relevant to inference optimization for large language models, though it focuses more on algorithmic improvements than system-level optimizations."
    }
  },
  {
    "id": "l42UGsdrNn",
    "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
    "abstract": "With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models.\nOur approach, X-EcoMLA, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers.\nX-EcoMLA achieves Transformer-level accuracy with near-SSM efficiency using only 7–11 billion training tokens (compared to the trillions required for pre-training) and an 8B teacher. Moreover, it dramatically reduces KV cache size—down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively—while preserving 100%, 100%, and over 97% of average zero-shot performance on LM Harness tasks.\nCompared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, our approach consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, X-EcoMLA-8B surpasses Minitron-8B in few-shot accuracy by 7%, while using 8× fewer training tokens, over 12× smaller KV cache, and a smaller teacher (8B vs. 15B). \nIt also achieves 1.4x–3.3x higher throughput (tokens/s) than MambaInLlama. The source code is\nreleased at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
    "authors": [
      "~Mingyu_Yang5",
      "~Mehdi_Rezagholizadeh1",
      "~Guihong_Li1",
      "~Vikram_Appia1",
      "~Emad_Barsoum1"
    ],
    "pdf": "/pdf/ba9ef0b7769e0db15d03dbd4788db3287b2d18a4.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Inference efficiency improvements, reduced KV cache size (2-3.9% of original), higher throughput (1.4x-3.3x), hybrid model combining SSMs and MLA layers, efficient knowledge transfer from pre-trained Transformers, reduced training requirements (7-11 billion tokens vs trillions), smaller teacher model usage (8B vs 15B), improved GPU utilization implied by efficiency gains, model compression and optimization techniques, post-training pipeline optimizations",
      "Irrelevant Aspects": "Limited discussion on system-level optimizations specific to GPU hardware, no detailed exploration of inference serving systems or deployment infrastructure, focus is on a specific hybrid approach rather than broader optimization techniques",
      "Summary": "This paper presents X-EcoMLA, a family of hybrid language models that combines State Space Models and Multi-head Latent Attention layers to achieve Transformer-level accuracy with near-SSM efficiency. The approach dramatically reduces KV cache requirements while maintaining high performance on LM Harness tasks. The models achieve significantly higher throughput compared to alternatives, use fewer training tokens, and require smaller teacher models, directly addressing key challenges in LLM inference optimization, particularly memory efficiency and throughput improvements."
    }
  },
  {
    "id": "tcisuhGsQZ",
    "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
    "abstract": "Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.",
    "authors": [
      "~Yuan_Feng10",
      "~Junlin_Lv1",
      "~Yukun_Cao1",
      "~Xike_Xie1",
      "~S_Kevin_Zhou1"
    ],
    "pdf": "/pdf/f766c4a703d424c73c10a5600d7340165f8e8f05.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on LLM inference efficiency optimization, addresses KV cache management crucial for memory efficiency, proposes adaptive budget allocation strategy, claims to improve both quality and efficiency (throughput/latency), offers plug-and-play benefits for practical implementation, extensive evaluation across multiple benchmarks",
      "Irrelevant Aspects": "Primarily focused on cache eviction rather than other optimization techniques like quantization or model parallelism, doesn't directly address hardware-specific optimizations",
      "Summary": "Ada-KV presents an adaptive budget allocation strategy for KV cache eviction during LLM inference. Unlike prior methods using uniform allocation across attention heads, Ada-KV optimizes allocation based on individual head characteristics, established through theoretical bounds, demonstrating substantial improvements over existing methods across multiple benchmarks."
    }
  },
  {
    "id": "QjJHUR0sXU",
    "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings",
    "abstract": "In this paper, we study the visual redundancy problem of multimodal large language models (MLLMs) from the perspective of attention behaviors. \nVia extensive empirical experiments, we observe and conclude three main inference stages of MLLMs:\n(i) Early fusion between tokens is first accomplished quickly. \n(ii) Intra-modality modeling then comes to play. \n(iii) Multimodal reasoning resumes and lasts until the end of inference. \nIn particular, we reveal that visual tokens will stop contributing to reasoning when the text tokens receive enough image information.\nBased on this observation, we propose an effective method to improve the efficiency of MLLMs, termed dynamic visual-token exit (DyVTE), which is orthogonal but collaborative to previous token-wise visual compression methods.\nTo validate the efficacy of DyVTE, we apply it to a set of MLLMs, including LLaVA, VILA, EAGLE and InternVL.\nThe experimental results not only show the effectiveness of our DyVTE in improving MLLMs' efficiency, e.g., DyVTE reduces the computation overhead of LLaVA-1.5 by up to 45.7% without performance drop, but also reveal a general pattern across multiple MLLMs, well facilitating the in-depth analysis of MLLMs. \nOur code is anonymously released at https://anonymous.4open.science/r/AnonymousDyVTE-26AB/.",
    "authors": [
      "~Qiong_Wu2",
      "~Wenhao_Lin1",
      "~Yiyi_Zhou1",
      "~Weihao_Ye2",
      "~Zhanpeng_Zeng1",
      "~Xiaoshuai_Sun3",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/d5416d1dc64a56c09d035bfb47e8889e59d6edc6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization for multimodal large language models, which is directly relevant to my expertise in LLM inference optimization. The proposed DyVTE method reduces computation overhead by up to 45.7% without performance drop, which relates to better GPU utilization. The approach is applied to multiple MLLMs (LLaVA, VILA, EAGLE, InternVL), showing broad applicability. The method identifies when visual tokens stop contributing to reasoning, reducing unnecessary computation, which is a key optimization strategy. It's described as orthogonal to existing compression methods, suggesting it could be combined with other optimization techniques.",
      "Irrelevant Aspects": "The paper focuses specifically on multimodal LLMs rather than general LLMs, which is a narrower domain. It doesn't address training optimization, only inference. There's no explicit mention of GPU utilization, scalability, throughput, or latency metrics in the abstract. While it mentions reducing computation overhead, it doesn't specifically quantify improvements in throughput or latency, which are key metrics in my research interest.",
      "Summary": "This paper presents a method called Dynamic Visual-Token Exit (DyVTE) to accelerate multimodal large language models by identifying when visual tokens stop contributing to reasoning and allowing them to exit early. Based on empirical findings about attention behaviors in MLLMs, the authors observe that visual tokens become unnecessary once text tokens have received sufficient image information. The method is applied to several MLLMs and achieves up to 45.7% reduction in computation overhead without performance drop. While the paper is relevant to inference optimization and GPU utilization, it's limited to the multimodal domain and doesn't explicitly address training optimization or provide specific metrics for throughput and latency."
    }
  },
  {
    "id": "twSE0WA1vh",
    "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation",
    "abstract": "Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10× higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.",
    "authors": [
      "~Liliang_Ren1",
      "~Congcong_Chen3",
      "~Haoran_Xu3",
      "~Young_Jin_Kim1",
      "~Adam_Atkinson1",
      "~Zheng_Zhan3",
      "~Jiankai_Sun6",
      "~Baolin_Peng2",
      "~Liyuan_Liu3",
      "~Shuohang_Wang1",
      "~Hao_Cheng4",
      "~Jianfeng_Gao1",
      "~Weizhu_Chen1",
      "~yelong_shen1"
    ],
    "pdf": "/pdf/6e476e748ca6dd590edc6d7b9f3ecc59e47dac70.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces architectural improvements for LLM efficiency with focus on decoding throughput optimization, claims of linear pre-filling time complexity, up to 10× higher decoding throughput on long generation tasks, implementation in vLLM framework, and specifically addresses GPU utilization through efficient memory sharing via the Gated Memory Unit (GMU). The paper demonstrates significant improvements in inference efficiency while maintaining or improving performance on reasoning tasks.",
      "Irrelevant Aspects": "The paper primarily focuses on architectural innovation rather than low-level implementation details of GPU utilization. It lacks detailed discussion of specific parallelization strategies across multiple GPUs, memory management techniques, or quantization methods. While it addresses inference throughput, it provides limited information on latency measurements.",
      "Summary": "This paper presents SambaY, a decoder-hybrid-decoder architecture with a novel Gated Memory Unit (GMU) for efficient memory sharing across layers. It combines State Space Models with decoder-decoder architecture to significantly improve decoding efficiency (up to 10× throughput) while maintaining linear pre-filling complexity. The architecture eliminates the need for explicit positional encoding and demonstrates superior performance scalability under large-scale compute regimes. The paper is highly relevant to LLM inference optimization research, showing substantial throughput improvements for long-context generation tasks."
    }
  },
  {
    "id": "tDG6bY48ch",
    "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation",
    "abstract": "Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
    "authors": [
      "~Ao_Wang2",
      "~Hui_Chen7",
      "~Jianchao_Tan1",
      "~Kefeng_Zhang1",
      "~Xunliang_Cai1",
      "~Zijia_Lin1",
      "~Jungong_Han2",
      "~Guiguang_Ding1"
    ],
    "pdf": "/pdf/2b8e3a2e7fb8f56c725d4a9f3068a2862e9a50a2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "KV cache optimization for inference efficiency, Adaptive layer-wise retention for better memory management, Focus on reducing computational and memory overhead, State-of-the-art performance in efficiency-quality trade-offs, Practical application orientation",
      "Irrelevant Aspects": "Primarily focused on vision-language models rather than pure language models, Only addresses inference, not training optimization, Abstract lacks specific mentions of GPU utilization metrics or scalability analysis",
      "Summary": "This paper introduces PrefixKV, an adaptive KV cache optimization method specifically designed for vision-language models. By implementing layer-wise KV retention based on binary search, it aims to reduce memory usage and improve inference efficiency while maintaining generation quality. The approach is directly relevant to inference optimization for large models, though its vision-language focus narrows its applicability to pure language models. The method appears promising for improving practical deployment scenarios where memory constraints are significant, but the abstract doesn't provide details on GPU utilization, throughput, or latency improvements specifically."
    }
  },
  {
    "id": "kke9TwtKi0",
    "title": "Subspace Networks: Scaling Decentralized Training with Communication-Efficient Model Parallelism",
    "abstract": "Scaling models has led to significant advancements in deep learning, but training these models in decentralized settings remains challenging due to communication bottlenecks. While existing compression techniques are effective in data-parallel, they do not extend to model parallelism. Unlike data-parallel training, where weight gradients are exchanged, model-parallel requires compressing activations and activation gradients as they propagate through layers, accumulating compression errors. We propose a novel  compression algorithm that  compresses both forward and backward passes, enabling up to 99% compression with no convergence degradation with negligible memory/compute overhead. By leveraging a recursive structure in transformer networks, we predefine a low-dimensional subspace to confine the activations and gradients, allowing full reconstruction in subsequent layers. Our method achieves up to 100x improvement in communication efficiency and enables training billion-parameter-scale models over low-end GPUs connected via consumer-grade internet speeds as low as 80Mbps, matching the convergence of centralized datacenter systems with 100Gbps connections with model parallel.",
    "authors": [
      "~Sameera_Ramasinghe1",
      "~Thalaiyasingam_Ajanthan1",
      "~Gil_Avraham1",
      "~Yan_Zuo1",
      "~Alexander_Long1"
    ],
    "pdf": "/pdf/df87e5b7f0958b6314d2b85234340a0e5080cbb7.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization, scalability to billion-parameter models, GPU utilization, communication efficiency (100x improvement), model parallelism, transformer networks, throughput improvements",
      "Irrelevant Aspects": "Focus on decentralized settings with consumer-grade internet rather than datacenter environments, no explicit mention of inference optimization, lack of explicit latency improvements",
      "Summary": "The paper presents a novel compression algorithm for model parallelism that enables efficient training of large language models. It achieves up to 99% compression without convergence degradation and leverages the recursive structure of transformers to define low-dimensional subspaces. The method allows training billion-parameter models on low-end GPUs with consumer-grade internet connections, offering up to 100x communication efficiency improvement. While highly relevant to training optimization and scalability, it doesn't address inference optimization and focuses on decentralized environments rather than traditional datacenter settings."
    }
  },
  {
    "id": "x3qnrhfhX0",
    "title": "Mixtures of Subspaces for Bandwidth Efficient Context Parallel Training",
    "abstract": "Pretraining language models with extended context windows enhances their ability to leverage rich information during generation. Existing methods split input sequences into chunks, broadcast them across multiple devices, and compute attention block by block which incurs significant communication overhead. While feasible in high-speed clusters, these methods are impractical for decentralized training over low-bandwidth connections. We propose a compression method for communication-efficient context parallelism in decentralized settings, achieving a remarkable compression rate of over 95% with negligible overhead and no loss in convergence. Our key insight is to exploit the intrinsic low-rank structure of activation outputs by dynamically constraining them to learned mixtures of subspaces via efficient reparameterizations. We demonstrate scaling billion-parameter decentralized models to context lengths exceeding 100K tokens on networks as slow as 300Mbps, matching the wall-clock convergence speed of centralized models on 100Gbps interconnects.",
    "authors": [
      "~Sameera_Ramasinghe1",
      "~Thalaiyasingam_Ajanthan1",
      "~Hadi_Mohaghegh_Dolatabadi2",
      "~Gil_Avraham1",
      "~Violetta_Shevchenko1",
      "~Yan_Zuo1",
      "~Chamin_P_Hewa_Koneputugodage1",
      "~Alexander_Long1"
    ],
    "pdf": "/pdf/140a635e9c87920004aa398a5608662c99b82a0b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents a novel compression technique for context parallel training of large language models, achieving over 95% compression with negligible impact on convergence. This directly addresses scalability challenges in training models with extended contexts (100K+ tokens). The method improves communication efficiency in distributed training, which is a critical factor for achieving higher throughput. The approach of exploiting low-rank structure in activation outputs could have broader applications for memory optimization in both training and inference scenarios.",
      "Irrelevant Aspects": "The paper focuses primarily on decentralized training scenarios over low-bandwidth connections, which may not directly apply to centralized high-bandwidth clusters. It doesn't explicitly address GPU utilization optimization or inference-specific challenges. The approach may have limited relevance for latency-critical inference scenarios where model size and memory bandwidth are different concerns.",
      "Summary": "This paper introduces a compression method for context parallel training that leverages the low-rank structure of activation outputs to reduce communication overhead by over 95%. It enables training billion-parameter models with 100K+ token contexts on slow networks (300Mbps) while matching the convergence speed of centralized models on fast interconnects. While primarily focused on decentralized training, the techniques presented could have valuable applications for improving scalability and throughput in broader distributed training scenarios."
    }
  },
  {
    "id": "Q3qAsZAEZw",
    "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
    "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. \nThis issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size.\nWe trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. \nThis work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge.\nOur analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices.\nInspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
    "authors": [
      "~Jiayi_Yuan1",
      "~Hao_Li102",
      "~Xinheng_Ding1",
      "~Wenya_Xie1",
      "~Yu-Jhe_Li1",
      "~Wentian_Zhao3",
      "~Kun_Wan1",
      "~Jing_Shi1",
      "~Xia_Hu4",
      "~Zirui_Liu1"
    ],
    "pdf": "/pdf/0025fad6ba784349766905dc7158027e8500db26.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "LLM inference optimization, GPU utilization considerations, numerical precision optimization (bfloat16 vs FP32), scalability across GPU configurations, memory efficiency vs numerical stability trade-offs, reproducibility in production systems",
      "Irrelevant Aspects": "Limited focus on raw throughput and latency optimization, minimal coverage of training optimization, not deeply addressing architectural optimizations beyond precision handling, focuses more on reproducibility than performance metrics",
      "Summary": "This paper investigates nondeterminism in LLM inference caused by numerical precision issues, showing how system configurations affect outputs. It proposes LayerCast, a lightweight inference pipeline balancing memory efficiency (16-bit storage) with numerical stability (FP32 computation). While addressing important reproducibility concerns for scalable LLM systems, it doesn't primarily focus on the performance optimization metrics most central to my research interests."
    }
  },
  {
    "id": "1ZuzFUMtx6",
    "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens",
    "abstract": "Chain-of-Thought (CoT) enhances the performance of Large Language Models (LLMs) on reasoning tasks by encouraging step-by-step solutions. However, the verbosity of CoT reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT reasoning by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed **SemCoT**. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.",
    "authors": [
      "~Yinhan_He1",
      "~Wendy_Zheng1",
      "~Yaochen_Zhu1",
      "~Zaiyi_Zheng1",
      "~Lin_Su3",
      "~Sriram_Vasudevan2",
      "~Qi_Guo9",
      "~Liangjie_Hong1",
      "~Jundong_Li2"
    ],
    "pdf": "/pdf/39a70c107ed5819e3435a94da518615c9e3db25c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on accelerating Chain-of-Thought reasoning in LLMs, which directly addresses inference optimization and latency reduction - key aspects of my research interest. It introduces implicit reasoning techniques to bypass certain LLM components and reduce reasoning length, improving computational efficiency. The approach includes knowledge distillation to create lightweight models for faster token generation, which aligns with my focus on optimizing model performance.",
      "Irrelevant Aspects": "The paper doesn't specifically address GPU utilization or hardware-specific optimizations, which are central to my research. There's no discussion of scalability across multiple GPUs or distributed systems. The focus is narrow on Chain-of-Thought reasoning rather than broader model optimization techniques. Training optimization is not the primary focus, though distillation is mentioned for a specific purpose.",
      "Summary": "SemCoT proposes a framework to accelerate Chain-of-Thought reasoning by using semantically-aligned implicit tokens instead of explicit reasoning steps. It addresses efficiency concerns by reducing reasoning length and optimizing token generation speed through a lightweight language model. While directly relevant to inference optimization and latency reduction, the paper doesn't explore GPU utilization or scalability aspects that would make it more aligned with my core research interests."
    }
  },
  {
    "id": "n4V3MSqK77",
    "title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents",
    "abstract": "LLM-based agent applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs and latency due to extensive planning and reasoning requirements. \nExisting LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agent applications where outputs depend on external data and environmental contexts. \nWe propose **Agentic Plan Caching (APC)**, a novel **test-time memory** that extracts, stores, adapts, and reuses structured plan templates from planning stages of agent applications across semantically similar tasks to reduce the cost and latency of serving. \nUnlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. \nEvaluation across multiple real-world agent applications shows that our system can reduce costs by 50.31\\% and latency by 27.28\\% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.",
    "authors": [
      "~Qizheng_Zhang1",
      "~Michael_Wornow1",
      "~Kunle_Olukotun1"
    ],
    "pdf": "/pdf/2362f2d245503d9d209dde5a45fbfbf3fb6a5990.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs, focusing on reducing latency and costs which are key metrics for my research. It proposes a test-time memory system that can improve serving infrastructure for LLMs, which relates to scalability. The approach could potentially improve GPU utilization by avoiding redundant computations during planning and reasoning stages of LLM agents.",
      "Irrelevant Aspects": "The paper doesn't explicitly address training optimization, which is one of my primary interests. It doesn't specifically discuss GPU utilization or throughput metrics, though improved efficiency would likely benefit these areas. The focus is specifically on agent applications rather than general LLM inference optimization.",
      "Summary": "This paper introduces 'Agentic Plan Caching (APC),' a novel test-time memory system for LLM agents that extracts, stores, adapts, and reuses structured plan templates to reduce the cost and latency of serving. Unlike traditional semantic caching, the system extracts plan templates from completed executions and uses lightweight models to adapt them to new, semantically similar tasks. Evaluations show the system can reduce costs by 50.31% and latency by 27.28% on average while maintaining performance across multiple real-world agent applications."
    }
  },
  {
    "id": "oDcAGSXZZP",
    "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
    "abstract": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.",
    "authors": [
      "~Jingbo_Yang3",
      "~Bairu_Hou2",
      "~Wei_Wei15",
      "~Yujia_Bao1",
      "~Shiyu_Chang2"
    ],
    "pdf": "/pdf/1313459390a6f2f26aba7c6b4aae71a261d1890c.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on inference optimization for large language models, which is a core research interest. It addresses reducing computational redundancy during inference by reusing KV caches, directly targeting efficiency improvements. The approach claims to reduce time-to-first-token by up to 96%, which is a significant latency improvement. The method aims to make LLM inference more scalable and efficient, aligning with the scalability interest. This technique would likely improve GPU utilization by avoiding recomputation of the same context across multiple queries. Additionally, the paper mentions that KVLink can be combined with KV cache compression for further optimization.",
      "Irrelevant Aspects": "The paper does not focus on training optimization, only addressing inference efficiency. It doesn't explicitly discuss GPU utilization metrics or throughput measurements. The main focus is on a specific technique (KV cache reuse) rather than general optimization principles that might be applicable across different scenarios. The research is narrowly focused on the KV cache management aspect without exploring broader system optimization considerations.",
      "Summary": "KVLink presents a specialized but highly relevant technique for optimizing LLM inference by reusing precomputed KV caches across queries with overlapping context. The approach addresses a common inefficiency in LLM deployment by eliminating redundant computation of the same context. With substantial claimed latency improvements (up to 96% reduction in time-to-first-token) and efficiency gains, this paper directly contributes to the goal of achieving better performance, scalability, and resource utilization in LLM systems. While it doesn't cover training optimization, its focus on inference efficiency makes it highly valuable for research in LLM system optimization."
    }
  },
  {
    "id": "hzMkfIrdDT",
    "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models",
    "abstract": "Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs).\nHowever, we observe that even state-of-the-art PRMs can be poorly calibrated. \nSpecifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory.\nTo address this, we present a calibration approach—performed via quantile regression—that adjusts PRM outputs to better align with true success probabilities. \nLeveraging these calibrated success estimates and their associated confidence bounds, we introduce an instance-adaptive scaling (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer.\nUnlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.",
    "authors": [
      "~Young-Jin_Park1",
      "~Kristjan_Greenewald1",
      "~Kaveh_Alim1",
      "~Hao_Wang22",
      "~Navid_Azizan1"
    ],
    "pdf": "/pdf/18533f919a9403854c7955628ad3488bf682b694.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs through calibrated process reward models and dynamic compute allocation. The instance-adaptive scaling framework addresses resource efficiency by adjusting compute budget based on confidence estimates, which could improve overall inference efficiency and reduce costs. This approach has implications for inference optimization and resource utilization in LLM systems.",
      "Irrelevant Aspects": "The paper primarily focuses on output quality and model calibration rather than system-level performance metrics. There's limited discussion of GPU utilization optimization, distributed systems scalability, or specific throughput and latency improvements. The calibration method is more about improving reasoning reliability than computational efficiency.",
      "Summary": "This paper presents a calibration approach for process reward models and an instance-adaptive scaling framework that dynamically adjusts compute budget during LLM inference. While it contributes to inference optimization and resource efficiency, its focus on output quality and uncertainty calibration makes it moderately relevant to my research interests in system-level LLM optimization for better GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "tle6DK5Tad",
    "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization",
    "abstract": "Large Vision-Language Models (LVLMs) have shown impressive performance across multi-modal tasks by encoding images into thousands of tokens. However, the large number of image tokens results in significant computational overhead, and the use of dynamic high-resolution inputs further increases this burden. Previous approaches have attempted to reduce the number of image tokens through token pruning, typically by selecting tokens based on attention scores or image token diversity. Through empirical studies, we observe that existing methods often overlook the joint impact of pruning on both the current layer's output (local) and the outputs of subsequent layers (global), leading to suboptimal pruning decisions. To address this challenge, we propose Balanced Token Pruning (BTP), a plug-and-play method for pruning vision tokens. Specifically, our method utilizes a small calibration set to divide the pruning process into multiple stages. In the early stages, our method emphasizes the impact of pruning on subsequent layers, whereas in the deeper stages, the focus shifts toward preserving the consistency of local outputs. Extensive experiments across various LVLMs demonstrate the broad effectiveness of our approach on multiple benchmarks. Our method achieves a 78\\% compression rate while preserving 96.7\\% of the original models' performance on average. Our code is available at https://github.com/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning.",
    "authors": [
      "~kaiyuan_Li1",
      "~Xiaoyue_Chen1",
      "~Chen_Gao3",
      "~Yong_Li7",
      "~Xinlei_Chen3"
    ],
    "pdf": "/pdf/3a885e73609444537c83531d0da901f7ab586557.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Directly addresses computational overhead in large models through token pruning",
        "Achieves 78% compression rate while preserving 96.7% of performance",
        "Plug-and-play method for inference optimization",
        "Focuses on reducing memory requirements for high-resolution inputs",
        "Multi-stage pruning approach that considers both local and global impact",
        "Potential to improve throughput and reduce latency in vision-language models"
      ],
      "Irrelevant Aspects": [
        "Specific to vision-language models rather than general language models",
        "No discussion of training optimization",
        "Doesn't address GPU-specific optimization techniques",
        "Limited mention of distributed computing or scaling across multiple GPUs",
        "No detailed analysis of actual throughput or latency improvements"
      ],
      "Summary": "This paper presents Balanced Token Pruning (BTP), a method for accelerating Vision-Language Models by strategically pruning image tokens. The approach considers both the immediate impact on the current layer and downstream effects, achieving significant compression (78%) while maintaining performance (96.7%). While focused on vision-language models rather than general language models, the token pruning techniques could have broader applications for inference optimization, GPU utilization, and reducing latency in large models."
    }
  },
  {
    "id": "ZdmmOAN4h3",
    "title": "Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining",
    "abstract": "Low-rank optimization has emerged as a promising approach to enabling memory-efficient training of large language models (LLMs). Existing low-rank optimization methods typically project gradients onto a low-rank subspace, reducing the memory cost of storing optimizer states. A key challenge in these methods is selecting suitable subspaces to ensure an effective optimization trajectory. Most existing approaches select the dominant subspace to preserve gradient information, as this intuitively provides the best approximation. However, we find that in practice, the dominant subspace stops changing during pretraining, thereby constraining weight updates to similar subspaces. In this paper, we propose importance sampling for low-rank optimization in LLM pretraining with a provable convergence guarantee, which the dominant subspace approach does not have. Empirically, we demonstrate that our method significantly outperforms previous methods in LLM pretraining tasks.",
    "authors": [
      "~Haochen_Zhang9",
      "~Junze_Yin1",
      "~Guanchu_Wang1",
      "~Zirui_Liu1",
      "~Lin_Yang12",
      "~Tianyi_Zhang6",
      "~Anshumali_Shrivastava1",
      "~Vladimir_Braverman1"
    ],
    "pdf": "/pdf/a5c087fda18ddf464374f4ba9f0eaf93e5414c84.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses LLM pretraining optimization, focusing on memory efficiency through low-rank optimization. This has clear implications for GPU utilization and scalability, as memory efficiency is a key bottleneck in training large models. The approach of projecting gradients onto a low-rank subspace to reduce optimizer state memory is directly relevant to resource utilization during training.",
      "Irrelevant Aspects": "The paper seems to focus solely on training optimization, not inference optimization. There's no explicit mention of latency improvements or direct throughput measurements. While it addresses memory efficiency, it doesn't specifically discuss GPU utilization strategies in detail. The importance sampling approach might add computational overhead that could offset memory benefits.",
      "Summary": "This paper presents a novel approach to low-rank optimization for LLM pretraining, addressing the issue of frozen subspaces in existing methods by using importance sampling. The method has provable convergence guarantees and claims to outperform previous approaches in pretraining tasks. While the focus is primarily on training efficiency rather than inference, the memory optimization strategies could have implications for the broader field of LLM optimization, particularly for resource-constrained environments."
    }
  },
  {
    "id": "RlqYCpTu1P",
    "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
    "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\n\nIn this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
    "authors": [
      "~Enzhe_Lu1",
      "~Zhejun_Jiang1",
      "~Jingyuan_Liu7",
      "~Yulun_Du1",
      "~Tao_Jiang16",
      "~Chao_Hong1",
      "~Shaowei_Liu1",
      "~Weiran_He2",
      "~Enming_Yuan1",
      "~Yuzhi_Wang1",
      "~Zhiqi_Huang2",
      "~Huan_Yuan1",
      "~Suting_Xu1",
      "~Xinran_Xu3",
      "~Guokun_Lai1",
      "~Yanru_Chen1",
      "~Huabin_Zheng1",
      "~Junjie_Yan5",
      "~Jianlin_Su1",
      "~Yuxin_Wu1",
      "~Yutao_Zhang1",
      "~Zhilin_Yang2",
      "~Xinyu_Zhou1",
      "~Mingxing_Zhang3",
      "~Jiezhong_Qiu1"
    ],
    "pdf": "/pdf/58c8bb4e2a6dc99072ee66e05abcf02af7db3e30.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Addresses quadratic computational complexity in attention mechanisms which directly impacts GPU utilization and scalability; Proposes MoBA which applies MoE principles to attention for efficiency gains; Offers transition between full and sparse attention which can optimize both training and inference performance; Demonstrates production deployment for long-context workloads showing practical value",
      "Irrelevant Aspects": "Primarily focused on long-context LLMs rather than general LLM optimization; No explicit mention of GPU utilization metrics in the abstract; More emphasis on architectural innovation than system-level optimizations",
      "Summary": "This paper introduces MoBA (Mixture of Block Attention), a novel approach that applies Mixture of Experts principles to attention mechanisms for long-context LLMs. It addresses the quadratic complexity problem in traditional attention, allowing models to autonomously determine attention patterns rather than relying on predefined biases. The approach enables seamless transition between full and sparse attention, enhancing efficiency while maintaining performance. With production deployment already implemented, MoBA presents a promising solution for computational efficiency in LLMs."
    }
  },
  {
    "id": "GJbrk55Bt8",
    "title": "List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression",
    "abstract": "We study a relaxation of the problem of coupling probability distributions — a list of samples is generated from one distribution and an *accept* is declared if any one of these samples is identical to the sample generated from the other distribution.\nWe propose a novel method for generating samples, which extends the Gumbel-max sampling suggested in Daliri et al. (2025) for coupling probability distributions. We also establish a corresponding lower bound on the acceptance probability, which we call the \\emph{list matching lemma}.\nWe next discuss two applications of our setup.\nFirst, we develop a new mechanism for multi-draft speculative sampling that is simple to implement and achieves performance competitive with baselines such as SpecTr and SpecInfer across a range of language tasks.\nOur method also guarantees a certain degree of *drafter invariance* with respect to the output tokens which is not supported by existing schemes.\nWe also provide a theoretical lower bound on the token level acceptance probability.\nAs our second application, we consider distributed lossy compression with side information in a setting where a source sample is compressed and available to multiple decoders, each with independent side information.\nWe propose a compression technique that is based on our generalization of Gumbel-max sampling and show that it provides significant gains in experiments involving synthetic Gaussian sources and the MNIST image dataset.",
    "authors": [
      "~Joseph_Rowan1",
      "~Buu_Phan3",
      "~Ashish_J_Khisti1"
    ],
    "pdf": "/pdf/74a2619228640bab3425872d8ea661d6b0a14289.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative decoding techniques which directly address LLM inference optimization. It proposes a novel method for multi-draft speculative sampling that achieves competitive performance with existing baselines like SpecTr and SpecInfer. The work includes theoretical guarantees on token level acceptance probability and introduces drafter invariance with respect to output tokens. These aspects are directly relevant to improving LLM throughput, reducing latency, and enhancing GPU utilization during inference.",
      "Irrelevant Aspects": "The second application of the work deals with distributed lossy compression with side information in a general setting, which is not specific to LLM optimization. Additionally, the experiments include synthetic Gaussian sources and MNIST image datasets rather than focusing exclusively on language model tasks. The theoretical foundations around list-level distribution coupling are more general than needed for the specific focus on LLM optimization.",
      "Summary": "This paper presents a novel approach to speculative decoding for LLM inference optimization through list-level distribution coupling. The proposed method extends Gumbel-max sampling for coupling probability distributions, providing theoretical guarantees on acceptance probability while maintaining drafter invariance. The work demonstrates competitive performance with existing speculative decoding methods across language tasks, making it highly relevant to research on improving LLM throughput, reducing latency, and enhancing GPU utilization. While the paper includes applications beyond LLMs in lossy compression, its primary contribution to speculative decoding makes it valuable for inference optimization research."
    }
  },
  {
    "id": "tIR9Naukr3",
    "title": "EUGens: Efficient, Unified and General Dense Layers",
    "abstract": "Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures.  To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, $\\textbf{E}$fficient, $\\textbf{U}$nified and $\\textbf{Gen}$eral dense layers (EUGens).  EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to $\\textbf{the first}$ unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to $\\textbf{27}$\\%) and memory efficiency (up to $\\textbf{30}$\\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.",
    "authors": [
      "~Sang_Min_Kim1",
      "~Byeongchan_Kim1",
      "~Arijit_Sehanobish1",
      "~Somnath_Basu_Roy_Chowdhury3",
      "~Rahul_Kidambi1",
      "~Dongseok_Shim1",
      "~Kumar_Avinava_Dubey1",
      "~Snigdha_Chaturvedi2",
      "~Min-hwan_Oh1",
      "~Krzysztof_Marcin_Choromanski1"
    ],
    "pdf": "/pdf/484f90110e0c072c495cddf24e0a09f293b9e8c6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on efficient neural networks addressing computation bottlenecks in fully-connected layers, which are crucial components in transformer architectures. It claims to improve inference efficiency by reducing complexity from quadratic to linear time, with reported improvements of up to 27% in inference speed and 30% in memory efficiency. The method is evaluated on Transformers and language model pre-training, which are directly relevant to large language model optimization. The layer-wise knowledge transfer technique that bypasses backpropagation could also contribute to training optimization.",
      "Irrelevant Aspects": "While the paper covers optimization relevant to LLMs, it also includes applications to 3D scene reconstruction and image classification which are less central to my specific focus. The paper may not deeply address distributed systems optimization or extreme scale-out scenarios typical of today's largest LLMs.",
      "Summary": "EUGens presents a method to optimize feedforward layers in neural networks by reducing computational complexity and parameter count, with demonstrated improvements in inference speed and memory efficiency. While not exclusively focused on large language models, it addresses key components of transformer architectures and shows promising results for language model applications. The paper is highly relevant to my interests in inference optimization for better GPU utilization, though it may not cover all aspects of LLM systems optimization."
    }
  },
  {
    "id": "a9MfGUHjF8",
    "title": "Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
    "abstract": "Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement—or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50\\%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models.",
    "authors": [
      "~Haotian_Luo1",
      "~Haiying_He1",
      "~Yibo_Wang7",
      "~Jinluan_Yang1",
      "~Rui_Liu34",
      "~Naiqiang_Tan1",
      "~Xiaochun_Cao3",
      "~Dacheng_Tao1",
      "~Li_Shen1"
    ],
    "pdf": "/pdf/a02fb3648324ee94fc387f1a68bfcb210d684f82.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on inference optimization for LLMs, reduces inference costs by 50%+ while maintaining performance, implements adaptive reasoning strategies to improve efficiency, directly addresses computational overhead of long reasoning chains, employs hybrid model approach combining different reasoning styles",
      "Irrelevant Aspects": "Primarily focused on mathematical reasoning tasks rather than general LLM optimization, limited discussion of specific GPU utilization metrics or throughput measurements",
      "Summary": "This paper is highly relevant to my research interests in LLM inference optimization. It introduces a sophisticated two-stage framework that adaptively selects between short and long reasoning paths based on input complexity, achieving significant inference cost reductions (>50%) while maintaining performance. The bi-level preference optimization approach is novel and directly addresses the efficiency-latency trade-off in LLM inference, which aligns perfectly with my focus on GPU utilization and scalability."
    }
  },
  {
    "id": "bFXbLQzRoZ",
    "title": "Power Lines: Scaling laws for weight decay and batch size in LLM pre-training",
    "abstract": "Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate η and weight decay λ. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B.  Recent work suggests the AdamW timescale, τ = B/(ηλD), should remain constant across training settings, and we verify the implication that optimal λ scales linearly with B, for a fixed N and D. However, as N and D scale, we show optimal τ obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict λopt in advance of large-scale training.  We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast to prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.",
    "authors": [
      "~Shane_Bergsma1",
      "~Nolan_Simran_Dey1",
      "~Gurpreet_Gosal2",
      "~Gavia_Gray1",
      "~Daria_Soboleva1",
      "~Joel_Hestness2"
    ],
    "pdf": "/pdf/f26602e9a24b6bf8d4e6a58a3b6aefecc2a58271.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM pre-training optimization, studying scaling laws for hyperparameters (learning rate, weight decay, batch size). It investigates how these parameters should scale with model size and dataset size, which is crucial for GPU utilization, scalability, and throughput optimization. The findings on optimal batch sizes and their scaling can directly contribute to more efficient training systems. The analysis of Pareto-optimal model and dataset sizes under time and compute constraints is highly relevant to resource-efficient training.",
      "Irrelevant Aspects": "The paper doesn't address inference optimization or latency reduction specifically. It focuses on theoretical scaling laws rather than implementation-specific GPU optimization techniques. There's no mention of novel hardware utilization strategies.",
      "Summary": "This paper is highly relevant to my research interests in LLM training optimization, particularly regarding GPU utilization and scalability. While it doesn't cover all aspects of my interests (especially inference optimization), its insights on training hyperparameter scaling laws directly contribute to optimizing resource utilization in large-scale language model training."
    }
  },
  {
    "id": "Wxh5Xz7NpJ",
    "title": "Remarkable Robustness of LLMs: Stages of Inference?",
    "abstract": "We investigate the robustness of Large Language Models (LLMs) to structural interventions by deleting and swapping adjacent layers during inference. Surprisingly, models retain 72–95\\% of their original top-1 prediction accuracy without any fine-tuning. We find that performance degradation is not uniform across layers: interventions to the early and final layers cause the most degradation, while the model is remarkably robust to dropping middle layers. This pattern of localized sensitivity motivates our hypothesis of four stages of inference, observed across diverse model families and sizes: (1) detokenization, where local context is integrated to lift raw token embeddings into higher-level representations; (2) feature engineering, where task- and entity-specific features are iteratively refined; (3) prediction ensembling, where hidden states are aggregated into plausible next-token predictions; and (4) residual calibration, where irrelevant features are suppressed to finalize the output distribution. Synthesizing behavioral and mechanistic evidence, we provide a hypothesis for interpreting depth-dependent computations in LLMs.",
    "authors": [
      "~Vedang_Lad1",
      "~Jin_Hwa_Lee1",
      "~Wes_Gurnee1",
      "~Max_Tegmark1"
    ],
    "pdf": "/pdf/7cac757c6fe3caa0014affc1447da1391c9e964a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly investigates LLM inference behavior and reveals that models are remarkably robust to dropping middle layers during inference. This finding could lead to significant inference optimizations by potentially skipping computational steps without substantial performance loss. The proposed four-stage inference model provides insights that could be leveraged to design more efficient inference pipelines. These discoveries could directly translate to improved throughput, reduced latency, and better GPU utilization by enabling selective computation strategies during inference.",
      "Irrelevant Aspects": "The paper doesn't directly address training optimization, which is part of my research interest. It focuses on model behavior rather than specific engineering implementations for production inference systems. There's limited discussion on how these findings translate to actual GPU utilization, memory footprint, or latency measurements in practical deployment scenarios. The paper doesn't provide concrete implementation details for leveraging these findings in real-world optimization systems.",
      "Summary": "This paper investigates the robustness of LLMs to structural interventions during inference, finding that models maintain 72-95% of original accuracy even when middle layers are removed. The authors identify four stages of inference with varying sensitivity across layers: detokenization, feature engineering, prediction ensembling, and residual calibration. These insights about model behavior during inference are highly relevant to my research on LLM inference optimization, as they suggest potential pathways to reduce computational requirements while maintaining performance, which directly impacts GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "miCXNqXyVS",
    "title": "Normalization in Attention Dynamics",
    "abstract": "We study the effect of normalization schemes on token representations in deep transformers. Modeling their evolution as interacting particles on the sphere, we show that normalization acts as a form of speed regulation. This perspective enables a unified analysis of several schemes---including **Post-LN**, **Pre-LN**, **Mix-LN**, **Peri-LN**, **nGPT**, and **LN-scaling**---revealing how they influence clustering dynamics and representation collapse. Our framework clarifies how different schemes shape token representations across layers and provides a principled basis for comparing them, identifying **Peri-LN** as a particularly effective choice.",
    "authors": [
      "~Nikita_Karagodin1",
      "~Shu_Ge1",
      "~Yury_Polyanskiy1",
      "~Philippe_Rigollet1"
    ],
    "pdf": "/pdf/f8d45e7d62c00e422536ec3877ae4d0719f5510c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper analyzes normalization schemes in transformers which are fundamental to efficient training and inference. Understanding how normalization affects token representation dynamics and stability can indirectly improve GPU utilization and training convergence speed. The identification of effective normalization schemes like Peri-LN can inform architectural choices that enhance model performance and potentially reduce computational requirements.",
      "Irrelevant Aspects": "The paper focuses on theoretical analysis of normalization rather than direct optimization of GPU utilization, throughput, or latency. It doesn't address memory management, parallelization strategies, or other system-level optimizations that are central to scaling LLMs efficiently.",
      "Summary": "This paper provides a unified theoretical framework for analyzing normalization schemes in transformers, viewing normalization as speed regulation for token representations. While it doesn't directly address GPU utilization or performance metrics, understanding how normalization affects representation dynamics and stability can inform architectural choices that indirectly impact training efficiency and model performance."
    }
  },
  {
    "id": "naAUSeyoZ7",
    "title": "Language Models (Mostly) Know When to Stop Reading",
    "abstract": "Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient when the information required to answer a query is localized within the context. We present dynamic context cutoff, a novel method enabling LLMs to self-terminate processing upon acquiring sufficient task-relevant information. Through analysis of model internals, we discover that specific attention heads inherently encode \"sufficiency signals\" -- detectable through lightweight classifiers -- that predict when critical information has been processed. This reveals a new efficiency paradigm: models' internal understanding naturally dictates processing needs rather than external compression heuristics. Comprehensive experiments across six QA datasets (up to 40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B-70B) demonstrate 3.4% accuracy improvement while achieving 1.33x token reduction on average. Furthermore, our method demonstrates superior performance compared to other context efficiency methods at equivalent token reduction rates. Additionally, we observe an emergent scaling phenomenon: while smaller models require probing for sufficiency detection, larger models exhibit intrinsic self-assessment capabilities through prompting.",
    "authors": [
      "~Roy_Xie1",
      "~Junlin_Wang1",
      "~Paul_Rosu1",
      "~Chunyuan_Deng1",
      "~Bolun_Sun2",
      "~Zihao_Lin1",
      "~Bhuwan_Dhingra1"
    ],
    "pdf": "/pdf/439eb89598f256e0d41e81b5824e6d10a5da8f7d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses inference optimization through dynamic context cutoff, reducing token processing by 1.33x on average while improving accuracy. This directly impacts inference latency, throughput, and computational efficiency. The method scales across different model sizes (1B-70B parameters) and demonstrates how model internals can be leveraged for efficiency gains. The approach shows promise for improving GPU utilization by processing fewer tokens per query.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization rather than training optimization. It doesn't address specific GPU hardware optimization techniques like memory management, kernel fusion, or other low-level implementation details. The research is more about model behavior analysis than system architecture optimization.",
      "Summary": "This paper introduces dynamic context cutoff, a method allowing LLMs to self-terminate processing when sufficient information is acquired. By detecting 'sufficiency signals' in attention heads, the approach reduces token processing while maintaining accuracy across multiple model sizes and datasets, offering a promising direction for improving inference efficiency and throughput."
    }
  },
  {
    "id": "AiZxn84Wdo",
    "title": "Training Language Models to Reason Efficiently",
    "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in  problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.\n\nIn this work, we propose to train large reasoning models to reason efficiently. Our method incentivizes models to minimize unnecessary computational overhead while largely maintaining accuracy, thereby achieving substantial deployment efficiency gains. It  enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
    "authors": [
      "~Daman_Arora1",
      "~Andrea_Zanette1"
    ],
    "pdf": "/pdf/c169ad531f568624e1f7af8211b9ff6b12391b63.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on reducing inference costs for large language models, directly addressing inference optimization which is central to my research. It tackles the trade-off between model performance and computational efficiency, crucial for optimizing GPU utilization. The method aims to maintain accuracy while reducing computational overhead, directly supporting higher throughput and lower latency goals. The ability to derive models with varying efficiency levels suggests scalability considerations. The reported 'significant reductions in inference cost while preserving most of the accuracy' aligns perfectly with my focus on efficient model deployment.",
      "Irrelevant Aspects": "The paper doesn't specifically address GPU utilization techniques, though reducing overhead would indirectly impact this. It lacks details on low-level implementation optimizations specific to hardware. There's no mention of parallelization strategies or distributed computing approaches. The abstract suggests a high-level approach rather than low-level optimization techniques. The paper appears focused on model behavior modification rather than computational architecture improvements.",
      "Summary": "This paper directly addresses my core research interest in LLM inference optimization by proposing methods to train models that maintain reasoning capabilities while reducing computational overhead. The work aligns well with my goals of improving GPU utilization, achieving higher throughput, and reducing latency. While it may not cover low-level implementation details, its focus on cost-effective deployment of large reasoning models makes it highly relevant to my research domain."
    }
  },
  {
    "id": "fru52tkjHf",
    "title": "ASGO: Adaptive Structured Gradient Optimization",
    "abstract": "Training deep neural networks (DNNs) is a structured optimization problem, because the parameters are naturally represented by matrices and tensors rather than simple vectors. Under this structural representation, it has been widely observed that gradients are low-rank and Hessians are approximately block-wise diagonal. These structured properties are crucial for designing efficient optimization algorithms but may not be utilized by current popular optimizers like Adam. In this paper, we present a novel optimization algorithm ASGO that capitalizes on these properties by employing a preconditioner that is adaptively updated using structured gradients. By fine-grained theoretical analysis, ASGO is proven to achieve superior convergence rates compared to existing structured gradient methods. Based on the convergence theory, we further demonstrate that ASGO can benefit from the low-rank and block-wise diagonal properties. We also discuss practical modifications of ASGO and empirically verify the effectiveness of the algorithm on language model tasks.",
    "authors": [
      "~Kang_An1",
      "~Yuxing_Liu1",
      "~Rui_Pan4",
      "~Yi_Ren7",
      "~Shiqian_Ma3",
      "~Donald_Goldfarb1",
      "~Tong_Zhang2"
    ],
    "pdf": "/pdf/c7301cf86ace83dfa738518cc4be720c5d52fd1f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents ASGO, a novel optimization algorithm that leverages the structured properties of gradients and Hessians in neural networks. It directly addresses training optimization for language models, claiming superior convergence rates compared to existing methods. This could potentially improve training efficiency for large language models.",
      "Irrelevant Aspects": "The paper doesn't explicitly address inference optimization, which is a key part of my research interest. It also doesn't specifically mention GPU utilization, scalability, throughput, or latency improvements. The focus appears more on theoretical optimization aspects rather than system-level optimizations.",
      "Summary": "ASGO introduces an optimization method that exploits the low-rank nature of gradients and block-wise diagonal structure of Hessians in neural networks. While it shows promising results for language model training efficiency, it lacks coverage of inference optimization and explicit system-level improvements in GPU utilization or scalability."
    }
  },
  {
    "id": "4E3I17pNEl",
    "title": "Yggdrasil: Bridging Dynamic Speculation and Static Runtime  for Latency-Optimal Tree-Based LLM Decoding",
    "abstract": "Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-optimal speculative decoding through context-aware tree drafting and compiler-friendly execution. Yggdrasil introduces an equal-growth tree structure for static graph compatibility, a latency-aware optimization objective for draft selection, and stage-based scheduling to reduce overhead. Yggdrasil supports unmodified LLMs and achieves up to $3.98\\times$ speedup over state-of-the-art baselines across multiple hardware setups.",
    "authors": [
      "~Yue_Guan2",
      "~Changming_Yu1",
      "~Shihan_Fang1",
      "~Weiming_Hu4",
      "~Zaifeng_Pan1",
      "~Zheng_Wang30",
      "~Zihan_Liu12",
      "~Yangjie_Zhou1",
      "~Yufei_Ding1",
      "~Minyi_Guo1",
      "~Jingwen_Leng1"
    ],
    "pdf": "/pdf/007d01438a2e9055b0f6e79a21da69ca6ab73aef.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "LLM inference optimization, speculative decoding for parallel token generation, latency reduction techniques, GPU utilization improvement through tree-based decoding, stage-based scheduling for performance optimization, significant speedup (3.98x) over baselines",
      "Irrelevant Aspects": "No focus on training optimization, limited discussion of scalability implications, primarily focused on latency rather than throughput optimization",
      "Summary": "Yggdrasil presents a novel co-designed system for LLM inference optimization using tree-based speculative decoding. It addresses performance gaps between dynamic speculation and static runtime through equal-growth tree structures, latency-aware draft selection, and stage-based scheduling. The system achieves substantial speedups (up to 3.98x) while maintaining compatibility with unmodified LLMs across various hardware platforms."
    }
  },
  {
    "id": "zswylB4Wnt",
    "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
    "abstract": "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
    "authors": [
      "~Siru_Ouyang1",
      "~Xinyu_Zhu2",
      "~Zilin_Xiao1",
      "~Minhao_Jiang1",
      "~Yu_Meng1",
      "~Jiawei_Han1"
    ],
    "pdf": "/pdf/4e1cf538a0cee20ba69772f7ede39e11ffddd493.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for LLMs by proposing a method that reduces GPU memory requirements for RL training. It focuses on improving GPU utilization and scalability by transferring RL-induced probability shifts from small models to larger ones, enabling cost-efficient enhancement of reasoning capabilities.",
      "Irrelevant Aspects": "The paper doesn't specifically address inference optimization techniques like quantization or pruning for faster inference. It lacks detailed discussion on latency optimization during generation and doesn't cover throughput optimization techniques for serving models.",
      "Summary": "RAST proposes a resource-efficient approach to enhance LLM reasoning capabilities by training small models with RL and transferring their induced probability shifts to larger models, significantly reducing GPU memory requirements while maintaining performance benefits."
    }
  },
  {
    "id": "RALtozQipi",
    "title": "OmniDraft: A cross-vocabulary, online adaptive drafter for on-device speculative decoding",
    "abstract": "Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the “one drafter for all” paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.",
    "authors": [
      "~Ramchalam_Kinattinkara_Ramakrishnan1",
      "~Zhaocong_Yuan1",
      "~Shaojie_Zhuo3",
      "~Chen_Feng9",
      "~Yicheng_Lin3",
      "~Chenzheng_Su1",
      "~Xiaopeng_Zhang1"
    ],
    "pdf": "/pdf/bb19bf4afb2b4dade07f71c9bb12255f0e584882.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on speculative decoding, an inference optimization technique for LLMs that can reduce latency and improve throughput. It addresses efficiency improvements in LLM deployment with online adaptation techniques. The paper reports 1.5-2x speedup, which directly relates to throughput and latency improvements. The approach enables a single small draft model to work with multiple target models, addressing scalability challenges in deployment.",
      "Irrelevant Aspects": "The focus is primarily on on-device deployment rather than data center GPU utilization. It doesn't address training optimization as much as inference optimization. The models discussed are relatively small (68M to 8B parameters) compared to the large models often used in data centers. The 'cross-vocabulary' aspect is more of a niche concern than a general optimization challenge for large-scale systems.",
      "Summary": "OmniDraft presents a cross-vocabulary, online adaptive drafter for speculative decoding that enables a single draft model to work with multiple target models while adapting to user data. The system addresses vocabulary mismatches between models through hybrid distillation fine-tuning and improves decoding speed with adaptive drafting techniques. While focused on on-device deployment rather than data center optimization, the paper provides valuable insights into inference optimization techniques that can improve latency and throughput, though with limited direct relevance to GPU utilization at scale or training optimization."
    }
  },
  {
    "id": "sEFDhxF1mG",
    "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
    "abstract": "Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than $10$% accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at https://github.com/SAI-Lab-NYU/QSVD.",
    "authors": [
      "~Yutong_Wang4",
      "~Haiyu_Wang1",
      "~Sai_Qian_Zhang1"
    ],
    "pdf": "/pdf/10e026defbf37f011f07d36f4da5e95854e23219.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses inference optimization through low-rank approximation of QKV matrices, reducing KV cache size and computational overhead. It combines SVD with quantization to improve hardware efficiency and enable real-time deployment. The rank allocation strategy dynamically adjusts based on accuracy impact, optimizing memory usage. These techniques directly relate to GPU utilization, scalability, throughput, and latency reduction in large models.",
      "Irrelevant Aspects": "The paper focuses on Vision-Language Models rather than pure language models, though techniques are transferable. There's no explicit discussion of training optimization, only inference. The paper doesn't specifically address GPU utilization techniques in detail, though hardware cost reduction is mentioned.",
      "Summary": "QSVD presents a method for compressing QKV weight matrices in VLMs using SVD with dynamic rank allocation and quantization. It achieves 10% accuracy improvement over previous methods while reducing hardware costs, making VLMs more suitable for resource-constrained deployment. The approach is highly relevant to inference optimization for large transformer models, though its focus on vision-language components and lack of training optimization details slightly limit its direct applicability to pure language model research."
    }
  },
  {
    "id": "oN5YVZ9JeF",
    "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning",
    "abstract": "Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high–quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promote robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples in 40 minutes on a single GPU. Our code is available at https://github.com/Dynamite321/T-SHIRT.",
    "authors": [
      "~Yanjun_Fu1",
      "~Faisal_Hamman1",
      "~Sanghamitra_Dutta2"
    ],
    "pdf": "/pdf/0fa7d6b8c3c7dc3cd17ee07f44b2a8c19ddb6cff.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses instruction tuning efficiency for LLMs by proposing a data selection framework that reduces dataset size to just 5% of the original while maintaining or improving model performance. This directly impacts training efficiency, reduces computational requirements, and optimizes resource utilization. The method is described as cost-effective and efficient, with the ability to process 52k samples in 40 minutes on a single GPU. The approach improves training outcomes with less data, potentially reducing training time and computational resources needed. The paper claims to surpass existing state-of-the-art data selection techniques, which suggests advancements in training optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on data selection methodology rather than directly addressing GPU utilization metrics or inference optimization. While it mentions efficiency, it doesn't provide deep analysis of throughput improvements or detailed GPU utilization patterns. The research doesn't appear to directly address inference latency optimization, which is a key aspect of the research interest. The focus is more on data quality aspects rather than system-level optimizations for LLM deployment.",
      "Summary": "T-SHIRT is a data selection framework for instruction tuning that improves training efficiency by selecting only 5% of the original dataset while outperforming models trained on the full dataset by up to 5.48 points on average. The method introduces a novel scoring approach that considers token-level informativeness and promotes robust samples. While the paper presents significant advances in training optimization through intelligent data selection, it doesn't deeply address the specific aspects of GPU utilization, throughput measurement, or inference latency that are central to the research interest. However, its demonstrated efficiency improvements (processing 52k samples in 40 minutes on a single GPU) make it relevant to LLM training optimization."
    }
  },
  {
    "id": "CGx4XU9rCA",
    "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training",
    "abstract": "As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets—such as cosine learning rate schedules—are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the \"river\" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.",
    "authors": [
      "~Minhak_Song1",
      "~Beomhan_Baek1",
      "~Kwangjun_Ahn2",
      "~Chulhee_Yun1"
    ],
    "pdf": "/pdf/7b88c40c3280ef6f9bf920d9b47afd3da8d06cd6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on training optimization for large language models, addressing scalability challenges in training with alternative learning rate schedules. It discusses memory optimization aspects by eliminating the need for auxiliary averaging. The refined SF variant improves performance under large batch sizes, which directly impacts throughput. The paper presents Schedule-Free methods as a practical and scalable approach for language model training, which aligns with my interest in scalable training solutions.",
      "Irrelevant Aspects": "The paper does not address inference optimization, which is a significant part of my expertise. There's no direct discussion of GPU utilization techniques or system-level optimizations. The focus appears to be more on algorithmic optimization rather than hardware-specific optimizations. The paper doesn't cover distributed training strategies or communication optimizations that would be relevant for scaling LLM training.",
      "Summary": "This paper is highly relevant to training optimization aspects of my expertise, particularly in developing scalable solutions for large language model training. The Schedule-Free approach addresses important challenges in conventional pretraining strategies, offering benefits in memory usage and large batch training. However, it lacks coverage of inference optimization and hardware-specific optimizations that are crucial to my overall research focus. The paper's focus on algorithmic improvements rather than system-level implementation limits its direct applicability to some aspects of my work on GPU utilization and low-latency inference."
    }
  },
  {
    "id": "W5Ht05jF4c",
    "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
    "abstract": "Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings—where training involves repeated passes over limited data—and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. Finally, we explain why diffusion models excel in this regime: their randomized masking objective implicitly trains over a rich distribution of token orderings, acting as an implicit data augmentation that AR’s fixed left-to-right factorization lacks. Our results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io",
    "authors": [
      "~Mihir_Prabhudesai1",
      "~Mengning_Wu1",
      "~Amir_Zadeh1",
      "~Katerina_Fragkiadaki1",
      "~Deepak_Pathak1"
    ],
    "pdf": "/pdf/1ef34bd9a9da38f85fb1499298290a5323378d49.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper compares language model architectures (diffusion vs. autoregressive) in terms of training efficiency, which is directly relevant to my expertise in LLM training optimization. It discusses scaling laws and resource allocation considerations, which relates to GPU utilization. The findings about data-constrained training could impact how we optimize compute resources. The architectural comparison might have implications for both training and inference performance.",
      "Irrelevant Aspects": "The paper doesn't explicitly focus on inference optimization, throughput, or latency metrics, which are central aspects of my expertise. It lacks detailed discussion about hardware utilization strategies or system-level optimizations. The main focus is on architectural comparison in data-constrained settings rather than general optimization of training and inference systems.",
      "Summary": "This paper investigates masked diffusion models versus autoregressive models in data-constrained settings, finding diffusion models outperform AR models when compute is abundant but data is scarce. It presents new scaling laws and identifies the compute threshold where diffusion begins to outperform AR, explaining this through randomized masking as implicit data augmentation. While relevant to training optimization, it doesn't directly address inference efficiency or system-level performance metrics."
    }
  },
  {
    "id": "9YNJ03jYsU",
    "title": "AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections",
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as an effective technique for reducing memory overhead in fine-tuning large language models. However, it often suffers from sub-optimal performance compared with full fine-tuning since the update is constrained in the low-rank space. Recent variants such as LoRA-Pro attempt to mitigate this by adjusting the gradients of the low-rank matrices to approximate the full gradient. However, LoRA-Pro's solution is not unique, and different solutions can lead to significantly varying performance in ablation studies. Besides, to incorporate momentum or adaptive optimization design, approaches like LoRA-Pro must first compute the equivalent gradient, causing a higher memory cost close to full fine-tuning. A key challenge remains in integrating momentum properly into the low-rank space with lower memory cost. In this work, we propose AltLoRA, an alternating projection method that avoids the difficulties in gradient approximation brought by the joint update design, meanwhile integrating momentum without higher memory complexity. Our theoretical analysis provides convergence guarantees and further shows that AltLoRA enables stable feature learning and robustness to transformation invariance. Extensive experiments across multiple tasks demonstrate that AltLoRA outperforms LoRA and its variants, narrowing the gap toward full fine-tuning while preserving superior memory efficiency.",
    "authors": [
      "~Xin_Yu12",
      "~Yujia_Wang3",
      "~Jinghui_Chen1",
      "~Lingzhou_Xue1"
    ],
    "pdf": "/pdf/eb9bb3dc04240b4ae3661f5da812e079c6c9b931.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on improving Low-Rank Adaptation (LoRA) for fine-tuning large language models, which is directly related to training optimization. It addresses memory efficiency during fine-tuning, a key factor in GPU utilization. The work aims to improve performance while maintaining low memory overhead, contributing to better resource utilization. The method integrates momentum optimization without increasing memory complexity, which is valuable for training optimization.",
      "Irrelevant Aspects": "The paper doesn't explicitly address inference optimization, which is part of the specified research interests. There's no mention of throughput or latency measurements, which are key metrics in the research focus. The work is primarily theoretical and experimental without clear connections to deployment or production systems where GPU utilization at scale would be most critical.",
      "Summary": "AltLoRA presents an alternating projection method to improve gradient approximation in Low-Rank Adaptation for fine-tuning large language models. While it addresses important aspects of training optimization and memory efficiency, it lacks explicit connections to inference optimization, throughput, and latency considerations that are central to the research focus on large language model systems optimization."
    }
  },
  {
    "id": "wIM0y07NGX",
    "title": "MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees",
    "abstract": "Open-weight large language model (LLM) zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. \nWe introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction.\nAcross a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of $2\\times$ cost savings compared to existing LLM routing techniques.",
    "authors": [
      "~Herbert_Woisetschläger1",
      "~Ryan_Zhang2",
      "~Shiqiang_Wang1",
      "~Hans_Arno_Jacobsen1"
    ],
    "pdf": "/pdf/912c7e017b59dbbec1e0942b1b1bce704b7cd27f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs through intelligent routing, which directly relates to improving resource utilization and efficiency. It addresses cost-optimal request routing while maintaining service quality, which is crucial for production ML systems. The stochastic optimization approach for real-time model selection can enhance throughput and resource allocation efficiency. The claimed 2x cost savings indicates significant efficiency improvements in inference systems. The paper's focus on dynamic learning and per-request optimization aligns with optimizing LLM deployment efficiency.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization optimization or direct latency measurements. There's limited discussion of scalability in terms of handling larger models or distributed computing environments. The focus is on model routing rather than training optimization or specific hardware optimization techniques. The paper doesn't appear to cover throughput benchmarking in detail or architectural improvements for faster inference.",
      "Summary": "MESS+ presents a routing system for LLM inference that dynamically selects models from a zoo to optimize costs while maintaining service quality guarantees. The approach uses stochastic optimization with virtual queues and real-time learning of request satisfaction probabilities. While the paper offers valuable insights for inference efficiency and cost optimization in production systems, it doesn't directly address core GPU utilization techniques, latency optimization strategies, or training optimization that are central to the research interest."
    }
  },
  {
    "id": "uNqTxj5brQ",
    "title": "Fast Inference for Augmented Large Language Models",
    "abstract": "Augmented Large Language Models (LLMs) enhance standalone LLMs by integrating external data sources through API calls. In interactive applications, efficient scheduling is crucial for maintaining low request completion times, directly impacting user engagement. However, these augmentations introduce new scheduling challenges: the size of augmented requests (in tokens) no longer correlates proportionally with execution time, making traditional size-based scheduling algorithms like Shortest Job First less effective. Additionally, requests may require different handling during API calls, which must be incorporated into scheduling.\nThis paper presents MARS, a novel inference framework that optimizes augmented LLM latency by explicitly incorporating system- and application-level considerations into scheduling. MARS introduces a predictive, memory-aware scheduling approach that integrates API handling and request prioritization to minimize completion time. We implement MARS on top of vLLM and evaluate its performance against baseline LLM inference systems, demonstrating improvements in end-to-end latency by 27%-85% and reductions in TTFT by 4%-96% compared to the existing augmented-LLM system, with even greater gains over vLLM. Our implementation is available online.",
    "authors": [
      "~Rana_Shahout1",
      "~Cong_Liang2",
      "~Shiji_Xin1",
      "~Qianru_Lao1",
      "~Yong_Cui1",
      "~Minlan_Yu1",
      "~Michael_Mitzenmacher1"
    ],
    "pdf": "/pdf/466ae7c6d881581805533176114b2bde5a683e8d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focus on LLM inference optimization, novel scheduling framework for augmented LLMs, significant latency improvements (27-85% end-to-end, 4-96% TTFT), implementation on vLLM, addresses API call handling in scheduling, predictive memory-aware approach",
      "Irrelevant Aspects": "No focus on training optimization, doesn't explicitly address GPU utilization, limited discussion on scalability beyond latency metrics",
      "Summary": "This paper presents MARS, a novel inference framework for augmented LLMs that integrate external data through API calls. It addresses unique scheduling challenges where token size no longer correlates with execution time by implementing predictive, memory-aware scheduling with integrated API handling. The framework demonstrates substantial latency improvements over existing systems, implemented on vLLM, directly targeting key metrics for LLM inference efficiency."
    }
  },
  {
    "id": "49Rc51iCso",
    "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering",
    "abstract": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as *overthinking* during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose **Manifold Steering**, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71\\% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.",
    "authors": [
      "~Yao_Huang2",
      "~Huanran_Chen1",
      "~Shouwei_Ruan1",
      "~Yichi_Zhang4",
      "~Xingxing_Wei1",
      "~Yinpeng_Dong2"
    ],
    "pdf": "/pdf/e526209110869ec8c63cf89731300146395ff4a2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Directly addresses inference optimization which is a core aspect of my expertise. Claims a 71% reduction in output tokens, which would significantly improve inference throughput and reduce latency. The approach appears to reduce computational overheads during inference, directly impacting GPU utilization. Shows cross-domain transferability suggesting broad applicability to different LLM tasks.",
      "Irrelevant Aspects": "Focuses specifically on 'overthinking' in reasoning models rather than general optimization techniques. Does not address training optimization, which is half of my stated expertise. The mechanistic interpretability focus is not directly related to computational efficiency goals.",
      "Summary": "This paper presents 'Manifold Steering' to mitigate overthinking in Large Reasoning Models, claiming up to 71% reduction in output tokens while maintaining accuracy. The approach operates on activation spaces during inference, which is highly relevant to my inference optimization interests, particularly for improving GPU utilization, throughput, and latency. However, its specific focus on the overthinking phenomenon in reasoning models limits its broader applicability to my research scope."
    }
  },
  {
    "id": "ENYvdnyhLl",
    "title": "Parallelizable Nonlinear State-Space Models: Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling",
    "abstract": "We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing its Jacobian matrix to be diagonal, the full sequence can be solved in parallel, giving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly, the diagonal Jacobian structure of our model results in no performance loss compared to the original model with dense Jacobian, and the approach can be generalized to other non-linear recurrent models, demonstrating broader applicability. On a suite of long-range forecasting tasks, we demonstrate that LrcSSM outperforms Transformers, LRU, S5, and Mamba.",
    "authors": [
      "~Mónika_Farsang1",
      "~Radu_Grosu1"
    ],
    "pdf": "/pdf/56a1c83a4538aab96c37803ed961f98878c7ed36.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces a parallelizable nonlinear state-space model (LrcSSM) with O(TD) time complexity and only O(log T) sequential depth, which could significantly improve GPU utilization, scalability, throughput, and potentially lower latency for sequence modeling tasks. The model outperforms Transformers and other popular sequence models, and its parallel processing capability and gradient stability guarantees are valuable for training and inference optimization.",
      "Irrelevant Aspects": "The paper focuses on state-space models rather than specifically on language models, though the techniques could be applicable to LLMs. It doesn't explicitly discuss GPU utilization optimization or implementation details for specific hardware optimizations. The evaluation appears to focus on forecasting tasks rather than NLP tasks, limiting direct applicability insights for language modeling.",
      "Summary": "LrcSSM is a nonlinear recurrent model that enables parallel processing of long sequences by enforcing a diagonal Jacobian structure. It offers computational efficiency with O(TD) time complexity and minimal sequential depth while providing gradient stability guarantees. The model outperforms Transformers and other popular sequence models on long-range forecasting tasks, and the approach can be generalized to other nonlinear recurrent models. This work presents important optimizations for sequence modeling that could enhance LLM training and inference efficiency, though its direct application to language modeling requires further exploration."
    }
  },
  {
    "id": "oTocSpfYvJ",
    "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning",
    "abstract": "Despite the significant breakthrough of Mixture-of-Experts (MoE), the increasing scale of these MoE models presents huge memory and storage challenges. Existing MoE pruning methods, which involve reducing parameter size with a uniform sparsity across all layers, often lead to suboptimal outcomes and performance degradation due to varying expert redundancy in different MoE layers. To address this, we propose a non-uniform pruning strategy, dubbed Differentiable Expert Pruning (DiEP), which adaptively adjusts pruning rates at the layer level while jointly learning inter-layer importance, effectively capturing the varying redundancy across different MoE layers. By transforming the global discrete search space into a continuous one, our method handles exponentially growing non-uniform expert combinations, enabling adaptive gradient-based pruning. Extensive experiments on five advanced MoE models demonstrate the efficacy of our method across various NLP tasks. Notably, \\textbf{DiEP} retains around 92\\% of original performance on Mixtral 8$\\times$7B with only half the experts, outperforming other pruning methods by up to 7.1% on the challenging MMLU dataset.",
    "authors": [
      "~Sikai_Bai1",
      "~Haoxi_Li1",
      "~Jie_ZHANG18",
      "~Zicong_Hong1",
      "~Song_Guo5"
    ],
    "pdf": "/pdf/552fa8d76f16e9c3626ec28f9b754c737380daed.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses memory and storage challenges in large-scale MoE models, directly impacting GPU utilization. It proposes a pruning method that reduces parameter size, which can lead to higher throughput and lower latency during inference. The approach maintains performance while reducing model size, crucial for practical deployment. Testing on Mixtral 8×7B shows retention of 92% performance with half the experts. The non-uniform pruning strategy handles different redundancy across MoE layers, potentially leading to more efficient resource allocation in GPU environments.",
      "Irrelevant Aspects": "The paper focuses primarily on the pruning method itself rather than system-level optimization. There's no explicit discussion of GPU utilization metrics or throughput/latency measurements. Evaluation appears to focus on model performance metrics rather than computational efficiency. The paper doesn't seem to address training optimization directly, focusing more on inference through compression. There's no indication of discussion about distributed computing or scalability in terms of GPU clusters.",
      "Summary": "DiEP presents a non-uniform pruning strategy for Mixture-of-Experts models that adaptively adjusts pruning rates at the layer level while learning inter-layer importance. The method transforms the discrete search space into a continuous one, enabling gradient-based pruning. Experiments show it can retain 92% of Mixtral's performance with half the experts, outperforming other methods by up to 7.1% on MMLU. While the paper is algorithmically focused on compression rather than system-level optimizations, the results suggest significant potential for improved GPU utilization and inference efficiency in large language models."
    }
  },
  {
    "id": "YPsJha5HXQ",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "abstract": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model’s reasoning capabilities or merely amplifies high-reward outputs already latent in the base model’s distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@$k$ evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time.  These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We will release model weights and data to support further research.",
    "authors": [
      "~Mingjie_Liu2",
      "~Shizhe_Diao2",
      "~Ximing_Lu1",
      "~Jian_Hu5",
      "~Xin_Dong1",
      "~Yejin_Choi1",
      "~Jan_Kautz1",
      "~Yi_Dong4"
    ],
    "pdf": "/pdf/75e49a843f0b6d00c0584a776fad3bea93496c83.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on training optimization through a novel reinforcement learning methodology (ProRL) for large language models. It addresses scalability by exploring how prolonged RL training and scaling up RL compute can improve reasoning capabilities. These aspects align well with my expertise in training optimization and scalability for LLMs.",
      "Irrelevant Aspects": "The paper appears to focus more on training rather than inference optimization. It doesn't seem to address specific GPU utilization techniques, hardware-level optimizations, or inference acceleration methods like quantization and pruning, which are important aspects of my expertise.",
      "Summary": "ProRL introduces a prolonged reinforcement learning methodology for expanding reasoning capabilities in large language models. While it provides valuable insights into training optimization and scalability, it appears less focused on inference optimization and specific GPU utilization techniques that are central to my expertise."
    }
  },
  {
    "id": "8ZiElzQxf1",
    "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "abstract": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key–value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8× compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference latency and memory load. For instance, we enhance Qwen-R1 32B by 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench on average for an equivalent number of memory reads.",
    "authors": [
      "~Adrian_Łańcucki1",
      "~Konrad_Staniszewski1",
      "~Piotr_Nawrot1",
      "~Edoardo_Ponti1"
    ],
    "pdf": "/pdf/bff876f643e028a8e0f2e4f8ae52a2b5e43de99b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization, KV cache compression (a critical component for optimizing transformer LLM inference), improving GPU utilization by reducing memory requirements, achieving higher throughput and lower latency, Dynamic Memory Sparsification (DMS) method for efficient KV cache compression, and quantitative improvements on benchmarks while maintaining comparable inference latency and memory load.",
      "Irrelevant Aspects": "The paper doesn't address training optimization, distributed training or model parallelism techniques, quantization techniques for inference optimization, or hardware-specific optimizations beyond general GPU utilization.",
      "Summary": "This paper introduces Dynamic Memory Sparsification (DMS), a novel method for compressing KV caches during LLM inference. The approach enables generating more tokens within the same compute budget, improving accuracy while maintaining comparable latency and memory load. The method requires only 1K training steps to achieve 8× compression while preserving accuracy. The paper demonstrates significant improvements on various benchmarks across multiple LLM families."
    }
  },
  {
    "id": "Fcs90Rwm8j",
    "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
    "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading,  underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents.",
    "authors": [
      "~Hao_Kang3",
      "~Qingru_Zhang2",
      "~Han_Cai1",
      "~Weiyuan_Xu2",
      "~Tushar_Krishna1",
      "~Yilun_Du1",
      "~Tsachy_Weissman1"
    ],
    "pdf": "/pdf/13fb7451afdc5c8796e7166332feddd0c8a38f8e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on latency-quality trade-offs in LLM inference optimization, proposes an adaptive framework (FPX) that dynamically selects model size and quantization levels based on real-time demands, and demonstrates significant improvements in latency-sensitive applications. These aspects directly align with research interests in inference optimization, GPU utilization, and latency reduction.",
      "Irrelevant Aspects": "The paper primarily focuses on specific application domains (high-frequency trading and competitive gaming) rather than broader LLM optimization techniques. It doesn't appear to address training optimization or general scalability issues beyond latency-constrained scenarios.",
      "Summary": "This paper presents a systematic study of latency-quality trade-offs in LLM-based agents, introducing benchmarks and an adaptive framework that dynamically adjusts model size and quantization levels based on real-time demands. The work demonstrates significant performance improvements in latency-sensitive applications, with up to 80% improvement in win rate and 26.52% boost in daily yield. While highly relevant to inference optimization and latency reduction, it focuses primarily on specific application domains rather than broader optimization techniques."
    }
  },
  {
    "id": "roKj4IwaVT",
    "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
    "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
    "authors": [
      "~Gleb_Rodionov1",
      "~Roman_Garipov1",
      "~Alina_Shutova1",
      "~George_Yakushev1",
      "~Erik_Schultheis1",
      "~Vage_Egiazarian1",
      "~Anton_Sinitsin1",
      "~Denis_Kuznedelev1",
      "~Dan_Alistarh7"
    ],
    "pdf": "/pdf/752690b760cb01f226ba630228b97af81df4d1d1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parallel LLM inference implementation, concurrently-updated attention cache, hardware utilization optimization, Rotary Position Embeddings (RoPE) to avoid recomputation, shared Key-Value cache for multiple instances, inference without additional fine-tuning",
      "Irrelevant Aspects": "Focus on collaboration strategies between LLM workers, human problem-solving analogies, voting mechanisms, task-solving approaches rather than system optimization techniques",
      "Summary": "This paper introduces Hogwild! Inference, a parallel LLM inference engine where multiple instances run concurrently with shared attention cache. It focuses on improving hardware utilization and throughput by allowing parallel token generation with instant access to other instances' progress. The approach leverages RoPE to avoid recomputation and enables shared KV cache usage without additional fine-tuning. While the collaboration aspects are less relevant, the technical implementation of concurrent attention directly addresses GPU utilization, scalability, and throughput optimization concerns in LLM inference systems."
    }
  },
  {
    "id": "cvFFar8cAr",
    "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs",
    "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\\it local homogeneity}), adjacent values demonstrate distinct {\\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this \\href{https://github.com/the-scale-lab/Asymkv}{link}.",
    "authors": [
      "~Wanyun_Cui1",
      "~Mingwei_Xu5"
    ],
    "pdf": "/pdf/67d3bdf6a6a508c5bf77ef6589fee67a1282c0f7.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on KV cache compression for long-context LLMs, which directly addresses inference optimization - a core part of my research interests. It presents a novel approach to improve memory efficiency during LLM inference by exploiting an asymmetry between keys and values in the attention mechanism. This can lead to better GPU utilization, higher throughput, and reduced latency for long-context tasks. The fact that it's a training-free method is particularly valuable as it can be applied to existing models without additional training overhead.",
      "Irrelevant Aspects": "The paper specifically focuses on inference optimization rather than training optimization, which is only one part of my broader research interest. It doesn't address aspects like distributed training strategies or model architecture optimizations for training efficiency.",
      "Summary": "This paper is highly relevant to my research on LLM optimization. It introduces AsymKV, a novel training-free compression framework that exploits an asymmetry between keys and values in KV caches to improve long-context inference efficiency. The method combines homogeneity-based key merging with lossless value compression, achieving state-of-the-art performance on long-context benchmarks. This work directly contributes to better GPU utilization, higher throughput, and lower latency during LLM inference, which are central goals in my research."
    }
  },
  {
    "id": "5MGClYw1cR",
    "title": "Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression",
    "abstract": "This paper proposes a novel matrix quantization method, Binary Quadratic Quan-\ntization (BQQ). In contrast to conventional first-order quantization approaches—\nsuch as uniform quantization and binary coding quantization—that approximate\nreal-valued matrices via linear combinations of binary bases, BQQ leverages the\nexpressive power of binary quadratic expressions while maintaining an extremely\ncompact data format. We validate our approach with two experiments: a matrix\ncompression benchmark and post-training quantization (PTQ) on pretrained Vision\nTransformer-based models. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction\nerror than conventional methods for compressing diverse matrix data. It also\ndelivers strong PTQ performance, even though we neither target state-of-the-art\nPTQ accuracy under tight memory constraints nor rely on PTQ-specific binary\nmatrix optimization. For example, our proposed method outperforms the state-of-\nthe-art PTQ method by up to 2.2% and 59.1% on the ImageNet dataset under the\ncalibration-based and data-free scenarios, respectively, with quantization equivalent\nto 2 bits. These findings highlight the surprising effectiveness of binary quadratic\nexpressions for efficient matrix approximation and neural network compression.",
    "authors": [
      "~Kyo_Kuroki1",
      "~Yasuyuki_Okoshi1",
      "~Thiem_Van_Chu1",
      "~Kazushi_Kawamura1",
      "~Masato_Motomura1"
    ],
    "pdf": "/pdf/b44925e5c048603572aa88deb867fd58d8c14df4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces Binary Quadratic Quantization (BQQ), a novel quantization method for matrix compression that directly addresses inference optimization through better memory efficiency. This has significant implications for GPU utilization and model scalability. The focus on extremely compact data formats and the superior trade-off between memory efficiency and reconstruction error are directly relevant to optimizing large models. The application to Transformer-based models (even if Vision Transformers) and strong PTQ performance (equivalent to 2-bit quantization) make the techniques potentially transferable to language models for reducing memory footprint and improving inference throughput.",
      "Irrelevant Aspects": "The paper focuses primarily on Vision Transformers rather than language models, which is my primary focus. It doesn't address training optimization aspects, only post-training quantization. The binary quadratic expressions, while mathematically interesting, may require significant adaptation for practical implementation in LLM systems. The evaluation doesn't measure system-level metrics like throughput, latency, or actual GPU utilization improvements, which are central to my research interests.",
      "Summary": "This paper presents a promising new quantization technique for matrix compression with applications to transformer models. While focused on vision models, the BQQ approach could potentially be adapted for language models to improve inference efficiency. The method shows significant improvements in memory efficiency with strong post-training quantization results equivalent to 2-bit quantization. The paper's relevance to my research interest in LLM optimization is moderate to high, particularly for inference optimization scenarios, though it lacks direct application to language models and doesn't address the broader system optimization challenges I typically focus on."
    }
  },
  {
    "id": "oanhUGY6un",
    "title": "Gradient Multi-Normalization for Efficient LLM Training",
    "abstract": "Training large language models (LLMs) commonly relies on adaptive optimizers such as Adam (Kingma & Ba 2015), which accelerate convergence through moment estimates but incur substantial memory overhead. Recent stateless approaches such as SWAN (Ma et al., 2024) have shown that appropriate preprocessing of instantaneous gradient matrices can match the performance of adaptive methods without storing optimizer states. Building on this insight, we introduce \\emph{gradient multi-normalization}, a principled framework for designing stateless optimizers that normalize gradients with respect to multiple norms simultaneously. Whereas standard first-order methods can be viewed as gradient normalization under a single norm (Bernstein & Newhouse, 2024), our formulation generalizes this perspective to a multi-norm setting. We derive an efficient alternating scheme that enforces these normalization constraints and show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem. This unifies and extends prior stateless optimizers, showing that SWAN arises as a specific instance with particular norm choices. Leveraging this principle, we develop SinkGD, a lightweight matrix optimizer that retains the memory footprint of SGD while substantially reducing computation relative to whitening-based methods. On the memory-efficient LLaMA training benchmark (Zhao et al., 2024), SinkGD achieves state-of-the-art performance, reaching the same evaluation perplexity as Adam using only 40\\% of the training tokens.",
    "authors": [
      "~Meyer_Scetbon1",
      "~Chao_Ma2",
      "~Wenbo_Gong1",
      "~Edward_Meeds1"
    ],
    "pdf": "/pdf/d68b4b1959a22981c5d5f82a95de6b8e6b31f253.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses memory efficiency in LLM training, which is crucial for GPU utilization and scalability. It proposes a stateless optimizer approach that reduces memory requirements while maintaining performance. SinkGD reduces computation while retaining the memory footprint of SGD. The method achieves the same perplexity as Adam using only 40% of training tokens, significantly improving training efficiency. It builds on and extends previous stateless optimizers like SWAN.",
      "Irrelevant Aspects": "The mathematical theoretical derivations around multi-normalization constraints may not directly contribute to practical implementation. There's no explicit discussion of how this approach affects inference optimization. The paper doesn't appear to address distributed training aspects in detail, which is important for scalability.",
      "Summary": "This paper proposes Gradient Multi-Normalization, a framework for designing stateless optimizers that normalize gradients with respect to multiple norms simultaneously. The resulting SinkGD optimizer maintains SGD's memory footprint while substantially reducing computation and training token requirements. The approach demonstrates significant potential for improving memory efficiency and reducing training costs for large language models."
    }
  },
  {
    "id": "iY1zuKydO0",
    "title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling",
    "abstract": "Inference scaling methods for LLMs often rely on decomposing problems into steps (or groups of tokens), followed by sampling and selecting the best next steps. However, these steps and their sizes are often predetermined or manually designed based on domain knowledge. We propose dynamic decomposition, a method that adaptively and automatically partitions solution and reasoning traces into manageable steps during inference. By more effectively allocating compute -- particularly through subdividing challenging steps and prioritizing their sampling -- dynamic decomposition significantly improves inference efficiency. Experiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate that dynamic decomposition outperforms static approaches, including token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These findings highlight the potential of dynamic decomposition to improve a wide range of inference scaling techniques.",
    "authors": [
      "~Jonathan_Light1",
      "~Wei_Cheng1",
      "~Benjamin_Riviere1",
      "~Yue_Wu12",
      "~Masafumi_Oyamada1",
      "~Mengdi_Wang1",
      "~Yisong_Yue1",
      "~Santiago_Paternain1",
      "~Haifeng_Chen1"
    ],
    "pdf": "/pdf/8e6f80833c03da451fac1cb9faac6f430ef6d525.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses LLM inference optimization and scaling techniques, which aligns with my expertise. It proposes a dynamic decomposition method that adaptively partitions reasoning traces to more effectively allocate compute resources during inference. The method focuses on improving inference efficiency, which relates to better resource utilization. The paper reports significant performance improvements (reductions in pass@10 error rates of 5.0-10.5%) on challenging benchmarks, indicating effective optimization of computational resources.",
      "Irrelevant Aspects": "The paper focuses on algorithmic-level decomposition rather than system-level optimizations like quantization, memory management, or kernel optimizations that directly impact GPU utilization. There's no explicit discussion of throughput and latency metrics, hardware-specific optimizations, or distributed inference scaling strategies. The research doesn't cover training optimization techniques, which is another aspect of my expertise.",
      "Summary": "DISC introduces a dynamic decomposition method for LLM inference that adaptively partitions reasoning traces during runtime. By automatically adjusting step sizes and prioritizing challenging steps, the approach improves compute allocation and inference efficiency compared to static decomposition methods. While it demonstrates significant improvements in error reduction on reasoning benchmarks, the paper focuses on algorithmic-level optimization rather than the system-level and hardware-aware optimizations that form a core part of my research interest."
    }
  },
  {
    "id": "eHRFb3DSZS",
    "title": "ZeCO: Zero-Communication Overhead Sequence Parallelism for Linear Attention",
    "abstract": "Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary performance bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve practically end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a novel collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.",
    "authors": [
      "~Yuhong_Chou1",
      "~Zehao_Liu4",
      "~Rui-Jie_Zhu2",
      "~Xinyi_Wan1",
      "~Tianjian_Li2",
      "~Congying_Chu2",
      "~Qian_Liu2",
      "~Jibin_Wu1",
      "~Zejun_MA1"
    ],
    "pdf": "/pdf/17933e31df2bb3be6cec18534124873195b1ad5b.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models by introducing ZeCO, a sequence parallelism method that eliminates communication overhead. It focuses on improving GPU utilization and scalability for LLM training, particularly for ultra-long sequences. The paper claims to achieve near-linear scalability and 60% speedup over state-of-the-art methods, which directly relates to the research interest in higher throughput and better GPU utilization. The introduction of All-Scan as a novel collective communication primitive represents a significant optimization technique that reduces bottlenecks in distributed training.",
      "Irrelevant Aspects": "The paper is specifically focused on linear attention mechanisms rather than general attention mechanisms in transformers. While this is still relevant to LLM optimization, it narrows the scope to a specific variant of attention. Additionally, the paper does not appear to address inference optimization, which is part of the research interest.",
      "Summary": "ZeCO introduces a sequence parallelism method designed to eliminate communication overhead in training linear attention models for LLMs. It achieves near-linear scalability through All-Scan, a novel communication primitive that provides necessary operator states with minimal communication footprint. The paper demonstrates significant performance improvements, enabling efficient training of models with extremely long sequences across many GPUs."
    }
  },
  {
    "id": "4Qe2Hga43N",
    "title": "Cost-Aware Contrastive Routing for LLMs",
    "abstract": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single $k$‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts.",
    "authors": [
      "~Reza_Shirkavand1",
      "~Shangqian_Gao1",
      "~Peiran_Yu1",
      "~Heng_Huang1"
    ],
    "pdf": "/pdf/bbf149b40a218151b3753f40ea5229b820765ca1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization through cost-aware routing, promising microsecond latency decisions. It introduces an efficient embedding-based approach for model selection that improves accuracy-cost tradeoffs. The lightweight framework uses compact representations and fast k-NN lookup, avoiding expensive profiling. The approach handles dynamic pools of models and generalizes to unseen models and out-of-distribution prompts, enhancing scalability. These aspects directly contribute to improving inference efficiency and resource allocation.",
      "Irrelevant Aspects": "The paper focuses primarily on routing between different models rather than optimizing a single model's inference process. It doesn't address hardware-specific optimizations for GPU utilization directly. Training optimization aspects aren't explicitly covered. The approach is more about model selection than maximizing GPU utilization within individual models. Details about throughput improvements in actual deployment scenarios are limited in the abstract.",
      "Summary": "This paper presents Cost-Spectrum Contrastive Routing (CSCR), a framework for efficiently routing prompts to appropriate language models in a cost-aware manner. It maps prompts and models to a shared embedding space using logit footprints for open-source models and perplexity fingerprints for black-box APIs. The contrastive encoder is trained to prefer the cheapest accurate expert within adaptive cost bands. At inference, routing is reduced to a single k-NN lookup via FAISS index, enabling microsecond-latency decisions without retraining when models change. The approach improves accuracy-cost tradeoffs by up to 25% and generalizes well to unseen models and out-of-distribution prompts, though it focuses more on model selection than direct GPU utilization optimization."
    }
  },
  {
    "id": "PNgG4H3q9D",
    "title": "Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training",
    "abstract": "Pre-training has proven effective in addressing data scarcity and performance limitations in solving PDE problems with neural operators.\nHowever, challenges remain due to the heterogeneity of PDE datasets in equation types, which leads to high errors in mixed training. \nAdditionally, dense pre-training models that scale parameters by increasing network width or depth incur significant inference costs.\nTo tackle these challenges, we propose a novel **M**ixture-**o**f-**E**xperts **P**re-training **O**perator **T**ransformer (**MoE-POT**), a sparse-activated architecture that scales parameters efficiently while controlling inference costs.\nSpecifically, our model adopts a layer-wise router-gating network to dynamically select 4 routed experts from 16 expert networks during inference, enabling the model to focus on equation-specific features. \nMeanwhile, we also integrate 2 shared experts, aiming to capture common properties of PDE and reduce redundancy among routed experts. \nThe final output is computed as the weighted average of the results from all activated experts.\nWe pre-train models with parameters from 30M to 0.5B on 6 public PDE datasets.\nOur model with 90M activated parameters achieves up to a 40\\% reduction in zero-shot error compared with existing models with 120M activated parameters.\nAdditionally, we conduct interpretability analysis, showing that dataset types can be inferred from router-gating network decisions, which validates the rationality and effectiveness of the MoE architecture.",
    "authors": [
      "~Hong_Wang14",
      "~Haiyang_Xin1",
      "~Jie_Wang1",
      "~Xuanze_Yang1",
      "~Fei_Zha1",
      "~huanshuo_dong1",
      "~Yan_Jiang16"
    ],
    "pdf": "/pdf/1600a95dda791a68fb16f6801a5da2dba72f784b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Mixture-of-Experts (MoE) architecture is directly relevant to large language model scaling and optimization. The sparse-activated approach is a key technique for efficient inference that I'm highly interested in. The router-gating network mechanism for dynamic expert selection aligns with my focus on resource utilization. The paper demonstrates parameter scaling efficiency while controlling inference costs - exactly the optimization goals I work on. The achievement of better performance with fewer activated parameters (90M vs 120M) directly addresses my interest in throughput and latency optimization.",
      "Irrelevant Aspects": "The paper focuses on PDE (Partial Differential Equation) problems rather than natural language processing, which is my primary domain. The specific details about PDE dataset heterogeneity and equation-specific features are not directly applicable to my LLM work. The architecture details specialized for solving PDEs would need adaptation for text-based models.",
      "Summary": "This paper presents MoE-POT, a Mixture-of-Experts architecture for PDE problems that uses sparse activation (selecting 4 out of 16 routed experts plus 2 shared experts) to improve parameter efficiency and control inference costs. While the application domain (PDEs) differs from my LLM focus, the MoE techniques, sparse activation patterns, and inference optimization strategies are highly relevant to my work on efficient large model training and inference."
    }
  },
  {
    "id": "xdNAVP7TGy",
    "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DFloat11)",
    "abstract": "Large-scale AI models, such as Large Language Models (LLMs) and Diffusion Models (DMs), have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM and DM size by 30\\% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in the existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) compact, hierarchical lookup tables (LUTs) that fit within GPU SRAM for efficient decoding, (ii) a two-phase GPU kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on Llama 3.3, Qwen 3, Mistral 3, FLUX.1, and others validate our hypothesis that DFloat11 achieves around 30\\% model size reduction while preserving bit-for-bit identical outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 2.3--46.2$\\times$ higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.7--14.9$\\times$ longer generation lengths than uncompressed models. Notably, our method enables lossless inference of Llama 3.1 405B, an 810GB model, on a single node equipped with 8$\\times$80GB GPUs.",
    "authors": [
      "~Tianyi_Zhang6",
      "~Mohsen_Hariri1",
      "~Shaochen_Zhong1",
      "~Vipin_Chaudhary2",
      "~Yang_Sui1",
      "~Xia_Hu4",
      "~Anshumali_Shrivastava1"
    ],
    "pdf": "/pdf/6b1607ff4d79f3a474030fbbf74f738e0e5350c9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Lossless compression for LLMs reducing memory footprint while maintaining bit-for-bit accuracy; Custom GPU kernel implementation for efficient decompression; Transformer-block-level decompression to minimize latency; Significant throughput improvements (2.3-46.2x higher than CPU offloading); Enables deployment of very large models (405B parameters) on limited GPU resources; Focuses on GPU memory optimization and utilization; Directly addresses inference bottlenecks; Uses hierarchical lookup tables within GPU SRAM; Two-phase GPU kernel for thread coordination; Enables longer generation lengths with fixed memory budget",
      "Irrelevant Aspects": "Focuses solely on inference optimization without addressing training efficiency; Limited discussion of distributed training strategies; No coverage of parameter-efficient fine-tuning methods; Specific to compression approach rather than broader system optimizations",
      "Summary": "This paper introduces DFloat11, a lossless compression framework that reduces LLM size by 30% while maintaining identical outputs. It leverages entropy coding of BFloat16 weights and implements custom GPU kernels for efficient decompression. The approach achieves significant throughput improvements and enables running massive models (like Llama 3.1 405B) on limited GPU resources through techniques like hierarchical lookup tables and transformer-block-level decompression. This work is highly relevant to GPU inference optimization, directly addressing memory constraints, throughput, and latency challenges in large language model deployment."
    }
  },
  {
    "id": "to1VYVar9W",
    "title": "Breaking AR’s Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
    "abstract": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.",
    "authors": [
      "~Gen_Li2",
      "~Changxiao_Cai1"
    ],
    "pdf": "/pdf/1261b5bf2bd40e9c3cabc341d1b097ab4ff4d743.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses the sampling bottleneck in autoregressive models which affects latency in LLM inference. It proposes using diffusion models for parallel generation, which could significantly improve throughput and reduce latency compared to sequential generation. The theoretical finding that high-quality samples can be generated with fewer iterations than sequence length is promising for inference optimization. The focus on breaking fundamental bottlenecks aligns with research interest in optimizing LLM generation efficiency.",
      "Irrelevant Aspects": "The paper appears to be primarily theoretical with focus on convergence guarantees rather than practical system implementation. There's no explicit discussion of GPU utilization optimization, memory management, or hardware-specific optimizations. The paper doesn't address training optimization, which is part of my research interest. It lacks details about scalability challenges in deployment or distributed inference settings. The analysis is from an information-theoretic perspective rather than a systems implementation perspective.",
      "Summary": "This paper provides theoretical justification for using diffusion language models to accelerate generation compared to autoregressive models, potentially breaking the fundamental sampling bottleneck. While the theoretical insights about parallel generation and reduced iterations are valuable for inference optimization research, the paper lacks practical implementation details about GPU utilization, scalability solutions, or system-level optimizations that would be directly applicable to my research focus on ML systems optimization."
    }
  },
  {
    "id": "zJzu9evD5K",
    "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
    "abstract": "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off—unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel level—and makes powerful LLMs practical for resource-constrained environments.",
    "authors": [
      "~Banseok_Lee3",
      "~Dongkyu_Kim2",
      "~Youngcheon_you1",
      "~Young-Min_Kim2"
    ],
    "pdf": "/pdf/2ea2c67c386eb0d3d7a4eedc6a821d38853641a9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Ultra low-bit quantization of LLMs, memory reduction techniques, performance-speedup trade-offs, novel factorization and compensation mechanisms, kernel-level speedup improvements",
      "Irrelevant Aspects": "Limited discussion of scalability across multiple GPUs (primarily focused on single model compression)",
      "Summary": "This paper introduces LittleBit, a novel method for extreme LLM compression achieving sub-1-bit quantization through latent matrix factorization and multi-scale compensation mechanisms. It demonstrates significant memory reduction (31×) and potential speedups (11.6×), making powerful LLMs practical for resource-constrained environments while maintaining performance superior to existing methods even at much higher bit rates."
    }
  },
  {
    "id": "ESELaMThLN",
    "title": "Controlling Thinking Speed in Reasoning Models",
    "abstract": "Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking.\nWhile current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency.\nIn this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs.\nOur approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance.\nFor the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space.\nUsing this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods.\nFor the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity.\nCombining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. \nWithout any training or additional cost, our plug-and-play method yields an average +1.3\\% accuracy with -8.6\\% token usage across leading LRMs and advanced reasoning benchmarks.\nAll of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research.",
    "authors": [
      "~Zhengkai_Lin1",
      "~Zhihang_Fu1",
      "~Ze_Chen3",
      "~Chao_Chen19",
      "~Liang_Xie3",
      "~Wenxiao_Wang2",
      "~Deng_Cai4",
      "~Zheng_Wang32",
      "~Jieping_Ye4"
    ],
    "pdf": "/pdf/ee862e36e40791c52d5b9c640e6eba71b0b72428.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization in large reasoning models by reducing computational overhead and latency through dynamic thinking speed adjustment. It achieves an 8.6% reduction in token usage, which directly impacts throughput and efficiency. The plug-and-play approach requires no additional training costs, and the implementation on vLLM is directly relevant to GPU utilization optimization. The real-time difficulty estimation and representation editing-based test-time scaling contribute to more efficient resource allocation.",
      "Irrelevant Aspects": "The cognitive theory background comparing System 1 and System 2 thinking is primarily conceptual rather than technical. The focus is specifically on reasoning models rather than general language models. The paper doesn't directly address distributed training strategies or GPU memory optimization techniques beyond token reduction.",
      "Summary": "This paper is highly relevant as it introduces a method to control thinking speed in large reasoning models, resulting in reduced token usage (8.6%) and maintained accuracy (+1.3%). By enabling dynamic adjustment between fast and slow thinking modes based on reasoning complexity, it optimizes the accuracy-efficiency trade-off. The plug-and-play approach implemented on vLLM directly addresses inference optimization without additional training costs, making it valuable for improving GPU utilization, throughput, and reducing latency in LLM deployment."
    }
  },
  {
    "id": "iyu4sLQZvW",
    "title": "First Attentions Last: Better Exploiting First Attentions for Efficient Parallel Training",
    "abstract": "As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block’s MHA–MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18×, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline.",
    "authors": [
      "~Gyudong_Kim1",
      "~Hyukju_Na1",
      "~Jin_Hyeon_Kim1",
      "~Hyunsung_Jang1",
      "~Jaemin_Park5",
      "~Jaegi_Hwang1",
      "~NAMKOO_HA1",
      "~Seungryong_Kim1",
      "~Young_Geun_Kim1"
    ],
    "pdf": "/pdf/ded9cce97d2f5f8c11be51b671368732ac2ac10b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses transformer training optimization for large language models, focusing on reducing communication overhead in Tensor Parallelism. It proposes architectural modifications (FAL and FAL+) that eliminate per-block MHA-MLP connections, enabling parallel execution on single GPUs. The work achieves significant performance improvements: up to 44% reduction in multi-GPU training time and 1.18× improvement in single-GPU throughput while maintaining or improving model quality. This directly relates to GPU utilization and scalability optimization in distributed training.",
      "Irrelevant Aspects": "The paper primarily focuses on training optimization rather than inference optimization. There's limited discussion of latency-specific optimizations which would be relevant for inference scenarios. The evaluation appears constrained to training metrics without comprehensive system-level analysis beyond the immediate architectural changes.",
      "Summary": "FAL (First Attentions Last) is a transformer architecture modification that eliminates communication bottlenecks in tensor parallel training by redirecting the first attention output to MLP inputs of subsequent layers. This removes all-reduce communication requirements and enables parallel execution of MHA and MLP on a single GPU. FAL+ further enhances this by adding normalized first attention outputs to MHA outputs of following layers. The approach achieves substantial training efficiency improvements while maintaining model quality, demonstrating a promising direction for scalable distributed training of large language models."
    }
  },
  {
    "id": "Rc489jcc30",
    "title": "AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining",
    "abstract": "Learning rate is widely regarded as crucial for effective foundation model pretraining.\nRecent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. \nNevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models.\nIn this work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities.\nWe provide theoretical and experimental analyzes to show that foundation model pretraining loss and its descent velocity are both convex and share the same optimal learning rate. \nRelying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. \nExperiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. \nWe also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, base learning rate scheduler choices, and hyperparameter settings.",
    "authors": [
      "~Hongyuan_Dong2",
      "~Dingkang_Yang1",
      "~LiangXiao3",
      "~ChaoFeng1",
      "~Ran_Jiao1"
    ],
    "pdf": "/pdf/fbc823935262760b602282e9ac61708ffc1c0259.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Directly addresses learning rate optimization for foundation model pretraining, including LLMs; claims to improve training efficiency and effectiveness; adaptable across different model sizes and training scenarios; potentially reduces resource-intensive hyperparameter tuning; designed as a plug-and-play solution for existing training pipelines",
      "Irrelevant Aspects": "Does not explicitly mention GPU utilization, throughput, or latency metrics; focuses on learning rate optimization rather than system-level optimizations; no mention of inference optimization which is also part of my research interests",
      "Summary": "This paper presents AdaLRS, a loss-guided adaptive learning rate search algorithm for foundation model pretraining. It's highly relevant to my research on LLM training optimization as learning rate schedules significantly impact training efficiency and resource utilization. The method's adaptability across different model sizes and scenarios aligns well with scalability concerns. While it doesn't explicitly address system-level metrics like GPU utilization, effective learning rate optimization inherently affects these aspects by reducing training time and resource requirements."
    }
  },
  {
    "id": "6geRIdlFWJ",
    "title": "SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training",
    "abstract": "Training large language models (LLMs) is highly resource-intensive due to their massive number of parameters and the overhead of optimizer states. While recent work has aimed to reduce memory consumption, such efforts often entail trade-offs among memory efficiency, training time, and model performance. Yet, true democratization of LLMs requires simultaneous progress across all three dimensions. To this end, we propose SubTrack++ that leverages Grassmannian gradient subspace tracking combined with projection-aware optimizers, enabling Adam’s internal statistics to adapt to subspace changes. Additionally, employing recovery scaling, a technique that restores information lost through low-rank projections, further enhances model performance. Our method demonstrates SOTA convergence by exploiting Grassmannian geometry, **reducing pre-training wall-time by up to 65% and fine-tuning time by 36%** compared to existing SOTA methods, while maintaining the same memory footprint. Code is at https://github.com/criticalml-uw/SubTrack.",
    "authors": [
      "~Sahar_Rajabi1",
      "~Nayeema_Nonta1",
      "~Sirisha_Rambhatla1"
    ],
    "pdf": "/pdf/8786674610d2fffb72f718be4dfff9160bca7c97.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Directly addresses LLM training optimization, claims to reduce pre-training wall-time by up to 65% and fine-tuning time by 36% while maintaining memory footprint, uses gradient subspace tracking techniques, focuses on balancing memory efficiency, training time, and model performance",
      "Irrelevant Aspects": "Primarily focuses on training optimization rather than inference optimization, limited details about specific GPU utilization techniques in the abstract",
      "Summary": "SubTrack++ presents a method for optimizing LLM training through gradient subspace tracking and projection-aware optimizers, achieving significant reductions in training time while maintaining memory efficiency. The paper directly addresses training optimization but has limited coverage of inference optimization aspects."
    }
  },
  {
    "id": "8ySOcf7UpM",
    "title": "Feedback Guidance of Diffusion Models",
    "abstract": "While Classifier-Free Guidance (CFG) has become standard for improving sample fidelity in conditional diffusion models, it can harm diversity and induce memorization by applying constant guidance regardless of whether a particular sample needs correction. We propose **F**eed**B**ack **G**uidance (FBG), which uses a state-dependent coefficient to self-regulate guidance amounts based on need. Our approach is derived from first principles by assuming the learned conditional distribution is linearly corrupted by the unconditional distribution, contrasting with CFG's implicit multiplicative assumption. Our scheme relies on feedback of its own predictions about the conditional signal informativeness to adapt guidance dynamically during inference, challenging the view of guidance as a fixed hyperparameter. The approach is benchmarked on ImageNet512x512, where it significantly outperforms Classifier-Free Guidance and is competitive to Limited Interval Guidance (LIG) while benefitting from a strong mathematical framework. On Text-To-Image generation, we demonstrate that, as anticipated, our approach automatically applies higher guidance scales for complex prompts than for simpler ones and that it can be easily combined with existing guidance schemes such as CFG or LIG.",
    "authors": [
      "~Felix_Koulischer1",
      "~Florian_Handke1",
      "~Johannes_Deleu1",
      "~Thomas_Demeester1",
      "~Luca_Ambrogioni1"
    ],
    "pdf": "/pdf/34dd54f5b6673334795dec83c8e463e75bc52f68.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization for generative models, Dynamic adaptation during inference, State-dependent optimization approach, Combinability with existing guidance schemes, Sample quality optimization",
      "Irrelevant Aspects": "Primary focus on image generation rather than text models, Benchmarking on ImageNet rather than language tasks, No explicit discussion of GPU utilization or scalability, Limited focus on training optimization (mostly inference), Absence of specific latency measurements",
      "Summary": "The paper introduces Feedback Guidance (FBG), an inference-time optimization technique for conditional diffusion models that dynamically adjusts guidance strength based on the current generation state. Unlike Classifier-Free Guidance which applies constant guidance, FBG uses feedback from its own predictions to self-regulate guidance amounts. The approach is theoretically grounded and shows improved performance on image generation tasks while being compatible with existing guidance methods."
    }
  },
  {
    "id": "xSHqNf5Pdc",
    "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
    "abstract": "Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA’s effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy.  We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",
    "authors": [
      "~Xinglin_Wang1",
      "~Yiwei_Li1",
      "~Shaoxiong_Feng1",
      "~Peiwen_Yuan1",
      "~Yueqi_Zhang2",
      "~Jiayi_Shi1",
      "~Chuyi_Tan1",
      "~Boyuan_Pan1",
      "~Yao_Hu1",
      "~Kan_Li3"
    ],
    "pdf": "/pdf/639ca4d7460b732c0c9d399142939d60bcdb29d2.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Test-Time Scaling (TTS) for LLMs, efficient use of compute at test time, resource allocation problem formulation, maximizing performance under fixed computational budget, mathematical reasoning benchmark improvements, DORA method for optimal resource allocation",
      "Irrelevant Aspects": "Training optimization, explicit low-latency techniques, specific GPU utilization methods, hardware-specific optimizations",
      "Summary": "This paper addresses efficient resource allocation during inference for LLMs, specifically focusing on Test-Time Scaling. It proposes DORA, a method for optimal resource allocation that maximizes performance under fixed computational budgets. While highly relevant to inference optimization and scalability, it doesn't cover training optimization or specific low-latency techniques. The approach demonstrates improved efficiency on mathematical reasoning benchmarks, making it valuable for understanding how to better utilize computational resources during LLM inference."
    }
  },
  {
    "id": "2Qn6skg175",
    "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
    "abstract": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific tasks,\nespecially when retraining from scratch is computationally infeasible. Fine-tuning\nenables LLMs to leverage task- or domain-specific data, producing models that\nmore effectively meet the requirements of targeted applications. However, con-\nventional FT approaches often suffer from catastrophic forgetting and suboptimal\ndata efficiency, limiting their real-world applicability. To address these challenges,\nthis paper proposes DEAL, a novel framework that integrates Low-Rank Adapta-\ntion (LoRA) with a continuous fine-tuning strategy. By incorporating knowledge\nretention and adaptive parameter update modules, the framework mitigates the\nlimitations of existing FT methods while maintaining efficiency. Experiments on\n15 diverse datasets show that DEAL consistently outperforms baseline methods,\nyielding substantial gains in task accuracy and resource efficiency. These find-\nings demonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency. The\nsource code is publicly available at https://github.com/Applied-Machine-Learning-Lab/DEAL.",
    "authors": [
      "~Xiao_Han11",
      "~ZIMO_ZHAO1",
      "~Wanyu_Wang1",
      "~Maolin_Wang1",
      "~Zitao_Liu1",
      "~Yi_Chang4",
      "~Xiangyu_Zhao1"
    ],
    "pdf": "/pdf/5d24184e802d70769bb984a1b66bf83f72ae7656.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on training optimization for Large Language Models through parameter-efficient fine-tuning techniques; integrates Low-Rank Adaptation (LoRA) which reduces computational requirements; addresses resource efficiency which relates to GPU utilization and scalability; improves data efficiency which can contribute to higher throughput in training scenarios.",
      "Irrelevant Aspects": "Does not directly address inference optimization; no explicit discussion of latency reduction for deployment scenarios; limited focus on throughput during inference phase; does not detail specific GPU utilization techniques or metrics.",
      "Summary": "This paper presents DEAL, a framework combining LoRA with continuous fine-tuning to improve adaptation efficiency in LLMs. While it strongly aligns with training optimization aspects of the research interest through its focus on parameter efficiency and resource utilization during training, it lacks substantial content on inference optimization and latency reduction, which are key components of the specified research focus."
    }
  },
  {
    "id": "movsqor65f",
    "title": "Parameter Efficient Fine-tuning via Explained Variance Adaptation",
    "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned for a specific downstream task. The most common fine-tuning method is to update pretrained weights via low-rank adaptation (LoRA). Existing initialization strategies for LoRA often rely on singular value decompositions (SVD) of gradients or weight matrices. However, they do not provably maximize the expected gradient signal, which is critical for fast adaptation. To this end, we introduce **E**xplained **V**ariance **A**daptation (EVA), an initialization scheme that uses the directions capturing the most activation variance, provably maximizing the expected gradient signal and accelerating fine-tuning. EVA performs incremental SVD on minibatches of activation vectors and selects the right-singular vectors for initialization once they converged. Further, by selecting the directions that capture the most activation-variance for a given rank budget, EVA accommodates adaptive ranks that reduce the number of trainable parameters. We apply EVA to a variety of fine-tuning tasks as language generation and understanding, image classification, and reinforcement learning. EVA exhibits faster convergence than competitors and achieves the highest average score across a multitude of tasks per domain while reducing the number of trainable parameters through rank redistribution. In summary, EVA establishes a new Pareto frontier compared to existing LoRA\ninitialization schemes in both accuracy and efficiency.",
    "authors": [
      "~Fabian_Paischer1",
      "~Lukas_Hauzenberger1",
      "~Thomas_Schmied1",
      "~Benedikt_Alkin1",
      "~Marc_Peter_Deisenroth1",
      "~Sepp_Hochreiter1"
    ],
    "pdf": "/pdf/c0aac1d842e3070cb84e0df925258a782577fda1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on parameter-efficient fine-tuning methods (LoRA) which is directly relevant to training optimization for large language models. The approach aims to reduce trainable parameters while achieving faster convergence, which can lead to better GPU utilization during fine-tuning. By establishing a new Pareto frontier in accuracy and efficiency, it contributes to the scalability of language model adaptation.",
      "Irrelevant Aspects": "Doesn't directly address inference optimization, throughput, or latency concerns during deployment. The paper includes evaluation on tasks beyond language models (image classification, reinforcement learning). Lacks discussion of hardware-specific optimizations or memory-efficient implementations during training.",
      "Summary": "The paper introduces EVA, an initialization scheme for LoRA that uses activation variance to accelerate fine-tuning of foundation models. While it advances parameter-efficient fine-tuning methods that could improve GPU utilization during training, it doesn't address inference optimization which is equally important for my research interests. The approach shows promise for making large language model adaptation more efficient, but doesn't provide insights into the broader system optimizations needed for deployment."
    }
  },
  {
    "id": "jaMPaFDAaZ",
    "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.",
    "authors": [
      "~Shoutao_Guo1",
      "~Shaolei_Zhang1",
      "~Qingkai_Fang1",
      "~Zhengrui_Ma1",
      "~Min_zhang14",
      "~Yang_Feng4"
    ],
    "pdf": "/pdf/ed9a9ed39891b0f6e01c98f12a13df9dc4c3be10.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses efficient processing of long sequences, improves inference efficiency, tackles high computational costs, implements compression strategies for better memory utilization, uses dynamic compression training for scalability, demonstrates performance-efficiency trade-offs, and develops systematic benchmarks - all directly relevant to LLM optimization for better GPU utilization, scalability, throughput and latency.",
      "Irrelevant Aspects": "The focus on speech-language models is domain-specific rather than general LLMs, some techniques may be speech-specific rather than broadly applicable, the emphasis on speech dataset scarcity is less central to computational optimization, and the attention to short-speech task maintenance is less directly relevant to my optimization focus.",
      "Summary": "FastLongSpeech presents a framework for efficient long-speech processing through sequence compression and dynamic training. While speech-domain specific, its computational optimization approaches for handling long sequences, reducing inference costs, and improving efficiency have significant relevance to general LLM optimization challenges."
    }
  },
  {
    "id": "qMm7tC1zvj",
    "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
    "abstract": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. \nEmbodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices.\nAt the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices.\nIn this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps.\nTo reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache.\nExtensive experimental results demonstrate that EfficientNav\nachieves 11.1\\% improvement in success rate on HM3D benchmark over GPT-4-based baselines, \nand demonstrates 6.7$\\times$ real-time latency reduction and 4.7$\\times$ end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
    "authors": [
      "~Zebin_Yang3",
      "~Sunjian_Zheng1",
      "~Tong_Xie4",
      "~Tianshi_Xu1",
      "~Bo_Yu8",
      "~Fan_Wang4",
      "~Jie_Tang9",
      "~Shaoshan_Liu1",
      "~Meng_Li1"
    ],
    "pdf": "/pdf/2001e2272aed429a8f96bf544c6ac5d5d8645cfb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on optimizing LLM inference for on-device deployment, addresses challenges of using smaller LLMs instead of cloud-based models, achieves significant latency reduction (6.7× real-time and 4.7× end-to-end), proposes discrete memory caching and attention-based memory clustering for efficient KV cache reuse, addresses long prompt latency issues on local devices, improves smaller LLM performance through semantics-aware memory retrieval",
      "Irrelevant Aspects": "Application domain is object-goal navigation rather than general LLM systems, uses navigation-specific benchmark (HM3D), focuses more on application-specific optimization rather than general LLM techniques",
      "Summary": "This paper addresses important LLM inference optimization challenges, particularly for edge deployment, though with a specific application focus. The techniques for reducing latency, optimizing memory usage, and improving smaller model performance are directly relevant to LLM optimization interests, despite the navigation-specific context."
    }
  },
  {
    "id": "iydmH9boLb",
    "title": "Advancing Expert Specialization for Better MoE",
    "abstract": "Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. \nHowever, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training.\nTo address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions.\nGradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process.\nExperimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. \nNotably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.",
    "authors": [
      "~Hongcan_Guo1",
      "~Haolang_Lu1",
      "~Guoshun_Nan1",
      "~Bolun_Chu1",
      "~Jialin_Zhuang1",
      "~Yuan_Yang6",
      "~Wenhao_Che1",
      "~Xinye_Cao1",
      "~Sicong_Leng1",
      "~Qimei_Cui1",
      "~Xudong_Jiang1"
    ],
    "pdf": "/pdf/39f723331b55f80ce561b9e84c01ce678e145b92.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses Mixture-of-Experts (MoE) models, which are crucial for efficient scaling of large language models and directly impact GPU utilization. It focuses on training optimization techniques that enhance expert specialization, which can lead to better computational efficiency during both training and inference. The proposed orthogonality loss and variance loss improve routing decisions and maintain load balancing, which are essential factors for distributed training systems and multi-GPU scenarios. The method's ability to improve performance without architectural changes makes it practical for existing systems, potentially enhancing throughput and scalability.",
      "Irrelevant Aspects": "The paper primarily focuses on the quality improvement through expert specialization rather than directly measuring or optimizing for GPU utilization metrics, throughput, or latency. There's no explicit discussion of inference optimization strategies beyond routing improvements. The abstract doesn't address low-level implementation details or specific hardware utilization strategies. The paper doesn't appear to cover memory optimization techniques, which are critical for large model training and inference.",
      "Summary": "This paper presents a method to improve Mixture-of-Experts models by enhancing expert specialization through orthogonality and variance losses. While it doesn't directly address system-level optimization metrics like GPU utilization or latency, its focus on improving training efficiency and routing decisions in MoE models makes it moderately relevant to my research interests. The improvements in expert specialization could indirectly lead to better computational efficiency, but the paper lacks explicit discussion of system-level performance metrics."
    }
  },
  {
    "id": "aNpj43Uh35",
    "title": "Multi-Objective One-Shot Pruning for Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but require substantial computational resources, limiting their deployment in resource-constrained environments. While one-shot pruning methods can reduce model size without expensive retraining, they typically optimize for single objectives, ignoring LLMs' multi-faceted applications. We introduce Multi-Objective One-Shot Pruning (MOSP), which formulates LLM pruning as a multi-objective optimization problem. MOSP efficiently generates a Pareto set of pruned models representing different capability trade-offs, allowing users to select solutions aligned with their preferences. The proposed approach identifies share core support while enabling specialized support. Experiments across various LLMs and sparsity levels demonstrate MOSP's superior performance in navigating multi-objective trade-offs compared to baseline methods.",
    "authors": [
      "~Weiyu_Chen1",
      "~Hansi_Yang1",
      "~Yunhao_GOU1",
      "~Han_Shi1",
      "~En-Liang_Hu1",
      "~Zhenguo_Li1",
      "~James_Kwok1"
    ],
    "pdf": "/pdf/57bc887b7564d9c4265d180d739174577189c519.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on Large Language Model optimization through pruning techniques, specifically one-shot pruning which reduces model size without expensive retraining. This directly impacts inference optimization and resource efficiency. The multi-objective optimization approach for generating pruned models with different capability trade-offs could be valuable for deployment scenarios with varying resource constraints. The method addresses computational resource requirements for LLMs, which is central to improving GPU utilization and enabling deployment in resource-constrained environments.",
      "Irrelevant Aspects": "The abstract emphasizes capability trade-offs over specific computational efficiency metrics like throughput and latency. There's limited focus on training optimization as the method avoids retraining. The paper doesn't explicitly address GPU utilization optimization or scalability challenges in distributed systems, which are key aspects of the research interest. The multi-objective formulation appears to prioritize model capability trade-offs rather than computational performance metrics.",
      "Summary": "This paper presents Multi-Objective One-Shot Pruning (MOSP) for LLMs, which formulates pruning as a multi-objective optimization problem to generate models with different capability trade-offs. While relevant to LLM optimization and resource efficiency, it focuses more on capability trade-offs than on specific computational metrics like GPU utilization, throughput, and latency. The one-shot pruning approach is valuable for reducing computational resources without retraining, but the abstract doesn't deeply address the system optimization aspects that are central to the research interest."
    }
  },
  {
    "id": "a3l3K9khbL",
    "title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
    "abstract": "Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.",
    "authors": [
      "~Yamato_Arai1",
      "~Yuma_Ichikawa1"
    ],
    "pdf": "/pdf/5ca6c379fbc6623e7cc32761925e35641bb88fbd.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on post-training quantization (PTQ) for large language models, which is directly relevant to inference optimization. Quantization is a key technique for reducing model size and computational requirements, leading to better GPU utilization and higher throughput. The method addresses extremely low-bit quantization regimes, which aligns with my interest in maximizing efficiency. The approach is described as 'lightweight and scalable,' directly relating to scalability concerns. By tackling quantization errors, the paper enables better performance even with aggressive compression, potentially improving latency/throughput tradeoffs. The experiments on multiple LLMs indicate broad applicability to the model types I'm interested in.",
      "Irrelevant Aspects": "The paper specifically focuses on post-training quantization rather than training optimization. It doesn't explicitly discuss GPU utilization metrics or detailed performance benchmarks related to throughput and latency. The focus is on the quantization methodology rather than comprehensive system-level optimizations. There's no mention of distributed inference or other scaling techniques beyond the model compression aspect.",
      "Summary": "This paper presents Quantization Error Propagation (QEP), a framework for improving layer-wise post-training quantization of large language models by addressing the propagation and accumulation of quantization errors across layers. The method is particularly effective in extremely low-bit quantization scenarios, offering a lightweight and scalable approach to maintaining model performance while significantly reducing model size. The research aligns well with my interests in inference optimization for LLMs, as quantization directly impacts GPU utilization, throughput, and latency, though the paper focuses more on the quantization methodology itself rather than comprehensive system-level performance analysis."
    }
  },
  {
    "id": "DbzREoPwmM",
    "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
    "abstract": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost.",
    "authors": [
      "~Luca_Eyring1",
      "~Shyamgopal_Karthik1",
      "~Alexey_Dosovitskiy1",
      "~Nataniel_Ruiz1",
      "~Zeynep_Akata1"
    ],
    "pdf": "/pdf/de61ca7ae49bbf649a4b6c3d85736ba55a32dc7b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization and reducing computational overhead, Amortizing test-time computation into model parameters, Maintaining quality gains while improving efficiency, Addressing latency issues in generative models",
      "Irrelevant Aspects": "Focus on diffusion models rather than LLMs, No explicit discussion of training optimization, Limited coverage of distributed computing or multi-GPU scenarios, Hypernetwork approach rather than more common LLM optimization techniques",
      "Summary": "This paper proposes a method to optimize diffusion model inference by using Noise Hypernetworks to incorporate test-time scaling knowledge into the model during post-training. The approach replaces explicit test-time noise optimization with a learned hypernetwork that modulates input noise, maintaining quality gains while significantly reducing inference computational cost. While the focus is on diffusion models rather than LLMs, the paper addresses important aspects of inference optimization, particularly the amortization of test-time compute to improve efficiency."
    }
  },
  {
    "id": "VfIOdGiBAv",
    "title": "Scaling Diffusion Transformers Efficiently via $\\mu$P",
    "abstract": "Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize $\\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate achieves 2.9$\\times$ faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\\mu$P on text-to-image generation by scaling PixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\\mu$P outperform their respective baselines while requiring small tuning cost—only 5.5% of one training run for PixArt-$\\alpha$ and 3% of consumption by human experts for MMDiT-18B. \\textit{These results establish $\\mu$P as a principled and efficient framework for scaling diffusion Transformers}.",
    "authors": [
      "~Chenyu_Zheng1",
      "~Xinyu_Zhang2",
      "~Rongzhen_Wang1",
      "~Wei_Huang6",
      "~Zhi_Tian2",
      "~Weilin_Huang1",
      "~Jun_Zhu2",
      "~Chongxuan_Li1"
    ],
    "pdf": "/pdf/c31485f7d0a8446f8f4f04b3b92001661aa138cb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization for large-scale Transformers, efficient scaling techniques from small to large models, GPU utilization through faster convergence (2.9×), hyperparameter transfer reducing tuning costs, demonstrated scalability to 18B parameters",
      "Irrelevant Aspects": "Application to diffusion models rather than language models, no focus on inference optimization, lack of discussion on latency reduction, image generation domain rather than text processing",
      "Summary": "This paper generalizes Maximal Update Parametrization (μP) to diffusion Transformers, enabling efficient scaling from small to large models with minimal hyperparameter tuning. It demonstrates 2.9× faster convergence and dramatically reduced tuning costs. The training optimization and scalability aspects are relevant to my research interests, but the focus on diffusion models and lack of inference optimization content limit its overall relevance."
    }
  },
  {
    "id": "Tjw0ACu3NL",
    "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
    "abstract": "While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, compared with exact gradients, ZO-based gradients usually exhibit an estimation error, which can significantly hurt the optimization process, leading to slower convergence and suboptimal solutions. In addition, we find that the estimation error will hurt more when adding to large weights instead of small weights. Based on this observation, this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task.",
    "authors": [
      "~Yong_Liu13",
      "~Zirui_Zhu2",
      "~Chaoyu_Gong1",
      "~Minhao_Cheng1",
      "~Cho-Jui_Hsieh1",
      "~Yang_You1"
    ],
    "pdf": "/pdf/317ff5a023f3c71c738a95b75c54d4f89202f41a.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Memory-efficient optimization for LLMs, GPU utilization improvements (fine-tuning LLaMA-30b on single A100 GPU), throughput improvements (3.5x speedup), scalability (enabling larger models on limited hardware), sparse optimization techniques",
      "Irrelevant Aspects": "The accuracy improvement (9% absolute) is less relevant to system-focused interests, the specific tasks used for evaluation (like RTE) are not as relevant as the optimization techniques themselves",
      "Summary": "The paper proposes Sparse MeZO, a memory-efficient zeroth-order optimization approach for LLM fine-tuning that significantly improves GPU utilization, enables fine-tuning of larger models on limited hardware (30B model on single A100 GPU), and achieves substantial throughput improvements (3.5x speedup), making it highly relevant to research on training optimization and resource utilization for large language models."
    }
  },
  {
    "id": "yISJGSdzdd",
    "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
    "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization.  To overcome these limitations, we propose **M**odule-wise **I**mportance **SA**mpling (**MISA**), a novel method that divides each layer into smaller modules and assigns importance scores to each module. \nMISA uses a weighted random sampling mechanism to activate modules, provably reducing\ngradient variance compared to layer-wise sampling. \nAdditionally, we establish an $\\mathcal{O}(1/\\sqrt{K})$ convergence rate under non-convex and stochastic conditions, where $K$ is the total number of training steps, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods.  Experiments on diverse learning tasks validate the effectiveness of MISA.",
    "authors": [
      "~Yuxi_Liu9",
      "~Renjia_Deng1",
      "~Yutong_He2",
      "~Xue_Wang9",
      "~Tao_Yao2",
      "~Kun_Yuan4"
    ],
    "pdf": "/pdf/ce2f92475278c309601629e80fcbf55142348547.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Memory-efficient optimization algorithms for LLMs, Module-wise optimization with importance sampling, Reducing memory footprint during training, Theoretical convergence rate analysis, Memory analysis compared to baseline methods, Pre-training and fine-tuning optimization",
      "Irrelevant Aspects": "No explicit discussion of inference optimization, Limited coverage of throughput and latency impacts, Insufficient information about distributed GPU scalability, Narrow focus on training optimization rather than end-to-end pipeline optimization",
      "Summary": "MISA proposes a module-wise optimization approach for LLMs that divides transformer layers into smaller modules and applies importance sampling. The method reduces memory requirements during training by selectively activating modules based on their importance scores, with theoretical guarantees on convergence. While innovative for training optimization and GPU memory utilization, the paper lacks focus on inference optimization, throughput, latency improvements, and distributed scaling considerations."
    }
  },
  {
    "id": "xWYL9Ki32T",
    "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs",
    "abstract": "The scaling law for large language models (LLMs) depicts that the path towards machine intelligence necessitates training at large scale. Thus, companies continuously build large-scale GPU clusters, and launch training jobs that span over thousands of computing nodes. However, LLM pre-training presents unique challenges due to its complex communication patterns, where GPUs exchange data in sparse yet high-volume bursts within specific groups. Inefficient resource scheduling exacerbates bandwidth contention, leading to suboptimal training performance. This paper presents Arnold, a scheduling system summarizing our experience to effectively align LLM communication patterns to data center topology at scale. In-depth characteristic study is performed to identify the impact of physical network topology to LLM pre-training jobs. Based on the insights, we develop a scheduling algorithm to effectively align communication patterns to physical network topology in data centers. Through simulation experiments, we show the effectiveness of our algorithm in reducing the maximum spread of communication groups by up to $1.67$x. In production training, our scheduling system improves the end-to-end performance by $10.6\\%$ when training with more than $9600$ Hopper GPUs, a significant improvement for our training pipeline.",
    "authors": [
      "~Guoliang_HE2",
      "~YOUHE_JIANG1",
      "~Wencong_Xiao1",
      "~Jiang_Kaihua1",
      "~Shuguang_Wang4",
      "~Jun_Wang99",
      "~Du_Zixian1",
      "~Zhuo_Jiang3",
      "~Xinlei_Zhang3",
      "~Binhang_Yuan1",
      "~Eiko_Yoneki1"
    ],
    "pdf": "/pdf/2a97b9aff754ccdbaac31b4b7efcf27f2b4f8122.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "LLM pre-training optimization, GPU utilization at scale (9600+ GPUs), throughput improvements (10.6% end-to-end performance gain), efficient communication patterns in distributed training, scalability solutions for large language models",
      "Irrelevant Aspects": "Focus on scheduling systems rather than direct optimization algorithms, emphasis on network topology details, reliance on simulation experiments rather than implementation details",
      "Summary": "This paper presents Arnold, a scheduling system that aligns LLM communication patterns to data center topology at scale. Through identifying the impact of physical network topology on LLM pre-training, the authors developed a scheduling algorithm that reduces communication group spread by up to 1.67x. In production with over 9600 Hopper GPUs, their system improves end-to-end performance by 10.6%, addressing critical challenges in large-scale LLM training efficiency."
    }
  },
  {
    "id": "24wDPGiDzA",
    "title": "Unified Scaling Laws for Compressed Representations",
    "abstract": "Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression strategies, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include proposing and validating a general scaling law formulation applicable both individually but also composably across compression types. We demonstrate both theoretically and empirically that a simple metric based on Gaussian mean squared error fitting can robustly predict parameter efficiency across compressed models. Additionally, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats. Finally, we identify conditions under which these unified scaling laws fail.",
    "authors": [
      "~Andrei_Panferov1",
      "~Alexandra_Volkova1",
      "~Ionut-Vlad_Modoranu1",
      "~Vage_Egiazarian1",
      "~Mher_Safaryan1",
      "~Dan_Alistarh7"
    ],
    "pdf": "/pdf/df76488d2d9bf398c00f4729771bf91f70d7378b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Model compression techniques (quantization and sparsification) for computational efficiency; Training optimization over compressed representations; Exploration of sparse, scalar-quantized, sparse-quantized, and vector-quantized formats; Algorithm development for training sparse-quantized formats; Performance prediction for compressed models",
      "Irrelevant Aspects": "Theoretical focus on scaling laws formulation rather than practical implementation; Academic aspects about identifying conditions where laws fail; Gaussian mean squared error fitting methodology",
      "Summary": "This paper explores the intersection of scaling laws and compression techniques for machine learning models. It develops a unified framework to predict performance when training with various compressed representations. The research validates a general scaling law applicable across compression types, proposes a Gaussian MSE-based metric for predicting parameter efficiency, and develops better algorithms for training sparse-quantized formats. While theoretically oriented, it addresses key aspects of model compression that impact computational efficiency and scalability."
    }
  },
  {
    "id": "Wc1VZ2bVJn",
    "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
    "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrated the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek‑R1‑Distill‑Qwen‑7B fine-tuned by using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at \\url{https://github.com/zhangyx1122/TokenSqueeze}.",
    "authors": [
      "~Yuxiang_Zhang8",
      "~Zhengxu_Yu1",
      "~Weihang_Pan2",
      "~Zhongming_Jin1",
      "~Qiang_Fu19",
      "~Deng_Cai4",
      "~Binbin_Lin3",
      "~Jieping_Ye4"
    ],
    "pdf": "/pdf/2658ef233811501fdccb69e12fe4d667bf1fe11d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization for reasoning LLMs, 50% token reduction while preserving accuracy, impact on latency and memory consumption, improved scalability, self-generated data approach, performance preservation during optimization",
      "Irrelevant Aspects": "Limited focus on training optimization itself, CoT-specific approach, methodology focused on linguistic refinement rather than system optimization, benchmark focus not directly measuring system optimization metrics",
      "Summary": "TokenSqueeze presents a method for compressing reasoning paths in LLMs, reducing token usage by 50% while preserving accuracy. While not directly addressing training optimization or GPU utilization techniques, it has significant implications for inference optimization, throughput, and scalability. The approach is specific to reasoning LLMs with chain-of-thought traces, which limits its broader applicability but provides valuable insights into a growing area of LLM deployment."
    }
  },
  {
    "id": "VCj7knCJhn",
    "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training",
    "abstract": "Recent large language models (LLMs) exhibit impressive reasoning but often \\textit{overthink}, generating excessively long responses that hinder efficiency. We introduce DIET (DIfficulty-AwarE Training), a framework that systematically cuts these \"token calories\" by integrating on-the-fly problem difficulty into the reinforcement learning (RL) process. DIET dynamically adapts token compression strategies by modulating token penalty strength and conditioning target lengths on estimated task difficulty, to optimize the performance-efficiency trade-off. We also theoretically analyze the pitfalls of naive reward weighting in group-normalized RL algorithms like GRPO, and propose \\textit{Advantage Weighting} technique, which enables stable and effective implementation of these difficulty-aware objectives. Experimental results demonstrate that DIET significantly reduces token counts while simultaneously improving reasoning performance. Beyond raw token reduction, we show two crucial benefits largely overlooked by prior work: (1) DIET leads to superior \\textbf{inference scaling}. By maintaining high per-sample quality with fewer tokens, it enables better scaling performance via majority voting under fixed computational budgets, an area where other methods falter. (2) DIET enhances the natural positive correlation between response length and problem difficulty, ensuring verbosity is appropriately allocated, unlike many existing compression methods that disrupt this relationship. Our analyses provide a principled and effective framework for developing more efficient, practical, and high-performing LLMs.",
    "authors": [
      "~Weize_Chen1",
      "~Jiarui_Yuan1",
      "~Tailin_Jin1",
      "~Ning_Ding5",
      "~Huimin_Chen3",
      "~Zhiyuan_Liu1",
      "~Maosong_Sun1"
    ],
    "pdf": "/pdf/33e46bebe7b8b298e40a0c3d81837e4d6c1f4673.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training optimization through difficulty-aware reinforcement learning; Inference optimization via token reduction; GPU utilization and throughput improvements; Latency reduction; Superior inference scaling under computational budgets; Performance-efficiency trade-off optimization; Theoretical analysis of RL algorithm pitfalls; Advantage Weighting technique for stable implementation",
      "Irrelevant Aspects": "Limited focus on reasoning tasks only; No model size compression techniques; Lack of hardware-specific optimizations; No discussion of distributed training approaches; Limited coverage of quantization or precision-based optimizations",
      "Summary": "The paper introduces DIET, a difficulty-aware training framework that optimizes LLMs to reduce token consumption while maintaining performance. It addresses both training and inference optimization by dynamically adapting token compression based on problem difficulty. The work demonstrates significant token reduction, improved reasoning performance, and better inference scaling under fixed computational budgets. The theoretical analysis of RL algorithm pitfalls and proposed Advantage Weighting technique provides a principled approach to optimization, making this highly relevant for LLM efficiency and scalability research."
    }
  },
  {
    "id": "qI95wZZCWh",
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
    "abstract": "Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision–language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process—where people skip reasoning for easy questions but think carefully when needed—we explore how to enable VLMs to first decide *when reasoning is necessary*.\nTo realize this, we propose \\ours, a two-stage training strategy:\n**(i)** a supervised fine-tuning (SFT) stage with a simple yet effective “**thought dropout**” operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning;  **(ii)** a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards.\nExperimental results show that \\ours can *reduce the completion length by up to **90%** compared to vanilla GRPO, without sacrificing performance or even improving it*. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR, GeoQA), and Agentic (AITZ) tasks—covering a range of reasoning difficulties under both 3B and 7B models—consistently reveal that the \\textit{model progressively learns to bypass unnecessary reasoning steps as training advances}.\nThese findings shed light on the path toward human-like reasoning patterns in RL approaches.\nOur code is available at https://github.com/kokolerk/TON.",
    "authors": [
      "~Jiaqi_WANG11",
      "~Kevin_Qinghong_Lin1",
      "~James_Cheng2",
      "~Mike_Zheng_Shou1"
    ],
    "pdf": "/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Reduction in completion length by up to 90%, which directly impacts inference efficiency and resource utilization. The two-stage training approach with 'thought dropout' represents a novel optimization strategy. The method enables selective reasoning, reducing unnecessary token generation and computational overhead during inference. Demonstrated scalability across different model sizes (3B and 7B parameters).",
      "Irrelevant Aspects": "Focuses specifically on vision-language models rather than general LLM systems. Limited discussion of system-level implementation details affecting GPU utilization. No detailed benchmarks of inference latency, throughput, or specific GPU memory utilization metrics. Emphasis is on reasoning performance rather than optimization metrics directly relevant to my expertise.",
      "Summary": "This paper introduces a selective reasoning approach for VLMs that learns when to generate reasoning traces and when to bypass them. The two-stage training method significantly reduces token usage (up to 90%) without sacrificing performance, which has direct implications for inference efficiency. While the computational cost reduction is significant for optimization goals, the paper focuses more on reasoning capabilities than detailed system performance metrics that would be most relevant to my research interests in LLM training and inference optimization."
    }
  },
  {
    "id": "cQxLCVa9u7",
    "title": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location",
    "abstract": "Large language models (LLMs) have facilitated a wide range of applications with distinct service-level objectives (SLOs), from latency-sensitive online tasks like interactive chatbots to throughput-oriented offline workloads like data synthesis. The existing deployment model, which dedicates machines to each workload, simplifies SLO management but often leads to poor resource utilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving SLOs. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor to estimate batch execution time and an SLO-aware profiler to quantify latency interference, and (2) SLO-aware offline scheduling policies that maximize serving throughput and prevent starvation. Our evaluation on production workloads shows that HyGen achieves up to 3.9-5.8× throughput gains over online and hybrid serving baselines, while ensuring latency SLOs. The code of HyGen is publicly available at https://github.com/UIUC-MLSys/HyGen.",
    "authors": [
      "~Ting_Sun2",
      "~Penghan_Wang1",
      "~Fan_Lai1"
    ],
    "pdf": "/pdf/40f47181fc0983cc4ba51054de34b5d6a4e75e14.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on improving GPU utilization and throughput while maintaining latency SLOs. It introduces an innovative co-location approach for online and offline workloads, which is a key optimization strategy for production LLM serving systems. The reported 3.9-5.8× throughput gains directly align with the interest in higher throughput. The latency predictor and SLO-aware profiler are relevant mechanisms for optimizing inference performance.",
      "Irrelevant Aspects": "The paper focuses specifically on serving (inference) rather than training optimization. It doesn't address model architecture improvements or training-specific optimizations. The approach is system-level rather than focusing on algorithmic improvements to the models themselves.",
      "Summary": "HyGen introduces an interference-aware LLM serving system that enables efficient co-location of latency-sensitive online tasks and throughput-oriented offline workloads. It achieves significant throughput improvements (3.9-5.8×) over baseline systems while preserving latency SLOs through performance control mechanisms including a latency predictor and SLO-aware profiler, combined with SLO-aware scheduling policies. The system addresses poor resource utilization in existing deployment models by enabling elastic co-location of different workload types."
    }
  },
  {
    "id": "aqpHTPC63N",
    "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Bootstrapping",
    "abstract": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model performance improves, raising a crucial question: How should the total budget for generation and training be allocated across iterations to maximize final performance? \nIn this work, we develop a theoretical framework for analyzing budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies---particularly exponential growth policies---exhibit significant theoretical advantages. \nExperiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance.",
    "authors": [
      "~Pu_Yang3",
      "~Yunzhen_Feng1",
      "~Ziyuan_Chen2",
      "~Yuhang_Wu2",
      "~Zhuoyuan_Li1"
    ],
    "pdf": "/pdf/63e35b6d090c274b0de2624b78d0aebfa54c3991.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses optimization of post-training processes for LLMs, focusing on budget allocation strategies across iterative training. It examines how resource allocation impacts GPU utilization and model performance. The research includes experiments with LLMs for math reasoning tasks, which is directly relevant to LLM training optimization. The findings about exponential vs. constant policies could significantly impact training efficiency and resource utilization.",
      "Irrelevant Aspects": "The paper includes work on image denoising with diffusion models, which is less relevant to my LLM focus. It doesn't appear to address inference optimization, which is a key aspect of my expertise. The theoretical focus seems to be on strategic resource allocation rather than technical implementation details of GPU optimization.",
      "Summary": "This paper provides valuable insights into optimizing post-training processes for LLMs through strategic budget allocation across iterative synthetic data bootstrapping. While highly relevant to training optimization aspects of my expertise, it lacks coverage of inference optimization and includes work on non-LLM models, making it moderately relevant to my overall research focus."
    }
  },
  {
    "id": "MI1uT5rReV",
    "title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning",
    "abstract": "When applying reinforcement learning—typically through GRPO—to large vision-language model reasoning struggles to effectively scale reasoning length or generates verbose outputs across all tasks with only marginal gains in accuracy.\nTo address this issue, we present FAST-GRPO, a variant of GRPO that dynamically adapts reasoning depth based on question characteristics. \nThrough empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance.\nInspired by these observations, we introduce two complementary metrics to estimate the difficulty of the questions, guiding the model to determine when fast or slow thinking is more appropriate. \nNext, we incorporate adaptive length-based rewards and difficulty-aware KL divergence into the GRPO algorithm.\nExperiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10% relative improvement compared to the base model, while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.",
    "authors": [
      "~Wenyi_Xiao2",
      "~Leilei_Gan1"
    ],
    "pdf": "/pdf/52fe68c14c7abde636af8fd396bd6a026575c7f0.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization by reducing token usage while maintaining accuracy, which directly relates to improving throughput and reducing latency. The adaptive reasoning depth approach represents dynamic resource allocation relevant to GPU utilization. The work on GRPO optimization connects to training optimization techniques. The 32.7-67.3% reduction in token usage could significantly improve system efficiency. The adaptive approach to determining computational resources based on task difficulty is highly relevant to optimization goals.",
      "Irrelevant Aspects": "The paper focuses specifically on vision-language models rather than pure language models. It centers on GRPO algorithm variants rather than broader optimization techniques. The abstract doesn't directly address GPU utilization techniques or distributed computing aspects. The fast-slow thinking paradigm is a specialized approach rather than a general optimization framework.",
      "Summary": "This paper presents FAST-GRPO, a method for optimizing large vision-language model reasoning through dynamic adaptation of thinking depth. It introduces difficulty estimation metrics and adaptive length-based rewards to balance accuracy and efficiency. While focused on vision-language models, the approach of reducing token usage by 32.7-67.3% while improving accuracy has direct implications for inference optimization, throughput improvement, and latency reduction, making it highly relevant to LLM optimization despite its vision-language focus."
    }
  },
  {
    "id": "5Iw1nDtYmT",
    "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows",
    "abstract": "Large language model (LLM) based agentic workflows have become a popular paradigm for coordinating multiple specialized agents to solve complex tasks. To improve serving efficiency, existing LLM systems employ prefix caching to reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby avoiding redundant computation across repeated invocations. However, current systems typically evict KV caches using a Least Recently Used (LRU) policy, which fails to anticipate future agent usage and often discards KV caches shortly before their reuse. This leads to frequent cache misses and substantial recomputation or swap- ping overhead. We present KVFlow, a workflow-aware KV cache management framework tailored for agentic workloads. KVFlow abstracts the agent execution schedule as an Agent Step Graph and assigns each agent a steps-to-execution value that estimates its temporal proximity to future activation. These values guide a fine-grained eviction policy at the KV node level, allowing KVFlow to preserve entries likely to be reused and efficiently manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a fully overlapped KV prefetching mecha- nism, which proactively loads required tensors from CPU to GPU in background threads for agents scheduled in the next step, thereby avoiding cache miss stalls during generation. Compared to SGLang with hierarchical radix cache, KVFlow achieves up to 1.83× speedup for single workflows with large prompts, and up to 2.19× speedup for scenarios with many concurrent workflows.",
    "authors": [
      "~Zaifeng_Pan1",
      "~AJJKUMAR_PATEL1",
      "~Yipeng_Shen2",
      "~Zhengding_Hu3",
      "~Yue_Guan2",
      "~Wan-Lu_Li1",
      "~Lianhui_Qin1",
      "~Yida_Wang1",
      "~Yufei_Ding1"
    ],
    "pdf": "/pdf/2c47adb29432f99879fceb1371b72f6e97e1f3ac.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on KV cache optimization for LLM inference, directly addressing GPU utilization, scalability, throughput, and latency improvements. Introduces novel eviction policies and prefetching mechanisms for efficient memory management. Handles multi-agent workflows, an increasingly important use case for LLMs. Demonstrates significant performance improvements (1.83× to 2.19× speedup) over existing systems like SGLang.",
      "Irrelevant Aspects": "Exclusively focuses on inference optimization rather than training optimization. Does not discuss model architecture changes or quantization techniques. Lacks coverage of distributed training or mixed-precision training methods. Limited discussion on hardware-specific optimizations beyond general GPU/CPU considerations.",
      "Summary": "KVFlow presents a workflow-aware KV cache management framework for LLM-based multi-agent workflows, addressing limitations of simple LRU eviction policies used in current systems. By abstracting agent execution as an Agent Step Graph and implementing fine-grained eviction based on temporal proximity to future use, along with background prefetching mechanisms, the system significantly improves inference efficiency while reducing redundant computations."
    }
  },
  {
    "id": "usOkGv1S7M",
    "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
    "abstract": "Post-training has demonstrated its importance in enhancing the reasoning capabilities of large language models (LLMs). The primary post-training methods can be categorized into supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). SFT is efficient and well-suited for small language models, but it may lead to overfitting and limit the reasoning abilities of larger models. In contrast, RFT generally yields better generalization but depends heavily on the strength of the base model. To address the limitations of SFT and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm that unifies SFT and RFT into a single, integrated process. UFT enables the model to effectively explore solutions while incorporating informative supervision signals, bridging the gap between memorizing and thinking underlying existing methods. Notably, UFT outperforms both SFT and RFT in general, regardless of model sizes. Furthermore, we theoretically prove that UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for the first time that unified training can exponentially accelerate convergence on long-horizon reasoning tasks.",
    "authors": [
      "~Mingyang_Liu1",
      "~Gabriele_Farina1",
      "~Asuman_E._Ozdaglar1"
    ],
    "pdf": "/pdf/4638c4fdf4fd757fe0b2cd9e2720d114e2529986.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents a novel post-training paradigm (UFT) that unifies supervised and reinforcement fine-tuning methods for LLMs. It claims to break RFT's exponential sample complexity bottleneck, which directly impacts training efficiency and scalability. The approach works across different model sizes, addressing both small and large model training challenges. This has potential implications for training optimization and resource utilization.",
      "Irrelevant Aspects": "The paper does not address inference optimization, which is a key part of my research focus. There's no explicit discussion of GPU utilization optimization, throughput improvement, or latency reduction during inference. The focus appears to be on algorithmic approach rather than system implementation details that would impact resource utilization.",
      "Summary": "UFT proposes a unified post-training paradigm that combines supervised and reinforcement fine-tuning methods. While it offers promising improvements to training efficiency and potentially reduces computational resources needed for post-training, it primarily focuses on the algorithmic aspect rather than the system-level optimizations needed for maximizing GPU utilization, throughput, and minimizing latency during inference."
    }
  },
  {
    "id": "9o7oH6DAHB",
    "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning",
    "abstract": "Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods. Code is available in github.com/KingdalfGoodman/LoTA-QAF.",
    "authors": [
      "~Junyu_Chen7",
      "~Junzhuo_Li1",
      "~Zhen_Peng7",
      "~Wenjie_Wang12",
      "~Yuxiang_Ren1",
      "~Long_Shi2",
      "~Xuming_Hu1"
    ],
    "pdf": "/pdf/87e9e056817eb2d02e742e66682de092d74a203b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses quantization of LLMs for deployment on edge devices, which relates to inference optimization. It proposes a novel fine-tuning method (LoTA-QAF) designed specifically for quantized LLMs that enables lossless merging of ternary adaptation weights into quantized weights. This has direct implications for improving computational efficiency during inference, potentially enhancing throughput and reducing latency. The method also deals with data type mismatches between low-precision quantized weights and high-precision adaptation weights, which is a practical concern in optimization. Testing on large models like Llama-3.1/3.3 and Qwen-2.5 families shows performance improvements over standard 16-bit LoRA.",
      "Irrelevant Aspects": "The paper primarily focuses on edge device deployment rather than data center GPU optimization, which may limit direct applicability to my research context. There's no explicit discussion of GPU utilization metrics or detailed throughput/latency measurements. The evaluation appears more focused on accuracy recovery (MMLU benchmark) rather than computational efficiency metrics. The emphasis on ternary adaptation and specific quantization techniques might be too specialized for general optimization strategies.",
      "Summary": "LoTA-QAF introduces a quantization-aware fine-tuning method that uses ternary adaptation to adjust quantized weights while maintaining computational efficiency. While it addresses important aspects of model optimization for deployment, it focuses more on accuracy preservation than the specific efficiency metrics (throughput, latency, GPU utilization) central to my research interests. The paper offers valuable insights into quantization techniques but lacks the system-level optimization perspective I prioritize."
    }
  },
  {
    "id": "sJd4DpYOis",
    "title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback",
    "abstract": "Ensembles of generative large language models (LLMs) are a promising way to compensate for individual model limitations, integrating the strengths of different LLMs. Existing LLM ensemble methods, however, face limitations such as first-token delay and challenges in long-range semantic collaboration between models, Moreover, they typically assume equal voting weights for all models during ensemble, ignoring performance differences between models for a given task. In this work, we propose SpecEM, a training-free, plug-and-play LLM ensemble framework that dynamically adjusts each model's model contribution in real time based on task performance. Inspired by speculative decoding, SpecFuse iteratively performs drafting and verification, allowing models to collaborate semantically at the segment level for integrated output. Furthermore, we introduce an online feedback mechanism with multiplicative weight updates, where each model's voting weight is adjusted on-the-fly according to how often it \"outperforms\" others during verification stage, ensuring that stronger models exert greater influence on the ensemble during generation. Experimental results on five popular LLMs (ranging from 7B to 72B parameters) and six benchmark tasks, spanning instruction following, reasoning, commonsense, and general instruction response, demonstrate consistent performance improvements compared to state-of-the-art LLM ensemble methods.",
    "authors": [
      "~Bo_Lv2",
      "~Nayu_Liu1",
      "~Chen_Tang5",
      "~Xin_Liu17",
      "~Yue_Yu8",
      "~Ping_Luo1"
    ],
    "pdf": "/pdf/ce36072cf898e279341e1ca31c51626ddead6950.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training-free optimization approach, speculative decoding-inspired methodology, real-time dynamic adjustment of model contributions, segment-level collaboration model, works across different model sizes (7B to 72B parameters), plug-and-play framework for practical deployment",
      "Irrelevant Aspects": "Focus on multi-model ensembling rather than single-model optimization, no explicit discussion of GPU utilization improvements, limited analysis of latency and throughput metrics, no memory optimization strategies, lacks discussion of quantization techniques",
      "Summary": "SpecEM presents a training-free LLM ensemble framework that uses speculative decoding principles to dynamically adjust model contributions in real-time. While it offers inference optimization through drafting-verification mechanisms and online feedback, it primarily focuses on ensembling multiple models rather than the single-model efficiency optimizations that are central to GPU utilization and scalability research. The paper demonstrates performance improvements but lacks detailed analysis of resource utilization, throughput, and latency metrics that would make it highly relevant to my core research interests."
    }
  },
  {
    "id": "2a36EMSSTp",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the **D**ecoupled Clip and **D**ynamic s**A**mpling **P**olicy **O**ptimization (**DAPO**) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
    "authors": [
      "~Qiying_Yu1",
      "~Zheng_Zhang12",
      "~Ruofei_Zhu1",
      "~Yufeng_Yuan1",
      "~Xiaochen_Zuo1",
      "~YuYue2",
      "~Weinan_Dai2",
      "~Tiantian_Fan1",
      "~Gaohong_Liu1",
      "~Juncai_Liu2",
      "~LingJun_Liu1",
      "~Xin_Liu51",
      "~Haibin_Lin1",
      "~Zhiqi_Lin1",
      "~Bole_Ma3",
      "~Guangming_Sheng1",
      "~Yuxuan_Tong2",
      "~Chi_Zhang80",
      "~Mofan_Zhang1",
      "~Ru_Zhang4",
      "~Wang_Zhang16",
      "~Hang_Zhu3",
      "~Jinhua_Zhu1",
      "~Jiaze_Chen1",
      "~Jiangjie_Chen1",
      "~Chengyi_Wang1",
      "~Hongli_Yu1",
      "~Yuxuan_Song2",
      "~Xiangpeng_Wei1",
      "~Hao_Zhou5",
      "~Jingjing_Liu2",
      "~Wei-Ying_Ma2",
      "~Ya-Qin_Zhang1",
      "~Lin_Yan1",
      "~Yonghui_Wu1",
      "~Mingxuan_Wang1"
    ],
    "pdf": "/pdf/a5ca4684c1debe30e4fde4bd063a262d61e13db7.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on large-scale RL system for LLMs, open-source implementation at scale, potential optimizations for scaling training, use of 32B model suggesting efficient utilization",
      "Irrelevant Aspects": "Primary focus on algorithm (DAPO) rather than system optimization, emphasis on reproducing RL results rather than GPU utilization, focus on mathematical reasoning benchmark rather than performance metrics",
      "Summary": "The paper describes DAPO, an open-source reinforcement learning system for training LLMs at scale. While potentially relevant for understanding scaling techniques, it appears to focus more on the algorithmic aspects rather than specific optimizations for GPU utilization, throughput, or latency. The open-source nature of their implementation could provide valuable insights into their approach to scaling, but the abstract doesn't highlight system optimization techniques."
    }
  },
  {
    "id": "gMHLQASj11",
    "title": "Learnable Sampler Distillation for Discrete Diffusion Models",
    "abstract": "Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps.",
    "authors": [
      "~Feiyang_Fu2",
      "~Tongxian_Guo1",
      "~Zhaoqiang_Liu1"
    ],
    "pdf": "/pdf/f048f8439aafb16ad90b59f9692785006d7ce9ef.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization techniques to reduce sampling steps, knowledge distillation approaches for faster generation, methods to improve throughput and reduce latency, applications to text generation tasks",
      "Irrelevant Aspects": "Focus on discrete diffusion models rather than standard transformer-based LLMs, limited GPU-specific optimizations, minimal discussion of memory optimization techniques, lack of emphasis on very large-scale model optimization",
      "Summary": "This paper presents learnable sampler distillation (LSD) to accelerate sampling in discrete diffusion models by distilling knowledge from a slow teacher sampler to a fast student sampler with learnable coefficients. The approach significantly reduces inference steps while maintaining generation quality. While not directly addressing standard large language models, the inference acceleration and distillation techniques could inform approaches for optimizing LLM inference, making it moderately relevant to research on improving GPU utilization, throughput, and reducing latency."
    }
  },
  {
    "id": "FAeU7516MR",
    "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
    "authors": [
      "~Zongle_Huang1",
      "~Lei_Zhu12",
      "~ZongYuan_Zhan3",
      "~Ting_Hu3",
      "~Weikai_Mao1",
      "~Xianzhi_Yu1",
      "~Yongpan_Liu1",
      "~Tianyu_Zhang12"
    ],
    "pdf": "/pdf/97aad2e71863d0f5a6cda9ae68d3109542792be2.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM inference optimization using speculative decoding for sparse MoE models, directly addressing GPU utilization and throughput improvements. It introduces a new metric 'target efficiency' to characterize system bottlenecks and demonstrates up to 2.29x speedup for MoE models. The analysis of batch size effects on performance and the focus on private serving scenarios align well with my expertise in inference optimization.",
      "Irrelevant Aspects": "The paper is specifically focused on MoE models and speculative decoding, rather than broader LLM optimization techniques. It doesn't address training optimization, GPU memory management, distributed systems, or other compression techniques like quantization that are within my expertise domain.",
      "Summary": "This paper explores how speculative decoding can accelerate inference for sparse Mixture of Experts models, showing that MoEs can benefit more from this technique than dense models. The authors develop theoretical models to understand the tradeoffs involved and introduce a new metric 'target efficiency' to identify system bottlenecks. The work demonstrates practical speedups (up to 2.29x) for MoE models at medium batch sizes, offering valuable insights for improving GPU utilization and throughput in LLM serving."
    }
  },
  {
    "id": "fXG1BvwqGt",
    "title": "Less is More: an Attention-free Sequence Prediction Modeling for Offline Embodied Learning",
    "abstract": "Offline reinforcement learning (offline RL) is increasingly approached as a sequence modeling task, with methods leveraging advanced architectures like Transformers to capture trajectory dependencies. Despite significant progress, the mechanisms underlying their effectiveness and limitations remain insufficiently understood. We conduct a thorough analysis on the representative Decision Transformer (DT) model using an entropy analysis and identify the inconsistencies in state-action-reward ($\\langle s, a, R \\rangle$) distributions causing attention ``dispersal\". To address this, we propose a hierarchical framework that decomposes sequence modeling into intra-step relational modeling—handled by a Token Merger that fuses each $\\langle s, a, R \\rangle$ triplet—and inter-step modeling—handled by a Token Mixer across timesteps. We investigate several Token Merger designs and validate their effectiveness across various offline RL methods. \nFurthermore, our theoretical analysis and experimental results suggest that while Token Mixers are important, lightweight architecture can also achieve even better performance to more complex ones. We therefore propose a parameter-free Average Pooling Token Mixer, which, combined with a convolutional Token Merger, forms our final model, Decision HiFormer (DHi). DHi achieves a \\textbf{73.6\\%} improvement in inference speed and an \\textbf{9.3\\%} gain in policy performance on the D4RL benchmark compared to DT. DHi also generalizes well to real-world robotic manipulation tasks, offering both practical benefits and insights into sequence-based policy design for offline RL. Code and models are public at \\href{https://wei-nijuan.github.io/DecisionHiFormer/}{project page}.",
    "authors": [
      "~Wei_Huang46",
      "~Jianshu_Zhang4",
      "~Leiyu_Wang1",
      "~Heyue_Li1",
      "~Luoyi_Fan1",
      "~Yichen_Zhu1",
      "~Nanyang_Ye1",
      "~Qinying_Gu1"
    ],
    "pdf": "/pdf/3845cfa9404b2e3ecb78f14ff172ee55e94408fc.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on improving sequence modeling efficiency, proposes an attention-free approach to replace computationally expensive attention mechanisms, reports a 73.6% improvement in inference speed which directly relates to lower latency and higher throughput, discusses architectural improvements for enhanced efficiency which affects GPU utilization and scalability, replaces complex attention with simpler alternatives like parameter-free Average Pooling Token Mixer to reduce computational requirements, and addresses the trade-off between model complexity and performance which is relevant to optimization goals.",
      "Irrelevant Aspects": "The paper focuses on offline reinforcement learning rather than language modeling specifically, the application domain is embodied learning/robotic manipulation rather than NLP tasks, the token merger designs are tailored to state-action-reward triplets rather than linguistic tokens, the benchmark used (D4RL) is for RL tasks rather than language tasks, there's no explicit mention of training optimization in depth, focusing more on architecture changes rather than training processes.",
      "Summary": "This paper proposes an attention-free sequence modeling approach for offline RL, decomposing modeling into intra-step and inter-step components. It achieves significant inference speed improvements while maintaining or improving policy performance. While the domain (offline RL) differs from the primary focus on language models, the techniques for attention replacement and inference optimization have clear relevance to large language model optimization research."
    }
  },
  {
    "id": "wsR7VYXbdR",
    "title": "HiMoLE: Towards OOD-Robust LoRA via Hierarchical Mixture of Experts",
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have enabled the efficient adaptation of large language models (LLMs) by updating only a small subset of parameters. However, their robustness under out-of-distribution (OOD) conditions remains insufficiently studied. In this paper, we identify the limitations of conventional LoRA in handling distributional shifts and propose $\\textbf{HiMoLE}$($\\textbf{Hi}$erarchical $\\textbf{M}$ixture of $\\textbf{L}$oRA $\\textbf{E}$xperts), a new framework designed to improve OOD generalization. HiMoLE integrates hierarchical expert modules and hierarchical routing strategies into the LoRA architecture and introduces a two-phase training procedure enhanced by a diversity-driven loss. This design mitigates negative transfer and promotes effective knowledge adaptation across diverse data distributions. We evaluate HiMoLE on three representative tasks in natural language processing. Experimental results evidence that HiMoLE consistently outperforms existing LoRA-based approaches, significantly reducing performance degradation on OOD data while improving in-distribution performance. Our work bridges the gap between parameter efficiency and distributional robustness, advancing the practical deployment of LLMs in real-world applications.",
    "authors": [
      "~Yinuo_Jiang3",
      "~Yan_Xiaodong1",
      "~Keyan_Ding1",
      "~Deng_Zhao2",
      "~Lei_Liang2",
      "~Qiang_Zhang6",
      "~Huajun_Chen1"
    ],
    "pdf": "/pdf/1d0816b77f135ae47ceb4baa8b84e588d6527937.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-efficient fine-tuning of LLMs using LoRA, hierarchical mixture of experts architecture, potential impact on GPU utilization patterns, implications for both training and inference optimization through parameter efficiency",
      "Irrelevant Aspects": "Primary focus on out-of-distribution robustness rather than computational efficiency metrics, evaluation based on task performance rather than throughput or latency, no explicit mention of memory efficiency optimizations",
      "Summary": "This paper proposes HiMoLE, a hierarchical mixture of LoRA experts framework to improve OOD robustness in parameter-efficient fine-tuning. While primarily focused on robustness rather than computational efficiency, the approach involves parameter-efficient methods and architectural changes that are relevant to training and inference optimization of LLMs."
    }
  },
  {
    "id": "xvxgG668th",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient Variable-Length VLMs",
    "abstract": "We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. \nUnlike previous approaches, our method dynamically determines token length based on the *image content*—not just resolution—and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks, demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models, across diverse VLM architectures.  Furthermore, qualitative analyses show that the adaptive token reduction from DToMe aligns well with human perception and enables users to better control computational costs through flexible integration with additional vision tools and models.",
    "authors": [
      "~Zhenhailong_Wang1",
      "~Senthil_Purushwalkam1",
      "~Caiming_Xiong1",
      "~Silvio_Savarese1",
      "~Heng_Ji3",
      "~Ran_Xu1"
    ],
    "pdf": "/pdf/7bb52b265e0feea0c6ca608921da2db2db8019c8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization for reducing computational burden, training-free efficiency improvements, dynamic resource allocation based on content complexity, token reduction techniques (32%-85% fewer tokens), GPU utilization improvements, latency reduction, throughput enhancement, scalability across different model architectures",
      "Irrelevant Aspects": "Primary focus on vision-language models rather than pure LLMs, vision-specific token merging mechanisms, image complexity analysis rather than text optimization, human perception alignment aspects, integration with vision tools and models",
      "Summary": "DyMU introduces Dynamic Token Merging (DToMe) and Virtual Token Unmerging (VTU) to efficiently reduce computational burden in VLMs during inference. The approach dynamically adjusts token length based on image content rather than fixed dimensions, operates without training, and achieves 32%-85% reduction in visual tokens while maintaining performance across diverse VLM architectures."
    }
  },
  {
    "id": "0mOBdNsI3L",
    "title": "Approximately Aligned Decoding",
    "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation to re-sample after a rejection, or distort the distribution of outputs by constraining the output to highly improbable tokens.\nWe present a method, Approximately Aligned Decoding (AprAD), to balance the distortion of the output distribution with computational efficiency, inspired by algorithms from the speculative decoding literature.\nAprAD allows for the generation of long sequences of text with difficult-to-satisfy constraints, while amplifying low probability outputs much less compared to existing methods.\nWe show through a series of experiments that the task-specific performance of AprAD is comparable to methods that do not distort the output distribution, while being much more computationally efficient.",
    "authors": [
      "~Daniel_Melcer1",
      "~Sujan_Kumar_Gonugondla1",
      "~Pramuditha_Perera3",
      "~Haifeng_Qian1",
      "~Wen-Hao_Chiang2",
      "~Yanjun_Wang2",
      "~Nihal_Jain1",
      "~Pranav_Garg1",
      "~Xiaofei_Ma1",
      "~Anoop_Deoras1"
    ],
    "pdf": "/pdf/fed4641ccc8eb690a2e8911e4b450fc13007947f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM inference optimization techniques, specifically addressing computational efficiency when rejecting undesired outputs. This directly relates to GPU utilization and scalability, as it proposes a method inspired by speculative decoding to improve efficiency. The approach potentially increases throughput and reduces latency while maintaining output quality, which aligns well with research interests in inference optimization.",
      "Irrelevant Aspects": "The paper doesn't address training optimization of LLMs, which is part of the research focus. It focuses on a specific niche aspect of inference (handling undesired outputs) rather than broader inference optimization techniques. The abstract provides limited information about how the method scales across different hardware configurations or model sizes.",
      "Summary": "Approximately Aligned Decoding (AprAD) presents an inference optimization technique for LLMs that balances output distribution quality with computational efficiency when rejecting undesired outputs. Building on speculative decoding approaches, the method aims to generate text with difficult constraints while minimizing distortion of low probability outputs. The paper claims AprAD maintains task-specific performance comparable to non-distorting methods while significantly improving computational efficiency, making it relevant for research focused on LLM inference optimization and resource utilization."
    }
  },
  {
    "id": "nGEq3D6FFX",
    "title": "Compress & Cache: Vision token compression for efficient generation and retrieval",
    "abstract": "This work aims to compress the vision tokens of an LVLM into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) storage-efficient. \nTo this end, we propose C&C, a novel compression method that leverages the LVLM itself for task-agnostic visual token compression.\nUnlike prior methods that perform token reduction on-the-fly, our approach offloads computation to a dedicated, upfront indexing stage, effectively decoupling compression from generation. This enables learning more powerful representations for generation during inference. \nAt the core of C&C is a ``double-forward pass'' training strategy. During the first forward pass, the LLM (of the LVLM) creates a bottleneck by compressing the dense visual tokens into a few summary tokens. Subsequently, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. \nThe training of C&C is guided by two key losses: an autoregressive loss applied after the second pass that provides a direct optimization objective for reconstructing the original information flow, and a contrastive loss applied after the first pass to bolster the representational strength of the summary tokens, particularly for discriminative tasks. Moreover, we propose stage-specific adapters for further enhancing performance. C&C  produces highly informative compressed representations. An in-depth ablation study confirms the efficacy of our approach. For generative tasks, we achieve a 2x higher compression rate without compromising capabilities, setting a new state-of-the-art. For discriminative tasks, we establish new state-of-the-art results on image retrieval and compositionality benchmarks.",
    "authors": [
      "~Adrian_Bulat1",
      "~Yassine_Ouali1",
      "~Georgios_Tzimiropoulos1"
    ],
    "pdf": "/pdf/82ef421c53f3faf122d6626fe524ba5fc9c8bdab.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on vision token compression for LVLMs, directly addressing inference optimization. It achieves 2x higher compression rate without compromising capabilities, improving efficiency. The method decouples compression from generation by offloading computation to an upfront indexing stage, which can improve throughput and reduce latency. The approach aims to enhance GPU utilization during inference and addresses storage efficiency, which is related to resource optimization.",
      "Irrelevant Aspects": "The paper specifically focuses on vision tokens rather than general language model optimization. It combines both generative and discriminative tasks, while the focus is more on general LLM optimization. The double-forward pass strategy is specific to their compression approach rather than a general optimization technique. The paper deals with contrastive loss for discriminative tasks, which is outside the core interest of LLM training and inference optimization.",
      "Summary": "This paper presents C&C, a vision token compression method for LVLMs that achieves efficient inference by decoupling compression from generation. It uses a double-forward pass strategy with autoregressive and contrastive losses to create compressed representations that work for both generative and discriminative tasks. The method achieves 2x higher compression rates without compromising capabilities, setting new state-of-the-art results. While specific to vision tokens, the paper's approach to offloading computation and improving inference efficiency is relevant to LLM optimization research."
    }
  },
  {
    "id": "jTBxyQempF",
    "title": "SparseDiT: Token Sparsification for Efficient Diffusion Transformer",
    "abstract": "Diffusion Transformers (DiT) are renowned for their impressive generative performance; however, they are significantly constrained by considerable computational costs due to the quadratic complexity in self-attention and the extensive sampling steps required. While advancements have been made in expediting the sampling process, the underlying architectural inefficiencies within DiT remain underexplored. We introduce SparseDiT, a novel framework that implements token sparsification across spatial and temporal dimensions to enhance computational efficiency while preserving generative quality. Spatially, SparseDiT employs a tri-segment architecture that allocates token density based on feature requirements at each layer: Poolingformer in the bottom layers for efficient global feature extraction, Sparse-Dense Token Modules (SDTM) in the middle layers to balance global context with local detail, and dense tokens in the top layers to refine high-frequency details. Temporally, SparseDiT dynamically modulates token density across denoising stages, progressively increasing token count as finer details emerge in later timesteps. This synergy between SparseDiT’s spatially adaptive architecture and its temporal pruning strategy enables a unified framework that balances efficiency and fidelity throughout the generation process. Our experiments demonstrate SparseDiT’s effectiveness, achieving a 55\\% reduction in FLOPs and a 175\\% improvement in inference speed on DiT-XL with similar FID score on 512$\\times$512 ImageNet, a 56\\% reduction in FLOPs across video generation datasets, and a 69\\% improvement in inference speed on PixArt-$\\alpha$ on text-to-image generation task with a 0.24 FID score decrease. SparseDiT provides a scalable solution for high-quality diffusion-based generation compatible with sampling optimization techniques.  Code is available at https://github.com/changsn/SparseDiT.",
    "authors": [
      "~Shuning_Chang1",
      "~Pichao_WANG3",
      "~Jiasheng_Tang1",
      "~Fan_Wang6",
      "~Yi_Yang22"
    ],
    "pdf": "/pdf/f82961ff7a65d91e46493e3b65c1cf3d16e4bec9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on computational efficiency optimization in transformer architectures through token sparsification, achieving significant reductions in FLOPs (55-56%) and improvements in inference speed (69-175%). These results directly relate to my research interests in inference optimization, GPU utilization, and throughput. The tri-segment architecture and temporal pruning strategy could potentially be applied to optimize transformer models beyond diffusion models, including large language models. The paper addresses the fundamental challenge of quadratic complexity in self-attention, which is a key bottleneck in many transformer models.",
      "Irrelevant Aspects": "The research is specifically focused on diffusion transformers for image and video generation rather than language models, which are my primary interest. The paper does not address training optimization techniques, only focusing on inference efficiency. There's no detailed discussion of GPU-specific architectural optimizations or memory management strategies that would be relevant to my research focus.",
      "Summary": "SparseDiT introduces a token sparsification framework for diffusion transformers that reduces computational costs while maintaining quality. The approach includes a tri-segment architecture that allocates token density based on feature requirements across different layers and dynamic temporal pruning that adjusts token density across denoising stages. While applied to image/video generation rather than language models, the optimization techniques for transformer architectures, particularly attention mechanisms, provide valuable insights for my research in ML systems optimization."
    }
  },
  {
    "id": "x5lITYXmW2",
    "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
    "abstract": "Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance, but at the cost of significant computational overhead. In this paper, we first show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in near future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, like Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, and Qwen2.5-32B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation. We make our code publicly available at https://github.com/Infini-AI-Lab/GRESO/.",
    "authors": [
      "~Haizhong_Zheng1",
      "~Yang_Zhou27",
      "~Brian_R._Bartoldson1",
      "~Bhavya_Kailkhura1",
      "~Fan_Lai1",
      "~Jiawei_Zhao2",
      "~Beidi_Chen1"
    ],
    "pdf": "/pdf/d42f9345769b084f75a03345feb31964fd301038.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models through selective rollout, reporting significant computational efficiency gains (2.4x rollout speedup, 2.0x total training time speedup). This approach improves GPU utilization by avoiding unnecessary computations and increases training throughput. The method is evaluated across multiple model sizes (1.5B to 32B parameters), demonstrating scalability.",
      "Irrelevant Aspects": "The research focuses exclusively on training optimization rather than inference optimization. It doesn't address inference latency concerns. While it improves computational efficiency, it doesn't provide detailed technical insights into GPU memory optimization or parallelization strategies. The approach is algorithmic (selective prompting) rather than system-level optimization.",
      "Summary": "GRESO introduces an efficient selective rollout method for reinforcement learning training of LLMs by predicting and skipping uninformative prompts. While it delivers impressive training speedups and improved computational efficiency, its relevance is limited to the training optimization aspect of your research interests, with no coverage of inference optimization or latency improvements."
    }
  },
  {
    "id": "45igeoC560",
    "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving",
    "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present **Loquetier**, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.",
    "authors": [
      "~Yuchen_Zhang20",
      "~Hanyue_Du3",
      "~Chun_Cao1",
      "~Jingwei_Xu3"
    ],
    "pdf": "/pdf/75759e7e0e48dbcb838d87e965d3de30dd9348af.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "LoRA fine-tuning optimization, unified training and serving framework, performance optimization (3.0x higher throughput, 46.4x higher SLO attainment), virtualized multi-LoRA architecture supporting multiple adapters on shared base model, optimized kernel design merging fine-tuning and inference paths, efficient batching with minimal kernel invocation overhead, scalability for serving multiple fine-tuned models.",
      "Irrelevant Aspects": "Limited details on specific GPU optimization techniques, no explicit mention of distributed strategies, no information on quantization or model compression techniques, lacking explicit latency measurements.",
      "Summary": "This paper presents Loquetier, a framework that unifies LoRA fine-tuning and serving in a single runtime with virtualized multi-LoRA architecture and optimized computation flow. It demonstrates significant performance improvements in throughput and SLO attainment, directly addressing my interests in LLM system optimization for better GPU utilization, scalability, and throughput. The unified approach for efficient batching and resource sharing is particularly relevant to my research focus."
    }
  },
  {
    "id": "8ZdWmpYxT0",
    "title": "AdaMSS: Adaptive Multi-Subspace Approach for Parameter-Efficient Fine-Tuning",
    "abstract": "In this paper, we propose AdaMSS, an adaptive multi-subspace approach for parameter-efficient fine-tuning  of large models. Unlike traditional parameter-efficient fine-tuning methods that operate within a large single subspace of the network weights, AdaMSS leverages subspace segmentation to obtain multiple smaller subspaces and adaptively reduces the number of trainable parameters during training, ultimately updating only those associated with a small subset of subspaces most relevant to the target downstream task. By using  the lowest-rank representation, AdaMSS achieves more compact expressiveness and finer tuning of the model parameters. Theoretical analyses demonstrate that AdaMSS has better generalization  guarantee than LoRA, PiSSA, and other single-subspace low-rank-based methods. Extensive experiments across image classification, natural language understanding, and natural language generation tasks show that AdaMSS achieves comparable performance to full fine-tuning  and outperforms other parameter-efficient fine-tuning   methods in most cases, all while requiring fewer trainable parameters. Notably, on the ViT-Large model, AdaMSS achieves 4.7\\% higher average accuracy than LoRA across seven tasks, using just 15.4\\% of the trainable parameters. On RoBERTa-Large, AdaMSS outperforms PiSSA by 7\\% in average accuracy across six tasks while reducing the number of trainable parameters by approximately 94.4\\%. These results demonstrate the effectiveness of AdaMSS in parameter-efficient fine-tuning. The code for AdaMSS is available at https://github.com/jzheng20/AdaMSS.",
    "authors": [
      "~Jingjing_Zheng3",
      "~Wanglong_Lu1",
      "~Yiming_Dong1",
      "~Chaojie_Ji2",
      "~Yankai_Cao1",
      "~Zhouchen_Lin1"
    ],
    "pdf": "/pdf/bcc2575dd670131f5c6cac5dbd9a55ed392b9a23.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-efficient fine-tuning for large models, reduction of trainable parameters leading to better GPU utilization, potential for faster training times, applicability to large language models like RoBERTa-Large, more compact representation with fine-tuned parameters",
      "Irrelevant Aspects": "No explicit focus on inference optimization, lack of throughput and latency measurements, limited discussion of distributed training or multi-GPU scalability, evaluation focuses more on accuracy than computational efficiency",
      "Summary": "AdaMSS proposes a novel parameter-efficient fine-tuning method that uses multi-subspace segmentation to reduce trainable parameters adaptively. While it shows significant improvements over methods like LoRA and PiSSA in terms of parameter efficiency and accuracy, it doesn't explicitly address key metrics like throughput, latency, or multi-GPU scalability that are central to the research interest. Nevertheless, parameter-efficient fine-tuning is crucial for optimizing large model training, making this paper relevant."
    }
  },
  {
    "id": "XfHfTqeXfZ",
    "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention",
    "abstract": "Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences $(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$ for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts.",
    "authors": [
      "~Can_Yaras1",
      "~Alec_S_Xu1",
      "~Pierre_Abillama1",
      "~Changwoo_Lee2",
      "~Laura_Balzano1"
    ],
    "pdf": "/pdf/e0b72bd679ffafe1324dd4e0f4a004e72bf961fd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses the quadratic complexity of attention mechanisms in transformers, which is a critical bottleneck in both training and inference of large language models. It specifically focuses on hardware efficiency, utilizing GPU tensor cores effectively, which aligns perfectly with GPU utilization goals. The reported speed-ups over FlashAttention-2 (1.4× to 8.2×) represent significant throughput and latency improvements. The method reduces memory complexity to Θ(Nd), crucial for large model processing. The technique shows particular effectiveness for longer sequences, addressing scalability challenges in modern LLMs. Its zero-shot transferability without requiring additional training makes it immediately applicable to existing models. The inclusion of optimized kernels demonstrates system-level optimizations rather than just theoretical improvements.",
      "Irrelevant Aspects": "The detailed mathematical formulation of Monarch matrices, while important for the method itself, is less central to the systems focus. Applications to vision tasks mentioned in the paper are less directly relevant to the primary interest in language models. If the paper includes extensive theoretical proofs about approximation quality, these would be secondary to the practical implementation and performance aspects.",
      "Summary": "MonarchAttention introduces a novel approach to optimize transformer attention mechanisms using Monarch matrices, achieving significant speed-ups over FlashAttention-2 (1.4× to 8.2× depending on sequence length) without requiring additional training. The method is specifically designed to efficiently utilize modern GPU tensor cores, reducing computational complexity to Θ(N√Nd) and memory complexity to Θ(Nd). This work is highly relevant for optimizing both training and inference of large language models, offering substantial improvements in GPU utilization, scalability, throughput, and latency while maintaining model quality."
    }
  },
  {
    "id": "IiEtQPGVyV",
    "title": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling",
    "abstract": "Accurately estimating *semantic* aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a **diversity-steered sampler** that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model’s proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.",
    "authors": [
      "~Ji_Won_Park1",
      "~Kyunghyun_Cho1"
    ],
    "pdf": "/pdf/e3fff0c5fce342dbe8f28571af7672ed46e7783f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Sample efficiency improvements in LLM inference, modular approach that doesn't require gradient access to the base LLM, drop-in enhancement suggesting easy integration, addresses computational cost of multiple generations, covers both autoregressive and masked diffusion paradigms",
      "Irrelevant Aspects": "Focus on uncertainty quantification rather than general performance optimization, limited focus on GPU utilization and distributed systems, doesn't address training optimization, less emphasis on throughput and latency metrics, specific application to QA tasks rather than general LLM use",
      "Summary": "This paper introduces a diversity-steered sampling method to improve efficiency in semantic uncertainty quantification for language models. The approach reduces the number of required samples by discouraging semantically redundant outputs during decoding, covering both autoregressive and masked diffusion models. The method injects a semantic similarity penalty using an NLI model and includes techniques for debiasing and variance reduction. While the primary focus is on uncertainty quantification rather than system-level optimization, the paper offers relevant insights into improving sample efficiency during inference, which impacts throughput and latency. The modular design and lack of gradient access requirements make it a practical enhancement for LLM deployments, particularly in risk-sensitive applications where uncertainty estimation is crucial."
    }
  },
  {
    "id": "m3huAdsaGI",
    "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning",
    "abstract": "Hybrid language models that combine Attention and State Space Models (SSMs) have been shown to achieve state-of-the-art accuracy and runtime performance. Recent work has also demonstrated that applying pruning and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. To this end, we introduce a novel group-aware pruning method for Mamba layers that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. We combine this method with FFN, embedding dimension, and layer pruning, along with knowledge distillation-based retraining to obtain a unified compression recipe for hybrid models. Using this recipe, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to $40\\times$ fewer training tokens compared to similarly-sized models. The resulting model surpasses the accuracy of similarly-sized models while achieving $\\sim2\\times$ faster inference throughput, significantly advancing the Pareto frontier.",
    "authors": [
      "~Ali_Taghibakhshi1",
      "~Sharath_Turuvekere_Sreenivas1",
      "~Saurav_Muralidharan1",
      "~Marcin_Chochowski1",
      "~Yashaswi_Karnati1",
      "~Raviraj_Bhuminand_Joshi1",
      "~Ameya_Sunil_Mahabaleshwarkar1",
      "~ZIJIA_CHEN1",
      "~Yoshi_Suhara1",
      "~Oluwatobi_Olabiyi1",
      "~Daniel_Korzekwa1",
      "~Mostofa_Patwary1",
      "~Mohammad_Shoeybi1",
      "~Jan_Kautz1",
      "~Bryan_Catanzaro1",
      "~Ashwath_Aithal1",
      "~Nima_Tajbakhsh1",
      "~Pavlo_Molchanov1"
    ],
    "pdf": "/pdf/acc974ee27273887dafd6ff29e70fca7be46ceb4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Model compression techniques for hybrid language models, group-aware pruning method for Mamba layers, knowledge distillation-based retraining, achieving 2x faster inference throughput, reducing training token requirements, advancing Pareto frontier for accuracy vs efficiency",
      "Irrelevant Aspects": "Limited discussion of GPU utilization specifically, focus on specific compression technique rather than broader optimization principles, limited scalability analysis beyond model size reduction",
      "Summary": "This paper introduces a novel group-aware pruning method for hybrid language models combining Attention and State Space Models. The approach compresses an 8B parameter model to 4B while maintaining accuracy and achieving 2x faster inference throughput with significantly fewer training tokens. The work directly addresses inference optimization through compression techniques, advancing the efficiency-accuracy tradeoff frontier, though it focuses on specific compression methods rather than broader optimization principles or GPU utilization specifically."
    }
  },
  {
    "id": "S6SKa97Gm0",
    "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence after the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the prior keys and values. This enables building what we call intrinsics, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while significantly improving inference efficiency. We contributed our Activated LoRA implementation to the Huggingface PEFT library.",
    "authors": [
      "~Kristjan_Greenewald1",
      "~Luis_A._Lastras1",
      "~Thomas_Parnell1",
      "~Vraj_Shah1",
      "~Lucian_Popa1",
      "~Giulio_Zizzo2",
      "~Chulaka_Gunasekara2",
      "~Ambrish_Rawat2",
      "~David_Daniel_Cox1"
    ],
    "pdf": "/pdf/3e772cdebadbd5f5afeffe2e7697ab860b68a4d6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization by eliminating KV cache recomputation when switching between LoRA adapters, which significantly reduces inference latency and improves throughput in multi-turn settings. It introduces 'intrinsics' - specialized models that can be instantly activated without reprocessing prior context, enhancing overall system efficiency. The contribution to the Huggingface PEFT library indicates practical implementation value. The approach focuses on making fine-tuned LLMs more efficient during inference, which aligns well with optimizing GPU utilization and scalability in production environments.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization rather than novel training optimization techniques. There's limited discussion of GPU memory optimization during model training or deployment. Hardware-specific optimizations and detailed quantization techniques are not covered. The paper doesn't explore distributed training or inference parallelization strategies, which are important aspects of LLM scalability.",
      "Summary": "Activated LoRA (aLoRA) introduces an optimized adapter architecture that allows LoRA-adapted models to be instantly activated without recomputing KV caches of prior conversation history. This approach enables efficient switching between specialized models (called intrinsics) in a single conversation, significantly improving inference efficiency while maintaining competitive accuracy with standard LoRA. The method contributes to better GPU utilization and lower latency in production scenarios requiring multiple specialized capabilities."
    }
  },
  {
    "id": "KTDAbnFsQj",
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "abstract": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints.While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. \nThis work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. \nSpecifically, we identify two central architectural factors: depth–width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth–width ratios, with the key finding that although deep–thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy–latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy–latency frontier.\nIn addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. This technique can serve as a generalizable component for future SLMs.\nCombining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy–efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5\\% average accuracy, 1.3$\\times$/1.9$\\times$ lower latency, and 18.7$\\times$/45.6$\\times$ higher throughput compared to Qwen3-1.7B/0.6B, respectively.",
    "authors": [
      "~Yonggan_Fu1",
      "~Xin_Dong1",
      "~Shizhe_Diao2",
      "~Matthijs_Van_keirsbilck1",
      "~Hanrong_Ye1",
      "~Wonmin_Byeon1",
      "~Yashaswi_Karnati1",
      "~Lucas_Liebenwein1",
      "~Maksim_Khadkevich1",
      "~Alexander_Keller1",
      "~Jan_Kautz1",
      "~Yingyan_Celine_Lin1",
      "~Pavlo_Molchanov1"
    ],
    "pdf": "/pdf/62408120ca69f23b26fb5c996b8d7085d35740e9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses latency optimization, throughput improvement, and architectural factors affecting GPU utilization for language models. It introduces an evolutionary search framework for discovering operator combinations, provides insights on depth-width ratios for latency optimization, and presents hybrid model design principles. The empirical results showing significant improvements in latency (1.3×/1.9× lower) and throughput (18.7×/45.6× higher) are highly relevant to inference optimization goals.",
      "Irrelevant Aspects": "The paper focuses specifically on small language models (SLMs) rather than large language models (LLMs), which is a primary focus of the specified research interest. There is limited discussion of distributed training approaches, multi-GPU scaling strategies, or large-scale model training optimizations. The weight normalization technique for training is a minor component and doesn't substantially address GPU utilization during training.",
      "Summary": "Nemotron-Flash introduces a family of hybrid small language models optimized for latency and throughput. The authors identify depth-width ratios and operator choices as key determinants of real-device performance, develop an evolutionary search framework for operator selection, and implement a weight normalization technique for training. Their models significantly outperform Qwen3 baselines in accuracy (+5.5%), latency (1.3×/1.9× lower), and throughput (18.7×/45.6× higher), advancing the efficiency frontier for SLMs."
    }
  },
  {
    "id": "b6SWqFEOSF",
    "title": "Private Training Large-scale Models with Efficient DP-SGD",
    "abstract": "As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to 50\\% but also cuts down redundant computations by 20\\%, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a 90\\% throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.",
    "authors": [
      "~Liangyu_Wang1",
      "~Junxiao_Wang1",
      "~Jie_Ren4",
      "~Zihang_Xiang1",
      "~David_E._Keyes1",
      "~Di_Wang1"
    ],
    "pdf": "/pdf/d768b7e7a1b434677c0dc7d91ab4df2d7b50f9f7.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on training optimization for large language models, specifically addressing GPU utilization and throughput improvements in the context of private training. Introduces FlashDP which reduces memory movement by 50% and computations by 20% through fused operations. Demonstrates 90% throughput compared to Non-DP methods on A100 GPUs for pre-training Llama-13B, showing strong scalability aspects.",
      "Irrelevant Aspects": "Primary focus on Differential Privacy rather than general optimization techniques. Doesn't address inference optimization which is part of my interest. Limited discussion of latency metrics, with primary focus on throughput in the context of privacy-preserving training.",
      "Summary": "The paper introduces FlashDP, an efficient implementation of DP-SGD for private training of large language models. It optimizes per-sample gradient clipping through a cache-friendly approach that fuses operations, reducing memory overhead and redundant computations. The method achieves high throughput (90% of Non-DP methods) when pre-training large models like Llama-13B on A100 GPUs without increasing memory demands."
    }
  },
  {
    "id": "C69741fMFX",
    "title": "MUSTAFAR: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
    "abstract": "We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70\\% without compromising accuracy or requiring fine-tuning. We conduct a systematic exploration of pruning strategies and find per-token magnitude-based pruning as highly effective for both Key and Value caches under unstructured sparsity, surpassing prior structured pruning schemes. The Key cache benefits from prominent outlier elements, while the Value cache surprisingly benefits from a simple magnitude-based pruning despite its uniform distribution. KV cache size is the major bottleneck in decode performance due to high memory overhead for large context lengths. To address this, we use a bitmap-based sparse format and a custom attention kernel capable of compressing and directly computing over compressed caches pruned to arbitrary sparsity patterns, significantly accelerating memory-bound operations in decode computations and thereby compensating for the overhead of runtime pruning and compression. Our custom attention kernel coupled with the bitmap-based format delivers substantial compression of KV cache up to 45\\% of dense inference and thereby enables longer context lengths and increased tokens/sec throughput of up to 2.23$\\times$ compared to dense inference. Our pruning mechanism and sparse attention kernel is available at https://github.com/dhjoo98/mustafar.",
    "authors": [
      "~Donghyeon_Joo1",
      "~Helya_Hosseini1",
      "~Ramyad_Hadidi1",
      "~Bahar_Asgari1"
    ],
    "pdf": "/pdf/3f12569e75bc44a8825d64bce1b98b485b326f52.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "LLM inference optimization, GPU utilization improvement, memory bottleneck reduction through KV cache compression, custom attention kernel development, throughput increase (up to 2.23×), enabling longer context lengths, requiring no fine-tuning.",
      "Irrelevant Aspects": "Only focuses on inference optimization, not training optimization. Limited to unstructured sparsity approach rather than comprehensive optimization strategies. Lacks detailed GPU utilization metrics beyond throughput.",
      "Summary": "MUSTAFAR introduces unstructured sparsity for KV cache pruning in LLM inference, achieving up to 70% sparsity without accuracy loss or fine-tuning. Using a bitmap-based sparse format and custom attention kernel, it reduces KV cache to 45% of dense inference and increases throughput up to 2.23×, enabling longer context lengths."
    }
  },
  {
    "id": "uBaFH7aQnC",
    "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments",
    "abstract": "We demonstrate that geometrically distinctive keys during LLM inference tend to have high attention scores. Based on the phenomenon we propose KeyDiff, a training-free KV cache eviction method based solely on key similarity. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses.\nWe provide a theoretical basis for KeyDiff by relating key diversity with attention scores. These results imply  KeyDiff can efficiently identify the most important tokens to retain. Notably KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. Under a strict memory allowance, we demonstrate the effectiveness of KeyDiff for the Llama and Qwen model families by observing a performance gap of less than 0.04\\% with 8K cache budget (~23\\% KV cache reduction) from the non-evicting baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning benchmark and decrease end-to-end inference latency by up to 30\\% compared to the other token-eviction methods.",
    "authors": [
      "~Junyoung_Park1",
      "~Dalton_Jones1",
      "~Matthew_J_Morse1",
      "~Raghavv_Goel1",
      "~Mingu_Lee1",
      "~Christopher_Lott1"
    ],
    "pdf": "/pdf/461c983f15c05127a5a4f31daa7628230411afbb.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on KV cache eviction for LLM inference optimization, addressing memory constraints in resource-limited environments. It demonstrates significant memory reduction (23% KV cache reduction) and latency improvements (up to 30% decrease) while maintaining near-baseline performance. The method is training-free and compatible with optimized attention mechanisms like FlashAttention, which aligns with GPU utilization goals. The ability to process arbitrarily long prompts within strict resource constraints addresses scalability concerns.",
      "Irrelevant Aspects": "The paper does not address training optimization aspects of LLMs, which is part of my research interest. It focuses primarily on latency rather than explicitly measuring throughput improvements. The scope is limited to KV cache management rather than broader LLM optimization techniques.",
      "Summary": "KeyDiff presents a training-free KV cache eviction method based on key similarity that enables efficient processing of long-context LLMs in resource-constrained environments. The approach maintains near-baseline performance while achieving significant memory reduction (23%) and latency improvements (up to 30%). Its compatibility with optimized attention mechanisms like FlashAttention makes it particularly valuable for GPU utilization optimization, addressing key aspects of my research interest in inference optimization for large language models."
    }
  },
  {
    "id": "M5jz47umjR",
    "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding",
    "abstract": "Speculative decoding (SD) has emerged as a powerful method for accelerating autoregressive generation in large language models (LLMs), yet its integration into vision-language models (VLMs) remains underexplored. We introduce DREAM, a novel speculative decoding framework tailored for VLMs that combines three key innovations: (1) a cross-attention-based mechanism to inject intermediate features from the target model into the draft model for improved alignment, (2) adaptive intermediate feature selection based on attention entropy to guide efficient draft model training, and (3) visual token compression to reduce draft model latency. DREAM enables efficient, accurate, and parallel multimodal decoding with significant throughput improvement. Experiments across a diverse set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3, demonstrate up to 3.6x speedup over conventional decoding and significantly outperform prior SD baselines in both inference throughput and speculative draft acceptance length across a broad range of multimodal benchmarks.",
    "authors": [
      "~Yunhai_Hu2",
      "~Tianhua_Xia1",
      "~Zining_Liu1",
      "~Rahul_Raman2",
      "~Xingyu_Liu19",
      "~BO_BAO1",
      "~Eric_Sather1",
      "~Vithursan_Thangarasa1",
      "~Sai_Qian_Zhang1"
    ],
    "pdf": "/pdf/a1515254cd60fb85f6f0b961ad9f212522373bb2.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization through speculative decoding for vision-language models, focusing on improving GPU utilization and throughput with claims of up to 3.6x speedup. It introduces techniques for parallel multimodal decoding, visual token compression to reduce latency, and efficient draft model training - all highly relevant to my research on inference optimization. The evaluation focuses on throughput metrics and is tested across multiple models, demonstrating scalability.",
      "Irrelevant Aspects": "The focus is specifically on multimodal (vision-language) models rather than pure language models, though many techniques may transfer. The paper appears to emphasize architectural innovations over system-level implementation details. There's limited discussion of distributed GPU utilization or memory optimization techniques.",
      "Summary": "DREAM introduces a speculative decoding framework for VLMs with three key innovations: cross-attention feature injection, entropy-adaptive feature selection, and visual token compression. It demonstrates significant throughput improvements (3.6x speedup) across multiple VLM architectures, directly addressing my research interests in inference acceleration and GPU utilization. While focused on multimodal models, the speculative decoding techniques presented are highly relevant to my work on optimizing large model inference."
    }
  },
  {
    "id": "XUKUx7Xu89",
    "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
    "abstract": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.",
    "authors": [
      "~William_Merrill1",
      "~Shane_Arora1",
      "~Dirk_Groeneveld1",
      "~Hannaneh_Hajishirzi1"
    ],
    "pdf": "/pdf/fc46293fc342e1f7b2794debf317afc3914462dd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses critical aspects of large language model training optimization, focusing on batch size selection which is fundamental to GPU utilization and training efficiency. It introduces an empirical approach to measure critical batch size (CBS), demonstrates how CBS evolves during training, and proposes batch size warmup to enable more efficient data parallelism. The paper shows concrete results of training OLMo models with fewer gradient steps while achieving better loss, indicating improved throughput and GPU utilization. The findings about CBS trends across different model sizes (1B and 7B) provide valuable insights for scalable training systems.",
      "Irrelevant Aspects": "The paper does not address inference optimization, latency reduction during deployment, or specific GPU kernel optimizations. While it focuses on training throughput, it doesn't delve into hardware-specific optimization techniques or detailed GPU memory management. The methodology is specific to language model training and may not generalize to other ML domains.",
      "Summary": "This paper presents a novel empirical approach to measuring critical batch size in language model training, addressing a fundamental optimization challenge in scaling LLM training. By analyzing how CBS evolves during training and introducing batch size warmup, the authors demonstrate a practical method to improve GPU utilization and reduce training steps while maintaining or improving model performance. The findings have direct implications for large-scale training systems and can help optimize resource utilization in LLM development."
    }
  },
  {
    "id": "4exx1hUffq",
    "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test",
    "abstract": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sam- pling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top- layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE’s feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE- 3 achieves a 1.38x throughput improvement at a batch size of 64.",
    "authors": [
      "~Yuhui_Li1",
      "~Fangyun_Wei1",
      "~Chao_Zhang1",
      "~Hongyang_Zhang1"
    ],
    "pdf": "/pdf/28c4c8cf58b0086a2136d73f6059ada87ac33e53.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on LLM inference acceleration using speculative sampling techniques, addresses the sequential nature bottleneck of LLMs, achieves significant speedup ratios (up to 6.5x), discusses balancing training data scaling with inference costs, demonstrates throughput improvements (1.38x at batch size of 64), and presents architectural innovations for efficiency",
      "Irrelevant Aspects": "Concentrates specifically on the EAGLE series rather than general optimization techniques, limited discussion of explicit GPU utilization metrics, no mention of memory optimization strategies, and lacks coverage of parallelism techniques crucial for LLM scaling",
      "Summary": "EAGLE-3 presents an approach to accelerate LLM inference by abandoning feature prediction in favor of direct token prediction and implementing multi-layer feature fusion through training-time test. This enables the draft model to better benefit from scaling up training data while maintaining improved inference performance, achieving up to 6.5x speedup ratio with about 1.4x improvement over its predecessor (EAGLE-2)."
    }
  },
  {
    "id": "AQsko3PPUe",
    "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains",
    "abstract": "Large Language Models (LLMs) achieve superior performance through Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are computationally expensive and inefficient.\nIn this paper, we introduce Compressed Latent Reasoning (CoLaR), a novel framework that dynamically compresses reasoning processes in latent space through a two-stage training approach.\nFirst, during supervised fine-tuning, CoLaR extends beyond next-token prediction by incorporating an auxiliary next compressed embedding prediction objective. This process merges embeddings of consecutive tokens using a compression factor $c$ randomly sampled from a predefined range, and trains a specialized latent head to predict distributions of subsequent compressed embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that leverages the latent head's non-deterministic nature to explore diverse reasoning paths and exploit more compact ones.\nThis approach enables CoLaR to: i) **perform reasoning at a dense latent level** (i.e., silently), substantially reducing reasoning chain length, and ii) **dynamically adjust reasoning speed** at inference time by simply prompting the desired compression factor.\nExtensive experiments across four mathematical reasoning datasets demonstrate that CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, and reduces reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit CoT method. Moreover, when applied to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%.\nThe code and models will be released upon acceptance.",
    "authors": [
      "~Wenhui_Tan1",
      "~Jiaze_Li3",
      "~Jianzhong_Ju1",
      "~Zhenbo_Luo2",
      "~Ruihua_Song1",
      "~Jian_Luan1"
    ],
    "pdf": "/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization through latent space compression, reduction of reasoning chain length (53.3-82.8%), dynamic adjustment of reasoning speed at inference time, two-stage training approach combining SFT with RL, computational efficiency improvements leading to latency reduction and throughput increase",
      "Irrelevant Aspects": "Limited discussion of GPU utilization and hardware-specific optimizations, no explicit coverage of distributed systems scalability, minimal focus on memory optimization techniques, lack of integration with other optimization methods like quantization",
      "Summary": "This paper introduces Compressed Latent Reasoning (CoLaR), a framework that optimizes LLM inference by compressing reasoning chains in latent space. The approach significantly reduces computational requirements while maintaining or improving accuracy. Its two-stage training process and dynamic compression adjustment offer flexible optimization potential. While it doesn't explicitly address GPU utilization or distributed scalability, the computational efficiency improvements are directly relevant to improving throughput and reducing latency in LLM systems."
    }
  },
  {
    "id": "2sG4ebgqBd",
    "title": "SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding",
    "abstract": "Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughput protein screening. Speculative decoding accelerates generation by employing a lightweight draft model to sample tokens, which a larger target model then verifies and refines. Yet in protein sequence generation, draft models are typically agnostic to the structural and functional constraints of the target protein, leading to biologically implausible outputs and a shift in the likelihood distribution of generated sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a novel framework that incorporates biological, structural, and functional priors using k-mer motifs extracted from multiple sequence alignments. By scoring candidate sequences in parallel and selecting those most consistent with known biological patterns, SpecMER significantly improves sequence plausibility while retaining the efficiency of speculative decoding. SpecMER achieves 24–32% speedup over standard autoregressive decoding, along with higher acceptance rates and improved sequence likelihoods.",
    "authors": [
      "~Thomas_Walton1",
      "~Darin_Tsui1",
      "~Aryan_Musharaf1",
      "~Amirali_Aghazadeh3"
    ],
    "pdf": "/pdf/50a20020c374f08848a57dadc434eea489374317.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on speculative decoding, which is a highly relevant inference optimization technique for large language models. It directly addresses latency reduction and throughput improvement (24-32% speedup), which aligns with my expertise in optimizing inference efficiency. The parallel scoring approach for candidate sequences relates to GPU utilization optimization. The technique of using a smaller draft model to accelerate a larger target model is directly applicable to LLMs beyond the protein domain.",
      "Irrelevant Aspects": "The application domain is protein generation rather than natural language processing. The paper uses k-mer motifs from multiple sequence alignments, which is domain-specific to bioinformatics rather than general NLP. It doesn't address training optimization or scaling to larger model architectures. The biological and structural constraints mentioned are specific to protein sequences rather than general text sequences.",
      "Summary": "While this paper focuses on protein generation rather than language models, its core contribution of speculative decoding is highly relevant to LLM inference optimization. The technique of using a smaller draft model to reduce inference latency while maintaining quality has direct parallels in LLM research. The 24-32% speedup demonstrates significant throughput improvements. Though domain-specific, the fundamental approach to reducing sequential inference bottlenecks aligns well with my expertise in optimizing GPU utilization and reducing latency in large model inference."
    }
  },
  {
    "id": "4QVLKwgg3S",
    "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs",
    "abstract": "Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by **1.91×** through achieving **2.22×** server throughput, and reduces inter token latency by **11.24\\%** compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge",
    "authors": [
      "~Jinwoo_Park2",
      "~Seunggeun_Cho1",
      "~Dongsu_Han1"
    ],
    "pdf": "/pdf/459a4e18b8ff1185a575fab6045c65a522b34c45.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Directly addresses LLM inference optimization, focuses on GPU utilization by leveraging edge GPUs, introduces pipeline-aware scheduling to increase throughput (2.22× improvement), reduces inter-token latency by 11.24%, employs speculative decoding to split workloads between edge and server, enhances overall cost efficiency by 1.91×, provides quantitative performance metrics relevant to scalability research",
      "Irrelevant Aspects": "Doesn't address training optimization aspects, limited focus on specific GPU architectures or hardware-level optimizations, edge-assisted approach may not be applicable to all LLM deployment scenarios, no mention of model compression or distillation techniques",
      "Summary": "This paper is highly relevant to my research interests in LLM inference optimization, specifically targeting GPU utilization, scalability, throughput, and latency improvements. SpecEdge introduces an innovative edge-assisted framework using speculative decoding to distribute workloads between edge and server GPUs, achieving significant performance gains. While it doesn't cover training optimization, its focus on inference efficiency and the reported quantitative improvements make it an important contribution to my research domain."
    }
  },
  {
    "id": "UPELg2oUo3",
    "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models",
    "abstract": "In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization.\nHowever, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: \"reorganizing\" the attention pattern to alleviate the challenges.\nInspired by the local aggregatin nature of visual feature extraction, we design a novel **P**attern-**A**ware token **R**e**O**rdering (**PARO**) technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization.\nWe evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern.\nOur approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (**20%-30%**) and bitwidth (**INT8/INT4**), achieving a **1.9 - 2.7x** end-to-end latency speedup.",
    "authors": [
      "~Tianchen_Zhao2",
      "~Ke_Hong1",
      "~Xinhao_Yang1",
      "~Xuefeng_Xiao1",
      "~Huixia_Li2",
      "~Feng_Ling1",
      "~Ruiqi_Xie1",
      "~SiQi_Chen11",
      "~Hongyu_Zhu8",
      "~Zhang_Yichong1",
      "~Yu_Wang3"
    ],
    "pdf": "/pdf/0af4f78381a38f6ee021c71f12b2d67896f5d6a5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Optimization of attention mechanisms through sparsification and quantization techniques; Hardware-friendly design patterns for improved GPU utilization; Significant latency reduction (1.9-2.7x speedup); Low bitwidth operation (INT8/INT4) for efficient inference; Memory and computational efficiency improvements; System-level performance optimization",
      "Irrelevant Aspects": "Focus on visual generation models rather than language models; Specialized handling of visual attention patterns which differ from language attention; No discussion of training optimization (only inference); Limited to attention mechanism without broader system optimization; Visual feature extraction properties that may not translate to language models",
      "Summary": "PAROAttention introduces a pattern-aware token reordering technique to optimize sparse and quantized attention in visual generation models. The approach unifies diverse attention patterns into hardware-friendly block-wise patterns, enabling efficient sparsification (20%-30% density) and quantization (INT8/INT4) while maintaining model quality. The method achieves 1.9-2.7x end-to-end latency improvements with lossless visual metrics. While the optimization principles for attention mechanisms and hardware-efficient design are relevant to ML system optimization, the visual generation focus differs from language model applications."
    }
  },
  {
    "id": "o2agVeHrW4",
    "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks",
    "abstract": "Network pruning reduces computational requirements of large neural networks, with N:M sparsity—retaining only N out of every M consecutive weights—offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100× speedup with only 1-10\\% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.  Our code is available at https://github.com/mazumder-lab/TSENOR.",
    "authors": [
      "~Xiang_Meng1",
      "~Mehdi_Makni1",
      "~Rahul_Mazumder1"
    ],
    "pdf": "/pdf/8e495ba4be6e42154fda6115ebe0adc3a42d9f45.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses N:M sparsity optimization for large language models, focusing on computational efficiency during both forward and backward passes. It proposes an efficient algorithm that scales to billion-parameter models like LLaMA3.2-8B, demonstrating up to 100× GPU speedup and integration with state-of-the-art pruning frameworks (Wanda, SparseGPT, ALPS). The approach directly targets GPU utilization and throughput optimization, which are central to training and inference efficiency.",
      "Irrelevant Aspects": "The paper focuses more on the mathematical formulation of sparse mask generation rather than detailed hardware implementation aspects. There's limited discussion specifically about inference-only optimizations or deployment considerations beyond the computational efficiency aspects.",
      "Summary": "TSENOR presents an efficient algorithm for finding transposable N:M sparse masks that scale to billion-parameter models, addressing a key limitation in existing sparsity approaches during matrix transposition. The method achieves significant GPU speedups while maintaining model quality, making it highly relevant for optimizing large language model training and potentially inference performance."
    }
  },
  {
    "id": "vqaWAmuzRt",
    "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
    "abstract": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model, which originally required 95GB of memory, on a single 24GB consumer GPU—bringing efficient and practical model adaptation to individual users.",
    "authors": [
      "~Hsi-Che_Lin1",
      "~Yu-Chu_Yu1",
      "~Kai-Po_Chang1",
      "~Yu-Chiang_Frank_Wang2"
    ],
    "pdf": "/pdf/48f5c3688bd6d8b15b10a4cc92a53dd7357badfc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses memory efficiency during fine-tuning of large language models, which is a critical optimization challenge. EMLoC enables fine-tuning a 38B model on a single 24GB consumer GPU, demonstrating significant improvements in GPU utilization and accessibility. The framework uses activation-aware SVD and LoRA correction, which are relevant optimization techniques for reducing memory overhead while maintaining model performance. This work contributes to making large model adaptation more practical and scalable.",
      "Irrelevant Aspects": "The paper primarily focuses on memory-efficient fine-tuning with limited discussion on inference optimization techniques. There's no explicit mention of throughput or latency measurements, which are key metrics for inference performance. The work doesn't appear to address distributed training or multi-GPU scaling strategies.",
      "Summary": "EMLoC proposes an emulator-based approach that enables fine-tuning large foundation models within inference memory budgets. By constructing lightweight emulators using activation-aware SVD and applying LoRA corrections, it achieves significant memory savings while maintaining performance. The framework enables fine-tuning a 38B model (originally requiring 95GB) on a single 24GB GPU, substantially improving accessibility of large model adaptation."
    }
  },
  {
    "id": "uazfjnFL0G",
    "title": "LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis",
    "abstract": "Electroencephalography (EEG) offers a non-invasive lens into human brain activity, but building large‐scale models is hampered by $\\textit{topological heterogeneity}$: each public corpus defines its own electrode layout, limiting generalization. We introduce $\\textbf{LUNA}$ ($\\textbf{L}$atent $\\textbf{U}$nified $\\textbf{N}$etwork $\\textbf{A}$rchitecture), a self-supervised foundation model that reconciles disparate electrode geometries while scaling linearly---not quadratically---with channel count. LUNA compresses multi-channel EEG into a fixed-size, topology-agnostic latent space via learned queries and cross-attention. Downstream transformer blocks then operate exclusively on this latent representation using patch-wise temporal self-attention, decoupling computation from electrode count. Pre-trained on TUEG and Siena ($\\>$21,000 h raw EEG across diverse montages) using a masked-patch reconstruction objective, LUNA transfers effectively to four downstream tasks: abnormality detection, artifact rejection, slowing classification, and emotion recognition. It demonstrates highly competitive performance across several benchmarks, achieving state-of-the-art results on TUAR and TUSL, e.g., $\\textbf{0.921 AUROC}$ on TUAR, while reducing FLOPs by $\\textbf{300}$$\\times$ and trimming GPU memory use by up to $\\textbf{10}$$\\times$. Critically,  these gains are consistent across all evaluated electrode configurations. Code is available at https://github.com/pulp-bio/biofoundation",
    "authors": [
      "~Berkay_Döner1",
      "~Thorir_Mar_Ingolfsson1",
      "~Luca_Benini2",
      "~Yawei_Li1"
    ],
    "pdf": "/pdf/d4cb59591e8ab0b37de23f9d4bb4c06db9952d1a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Efficient transformer architecture design that scales linearly rather than quadratically with input size; Use of cross-attention to compress inputs before processing; Self-attention on compressed latent representations; Significant computational efficiency gains (300× FLOP reduction, 10× memory reduction); Self-supervised foundation model training on large datasets; Latent space compression techniques that decouple computation from input size.",
      "Irrelevant Aspects": "Domain-specific application to EEG signal analysis rather than natural language; Focus on EEG-specific challenges like electrode layouts and topological heterogeneity; Signal processing techniques tailored for brain activity data; Different modality (temporal-spatial signals vs. text).",
      "Summary": "LUNA presents an efficient foundation model for EEG signal analysis with notable architectural innovations for transformer efficiency. While domain-specific to neurology, its core techniques for attention optimization and input compression achieve substantial computational savings that have broader relevance to transformer optimization. The paper demonstrates how latent space compression and selective attention can dramatically reduce computational requirements while maintaining performance, offering valuable insights for optimizing large language model training and inference."
    }
  },
  {
    "id": "2Z0OFReqkT",
    "title": "UMoE: Unifying Attention and FFN with Shared Experts",
    "abstract": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, that reveals an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.",
    "authors": [
      "~Yuanhang_Yang1",
      "~Chaozheng_Wang1",
      "~Jing_Li18"
    ],
    "pdf": "/pdf/5300d80df6d724e66f4a137004e8d1ca147f08be.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses Mixture of Experts (MoE) architectures for Transformer scaling, which is directly related to GPU utilization and scalability. It proposes unifying MoE designs in both attention and FFN layers, enabling efficient parameter sharing. The reformulation of attention mechanism to reveal FFN-like structure could lead to more efficient implementations and potentially better GPU utilization. The focus on improving performance through architectural design aligns with optimizing throughput.",
      "Irrelevant Aspects": "The abstract does not explicitly discuss GPU utilization optimization, memory efficiency, or implementation-specific optimizations. There's no mention of inference optimization techniques, latency measurements, or throughput benchmarks. The paper appears to focus more on theoretical architectural reformulation than practical engineering optimizations for deployment.",
      "Summary": "UMoE introduces a novel reformulation of attention mechanism that reveals FFN-like structures, enabling unified MoE designs across attention and FFN layers. This approach could improve model scalability and performance through efficient parameter sharing. While not explicitly focused on implementation aspects, the architectural innovations could impact GPU utilization and throughput in practice, making it relevant to optimization research for large language models."
    }
  },
  {
    "id": "vVU1KTOsju",
    "title": "Scaling Laws for Optimal Data Mixtures",
    "abstract": "Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.",
    "authors": [
      "~Mustafa_Shukor1",
      "~Louis_Béthune1",
      "~Dan_Busbridge1",
      "~David_Grangier1",
      "~Enrico_Fini1",
      "~Alaaeldin_El-Nouby1",
      "~Pierre_Ablin2"
    ],
    "pdf": "/pdf/0c763c984dae4c3b82805d215708d4f45f9f9814.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses large language model training optimization through systematic data mixture selection using scaling laws, which can improve training efficiency. The approach is validated in large-scale settings and can extrapolate across scales, which relates to scalability concerns. The method offers a more efficient alternative to trial-and-error approaches, potentially improving resource utilization and training throughput.",
      "Irrelevant Aspects": "The paper doesn't address inference optimization, latency concerns, or explicit GPU utilization techniques. While it validates across multiple model types including vision and multimodal models, these are not the primary focus of my research. The focus is specifically on data mixture optimization rather than broader aspects of training optimization.",
      "Summary": "This paper presents a systematic method for determining optimal data mixtures in large foundation model training using scaling laws. The approach is validated across LLMs, multimodal models, and vision models, demonstrating predictive power across scales. While directly relevant to training optimization and potentially improving resource utilization, it doesn't address inference optimization or latency concerns that are also part of my research interests."
    }
  },
  {
    "id": "onhjdWCxZY",
    "title": "HiFC: High-efficiency Flash-based KV Cache Swapping for Scaling LLM Inference",
    "abstract": "Large‑language‑model inference with long contexts often produces key–value (KV) caches whose footprint exceeds the capacity of high‑bandwidth memory on a GPU. Prior LLM inference frameworks such as vLLM mitigate this pressure by swapping KV cache pages to host DRAM. However, the high cost of large DRAM pools makes this solution economically unattractive. Although offloading to SSDs can be a cost-effective way to expand memory capacity relative to DRAM, conventional frameworks such as FlexGen experience a substantial throughput drop since the data path that routes SSD traffic through CPU to GPU is severely bandwidth-constrained. To overcome these limitations, we introduce HiFC, a novel DRAM‑free swapping scheme that enables direct access to SSD-resident memory with low latency and high effective bandwidth. HiFC stores KV pages in pseudo-SLC (pSLC) regions of commodity NVMe SSDs, sustaining high throughput under sequential I/O and improving write endurance by up to 8$\\times$. Leveraging GPU Direct Storage, HiFC enables direct transfers between SSD and GPU, bypassing host DRAM and alleviating PCIe bottlenecks. HiFC employs fine-grained block mapping to confine writes to high-performance pSLC zones, stabilizing latency and throughput under load. HiFC achieves inference throughput comparable to DRAM-based swapping under diverse long-context workloads, such as NarrativeQA, while significantly lowering the memory expansion cost of a GPU server system by 4.5$\\times$ over three years.",
    "authors": [
      "~Inho_Jeong1",
      "~Sunghyeon_Woo1",
      "~Sol_Namkung1",
      "~Dongsuk_Jeon1"
    ],
    "pdf": "/pdf/54ad85c547f1d3f857eaf95351118ce21c8de1d6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization with focus on GPU utilization, scalability, throughput, and latency. It introduces novel memory management techniques for KV caches, presents hardware-software co-design using GPU Direct Storage, and addresses the challenge of long-context processing.",
      "Irrelevant Aspects": "Focuses primarily on inference rather than training optimization. Relies on specific hardware optimizations (pSLC SSDs) that might have limited generalizability. Doesn't address algorithmic model improvements or quantization techniques that are also relevant to inference optimization.",
      "Summary": "HiFC presents a DRAM-free KV cache swapping system that enables direct GPU-to-SSD transfers to handle long-context LLM inference more efficiently. By leveraging pseudo-SLC regions of NVMe SSDs and GPU Direct Storage, it achieves throughput comparable to DRAM-based solutions at significantly lower cost, addressing key challenges in LLM inference scalability."
    }
  },
  {
    "id": "QIv5aXEAcc",
    "title": "Theoretical Investigation of Adafactor for Non-Convex Smooth Optimization",
    "abstract": "Adafactor is an early memory-efficient optimization algorithm proposed as an alternative to Adam. By eliminating first-order momentum and employing a\nrank-$1$ matrix factorization to approximate the second-moment matrix, Adafactor achieves near-zero memory overhead compared to traditional gradient descent methods.\nDespite its practical suitability for large-scale training tasks where memory efficiency is critical, its theoretical convergence analysis remains unexplored, largely due to the challenges posed by its matrix factorization and update clipping mechanisms. In this work, we provide a convergence analysis of Adafactor for non-convex smooth optimization.\nWe establish optimal convergence rates (up to logarithmic factors) for finding stationary points in both deterministic and stochastic settings, the latter under sub-Gaussian noises.\nCentral to our analysis involves viewing Adafactor as an approximation of Adam, and the use of a new proxy step-size to approximate the unique\nadaptive step-size induced by Adafactor's matrix factorization and update clipping, along with an induction argument to control the gradient magnitude. \nOur finding may theoretically suggest that involving rank-$1$ matrix approximation of the second-moment matrix in Adam does not fundamentally hinder the convergence.",
    "authors": [
      "~Yusu_Hong1",
      "~Junhong_Lin1"
    ],
    "pdf": "/pdf/c82e79619ed45238d1610713f87bc093270f0d3d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Memory-efficient optimization algorithms, Adafactor as alternative to Adam, convergence analysis for large-scale training, GPU memory optimization implications, rank-1 matrix factorization technique",
      "Irrelevant Aspects": "Purely theoretical focus without empirical LLM results, lacks practical implementation details, no discussion of inference optimization, limited analysis on transformer-specific optimization",
      "Summary": "This paper provides theoretical convergence analysis of Adafactor, a memory-efficient optimizer that uses rank-1 matrix factorization. While Adafactor is relevant for LLM training where memory efficiency is critical, the paper focuses on theoretical aspects rather than practical implementation details or performance on actual language models. The analysis suggests the rank-1 approximation doesn't fundamentally hinder convergence, which has implications for optimization selection in memory-constrained environments."
    }
  },
  {
    "id": "GBMzJLhsRj",
    "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
    "abstract": "We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.",
    "authors": [
      "~Yanxi_Chen1",
      "~Xuchen_Pan1",
      "~Yaliang_Li1",
      "~Bolin_Ding3",
      "~Jingren_Zhou1"
    ],
    "pdf": "/pdf/dfab05f4543d811a499e3234d231318075ea52aa.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Test-time compute optimization for LLMs, Provable scaling laws for inference performance, Two-stage knockout and league-style algorithms requiring no additional training, Black-box approach without need for verifiers or reward models, GPU utilization implications for inference phase, Throughput and latency optimization through better compute allocation, Practical implementations that improve model performance with additional compute",
      "Irrelevant Aspects": "No focus on training phase optimization, Lacks GPU-specific optimization techniques, Doesn't discuss multi-GPU or distributed inference strategies, No mention of quantization or model compression methods, Limited discussion of specific hardware implementation details",
      "Summary": "This paper presents highly relevant research on optimizing LLM inference through test-time compute scaling. The proposed knockout and league-style algorithms offer practical ways to improve model performance by efficiently allocating computational resources during inference, directly addressing throughput and latency concerns. While it focuses primarily on the inference phase rather than training, its black-box approach and provable scaling properties make it valuable for improving GPU utilization and overall system efficiency."
    }
  },
  {
    "id": "HW55AwGEC8",
    "title": "Model Merging in Pre-training of Large Language Models",
    "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
    "authors": [
      "~Yunshui_Li1",
      "~Yiyuan_Ma1",
      "~Shen_Yan5",
      "~Chaoyi_Zhang2",
      "~Jing_Liu8",
      "~Jianqiao_Lu1",
      "~Ziwen_Xu4",
      "~Mengzhao_Chen1",
      "~Minrui_Wang1",
      "~Shiyi_Zhan1",
      "~Jin_Ma6",
      "~Xunhao_Lai1",
      "~Yao_Luo3",
      "~Xingyan_Bin1",
      "~Hongbin_Ren2",
      "~Mingji_Han1",
      "~Wenhao_Hao1",
      "~Bairen_Yi1",
      "~LingJun_Liu1",
      "~Bole_Ma3",
      "~Xiaoying_Jia1",
      "~zhou_Xun2",
      "~liang_xiang3",
      "~Yonghui_Wu1"
    ],
    "pdf": "/pdf/9703feed654b63629cceff7758b9964422f37467.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Model merging during pre-training directly impacts training optimization and GPU utilization efficiency. The paper's experiments with models from millions to over 100 billion parameters address scalability challenges. The focus on more efficient model development and lower training costs aligns with goals of higher throughput. Analysis of MoE architectures is particularly relevant for scalability at large scale. The practical guidelines for model merging can inform optimization strategies.",
      "Irrelevant Aspects": "Limited focus on inference optimization which is a key part of my research interests. Doesn't explicitly address GPU memory utilization strategies in detail. Minimal discussion of latency optimization techniques during inference. The focus appears to be on model quality improvements rather than system-level performance metrics.",
      "Summary": "This paper is moderately relevant to my research interests, particularly for training optimization and scalability aspects of large language models. It provides valuable insights into model merging techniques during pre-training that can enhance training efficiency and reduce costs, potentially improving GPU utilization and throughput. However, its limited focus on inference optimization and latency concerns reduces its overall relevance to my specific research goals."
    }
  },
  {
    "id": "qXSFkP0ELS",
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "abstract": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. \nHowever, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. \nTo address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \\textit{atomic questions}, exhibiting the memoryless property similar to Markov processes.\nBased on this observation, we propose Atom of Thoughts (\\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \\textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process.\nFurthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \\our to serve as a plug-in enhancement for improving reasoning capabilities.",
    "authors": [
      "~Fengwei_Teng1",
      "~Quan_Shi4",
      "~Zhaoyang_Yu4",
      "~Jiayi_Zhang7",
      "~Yuyu_Luo1",
      "~Chenglin_Wu2",
      "~Zhijiang_Guo2"
    ],
    "pdf": "/pdf/aee7e5e0ac85edb676698e634deccc28c92e1407.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on test-time scaling for LLMs, which is directly related to inference optimization. It addresses computational resource waste during reasoning, which relates to GPU utilization efficiency. The approach aims to make reasoning more efficient by avoiding accumulated historical information. The method is described as a 'plug-in enhancement' for improving reasoning capabilities, suggesting practical implementation for inference systems. The decomposition-contraction process could potentially reduce computational overhead for complex reasoning tasks. The paper deals with the scalability of LLM inference systems.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU hardware utilization or memory optimization. There's no mention of parallel processing or distributed computing strategies for inference. The abstract doesn't cover quantitative metrics about throughput or latency improvements. It doesn't appear to address model compression or quantization techniques often relevant to inference optimization. There's no mention of batch processing or serving optimizations.",
      "Summary": "This paper presents Atom of Thoughts, a test-time scaling method for LLMs that decomposes complex reasoning into atomic subquestions with Markov properties. While it addresses computational efficiency in reasoning, it appears more focused on the reasoning methodology than system-level optimizations for GPU utilization, throughput, or latency metrics. The approach could enhance inference systems but lacks explicit discussion of hardware-specific optimizations."
    }
  },
  {
    "id": "fDjDVE4qdj",
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform reasoning based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate reasoning mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model’s capability for hybrid reasoning. Extensive experimental results show that LHRMs can adaptively perform hybrid reasoning on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended reasoning processes and provides a solid starting point for building hybrid reasoning systems.",
    "authors": [
      "~Lingjie_Jiang1",
      "~Xun_Wu1",
      "~Shaohan_Huang1",
      "~Qingxiu_Dong1",
      "~Zewen_Chi1",
      "~Li_Dong1",
      "~Xingxing_Zhang1",
      "~Tengchao_Lv1",
      "~Lei_Cui2",
      "~Furu_Wei1"
    ],
    "pdf": "/pdf/33fb886d1cda050a0c29d0bdee85176c1c3f7f31.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization through adaptive reasoning mechanisms, which can reduce computational overhead for simple queries and improve GPU utilization. Their two-stage training pipeline (Hybrid Fine-Tuning followed by Hybrid Group Policy Optimization) is relevant to training optimization for large models. The focus on reducing latency and token consumption directly aligns with research goals for more efficient inference systems.",
      "Irrelevant Aspects": "The paper primarily focuses on reasoning capabilities rather than the technical details of GPU utilization. There's no explicit discussion of scalability across multiple GPUs or distributed systems. The Hybrid Accuracy metric they introduce is focused on reasoning assessment rather than system performance metrics.",
      "Summary": "This paper presents Large Hybrid-Reasoning Models (LHRMs) that can adaptively determine when to perform extended reasoning based on query complexity, offering potential efficiency gains by avoiding unnecessary computation for simple queries. While relevant to inference optimization and reducing latency, it doesn't deeply explore the technical aspects of GPU utilization and scalability that are central to advanced machine learning system optimization."
    }
  },
  {
    "id": "eIojV2epgX",
    "title": "Synergistic Tensor and Pipeline Parallelism",
    "abstract": "In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communication overheads, while PP suffers from synchronization inefficiencies such as pipeline bubbles. Existing works primarily address these challenges from isolated perspectives, focusing either on overlapping TP communication or on flexible PP scheduling to mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor and pipeline parallelism schedule that simultaneously reduces both types of bubbles. Our proposed schedule decouples the forward and backward passes in PP into fine-grained computation units, which are then braided to form a composite computation sequence. This compositional structure enables near-complete elimination of TP-related bubbles. Building upon this structure, we further design the PP schedule to minimize PP bubbles. Experimental results demonstrate that our approach improves training throughput by up to 12\\% for LLMs and 16\\% for MLLMs compared to existing scheduling methods. Our source code is avaiable at https://github.com/MICLAB-BUPT/STP.",
    "authors": [
      "~Mengshi_Qi1",
      "~Jiaxuan_Peng1",
      "~Jie_Zhang17",
      "~Juan_Zhu1",
      "~Yong_Li11",
      "~Huadong_Ma1"
    ],
    "pdf": "/pdf/bec908639def2b2c30972be14736f60f3b75648e.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses key challenges in LLM training optimization through hybrid model parallelism. It focuses on reducing tensor parallelism communication overheads and pipeline parallelism bubbles, which are critical for improving GPU utilization and scalability. The proposed synergistic schedule decouples forward and backward passes into fine-grained units to eliminate TP-related bubbles and optimize PP scheduling. The demonstrated throughput improvements of 12% for LLMs and 16% for MLLMs directly align with research goals of higher throughput and better GPU utilization.",
      "Irrelevant Aspects": "The paper appears to focus exclusively on training optimization without addressing inference optimization, which is part of the research interest. There's no explicit discussion of latency improvements, though reducing bubbles would indirectly help with latency. The scope is limited to LLMs and MLLMs without broader application to other machine learning models.",
      "Summary": "This paper presents a highly relevant approach to optimizing large language model training by synergistically combining tensor and pipeline parallelism. The novel scheduling technique effectively addresses two major bottlenecks in distributed training: communication overheads from tensor parallelism and pipeline bubbles from pipeline parallelism. By braiding fine-grained computation units, the method achieves significant throughput improvements of up to 16% for multimodal models. The work directly contributes to research goals of enhancing GPU utilization and scalability in large-scale model training, making it highly valuable for the research focus on ML system optimization."
    }
  },
  {
    "id": "JRFMzQnYXl",
    "title": "Stab-SGD: Noise-Adaptivity in Smooth Optimization with Stability Ratios",
    "abstract": "In the context of smooth stochastic optimization with first order methods, we introduce the stability ratio of gradient estimates, as a measure of local relative noise level, from zero for pure noise to one for negligible noise. We show that a schedule-free variant (Stab-SGD) of stochastic gradient descent obtained by just shrinking the learning rate by the stability ratio achieves real adaptivity to noise levels (i.e. without tuning hyperparameters to the gradient's variance), with all key properties of a good schedule-free algorithm: neither plateau nor explosion at intialization, and no saturation of the loss.\nWe believe this theoretical development reveals the importance of estimating the local stability ratio in the construction of well-behaved (last-iterate) schedule-free algorithms, particularly when hyperparameter-tuning budgets are a small fraction of the total budget since noise-adaptivity and cheaper horizon-free tuning are most crucial in this regime.",
    "authors": [
      "~David_A._R._Robin1",
      "~Killian_Bakong1",
      "~Kevin_Scaman1"
    ],
    "pdf": "/pdf/3cabd32b31bcbc6af4ee33dda9c64317e6a56212.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces Stab-SGD, a noise-adaptive variant of SGD that could significantly improve LLM training efficiency. By adapting learning rates based on local noise levels without manual hyperparameter tuning, it could reduce training time and computational costs, leading to better GPU utilization and higher throughput. The 'schedule-free' approach is particularly valuable for large-scale LLM training, where hyperparameter tuning is expensive. The theoretical development could inform the design of more efficient training algorithms for large models.",
      "Irrelevant Aspects": "The paper doesn't directly address inference optimization, which is a key part of my research interest. It's also more theoretical than implementation-focused, so the practical applicability to LLM training pipelines may require additional development. There's no explicit mention of scalability across multiple GPUs or distributed training systems.",
      "Summary": "This paper presents Stab-SGD, a noise-adaptive optimization algorithm that could enhance LLM training efficiency by automatically adjusting learning rates based on local noise conditions. The 'schedule-free' approach could reduce the need for expensive hyperparameter searches, which is crucial for large-scale LLM training. While it doesn't directly address inference optimization or specific scalability concerns, its potential to improve training efficiency and reduce resource requirements makes it highly relevant to my research on LLM optimization."
    }
  },
  {
    "id": "bjV8Y38aFF",
    "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads",
    "abstract": "Transformer models have driven breakthroughs across various language tasks by their strong capability to learn rich contextual representations. Scaling them to improve representation, however, often demands substantial memory and compute costs, such as the Key-Value (KV) cache used during auto-regressive decoding. Skip connections offer a promising way to improve representation without bloating resource usage, yet most prior works either improve expressivity while leaving KV costs unchanged, or reduce memory at the cost of weaker representation. In this work, we propose SkipV1Former, a  Transformer variant that uses skip connections from the first layer's Value heads to strengthen model representation and reduce KV cache.  Specifically, from the second block onward, each layer reuses half of its Value heads from the very first layer, while computing the other half as usual-cutting Value projections and V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed first-layer Values into deeper layers restores information lost to compression and accelerates the model’s implicit mesa-optimization-a key pattern of Transformer in auto-regressive tasks. Empirically, across different model scales, SkipV1Former delivers consistent reductions of approximately 25 \\% in KV cache while improving perplexity relative to standard Multi-Head Attention (MHA) Transformers and some advanced variants. Moreover, we propose a recipe for uptraining existing MHA Transformer checkpoints to SkipV1Former with only 10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine advanced methods like Group-Query Attention and Multi-Latent Attention to achieve further KV cache savings and performance improvement. When combined with YOCO, it cuts KV cache size by nearly 50 \\% while still improving performance. The code is\n available at: https://github.com/Zhoutong-Wu/SkipV1Former.",
    "authors": [
      "~Zhoutong_Wu1",
      "~Yuan_Zhang51",
      "~Yiming_Dong1",
      "~Chenheng_Zhang2",
      "~Cong_Fang1",
      "~Kun_Yuan4",
      "~Zhouchen_Lin1"
    ],
    "pdf": "/pdf/b6a5b9b8c6fbc4cd315ac76ae37e16806cea1ae5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Reduces KV cache memory usage during inference, addressing a key bottleneck in LLM deployment",
        "Maintains or improves model performance while reducing resource requirements",
        "Compatible with other optimization techniques like Group-Query Attention",
        "Offers method to uptrain existing models with minimal additional compute",
        "Potential to reduce inference latency through decreased memory requirements"
      ],
      "Irrelevant Aspects": [
        "Primarily focuses on architectural changes rather than system-level optimizations",
        "Limited discussion of throughput optimization",
        "Lacks coverage on distributed training or inference strategies",
        "Theoretical aspects may not directly translate to practical system improvements"
      ],
      "Summary": "SkipV1Former introduces a novel approach to reduce KV cache memory usage in Transformers by reusing Value heads from the first layer in deeper layers, achieving approximately 25% reduction in KV cache while improving performance. The method is highly relevant for memory optimization in large language models and addresses critical GPU utilization challenges during inference."
    }
  },
  {
    "id": "ixOpURt7wC",
    "title": "Exploiting Dynamic Sparsity in Einsum",
    "abstract": "Einsum expressions specify an output tensor in terms of several input tensors. They offer a simple yet expressive abstraction for many computational tasks in artificial intelligence and beyond. However, evaluating einsum expressions poses hard algorithmic problems that depend on the representation of the tensors. Two popular representations are multidimensional arrays and coordinate lists. The latter is a more compact representation for sparse tensors, that is, tensors where a significant proportion of the entries are zero. So far, however, most of the popular einsum implementations use the multidimensional array representation for tensors. Here, we show on a non-trivial example that, when evaluating einsum expressions, coordinate lists can be exponentially more efficient than multidimensional arrays. In practice, however, coordinate lists can also be significantly less efficient than multidimensional arrays, but it is hard to decide from the input tensors whether this will be the case. Sparsity evolves dynamically in intermediate tensors during the evaluation of an einsum expression. Therefore, we introduce a hybrid solution where the representation is switched on the fly from multidimensional arrays to coordinate lists depending on the sparsity of the remaining tensors. In our experiments on established benchmark einsum expressions, the hybrid solution is consistently competitive with or outperforms the better of the two static representations.",
    "authors": [
      "~Christoph_Staudt1",
      "~Mark_Blacher1",
      "~Tim_Hoffmann1",
      "~Kaspar_Kasche1",
      "~Olaf_Beyersdorff1",
      "~Joachim_Giesen1"
    ],
    "pdf": "/pdf/0e553fb0442814f2f9772c6cf507c83d48f7c8f0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses optimization of einsum operations which are fundamental to deep learning computations including LLMs. It introduces a hybrid approach for dynamically switching between dense and sparse tensor representations based on evolving sparsity patterns. This could improve memory efficiency and computational performance, potentially leading to better GPU utilization and reduced latency in ML workloads. The dynamic handling of sparsity is particularly relevant for efficient tensor operations in ML systems.",
      "Irrelevant Aspects": "The paper does not specifically focus on GPU optimizations or directly address large language model implementations. The abstract doesn't mention scalability across multiple GPUs or distributed systems. The approach appears to be limited to einsum expressions rather than broader optimization strategies for LLM training or inference pipelines. The evaluation seems focused on established benchmark expressions rather than real-world LLM scenarios.",
      "Summary": "This paper presents a hybrid solution for optimizing einsum operations by dynamically switching between multidimensional arrays and coordinate list representations based on sparsity evolution during computation. While not directly targeting LLMs, the techniques could contribute to more efficient tensor computations in ML systems by better utilizing resources when processing sparse data, potentially improving both throughput and latency in scenarios where sparsity is present."
    }
  },
  {
    "id": "w4qJ056WhI",
    "title": "SpaceServe: Spatial Multiplexing of Complementary Encoders and Decoders for Multimodal LLMs",
    "abstract": "Recent multimodal large language models (MLLMs) marry modality-specific\nvision or audio encoders with a shared text decoder. While the encoder is compute-\nintensive but memory-light, the decoder is the opposite, yet state-of-the-art serving\nstacks still time-multiplex these complementary kernels, idling SMs or HBM in\nturn. We introduce SpaceServe, a serving system that space-multiplexes MLLMs:\nit decouples all modality encoders from the decoder, and co-locates them on the\nsame GPU using fine-grained SM partitioning available in modern runtimes. A\ncost-model-guided Space-Inference Scheduler (SIS) dynamically assigns SM slices,\nwhile a Time-Windowed Shortest-Remaining-First (TWSRFT) policy batches en-\ncoder requests to minimise completion latency and smooth decoder arrivals. \nEvaluation shows that SpaceServe reduces time-per-output-token by 4.81×\non average and up to 28.9× on Nvidia A100 GPUs. SpaceServe is available at\nhttps://github.com/gofreelee/SpaceServe",
    "authors": [
      "~zhicheng_li4",
      "~Shuoming_Zhang1",
      "~Jiacheng_Zhao1",
      "~Siqi_Li13",
      "~Xiyu_Shi2",
      "~Yangyu_Zhang2",
      "~Shuaijiang_Li2",
      "~Donglin_Yu1",
      "~Zheming_Yang1",
      "~YUAN_WEN2",
      "~Huimin_Cui1"
    ],
    "pdf": "/pdf/3b4c3201ed20b1e72b79554b27c1dbf4768bb9b9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "SpaceServe directly addresses GPU utilization optimization for LLM inference through spatial multiplexing of complementary compute kernels. It specifically focuses on improving throughput and reducing latency by co-locating encoders and decoders on the same GPU using fine-grained SM partitioning. The system introduces a cost-model-guided scheduler and batching policy that achieves 4.81× reduction in time-per-output-token on average. This represents significant advances in inference optimization, resource utilization, and scalability for serving multimodal LLMs on modern GPU hardware.",
      "Irrelevant Aspects": "The paper focuses exclusively on multimodal LLMs rather than text-only models, and addresses inference optimization rather than training optimization. The approach is limited to single-GPU optimization rather than distributed serving across multiple GPUs or nodes.",
      "Summary": "SpaceServe introduces a novel serving system for multimodal LLMs that addresses the complementary resource usage patterns of encoders (compute-intensive, memory-light) and decoders (compute-light, memory-intensive). By spatially multiplexing these components on the same GPU with fine-grained SM partitioning, SpaceServe achieves significantly better GPU utilization compared to traditional time-multiplexing approaches. The system achieves substantial performance improvements with up to 28.9× reduction in time-per-output-token on Nvidia A100 GPUs, representing important advances in inference optimization for large language models."
    }
  },
  {
    "id": "D6w7wIN360",
    "title": "DynaPipe: Dynamic Layer Redistribution for Efficient Serving of LLMs with Pipeline Parallelism",
    "abstract": "To accelerate large language model (LLM) inference, pipeline parallelism partitions model layers into sequential stages, each assigned to a different device for concurrent execution. However, this method often suffers from pipeline bubbles caused by imbalanced computation in the tail stage. While upstream stages focus solely on layer-forward operations, the final stage must also handle post-processing tasks like sampling, introducing significant latency. This uneven workload leads to pipeline misalignment, forcing upstream stages to idle and degrading overall performance. Existing frameworks typically distribute layers evenly across stages without accounting for computational load differences. To address this, we propose DynaPipe, a dynamic layer redistribution scheme that adaptively balances computation by predicting execution latency in real time. Moreover, we introduce an asynchronous key-value (KV) cache migration coordinator to enable\nnon-blocking layer redistribution during inference. Experiments on representative LLMs demonstrate that DynaPipe reduces average end-to-end request latency by 8% to 49% across diverse workloads, outperforming state-of-the-art pipeline parallelism systems.",
    "authors": [
      "~HongXin_Xu1",
      "~Tianyu_Guo5",
      "~Xianwei_Zhang1"
    ],
    "pdf": "/pdf/8504f4e482ff780ff39a92c3a62cfc54193c6957.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Pipeline parallelism optimization for LLMs, dynamic layer redistribution to balance computational workload, real-time latency prediction, asynchronous KV cache migration, reduction of pipeline bubbles, GPU utilization improvement, latency reduction techniques (8-49% improvement), LLM inference optimization",
      "Irrelevant Aspects": "Limited focus on training optimization (inference-only focus), minimal details on hardware-specific optimizations",
      "Summary": "DynaPipe addresses the pipeline parallelism bottleneck in LLM serving where the final stage's additional post-processing workload creates imbalanced computation and pipeline bubbles. The system dynamically redistributes layers across stages based on real-time execution latency predictions and implements asynchronous KV cache migration to enable non-blocking redistribution. This approach improves GPU utilization by reducing idle time in upstream stages, resulting in 8-49% reduction in end-to-end request latency across diverse workloads, demonstrating significant improvements in throughput and scalability for LLM serving systems."
    }
  },
  {
    "id": "gUPGGCM4WH",
    "title": "Chain-of-Retrieval Augmented Generation",
    "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than $10$ points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
    "authors": [
      "~Liang_Wang2",
      "~Haonan_Chen5",
      "~Nan_Yang5",
      "~Xiaolong_Huang1",
      "~Zhicheng_Dou1",
      "~Furu_Wei1"
    ],
    "pdf": "/pdf/5a6faf62c69cb2470dc5a358601ce9be1015dcdb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization through rejection sampling for generating intermediate retrieval chains, which relates to efficient training methodologies. It discusses various decoding strategies to scale test-time compute, directly connecting to inference optimization. The paper analyzes scaling behavior of the CoRAG system, which is relevant to my research on scalability. The multi-step retrieval approach could impact GPU utilization patterns, throughput, and latency considerations in deployment scenarios.",
      "Irrelevant Aspects": "The paper focuses primarily on algorithmic innovations rather than system-level optimizations for GPUs. There is no explicit discussion of hardware-specific optimizations, kernel implementations, or memory management techniques. The scaling analysis appears to focus on performance scaling (accuracy improvements) rather than computational resource scaling. The paper doesn't address distributed training techniques or memory-efficient implementations that would be central to optimizing GPU utilization.",
      "Summary": "This paper introduces CoRAG, a chain-of-retrieval approach for RAG systems that performs multiple, dynamically formulated retrieval steps before generation. While it addresses some aspects of my research interest through its novel training methodology using rejection sampling and various inference-time decoding strategies, it primarily focuses on algorithmic improvements rather than system-level optimizations. The paper's analysis of scaling behavior and test-time compute provides some relevant insights, but lacks the GPU-specific optimization techniques that are central to my research focus on hardware utilization and efficiency."
    }
  },
  {
    "id": "4VKVUmE1I8",
    "title": "RAGRouter: Learning to Route Queries to Multiple Retrieval-Augmented Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) significantly improves the performance of Large Language Models (LLMs) on knowledge-intensive tasks. However, varying response quality across LLMs under RAG necessitates intelligent routing mechanisms, which select the most suitable model for each query from multiple retrieval-augmented LLMs via a dedicated router model. We observe that external documents dynamically affect LLMs' ability to answer queries, while existing routing methods, which rely on static parametric knowledge representations, exhibit suboptimal performance in RAG scenarios. To address this, we formally define the new retrieval-augmented LLM routing problem, incorporating the influence of retrieved documents into the routing framework. We propose RAGRouter, a RAG-aware routing design, which leverages document embeddings and RAG capability embeddings with contrastive learning to capture knowledge representation shifts and enable informed routing decisions. Extensive experiments on diverse knowledge-intensive tasks and retrieval settings, covering open and closed-source LLMs, show that RAGRouter outperforms the best individual LLM and existing routing methods. With an extended score-threshold-based mechanism, it also achieves strong performance-efficiency trade-offs under low-latency constraints. The code and data are available at https://github.com/OwwO99/RAGRouter.",
    "authors": [
      "~Jiarui_Zhang12",
      "~Xiangyu_Liu8",
      "~Yong_Hu1",
      "~Chaoyue_Niu1",
      "~Fan_Wu10",
      "~Guihai_Chen3"
    ],
    "pdf": "/pdf/8f68f92dc478f3973bdaeafa880e5011aa1d3784.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization through intelligent routing between multiple LLMs, addressing low-latency constraints with a score-threshold-based mechanism, improving system efficiency and throughput by selecting the most suitable model for each query, enabling better resource utilization across multiple models",
      "Irrelevant Aspects": "Limited focus on training optimization techniques, minimal discussion of GPU-level optimization, doesn't address model architecture improvements, primarily focuses on routing rather than model-specific optimizations",
      "Summary": "RAGRouter introduces a routing mechanism for selecting the most suitable retrieval-augmented language model for each query. The paper addresses inference optimization by incorporating document embeddings and RAG capability embeddings with contrastive learning to make routing decisions. It demonstrates performance improvements across knowledge-intensive tasks and specifically mentions achieving strong performance-efficiency trade-offs under low-latency constraints. While relevant to system-level optimization and efficiency, the paper doesn't deeply address training optimization or GPU-specific utilization techniques, making it moderately relevant to my research focus on large language model optimization."
    }
  },
  {
    "id": "x2BsIdJJJW",
    "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding",
    "abstract": "Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups ($<1.5\\times$). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model's attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model's hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding.",
    "authors": [
      "~Jialiang_Kang1",
      "~Han_Shu1",
      "~Wenshuo_Li2",
      "~Yingjie_Zhai1",
      "~Xinghao_Chen1"
    ],
    "pdf": "/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Speculative decoding for inference acceleration, model throughput optimization, efficient token processing and compression, training strategies for draft models, multimodal model optimization, inference latency reduction",
      "Irrelevant Aspects": "Primarily focused on VLMs rather than general LLMs, limited discussion of GPU utilization metrics, scalability in distributed settings not explicitly covered, vision-specific aspects that don't translate to text-only models",
      "Summary": "ViSpec introduces Vision-Aware Speculative Decoding, a framework designed to accelerate Vision-Language Model inference through optimized draft model integration, vision token compression, and specialized training techniques. It represents a significant advancement in VLM inference optimization, achieving substantial speedups over previous methods, though with less explicit focus on GPU utilization and distributed scalability aspects."
    }
  },
  {
    "id": "ppKDXf55lY",
    "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment",
    "abstract": "How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding steps. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.",
    "authors": [
      "~Sangwoo_Kwon1",
      "~Seong_Hoon_Seo1",
      "~Jae_W._Lee1",
      "~Yeonhong_Park1"
    ],
    "pdf": "/pdf/523b6f8097aa032b4da47977d854556f0113ad67.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Dynamic layer-wise precision assignment, runtime model adaptation for LLMs, performance-latency trade-offs, multi-scale quantization techniques, observation of layer sensitivity changes during decoding steps",
      "Irrelevant Aspects": "Focus on on-device deployment rather than data center GPU optimization, limited discussion of training optimization, primarily memory-focused rather than GPU utilization optimization",
      "Summary": "DP-LLM introduces a dynamic precision assignment mechanism for LLM inference that adapts layer precision based on input values and decoding step context. The paper demonstrates how different layer sensitivities vary during runtime, allowing for more efficient quantization strategies that maintain accuracy while improving latency. This approach offers superior performance-latency trade-offs compared to static quantization methods, making it valuable for inference optimization scenarios with varying runtime constraints."
    }
  },
  {
    "id": "KstkQi6e7q",
    "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning",
    "abstract": "Despite significant advances in inference-time search for vision–language models (VLMs), existing approaches remain both computationally expensive and prone to unpenalized, low-confidence generations which often lead to persistent hallucinations. We introduce \\textbf{Value-guided Inference with Margin-based Reward (ViMaR)}, a two-stage inference framework that improves both efficiency and output fidelity by combining a temporal-difference value model with a margin-aware reward adjustment. In the first stage, we perform a single pass to identify the highest-value caption among diverse candidates. In the second stage, we selectively refine only those segments that were overlooked or exhibit weak visual grounding, thereby eliminating frequently rewarded evaluations. A calibrated margin-based penalty discourages low-confidence continuations while preserving descriptive richness. Extensive experiments across multiple VLM architectures demonstrate that ViMaR generates captions that are significantly more reliable, factually accurate, detailed, and explanatory, while achieving over 4$\\times$ speedup compared to existing value-guided methods. Specifically, we show that ViMaR trained solely on LLaVA Mistral-7B \\textit{generalizes effectively to guide decoding in stronger unseen models}. To further validate this, we adapt ViMaR to steer generation in both LLaVA-OneVision-Qwen2-7B and Qwen2.5-VL-3B, leading to consistent improvements in caption quality and demonstrating robust cross-model guidance. This cross-model generalization highlights ViMaR's flexibility and modularity, positioning it as a scalable and transferable inference-time decoding strategy. Furthermore, when ViMaR-generated captions are used for self-training, the underlying models achieve substantial gains across a broad suite of visual comprehension benchmarks, underscoring the potential of fast, accurate, and self-improving VLM pipelines.\n\nCode: https://github.com/ankan8145/ViMaR",
    "authors": [
      "~Ankan_Deria1",
      "~Adinath_Madhavrao_Dukre1",
      "~Feilong_Tang2",
      "~Sara_Atito1",
      "~Sudipta_Roy1",
      "~Muhammad_Awais3",
      "~Muhammad_Haris_Khan3",
      "~Imran_Razzak2"
    ],
    "pdf": "/pdf/8d9df6d290dbe7ca260a923b4f2357a4855ede26.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for vision-language models, introducing a two-stage framework that achieves 4× speedup compared to existing methods. It addresses efficiency in model inference, which is directly relevant to my interest in inference optimization. The approach demonstrates scalability through cross-model generalization across different VLM architectures, showing its flexibility as a decoding strategy. The significant speedup indicates potential for higher throughput, which is a key concern in my research.",
      "Irrelevant Aspects": "The paper specifically focuses on vision-language models for captioning tasks rather than general LLM optimization. It doesn't explicitly discuss GPU utilization, which is a central aspect of my research interest. The margin-based reward adjustment technique is specialized for captioning tasks and may not directly generalize to other LLM applications. While it shows speed improvements, it doesn't provide detailed analysis of latency characteristics.",
      "Summary": "ViMaR presents a dual-stage inference framework for vision-language models that combines a temporal-difference value model with margin-based reward adjustment. The approach identifies high-value captions in the first stage and selectively refines weak segments in the second stage, achieving over 4× speedup compared to existing methods while improving caption quality. The framework demonstrates cross-model generalization across different VLM architectures and can be used for self-training to improve model performance on visual comprehension tasks. While specialized for VLM captioning, the two-stage approach and efficiency gains offer valuable insights for general inference optimization of large models."
    }
  },
  {
    "id": "dpllevHMbc",
    "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules",
    "abstract": "Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs).  However, existing studies predominantly focus on the final-step loss, leaving open whether the entire $\\textit{loss dynamics}$ obey similar laws and, crucially, how the $\\textit{learning rate schedule}$ (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel $\\textbf{intrinsic-time}$ viewpoint, which captures the training progress more faithfully than iteration count. We then establish a $\\textbf{Functional Scaling Law (FSL)}$ that captures the full loss trajectory under arbitrary LRSs, with the schedule’s influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training.",
    "authors": [
      "~Binghui_Li1",
      "~Fengling_Chen2",
      "~Zixun_Huang4",
      "~Lean_Wang1",
      "~Lei_Wu1"
    ],
    "pdf": "/pdf/4db46b2ba3f9713e509ad6852a99e0a0e72f0637.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on scaling laws for large language models (LLMs) and provides theoretical understanding of loss dynamics and learning rate schedules, which are directly relevant to training optimization. It analyzes training efficiency in both data-limited and compute-limited regimes and validates the theory on LLMs ranging from 0.1B to 1B parameters. The paper explains key phenomena like why higher-capacity models are more compute-efficient and why warmup-stable-decay schedules outperform pure decay, offering practical insights for LLM training optimization.",
      "Irrelevant Aspects": "The paper's analysis is based on kernel regression models rather than directly on LLM architectures, which may limit direct applicability. It focuses primarily on training dynamics without addressing inference optimization, GPU utilization, throughput, latency, or distributed training scalability. The 'intrinsic-time' viewpoint is more theoretical than practical for systems optimization.",
      "Summary": "This paper establishes Functional Scaling Laws for loss dynamics under various learning rate schedules, providing theoretical insights into LLM training efficiency. While highly relevant for understanding training optimization dynamics, it doesn't address key systems aspects like GPU utilization or inference performance. Its experimental validation on actual LLMs and practical insights on learning rate schedules make it valuable for training optimization research."
    }
  },
  {
    "id": "gm65gK3uOJ",
    "title": "Glance2Gaze: Efficient Vision-Language Models from Glance Fusion to Gaze Compression",
    "abstract": "Vision-language models heavily rely on visual representations, yet ensuring its efficiency remains a critical challenge. Most existing approaches focus on reducing visual tokens either at the visual encoder phase or during the LLM decoder stage. Inspired by human visual cognition, where an initial global glance precedes focused attention on semantically salient regions, we introduce Glance2Gaze, a cognitively inspired framework that mimics the human two-stage attention process. The framework consists of two key components: the Glance Fusion module, which integrates multi-layer vision transformer features with text-aware attention to generate a semantically enriched global representation, and the Gaze Compression module, which utilizes a novel query-guided mechanism to selectively compress visual tokens based on their semantic relevance. Experimental results on widely adopted benchmarks demonstrate that Glance2Gaze outperforms existing methods, achieving superior performance with equal or lower computational cost. Furthermore, it generalizes well to high-resolution and video scenarios, showcasing robust and scalable efficiency improvements in VLMs.",
    "authors": [
      "~Juan_Chen2",
      "~Honglin_liu4",
      "~Yingying_Ao1",
      "~Ting_Zhang7",
      "~Yan_Huang16",
      "~Xudong_Liu9",
      "~Biao_Li7",
      "~Jintao_Fang1"
    ],
    "pdf": "/pdf/cace55a64d956ca69ab6942930e4d833d27e5559.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on improving computational efficiency in large models through token reduction techniques, which directly aligns with research interests in GPU utilization and throughput. The Glance2Gaze framework introduces methods to reduce computational cost while maintaining performance, which is central to training and inference optimization. The scalability claims and generalization to high-resolution scenarios suggest practical applications for resource-constrained environments. The selective compression mechanism could inform similar techniques for pure language models, potentially reducing memory footprint and latency during inference.",
      "Irrelevant Aspects": "The primary focus is on vision-language models rather than pure language models, making some of the vision-specific techniques less directly applicable. The cognitive inspiration aspect, while interesting, is not central to the technical optimization concerns. The visual representation processing details are outside the core language model optimization domain.",
      "Summary": "Glance2Gaze introduces a cognitively-inspired framework for efficient vision-language models through a two-stage process: Glance Fusion (integrating multi-layer visual features with text-aware attention) and Gaze Compression (selectively compressing visual tokens based on semantic relevance). The approach achieves superior performance with equal or lower computational cost and demonstrates scalability to high-resolution and video scenarios, making it highly relevant to research on model efficiency and optimization."
    }
  },
  {
    "id": "TDFSKAspoQ",
    "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
    "abstract": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP.",
    "authors": [
      "~Da_Chang1",
      "~Ganzhao_Yuan1"
    ],
    "pdf": "/pdf/106ecdcc2b8ad7549b92b28b9bf4087cd5c12e7d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on LLM training optimization, selective parameter updates mechanism, plug-and-play integration with standard optimizers, theoretical convergence guarantees, experiments on LLM pretraining, potential for improved training speed and GPU utilization",
      "Irrelevant Aspects": "Does not address inference optimization, limited discussion on computational overhead of selection mechanism, no specific focus on memory usage optimization",
      "Summary": "MGUP presents a momentum-gradient alignment update policy for selective parameter updates in LLM training, offering a plug-and-play enhancement to standard optimizers that can accelerate training speed while maintaining theoretical guarantees."
    }
  },
  {
    "id": "8FN25PlktS",
    "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization",
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies,  but these methods often overlook the impact of the evolving states of the language model during the optimization process.\nIn this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving batch-wise states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance.\nNotably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. \nThis work points to a promising new direction for improving LLM alignment through batch-wise sample selection, with potential generalization to RLHF and broader supervised learning paradigms.",
    "authors": [
      "~Zixuan_Huang12",
      "~Yikun_Ban1",
      "~Lean_Fu1",
      "~Xiaojie_Li12",
      "~Zhongxiang_Dai1",
      "~Jianxin_Li3",
      "~deqing_wang2"
    ],
    "pdf": "/pdf/c150f4ed4e4459a8689553f5058a3b3b63d45035.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on training optimization for LLMs, Batch-wise sample scheduling approach, Claims of minimal computational overhead, Potential improvement in training efficiency, Adaptive selection based on learning feedback, Implications for GPU utilization through efficient batch processing, Potential impact on training throughput",
      "Irrelevant Aspects": "No direct focus on inference optimization, No specific attention to inference latency optimization, Limited discussion of scalability in distributed settings",
      "Summary": "This paper introduces an adaptive batch-wise sample scheduling algorithm for Direct Preference Optimization (DPO) that dynamically selects training samples based on the model's evolving states. It's relevant to my interests in LLM training optimization, with some implications for GPU utilization and potential improvements in training throughput through more efficient sample selection. However, it doesn't directly address inference optimization, latency, or distributed scalability, which are also key aspects of my expertise, limiting its overall relevance to my broader research interests."
    }
  },
  {
    "id": "d4mZyZB5I9",
    "title": "Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving",
    "abstract": "Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. \nLLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features.\nHowever, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets.\nIn this work, we introduce the first training-free algorithm for online routing scenarios.\nOur algorithm leverages approximate nearest neighbor search to efficiently estimate the features of queries and performs a one-time optimization over a small set of initial queries to learn a set of routing weights that guide future routing.\nWe provide a theoretical guarantee that the algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\\times$ in performance, 1.85$\\times$ in cost efficiency, and nearly 4.25$\\times$ in throughput.\nOur code is available at https://github.com/fzwark/PORT.",
    "authors": [
      "~Fangzhou_Wu1",
      "~Sandeep_Silwal1"
    ],
    "pdf": "/pdf/463ab498b4d2ef7b5a0cafd7ac4276fc50fa3cb1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM routing for cost-efficient query management, which is directly relevant to inference optimization. It reports significant improvements in throughput (4.25×), cost efficiency (1.85×), and addresses high-volume serving challenges. The training-free approach is valuable for deployment optimization and potentially better GPU utilization across multiple models.",
      "Irrelevant Aspects": "The paper doesn't cover LLM training techniques, low-latency specifics, hardware-specific optimizations, or model architecture improvements. It focuses on routing between models rather than optimizing the models themselves.",
      "Summary": "This paper introduces a training-free online routing algorithm for efficiently serving multiple LLMs by directing queries to optimal models. It uses approximate nearest neighbor search to estimate query features and optimizes routing weights. The approach demonstrates significant improvements in throughput, performance, and cost efficiency across benchmark datasets, making it highly relevant for LLM inference optimization and scalable serving systems."
    }
  },
  {
    "id": "rAuRLePL2R",
    "title": "Týr-the-Pruner: Structural Pruning LLMs via Global Sparsity Distribution Optimization",
    "abstract": "Structural pruning enhances hardware-agnostic inference efficiency for large language models (LLMs) yet often fails to maintain comparable performance. Local pruning performs efficient layer-by-layer compression but ignores global topology. Although global pruning aims to identify an optimal sparse model, intuitive methods typically adopt a two-stage paradigm that first evaluates substructure saliency and then applies global pruning, which ignores inter-structure dependencies and fails to achieve end-to-end optimization. To address these limitations, we propose Týr-the-Pruner, an efficient end-to-end search-based global structural pruning framework. This framework constructs a supernet by repeatedly applying local pruning across a range of sparsity ratios to each layer in an LLM, with the core goal of determining the optimal sparsity distribution under a target overall sparsity ratio. Concretely, we introduce an effective local pruning and an expectation error accumulation approach to improve supernet construction. Furthermore, we employ an iterative prune-and-search strategy with coarse-to-fine sparsity granularity to ensure efficient search convergence. Experimental results show that Týr-the-Pruner achieves state-of-the-art structural pruning, retaining 97% of the dense model's performance while removing a challenging 50% of Llama-3.1-70B's parameters.",
    "authors": [
      "~Guanchen_Li1",
      "~Yixing_Xu2",
      "~Zeping_Li4",
      "~Ji_Liu11",
      "~Xuanwu_Yin1",
      "~Dong_Li13",
      "~Emad_Barsoum1"
    ],
    "pdf": "/pdf/b8d4af71f4e823cde143ab63fa780a2fddb141b0.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Directly addresses LLM inference optimization through structural pruning, claims significant parameter reduction (50%) while maintaining performance (97%), uses global sparsity optimization which impacts GPU utilization, demonstrates scalability by applying to large models like Llama-3.1-70B, presents hardware-agnostic approach for improved deployment efficiency",
      "Irrelevant Aspects": "Limited explicit focus on GPU utilization metrics, may not directly address latency and throughput measurements in abstract, focuses more on pruning methodology than specific hardware optimizations",
      "Summary": "This paper is highly relevant to my research interests as it presents a novel approach to structural pruning for LLMs that significantly reduces model size while maintaining performance. The Týr-the-Pruner framework optimizes global sparsity distribution, which directly impacts inference efficiency and resource utilization. The demonstrated 50% parameter reduction on Llama-3.1-70B while retaining 97% performance could substantially improve GPU utilization, throughput, and reduce latency in production environments."
    }
  },
  {
    "id": "OH7U836jKk",
    "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
    "abstract": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency–memory–accuracy trade-offs under a unified implementation.\nOn Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.",
    "authors": [
      "~Gunho_Park1",
      "~Jeongin_Bae1",
      "~Byeongwook_Kim1",
      "~Baeseong_park1",
      "~Jiwon_Ryu2",
      "~Hoseung_Kim2",
      "~Se_Jung_Kwon1",
      "~Dongsoo_Lee1"
    ],
    "pdf": "/pdf/fcb1fcc57d0a70fa97286b5dbcc27c0c5fbf4fcd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on optimizing GEMM operations in quantized LLMs, directly addressing GPU utilization and inference optimization. It proposes a novel kernel that improves memory subsystem utilization and reduces latency by eliminating per-element lookups. The work achieves substantial speedups (1.83x for 8B and 8.93x for 70B models) at comparable accuracy, which demonstrates significant improvements in computational efficiency. The approach specifically targets low-bit quantization (2-bit), addressing the memory-bound nature of LLM inference and exploring latency-memory-accuracy trade-offs.",
      "Irrelevant Aspects": "The paper appears to focus solely on inference optimization rather than training optimization. It doesn't explicitly address scalability across multiple GPUs or distributed systems. The abstract doesn't provide specific throughput metrics beyond the speedup comparisons.",
      "Summary": "CodeGEMM presents a codebook-centric approach to optimize GEMM operations in quantized LLMs by replacing dequantization with precomputed inner products stored in a Psumbook. This method eliminates repeated centroid fetching and weight reconstruction, reducing cache pressure and on-chip footprint. The approach enables direct gathering of partial sums using code indices, improving both latency and memory utilization. Applied to Llama-3 models, CodeGEMM achieves significant speedups (1.83x for 8B, 8.93x for 70B) in 2-bit configurations while maintaining accuracy, representing a substantial advancement in efficient LLM inference."
    }
  },
  {
    "id": "8q2kReYRDn",
    "title": "TF-MAS: Training-free Mamba2 Architecture Search",
    "abstract": "The Mamba-type neural networks have gained significant popularity recently. To effectively and efficiently establish model architectures of Mamba, it is natural to introduce Neural Architecture Search (NAS) methods into Mamba. However, existing NAS methods tailored for Mamba are training-based, leading to substantial time and computational resource expenditure. To address this issue, and considering that Mamba2 is an improved version of the original Mamba, we propose a training-free NAS method specifically designed for Mamba2. Based on rank collapse in stacked State Space Duality (SSD) blocks, we design a proxy that only requires the computation of the transformation matrix and its gradient between two tensors within the network. Additionally, we develop a corresponding search space and introduce a novel approach for determining adjustable hyperparameter ranges. Experimental results show that our method outperforms all existing training-free NAS approaches in terms of both ranking correlation and the performance of search results for Mamba2 architecture. To the best of our knowledge, this is the first training-free NAS method designed for Mamba-type architectures. Our codes are available at https://github.com/fanyi-plus/tf-nas.",
    "authors": [
      "~Yi_Fan3",
      "~Yu-Bin_Yang3"
    ],
    "pdf": "/pdf/2fbe01aafe994ac7bd569b82f2101676d6717042.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Focuses on Mamba/Mamba2 architectures which are important in large language models",
        "Proposes training-free NAS method which reduces computational resources",
        "Efficient architecture search can lead to better inference performance",
        "Architecture optimization directly impacts system performance",
        "Reduces computational requirements for NAS process"
      ],
      "Irrelevant Aspects": [
        "Limited focus on implementation details on specific hardware",
        "No explicit discussion of GPU utilization or throughput measurements",
        "More focused on NAS methodology than training/inference optimization",
        "Doesn't address scalability in distributed systems context",
        "Lacks detailed performance metrics relevant to optimization goals"
      ],
      "Summary": "This paper presents a training-free neural architecture search method specifically designed for Mamba2, an improved version of Mamba neural networks. The approach uses rank collapse in stacked State Space Duality blocks to create a proxy that only requires computation of transformation matrices and gradients, eliminating the need for expensive training during architecture search. While this work is relevant to architecture optimization for efficient language models, it focuses more on the NAS methodology than on specific implementation details related to GPU utilization, scalability, throughput, or latency that would be most relevant to optimization systems."
    }
  },
  {
    "id": "8s0qknrCVK",
    "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards",
    "abstract": "Fine-tuning foundation models has emerged as a powerful approach for generating objects with specific desired properties. Reinforcement learning (RL) provides an effective framework for this purpose, enabling models to generate outputs that maximize a given reward function. However, in many applications such as text generation and drug discovery, it can be suboptimal to optimize using a single reward signal, as multiple evaluation criteria are often necessary. This paper proposes a novel reinforcement learning-based method for fine-tuning foundation models using multiple reward signals. By employing an iterative fine-tuning strategy across these rewards, our approach generalizes state-of-the-art RL-based methods. We further provide a theoretical analysis that offers insights into the performance of multi-reward RL fine-tuning. Experimental results across diverse domains including text, biological sequence, and small molecule generation, demonstrate the effectiveness of the proposed algorithm compared to state-of-the-art baselines.",
    "authors": [
      "~Pouya_M._Ghari1",
      "~simone_sciabola1",
      "~Ye_Wang11"
    ],
    "pdf": "/pdf/f8dd9399122d37b232c077a459581d934b377a2d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Foundation model fine-tuning, reinforcement learning approach for model optimization, multi-reward optimization framework, experimental validation on text generation",
      "Irrelevant Aspects": "Applications to biological sequences and small molecule generation, no explicit focus on GPU utilization, no clear mention of throughput or latency metrics, theoretical analysis rather than implementation optimizations",
      "Summary": "The paper presents a novel RL-based method for fine-tuning foundation models using multiple reward signals through an iterative strategy. While it addresses important aspects of model optimization, it doesn't appear to focus on the system-level performance metrics like GPU utilization, throughput, and latency that are central to my research interests."
    }
  },
  {
    "id": "5OnejG2SQH",
    "title": "Masked Gated Linear Unit",
    "abstract": "Gated Linear Units (GLUs) have become essential components in the feed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward layers without gating, due to the use of separate weight matrices for the gate and value streams.\nTo address this bottleneck, we introduce Masked Gated Linear Units (MGLUs),  a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include:\n(1) the Mixture of Element-wise Gating (MoEG) architecture that learns multiple binary masks, each determining gate or value assignments at the element level on a single shared weight matrix resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly kernel that yields up to a 19.7$\\times$ inference-time speed-up over a na\\\"ive PyTorch MGLU and is 47\\% more memory-efficient and 34\\% faster than standard GLUs despite added architectural complexity on an RTX5090 GPU.\nIn LLM experiments, the Swish-activated variant SwiMGLU preserves its memory advantages while matching—or even surpassing—the downstream accuracy of the SwiGLU baseline.",
    "authors": [
      "~Yukito_Tajima1",
      "~Nakamasa_Inoue1",
      "~Yusuke_Sekikawa1",
      "~Ikuro_Sato1",
      "~Rio_Yokota1"
    ],
    "pdf": "/pdf/22794ba8531d3c67b3f070a88fb885f40e5a0f6c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization with a focus on reducing memory requirements and improving GPU utilization. It introduces FlashMGLU, an efficient kernel implementation that achieves 19.7× inference speedup over naive implementations and is 47% more memory-efficient than standard GLUs. The work maintains accuracy while improving hardware performance, which is central to my research on optimizing LLM systems. The evaluation on RTX5090 GPU and focus on element-level operations with a single shared weight matrix directly relates to GPU utilization and throughput optimization.",
      "Irrelevant Aspects": "The paper appears to focus primarily on inference optimization rather than training optimization, which is only half of my research interest. It doesn't extensively address distributed training or multi-GPU scaling, which are important aspects of my work. The evaluation seems limited to single GPU scenarios rather than large-scale deployments across multiple GPUs or nodes.",
      "Summary": "This paper introduces Masked Gated Linear Units (MGLUs) as a novel optimization for GLU components in LLMs, addressing memory bottlenecks through a Mixture of Element-wise Gating (MoEG) architecture that uses binary masks on a single shared weight matrix. The FlashMGLU kernel implementation achieves significant performance improvements (19.7× speedup, 47% memory reduction) while maintaining or improving model accuracy. This work directly contributes to my research focus on LLM inference optimization and GPU utilization, though it emphasizes single-GPU scenarios rather than distributed training or multi-GPU scalability."
    }
  },
  {
    "id": "gH4BRa4ZP3",
    "title": "Scaling Embedding Layers in Language Models",
    "abstract": "We propose SCONE (**S**calable, **C**ontextualized, **O**ffloaded, **N**-gram **E**mbedding), a new method for extending input embedding layers to enhance language model performance. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. After training, embeddings are precomputed and stored in off-accelerator memory; during inference, querying them has minimal impact on latency due to the low complexity of embedding lookups. SCONE enables two new scaling strategies: increasing the number of $n$-gram embeddings and scaling the model used to learn them, both while maintaining fixed accelerator usage  during inference (in terms of FLOPS and memory). We show that scaling both aspects enables a model with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline across diverse corpora, while using only about half the FLOPS and accelerator memory during inference.",
    "authors": [
      "~Da_Yu1",
      "~Edith_Cohen1",
      "~Badih_Ghazi1",
      "~Yangsibo_Huang2",
      "~Pritish_Kamath2",
      "~Ravi_Kumar1",
      "~Daogao_Liu1",
      "~Chiyuan_Zhang1"
    ],
    "pdf": "/pdf/da60d0485fcb80490aa64b9fa7460c364fb8d084.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on optimizing embedding layers for better performance without increasing inference costs; uses off-accelerator memory to store embeddings; aims to reduce FLOPS and accelerator memory during inference; maintains low latency; enables scaling while keeping accelerator usage fixed; demonstrates performance improvements with smaller models (1B outperforming 1.9B baseline); addresses both training and inference optimization",
      "Irrelevant Aspects": "Primarily focuses on embedding layers rather than comprehensive model optimization; specific technique (SCONE) rather than general optimization framework",
      "Summary": "This paper presents SCONE, a method for scaling embedding layers in language models by adding n-gram embeddings stored off-accelerator. It enables improved performance with reduced computational resources during inference, maintaining low latency while allowing models to scale capabilities without increasing accelerator memory or FLOPS usage. This approach directly addresses key concerns in LLM optimization including resource utilization, inference speed, and performance efficiency."
    }
  },
  {
    "id": "VwPt1WDQNB",
    "title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training",
    "abstract": "Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, on-policy algorithms used for post-training are not naturally robust to a diversified content of experience replay buffers, which asynchronous off-policy actors can efficiently populate in parallel to training. We propose efficiently learning on such off-policy data via Trajectory Balance with Asynchrony (TBA), an approach to asynchronous RL for LLMs that leverages the principled off-policy TB objective. On math, preference-tuning, and automated red-teaming tasks, we post-train models ranging from Pythia 410M to Qwen 2.5 7B, finding TBA offers speed and performance boosts over strong baselines like Online DPO and Dr. GRPO. Beyond TBA's performance benefits (high accuracy even as asynchrony grows) and speedups ($4\\times$ or more), we show its reward- and recency-prioritizing sampling enable further gains as data generation is scaled. Our code is available at https://github.com/bbartoldson/TBA.",
    "authors": [
      "~Brian_R._Bartoldson1",
      "~Siddarth_Venkatraman1",
      "~James_Diffenderfer1",
      "~Moksh_Jain1",
      "~Tal_Ben-Nun1",
      "~Seanie_Lee1",
      "~Minsu_Kim2",
      "~Johan_Obando-Ceron1",
      "~Yoshua_Bengio1",
      "~Bhavya_Kailkhura1"
    ],
    "pdf": "/pdf/13f871efbbbe385c376ef8105b8c5e2aaeddb542.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM post-training through asynchronous reinforcement learning, directly addressing training efficiency, GPU utilization, and throughput improvements. TBA demonstrates significant speedups (4x or more) while maintaining performance across various model sizes (410M to 7B), showing scalability. The method efficiently handles parallel compute resources by leveraging off-policy data and compares against current strong baselines.",
      "Irrelevant Aspects": "The abstract doesn't specifically address inference optimization or latency improvements during inference. The specific tasks mentioned (math, preference-tuning, automated red-teaming) are application domains rather than core optimization techniques.",
      "Summary": "Trajectory Balance with Asynchrony (TBA) is an approach to asynchronous RL for LLM post-training that leverages off-policy TB objective to efficiently learn from data generated in parallel. The method achieves significant speedups while maintaining performance across various model sizes and tasks, demonstrating robustness as asynchrony increases and potential for further scaling through specialized sampling strategies."
    }
  },
  {
    "id": "HKfZwLjSwQ",
    "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
    "abstract": "The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.",
    "authors": [
      "~Gregory_Dexter1",
      "~Shao_Tang1",
      "~Ata_Fatahibaarzi1",
      "~Qingquan_Song2",
      "~Tejas_Dharamsi1",
      "~Aman_Gupta1"
    ],
    "pdf": "/pdf/326025d0c9cbf126be1b4557c729aa696bf67856.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on query scheduling with prefix reuse techniques to reduce computational overhead. It targets latency constraints (TTFT and TPOT), which aligns perfectly with the goal of lower latency. The RadixAttention mechanism and k-LPM algorithm for better resource utilization can improve GPU utilization and throughput. The work addresses scalability in online serving settings, which matches the research focus on scalable LLM systems.",
      "Irrelevant Aspects": "The paper does not address training optimization, which is one of the specified interests. It focuses more on theoretical analysis and algorithmic approaches rather than detailed implementation aspects that might directly impact GPU utilization. There is no explicit discussion of system architecture or hardware-specific optimizations beyond the conceptual level.",
      "Summary": "This paper presents a novel approach to optimizing LLM inference performance through improved query scheduling with prefix reuse. It establishes the NP-hardness of the scheduling problem under TTFT constraints and proposes the k-LPM algorithm that balances prefix reuse and fairness. The work focuses on reducing latency, specifically TTFT and TPOT, which directly contributes to the research goals of lower latency, better resource utilization, and higher throughput in LLM serving systems."
    }
  },
  {
    "id": "b6H64u6TqI",
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
    "abstract": "Linear RNNs with gating recently demonstrated competitive performance compared to Transformers in language modeling. Although their linear compute scaling in sequence length offers theoretical runtime advantages over Transformers, realizing these benefits in practice requires optimized custom kernels, as Transformers rely on the highly efficient Flash Attention kernels (Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels are faster than Flash Attention, by parallelizing over chunks of the input sequence. However, since the chunk size of FLA is limited, many intermediate states must be materialized in GPU memory. This leads to low arithmetic intensity and causes high memory consumption and IO cost, especially for long-context pre-training. In this work, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm for linear RNNs, that enables arbitrary large chunk sizes and high arithmetic intensity by introducing an additional level of sequence parallelization within each chunk. First, we apply TFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we propose an mLSTM variant with sigmoid input gate and reduced computation for even faster kernel runtimes at equal language modeling performance. In our speed benchmarks, we show that our new mLSTM kernels based on TFLA outperform highly optimized Flash Attention, Linear Attention and Mamba kernels, setting a new state of the art for efficient long-context sequence modeling primitives. Our code is available at: https://github.com/NX-AI/mlstm_kernels",
    "authors": [
      "~Maximilian_Beck1",
      "~Korbinian_Pöppel1",
      "~Phillip_Lippe1",
      "~Sepp_Hochreiter1"
    ],
    "pdf": "/pdf/2aade266e5cf669740d1a6576caff6b3d7c10d8c.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper introduces Tiled Flash Linear Attention (TFLA), a kernel optimization directly addressing GPU utilization efficiency for sequence models. It compares against Flash Attention and claims performance improvements over Flash Attention, Linear Attention, and Mamba kernels. The work focuses on reducing memory consumption, improving arithmetic intensity, and lowering IO costs - all critical factors for GPU optimization. It addresses both training (pre-training) and inference scenarios with empirical speed benchmarks showing throughput and latency improvements.",
      "Irrelevant Aspects": "The paper primarily focuses on linear RNNs and xLSTM architectures rather than standard Transformer models. The architectural modifications to mLSTM (sigmoid input gate) are model-specific rather than general systems optimizations.",
      "Summary": "This paper presents Tiled Flash Linear Attention (TFLA), a kernel algorithm for linear RNNs that enables larger chunk sizes and higher arithmetic intensity than previous approaches. The authors demonstrate that their optimized mLSTM kernels outperform existing attention kernels, potentially offering significant improvements for long-context modeling scenarios. The work addresses key systems-level concerns including memory efficiency, compute optimization, and GPU utilization, making it highly relevant to research on ML systems optimization."
    }
  },
  {
    "id": "Zd6VyjmN1S",
    "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
    "abstract": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2$\\times$ and achieving 3.2–4.5$\\times$ higher throughput while meeting service-level objectives (SLOs).",
    "authors": [
      "~Zedong_Liu4",
      "~Shenggan_Cheng1",
      "~Guangming_Tan1",
      "~Yang_You1",
      "~Dingwen_Tao1"
    ],
    "pdf": "/pdf/7250c50e4dd72db4731ced5dc34ecbc4256eaf90.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper addresses multimodal LLM inference optimization, which directly aligns with my expertise. It focuses on reducing TTFT (latency), increasing throughput (3.2-4.5× higher), and improving resource utilization. The Elastic Multimodal Parallelism (EMP) approach enables dynamic resource allocation, elastic partition scheduling, and adaptive scaling, which are key optimization techniques for GPU utilization. The system also implements prefix caching, a valuable inference optimization strategy. The paper demonstrates significant improvements in both latency and throughput metrics, which are central to my research interests.",
      "Irrelevant Aspects": "The paper focuses exclusively on multimodal models (handling images, videos, and audio) rather than text-only LLMs, which is a specialized subset of my broader expertise. It doesn't address training optimization, only serving/inference. There's no explicit mention of specific GPU kernel optimizations or inter-node communication strategies, which are important aspects of my research. The work appears to be primarily a serving system architecture rather than fundamental algorithmic improvements to LLM inference.",
      "Summary": "This paper introduces ElasticMM, a serving system for multimodal LLMs that implements Elastic Multimodal Parallelism to optimize inference efficiency. It reduces TTFT by up to 4.2× and achieves 3.2-4.5× higher throughput through techniques including modality-aware load balancing, dynamic resource allocation, elastic partition scheduling, and prefix caching. The work is highly relevant to my expertise in LLM inference optimization, particularly regarding GPU utilization, scalability, throughput, and latency improvements."
    }
  },
  {
    "id": "0BVrpXMr5Y",
    "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
    "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens uniformly, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs with different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model’s attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.",
    "authors": [
      "~Yi_Zhao20",
      "~Yajuan_Peng1",
      "~Nguyen_Cam-Tu2",
      "~Zuchao_Li1",
      "~Wang_Xiaoliang1",
      "~hai_zhao1",
      "~Xiaoming_Fu1"
    ],
    "pdf": "/pdf/a9bf67e94ca9d4f29d0cf15f37fae223436e1ed1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through KV cache compression, which is a critical component for improving throughput and resource utilization. It presents a novel approach that claims 1.75-2.56 times higher throughput than baseline methods, directly targeting my interest in optimizing inference performance. The method focuses on efficient handling of long-context scenarios in resource-constrained environments, which aligns with my research on GPU utilization and scalability. The compensation mechanisms using attention similarity between different scale models represent an interesting approach to maintaining performance while reducing resource requirements.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization, which is another core part of my research interest. There is limited explicit discussion of GPU utilization optimization specific metrics or distributed system scalability. The small model assistance approach might not directly address all aspects of my research on LLM optimization, particularly regarding end-to-end latency improvements. The specialized nature of KV cache optimization means it doesn't provide a comprehensive view of LLM optimization strategies.",
      "Summary": "SmallKV presents a novel approach for KV cache compression in LLM inference, using a small model to assist a larger model in maintaining attention quality while reducing memory usage. The method addresses two key problems in existing token eviction strategies: the saliency shift problem and marginal information over-compression problem. By leveraging the similarity of attention matrices between different scale LLMs, SmallKV achieves significant throughput improvements (1.75-2.56x) while maintaining model performance across various benchmarks. This work is highly relevant to inference optimization for LLMs, particularly for long-context scenarios in resource-constrained environments, though it does not address training optimization or provide a holistic view of LLM optimization strategies."
    }
  },
  {
    "id": "kd6hcHUl9C",
    "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM Inference",
    "abstract": "Private large language model (LLM) inference based on secure multi-party computation (MPC) achieves formal data privacy protection but suffers from significant latency overhead, especially for long input sequences. While key-value (KV) cache eviction and sparse attention algorithms have been proposed for efficient LLM inference in plaintext, they are not designed for MPC and cannot benefit private LLM inference directly. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache, building on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant KV cache and a query-aware dynamic selection algorithm to activate only a small subset of KV cache for attention computation. MPCache further incorporates a series of optimizations for efficient dynamic KV cache selection, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index-sharing strategy. Extensive experiments demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different generation tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and communication reduction on different sequence lengths, respectively.",
    "authors": [
      "~Wenxuan_Zeng1",
      "~Ye_Dong1",
      "~Jinjin_Zhou1",
      "~Jin_Tan2",
      "~Lei_Wang30",
      "~Tao_Wei5",
      "~Runsheng_Wang3",
      "~Meng_Li1"
    ],
    "pdf": "/pdf/20929dac9b67fad4ccdf8d864b7ebe09c4e8ca00.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM inference, specifically addressing KV cache eviction which is crucial for improving inference efficiency. It targets latency reduction, a key metric in my research, and addresses challenges with long input sequences. The proposed optimizations for efficient dynamic KV cache selection and reported improvements in decoding latency (1.8~2.01x) and communication reduction (3.39~8.37x) directly align with my research goals of improving LLM throughput and reducing latency.",
      "Irrelevant Aspects": "The specific focus on secure multi-party computation (MPC) for privacy protection is somewhat peripheral to my core interests, which concentrate on general optimization techniques rather than privacy-preserving computation methods. While privacy aspects aren't my primary focus, they don't detract significantly from the overall relevance.",
      "Summary": "MPCache presents an MPC-friendly KV cache eviction framework for efficient private LLM inference. The paper combines static eviction and dynamic selection algorithms to optimize attention computation, with additional optimizations for efficient KV cache selection. While the privacy-focused approach through MPC adds a dimension outside my core research, the fundamental techniques for reducing computational overhead and improving inference efficiency are highly relevant to my work on LLM inference optimization."
    }
  },
  {
    "id": "tM4cHBD7kD",
    "title": "PseuZO: Pseudo-Zeroth-Order Algorithm for Training Deep Neural Networks",
    "abstract": "Zeroth-order Optimization (ZO) has received wide attention in machine learning, especially when computing full gradient is expensive or even impossible. Recently, ZO has emerged as an important paradigm for memory-efficient fine-tuning of large language models (LLMs), circumventing the memory overhead of backpropagation. However, existing ZO gradient estimators exhibit dimension-dependent variance scaling as $\\Theta(d)$, leading to dimension-dependent convergence rates without further assumptions on the objective function, which is prohibitive for large-scale LLM parameters. To address this problem, we present a Pseudo-Zeroth-Order (PseuZO) framework for optimizing composite objective functions, especially large-scale models: $ \\min_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{F}(\\mathbf{x})= \\bbE_{\\mathbf{z}} g\\circ h(\\mathbf{x};\\mathbf{z}) $, where $h$ represents complex, high-dimensional representations and $g$ is a task-specific loss. While existing zeroth-order methods estimate gradients with final loss functions, our PseuZO algorithm estimate the Jacobian matrix of $h(\\mathbf{x})$ with the model output $\\mathbf{o}= h(\\mathbf{x})$, and the gradient of the loss function on model output $\\mathbf{e} = \\nabla_{\\mathbf{o}} g(\\mathbf{o})$, and apply exponential moving average on Jacobian estimators to reduce the variance. Moreover, we use the sliding window technique to reduce memory costs. Our algorithm achieves an $O( \\max \\lbrace \\alpha_1 L\\epsilon^{-2}, \\alpha_1 L \\sigma_2^2\\epsilon^{-4} \\rbrace )$ convergence rate, where $\\alpha_1$ is the effective dimension of $\\mathcal{F}$.\n  Experimental results demonstrate that PseuZO outperforms MeZO and MeZO-SVRG in classification, multiple choice and generation tasks in both full-parameter and PEFT fine-tuning settings by boosting convergence in the early stages of training. For instance, under the same computation time, with respect to SST2 task, PesuZO gets 9.8\\% higher accuracy than MeZO (91.2\\% v.s. 82.4\\%). With the sliding window technique, our PseuZO achieves $70\\%\\sim80\\%$ memory reduction compared to FO-SGD for different model sizes as PseuZO only introduced a small dimension-independent memory overhead, which enables efficient scaling of the model size. The code is available at https://github.com/YangBigMn/PseuZO.\n$\\newcommand{\\bbE}{\\mathbb{E}}$",
    "authors": [
      "~Pengyun_Yue1",
      "~Xuanlin_Yang1",
      "~Mingqing_Xiao1",
      "~Zhouchen_Lin1"
    ],
    "pdf": "/pdf/43a9d24dd5077fb66c068a4fc5e540e2ab045de6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses memory efficiency for training large language models, which is crucial for GPU utilization and scalability. It proposes a method to achieve 70-80% memory reduction compared to standard approaches, enabling better utilization of GPU resources. The focus on LLM optimization aligns perfectly with the research interest, and the reported faster convergence rates would improve training throughput. The approach allows for efficient scaling of model sizes, which directly addresses scalability concerns.",
      "Irrelevant Aspects": "The paper focuses primarily on training optimization rather than inference optimization, which is another important aspect of the research interest. There is limited discussion of latency considerations for inference scenarios. The paper doesn't address specific GPU utilization techniques beyond memory efficiency, and there's no mention of hardware-specific optimizations that might affect overall system performance.",
      "Summary": "PseuZO presents a zeroth-order optimization algorithm for training large language models with significantly reduced memory requirements (70-80% less than standard methods). By estimating Jacobian matrices rather than full gradients, it achieves better convergence rates and enables efficient scaling of model sizes. While the paper offers valuable insights for training optimization and GPU memory utilization, it lacks coverage of inference optimization and latency aspects."
    }
  },
  {
    "id": "V13dSX1wAs",
    "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models",
    "abstract": "Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. \nHowever, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. \nTo address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. \nLCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. \nAdditionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. \nPermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. \nExtensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models.",
    "authors": [
      "~Lancheng_Zou1",
      "~Shuo_Yin2",
      "~Zehua_Pei2",
      "~Tsung-Yi_Ho2",
      "~Farzan_Farnia1",
      "~Bei_Yu2"
    ],
    "pdf": "/pdf/c4efdba2903a535d6f7ad1014f1e6e31076d7792.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on N:M sparse large language models, which is directly relevant to inference optimization for better GPU utilization and lower latency. It presents optimization techniques for popular LLM architectures (LLaMA series, Qwen, OPT), which are models commonly used in my research area. The learnable channel permutation approach maintains sparse structure that enables efficient inference, which is a key concern in LLM optimization. The paper also mentions computational efficiency through a block-wise channel permutation strategy that reduces computational complexity.",
      "Irrelevant Aspects": "The paper appears to focus more on the mathematical technique (Sinkhorn normalization) rather than practical implementation details that directly impact GPU utilization. It seems to be primarily a post-training pruning framework rather than a comprehensive optimization strategy covering the entire training and inference pipeline. There's no explicit mention of throughput measurements, latency benchmarks, or GPU utilization metrics in the abstract. The focus appears to be on accuracy improvement of sparse models rather than comprehensive evaluation of inference efficiency benefits.",
      "Summary": "PermLLM introduces a learnable channel permutation technique for N:M sparse LLMs, using Sinkhorn normalization to create differentiable permutation matrices. The method aims to improve the accuracy of sparse models while maintaining their computational advantages. While relevant to my expertise in LLM optimization, it focuses primarily on a specific mathematical technique rather than broader optimization strategies or empirical performance metrics that would be most directly applicable to my research interests."
    }
  },
  {
    "id": "1b7whO4SfY",
    "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
    "abstract": "Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention.\nYet, existing literature rarely examines the specific effects of gating.\nIn this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset.\nOur central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance.\nThis modification also enhances training stability, tolerates larger learning rates, and improves scaling properties.\nBy comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. \nWe also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research.\nFurthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).",
    "authors": [
      "~Zihan_Qiu1",
      "~Zekun_Wang1",
      "~Bo_Zheng4",
      "~Zeyu_Huang1",
      "~Kaiyue_Wen1",
      "~Songlin_Yang1",
      "~Rui_Men2",
      "~Le_Yu2",
      "~Fei_Huang3",
      "~Suozhi_Huang1",
      "~Dayiheng_Liu1",
      "~Jingren_Zhou1",
      "~Junyang_Lin1"
    ],
    "pdf": "/pdf/363b779a6787084e8392405b524f10679d6da8a3.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Introduction of sparse attention mechanisms through gating, which could improve computational efficiency; mitigation of 'attention sink' problems affecting performance; experiments with large-scale models (15B MoE and 1.7B dense models); improved training stability and better scaling properties; enhanced long-context extrapolation potential; technique already implemented in Qwen3-Next models indicating practical applicability; ability to tolerate larger learning rates which may improve training efficiency",
      "Irrelevant Aspects": "Primary focus on model architecture rather than system-level optimization; lack of direct discussion of GPU utilization metrics; absence of explicit throughput or latency measurements; limited analysis of computational savings from the sparsity introduced",
      "Summary": "This paper proposes a simple but effective modification to attention mechanisms in LLMs by applying head-specific sigmoid gates after Scaled Dot-Product Attention. Through extensive experiments on large models, the authors demonstrate improved performance, training stability, and scaling properties. The gated attention introduces both non-linearity and query-dependent sparsity, addressing issues like massive activation and attention sink while enhancing long-context capabilities. While focused on architecture improvements rather than system optimization, the approach has potential efficiency benefits for both training and inference, evidenced by its adoption in Qwen3-Next models."
    }
  },
  {
    "id": "xtKNbPTnMA",
    "title": "Sampled Estimators For Softmax Must Be Biased",
    "abstract": "Models requiring probabilistic outputs are ubiquitous and used in fields such as natural language processing, contrastive learning, and recommendation systems. The standard method of designing such a model is to output unconstrained logits, which are normalized into probabilities with the softmax function. The normalization involves computing a summation across all classes, which becomes prohibitively expensive for problems with a large number of classes. An important strategy to reduce the cost is to sum over a sampled subset of classes in the softmax function, known as the sampled softmax. It was known that the sampled softmax is biased; the expectation taken over the sampled classes is not equal to the softmax function. Many works focused on reducing the bias by using a better way of sampling the subset. However, while sampled softmax is biased, it is unclear whether an unbiased function different from sampled softmax exists. In this paper, we show that all functions that only access a sampled subset of classes must be biased.\nWith this result, we prevent efforts in finding unbiased loss functions and validate that past efforts devoted to reducing bias are the best we can do.",
    "authors": [
      "~Li-Chung_Lin1",
      "~Yaxu_Liu1",
      "~Chih-Jen_Lin1"
    ],
    "pdf": "/pdf/be9b1a0a39f76c94bb849fe701edfe3585705c14.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "The softmax function is a fundamental component of language models, particularly in the final layer for generating token probabilities",
        "Large language models have very large vocabularies (often 50,000+ tokens), making full softmax computation extremely expensive",
        "Sampled softmax methods are relevant for training efficiency in LLMs, as they can significantly reduce computational requirements",
        "The paper addresses computational optimization strategies for large-scale models, which directly relates to my focus on GPU utilization and throughput",
        "Efficient softmax computation is critical for both training and inference performance",
        "The paper provides theoretical limits on what can be achieved with sampling-based approaches, which is valuable for making informed optimization decisions"
      ],
      "Irrelevant Aspects": [
        "The paper appears to be purely theoretical without implementation details or performance benchmarks",
        "There's no discussion of hardware-specific optimizations or GPU utilization strategies",
        "The paper doesn't address inference-specific optimizations or latency reduction techniques",
        "There's no mention of parallelization strategies or distributed computing aspects",
        "The paper focuses on the theoretical impossibility of unbiased estimators rather than practical optimization techniques"
      ],
      "Summary": "This paper provides theoretical insights into the limitations of sampled softmax estimators, proving that unbiased estimators are impossible when only accessing a subset of classes. While this has implications for LLM optimization through reduced computational requirements for large vocabularies, the paper is primarily theoretical without practical implementation details or performance measurements that would directly inform GPU utilization, scalability, throughput, or latency optimization strategies."
    }
  },
  {
    "id": "ECTxVRFhUa",
    "title": "Tensor Product Attention Is All You Need",
    "abstract": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.",
    "authors": [
      "~Yifan_Zhang16",
      "~Yifeng_Liu3",
      "~Huizhuo_Yuan1",
      "~Zhen_Qin6",
      "~Yang_Yuan4",
      "~Quanquan_Gu1",
      "~Andrew_C_Yao1"
    ],
    "pdf": "/pdf/59c444020ea921a03150952b9c020c13acda6605.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization by proposing Tensor Product Attention (TPA) to substantially shrink KV cache size during inference. It focuses on memory efficiency and computational efficiency at the decoding stage, enabling processing of longer sequences under fixed resource constraints. The work compares against various attention mechanisms (MHA, MQA, GQA, MLA) which are relevant to GPU utilization and scalability. The T6 architecture claims improved throughput by reducing memory overhead.",
      "Irrelevant Aspects": "The paper primarily focuses on inference optimization with limited discussion of training optimization techniques. There's no explicit analysis of distributed training or scaling across multiple GPUs. While it mentions computational efficiency, it doesn't provide detailed latency measurements or thorough analysis of GPU utilization patterns.",
      "Summary": "This paper introduces Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to compactly represent queries, keys, and values, significantly reducing KV cache memory overhead during inference. The proposed T6 architecture claims to match or surpass existing attention mechanisms while improving memory efficiency, enabling longer sequence processing under fixed resource constraints. The work is highly relevant to inference optimization and partially relevant to GPU utilization and scalability, but has limited coverage of training optimization."
    }
  },
  {
    "id": "PBvlBI6c30",
    "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training",
    "abstract": "The parameter size of modern large language models (LLMs) can be scaled up to the trillion-level via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid excessive increase of the computational costs. To further improve training efficiency, pipelining computation and communication has become a promising solution for distributed MoE training. However, existing work primarily focuses on scheduling tasks within the MoE layer, such as expert computing and all-to-all (A2A) communication, while neglecting other key operations including multi-head attention (MHA) computing, gating, and all-reduce communication. In this paper, we propose FlowMoE, a scalable framework for scheduling multi-type task pipelines. First, FlowMoE constructs a unified pipeline to consistently scheduling MHA computing, gating, expert computing, and A2A communication. Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism to overlap the all-reduce communication with all computing tasks. We implement FlowMoE as an adaptive and generic framework atop PyTorch. Extensive experiments with 675 typical MoE layers and four real-world MoE models across two GPU clusters demonstrate that our proposed FlowMoE framework outperforms state-of-the-art MoE training frameworks, reducing training time by14%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%. FlowMoE’s code is anonymously available at https://anonymous.4open.science/r/FlowMoE.",
    "authors": [
      "~Yunqi_Gao3",
      "~Bing_Hu5",
      "~Mahdi_Boloursaz_Mashhadi1",
      "~A-Long_Jin1",
      "~Yanfeng_Zhang1",
      "~Pei_Xiao4",
      "~Rahim_Tafazolli1",
      "~Merouane_Abdelkader_DEBBAH1"
    ],
    "pdf": "/pdf/86a6cebe3baa8302580dfb92f922f61b378fb4fb.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on optimizing training efficiency for MoE-based LLMs through pipeline scheduling. It addresses key techniques for improving GPU utilization and throughput by overlapping computation and communication. FlowMoE schedules various operations including multi-head attention computing, gating, expert computing, and all-to-all communication. It reports significant improvements in training time (14%-57%), energy consumption (10%-39%), and memory usage (7%-32%). The framework is implemented atop PyTorch, making it practical for real-world applications.",
      "Irrelevant Aspects": "The paper is specifically focused on Mixture-of-Experts models, which is a subset of LLM architectures rather than all LLM training optimization. It doesn't address inference optimization, which is another key area of my research interest. The framework's implementation atop PyTorch might limit its generalizability to other ML frameworks.",
      "Summary": "FlowMoE presents a pipeline scheduling framework for distributed Mixture-of-Experts training that addresses both computation and communication optimization. It constructs a unified pipeline to consistently schedule multiple task types and introduces a tensor chunk-based priority mechanism to overlap communication with computation. The framework demonstrates significant improvements in training efficiency, making it highly relevant to research on LLM training optimization and GPU utilization, though its focus is limited to training rather than inference optimization."
    }
  },
  {
    "id": "wNMK5o0Vfg",
    "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
    "abstract": "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. \nHowever, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking issue arises from the inherent limitations of conventional outcome-reward reinforcement learning, which systematically overlooks the regulation of intermediate reasoning processes. This paper introduces Serial-Group Decaying-Reward Policy Optimization (S-GRPO), a novel reinforcement learning paradigm that enables models to implicitly evaluate the sufficiency of intermediate reasoning steps, thereby facilitating early exit in CoT generation.\nUnlike GRPO, which samples multiple possible reasoning paths in parallel (parallel group), S-GRPO only samples one reasoning path and serially selects multiple temporal positions from the path to exit thinking and directly generate answers (serial group). For correct answers within a serial group, rewards gradually decrease based on the exit positions along the reasoning path from front to back. This design encourages the model to produce more accurate and concise thoughts, while also incentivizing early thinking termination when appropriate. Empirical evaluations demonstrate that S-GRPO is compatible with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill. Across diverse benchmarks such as GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond, S-GRPO achieves a substantial reduction in sequence length (40.4%～61.1%) while simultaneously improving accuracy (absolute 0.72%～3.92%).",
    "authors": [
      "~Mz_Dai2",
      "~Chenxu_Yang1",
      "~Qingyi_Si1"
    ],
    "pdf": "/pdf/569e607e6ef3208e833f0f3b98e88117311fc562.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Directly addresses inference optimization through sequence length reduction (40.4-61.1%), which translates to improved throughput and reduced latency. The early exit mechanism enables more efficient resource utilization during inference. Compatible with existing models like Qwen3 and Deepseek-distill, showing practical scalability.",
      "Irrelevant Aspects": "Focuses primarily on reasoning quality improvement rather than system-level optimization metrics. Lacks explicit discussion of GPU utilization, memory management, or distributed computing considerations. The approach is from a machine learning methodology perspective rather than systems architecture.",
      "Summary": "S-GRPO introduces a reinforcement learning method that reduces chain-of-thought sequence length while maintaining or improving accuracy, directly contributing to inference efficiency. The serial group approach with decaying rewards enables early termination of reasoning paths, offering significant potential for reducing computational overhead, latency, and improving throughput in production systems."
    }
  },
  {
    "id": "OJMLzo8rEX",
    "title": "HubGT: Fast Graph Transformer with Decoupled Hierarchy Labeling",
    "abstract": "Graph Transformer (GT) has recently emerged as a promising neural network architecture for learning graph-structured data. However, its global attention mechanism with quadratic complexity concerning the graph scale prevents wider application to large graphs. Effectively representing graph information while ensuring learning efficiency remains challenging, as our analysis reveals that current GT designs targeting scalability still suffer from the computational bottleneck related to graph-scale operations. In this work, we tackle the GT scalability issue by proposing HubGT, a scalable Graph Transformer boosted by fully decoupled graph processing and simplified learning. HubGT represents the graph by a novel hierarchical scheme exploiting hub labels, which is shown to be more informative than plain adjacency by offering global connections while promoting locality, and is particularly suitable for handling complex graph patterns such as heterophily. We also design algorithms for efficiently constructing and querying the hub label hierarchy tailored for the GT attention training in scalable deployments. Notably, the precomputation and training processes of HubGT achieve complexities linear to the number of graph edges and nodes, respectively, while the training stage completely removes graph-related computations, leading to favorable mini-batch capability and GPU utilization. Extensive experiments demonstrate that HubGT is efficient in terms of computational enhancement and mini-batch capability over existing GT designs on large-scale benchmarks, while achieving top-tier effectiveness on both homophilous and heterophilous graphs.",
    "authors": [
      "~Ningyi_Liao1",
      "~Zihao_Yu2",
      "~Siqiang_Luo1",
      "~Gao_Cong1"
    ],
    "pdf": "/pdf/0b50e4270f26937bed85e3f41d1562416fdfbe9b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses transformer architecture optimization with focus on computational efficiency, GPU utilization, and scalability. It introduces techniques to reduce quadratic complexity to linear complexity, improves mini-batch processing capabilities, and eliminates graph-related computations during training - all of which are relevant to optimizing ML systems for better GPU utilization and throughput.",
      "Irrelevant Aspects": "The paper focuses on Graph Transformers rather than Language Transformers, which are the primary interest. The domain application is graphs rather than natural language processing. There's less emphasis on inference optimization compared to training optimization.",
      "Summary": "HubGT presents a scalable Graph Transformer architecture that reduces computational complexity through a hierarchical labeling scheme. While focused on graph processing rather than language models, its optimization techniques for reducing computational bottlenecks and improving GPU utilization are transferable to transformer optimization in general. The paper's contributions to linear complexity and improved mini-batch capability make it moderately relevant to ML systems optimization."
    }
  },
  {
    "id": "cCYUFaR6En",
    "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training",
    "abstract": "Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. As a next step, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.",
    "authors": [
      "~Kaichen_Zhang1",
      "~Yuzhong_Hong1",
      "~Junwei_Bao1",
      "~Hongfei_Jiang2",
      "~yang_song8",
      "~Hong_Dingqian1",
      "~Hui_Xiong1"
    ],
    "pdf": "/pdf/b507d480bd87c1ba0e05bf79ce938cd0b3a0b4e6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on post-training optimization for LLMs, addressing training instability issues that impact practical deployment. GVPO provides theoretical guarantees for KL-constrained reward maximization and supports flexible sampling distributions that avoid on-policy and importance sampling limitations. This could lead to better GPU utilization during the post-training phase and more efficient training processes overall.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss inference optimization or latency improvements, which are key aspects of my research focus. It also doesn't appear to address system-level optimizations for better throughput or distributed training considerations. There's no mention of hardware-specific optimizations that would improve scalability for deployment.",
      "Summary": "GVPO presents a novel approach to LLM post-training that addresses training instability by incorporating the analytical solution to KL-constrained reward maximization. While this is highly relevant to training optimization aspects of my work, the paper lacks focus on inference optimization, GPU utilization during deployment, and system-level scalability considerations that are equally important to my research interests."
    }
  },
  {
    "id": "kuzye4EPLR",
    "title": "FP4 All the Way: Fully Quantized Training of Large Language Models",
    "abstract": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way.",
    "authors": [
      "~Brian_Chmiel1",
      "~Maxim_Fishman1",
      "~Ron_Banner1",
      "~Daniel_Soudry1"
    ],
    "pdf": "/pdf/43b9e7285904d70ba2b9386b736d51555d621885.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Fully quantized training using 4-bit floating-point precision for LLMs directly addresses training optimization, which impacts GPU utilization and scalability. The paper investigates design choices for FP4 quantization including block sizes, scaling formats, and rounding methods, which are crucial for implementation. The demonstration of training a 7B parameter model across 256 accelerators shows scalability potential. The comparison with BF16 baseline provides insights into performance trade-offs. The quantization approach would naturally lead to higher throughput and lower memory requirements during training.",
      "Irrelevant Aspects": "The paper focuses on Intel Gaudi2 accelerators rather than GPUs, requiring translation of insights. Some theoretical quantization noise analysis may be overly academic for practical implementation. The reference implementation might be hardware-specific and not directly applicable to GPU environments.",
      "Summary": "This paper presents highly relevant research on fully quantized training of LLMs using 4-bit floating-point precision for all components (weights, activations, gradients). It identifies optimal design choices for FP4 training and demonstrates successful training of a 7B parameter model with performance comparable to BF16 baseline. The work directly impacts training efficiency, memory usage, and computational throughput, which are central concerns for LLM training optimization."
    }
  },
  {
    "id": "QlDyoo8qLY",
    "title": "Approximate Gradient Coding for Distributed Learning with Heterogeneous Stragglers",
    "abstract": "In this paper, we propose an optimally structured gradient coding scheme to mitigate the straggler problem in distributed learning. Conventional gradient coding methods often assume homogeneous straggler models or rely on excessive data replication, limiting performance in real-world heterogeneous systems. To address these limitations, we formulate an optimization problem minimizing residual error while ensuring unbiased gradient estimation by explicitly considering individual straggler probabilities. We derive closed-form solutions for optimal encoding and decoding coefficients via Lagrangian duality and convex optimization, and propose data allocation strategies that reduce both redundancy and computational load. We also analyze convergence behavior for $\\lambda$-strongly convex and $\\mu$-smooth loss functions. Numerical results show that our approach significantly reduces the impact of stragglers and accelerates convergence compared to existing methods.",
    "authors": [
      "~Heekang_Song1",
      "~Wan_Choi1"
    ],
    "pdf": "/pdf/0e543e1254d36f803615e0b40955221b352e299e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Distributed learning optimization, straggler mitigation techniques which impact training throughput, convergence rate improvement, redundancy reduction for better resource utilization, optimization formulation for distributed systems",
      "Irrelevant Aspects": "Not specifically focused on large language models, no explicit GPU-level optimization, lacks inference optimization components, primarily theoretical rather than implementation-focused, doesn't address memory constraints specific to LLMs",
      "Summary": "This paper addresses distributed learning optimization through gradient coding to mitigate stragglers, which is directly relevant to my interest in improving training efficiency and throughput. The approach considers heterogeneous systems and optimizes convergence behavior, which could benefit large language model training. However, the paper lacks specific focus on GPU utilization optimization and inference aspects that are central to my research. The theoretical optimization framework and straggler mitigation strategies provide valuable insights for distributed LLM training, but the paper doesn't address the unique challenges of massive-scale language models or GPU-specific optimizations."
    }
  },
  {
    "id": "cg2S1qqNSq",
    "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity",
    "abstract": "Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop Selective Head Attention with hardware-efficient, sparsity-aware GPU kernels, delivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2 \\& 3, Qwen, Mistral across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems.",
    "authors": [
      "~Susav_Shrestha1",
      "~Bradley_Settlemyer1",
      "~Nikoli_Dryden1",
      "~A._L._Narasimha_Reddy1"
    ],
    "pdf": "/pdf/43bf702e03ce0cb3a890cf8a35f3ded98d30b94d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization with focus on GPU utilization through sparsity-aware kernels, achieving up to 2.2× speedup. It tackles scalability with large batch sizes, maintains high throughput and low latency, and works across multiple models (OPT, LLaMA-2 & 3, Qwen, Mistral). The approach requires minimal changes to existing systems, making it practical for deployment. The paper specifically addresses the challenge of maintaining efficiency when scaling batch sizes, which is a key concern for production systems.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization. It doesn't appear to cover quantization techniques, memory optimization strategies beyond sparsity, or distributed/multi-GPU inference approaches. There's limited discussion of hardware-specific optimizations beyond general GPU kernels.",
      "Summary": "Polar Sparsity introduces a novel approach to contextual sparsity that scales effectively to large batch sizes by shifting sparsity focus from MLP to Attention layers. The paper develops Selective Head Attention with hardware-efficient GPU kernels to achieve significant inference speedups (up to 2.2×) across multiple models without accuracy loss. This work is highly relevant as it directly addresses key challenges in LLM deployment: maintaining efficiency, throughput and latency at scale while optimizing GPU utilization."
    }
  },
  {
    "id": "jTUIo4Kl6u",
    "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts",
    "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters. However, this means that MoEs only receive a sparse backward update, leading to training instability and suboptimal performance. We present a lightweight approximation method that gives the MoE router a dense gradient update while continuing to sparsely activate its parameters. Our method, which we refer to as Default MoE, substitutes missing expert activations with default outputs consisting of an exponential moving average of expert outputs previously seen over the course of training. This allows the router to receive signals from every expert for each token, leading to significant improvements in training performance. Our Default MoE outperforms standard TopK routing in a variety of settings without requiring significant computational overhead.",
    "authors": [
      "~Ashwinee_Panda1",
      "~Vatsal_Baherwani1",
      "~Zain_Sarwar1",
      "~Benjamin_Thérien1",
      "~Sambit_Sahu2",
      "~Tom_Goldstein1",
      "~Supriyo_Chakraborty1"
    ],
    "pdf": "/pdf/cd31201f8ec5523e5cff4ebf6b5b11000f8afa4b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for Mixture-of-Experts models, focusing on improving training stability and performance. It addresses scalability concerns by enhancing MoE pretraining which is already more scalable than dense Transformer pretraining. The method aims to optimize training efficiency without significant computational overhead, which relates to better GPU utilization and throughput during training.",
      "Irrelevant Aspects": "The paper primarily focuses on training optimization rather than inference optimization, which is a key aspect of my research interests. It doesn't explicitly address latency concerns during inference or detailed GPU utilization metrics. The proposed solution is limited to the backward propagation optimization without broader system-level optimizations.",
      "Summary": "This paper introduces 'Default MoE,' a method that improves training stability and performance for sparse Mixture-of-Experts models by providing dense gradient updates to the router while maintaining sparse activation. It addresses the training instability caused by sparse backward updates in MoE systems by substituting missing expert activations with default outputs based on exponential moving averages. The approach shows improved training performance without significant computational overhead, making it relevant for training optimization and scalability aspects of large language model development."
    }
  },
  {
    "id": "lF6SHARvmG",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
    "abstract": "Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to turn a pre-trained latent video diffusion model into\na real-time, interactive, streaming video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as control to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This allows us to design a more efficient architecture for one-step generation and to train the model in a student-forcing way to mitigate error accumulation. The adversarial approach also enables us to train the model for long-duration generation fully utilizing the KV cache. As a result, our 8B model achieves real-time, 24fps, nonstop, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames).",
    "authors": [
      "~Shanchuan_Lin1",
      "~Ceyuan_Yang2",
      "~Hao_He7",
      "~Jianwen_Jiang2",
      "~Yuxi_Ren1",
      "~Xin_Xia1",
      "~Yang_Zhao14",
      "~Xuefeng_Xiao1",
      "~Lu_Jiang1"
    ],
    "pdf": "/pdf/dd77f1b02f7344fe92d5c026cf82bd33d98e1c51.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization with real-time performance (24fps on a single H100), demonstrating excellent GPU utilization. The use of single neural function evaluation (1NFE) is a significant efficiency gain. The paper leverages KV cache optimization for long-duration generation, which is directly applicable to LLM inference. The post-training approach to optimize pre-trained models aligns with my interest in model optimization techniques without full retraining.",
      "Irrelevant Aspects": "The focus on video generation rather than text/language models makes some aspects less directly transferable. The adversarial training paradigm, while important for their approach, isn't a primary focus of my LLM research. The specific resolution requirements and streaming aspects of video generation present unique challenges not always present in LLM deployment.",
      "Summary": "This paper presents valuable insights for inference optimization, particularly around reducing computational steps (1NFE), efficient cache utilization, and achieving real-time performance on resource-constrained hardware. While the domain is video generation rather than language models, many of the optimization techniques are directly applicable to LLM deployment scenarios where latency and throughput are critical. The paper demonstrates how to transform a computationally intensive model into an efficient streaming system, which aligns well with my research interests."
    }
  },
  {
    "id": "dRjt4vlYVQ",
    "title": "Weaver: Shrinking the Generation-Verification Gap by Scaling Compute for Verification",
    "abstract": "Verifiers can improve language model (LM) capabilities by providing feedback or selecting the best response from a pool of generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean for formal proofs). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers. To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. First we find that weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in the verifiers. To reduce the dependency on labeled data, Weaver leverages weak supervision to estimate each verifier’s accuracy and combines their outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses several challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these challenges by using dataset statistics to normalize outputs and filter specific verifiers. We study the effectiveness of Weaver in repeated sampling settings, where a model generates multiple candidate responses at test time and a verifier is used to select the correct one. Our evaluations demonstrate that Weaver significantly improves the pass@1 performance across several reasoning and math tasks, achieving o3-mini level accuracy with Llama 3.3 70B Instruct (a much cheaper non-reasoning model) as the generator, and an ensemble of smaller judge and reward models as the verifiers (86.2% average). This gain mirrors the jump achieved between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training interventions. To make Weaver more efficient, we train a compact 400M cross-encoder using Weaver's combined output scores. This distilled model retains 98.7% of Weaver's full accuracy while reducing verification compute by up to 99.97%.",
    "authors": [
      "~Jon_Saad-Falcon1",
      "~E._Kelly_Buchanan1",
      "~Mayee_F_Chen1",
      "~Tzu-Heng_Huang1",
      "~Brendan_McLaughlin2",
      "~Tanvir_Bhathal1",
      "~Shang_Zhu1",
      "~Ben_Athiwaratkun1",
      "~Frederic_Sala1",
      "~Scott_Linderman1",
      "~Azalia_Mirhoseini3",
      "~Christopher_Re1"
    ],
    "pdf": "/pdf/e96545cd91e3d21ec915c2a775d66c7fe5f57eb6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses scaling compute for verification, which relates to GPU utilization and scalability. The distillation of a compact 400M cross-encoder that retains 98.7% of accuracy while reducing compute by up to 99.97% is highly relevant to resource optimization and efficiency. The framework demonstrates how to achieve high performance with smaller models through verification optimization, impacting the inference pipeline efficiency. The weak supervision approach to ensemble multiple verifiers without extensive labeled data provides a practical optimization strategy.",
      "Irrelevant Aspects": "The paper focuses primarily on verification methodology rather than core training optimization or detailed inference system design. It doesn't deeply address GPU utilization metrics, memory optimization, or distributed computing aspects. The evaluation emphasizes accuracy metrics (pass@1) rather than system performance metrics like throughput and latency. The weak supervision and ensemble methods are more about verification strategy than computational efficiency.",
      "Summary": "Weaver introduces a framework that combines multiple weak verifiers into a stronger one using weak supervision, reducing the generation-verification gap in language models. The approach achieves o3-mini level accuracy using Llama 3.3 70B as generator with smaller judge and reward models as verifiers. Notably, the paper includes a distillation component that creates a compact 400M cross-encoder maintaining 98.7% of accuracy while reducing verification compute by up to 99.97%, representing significant efficiency gains. While the focus is on verification methodology rather than system optimization, the paper provides relevant insights for resource-efficient inference optimization."
    }
  },
  {
    "id": "GMvRmyunw4",
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape",
    "abstract": "Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference.",
    "authors": [
      "~Ruichen_Chen1",
      "~Keith_G._Mills1",
      "~Liyao_Jiang1",
      "~Chao_Gao1",
      "~Di_Niu1"
    ],
    "pdf": "/pdf/106856da88decbaaf770621f71550ed936a5bfad.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization through sparse attention, GPU utilization improvement by processing only 3.1% of tokens, addressing quadratic complexity of attention mechanisms, throughput and latency optimization, attention mechanism efficiency for transformer architectures",
      "Irrelevant Aspects": "Focus on visual generation (T2V/T2I) rather than language models, specific to diffusion models rather than general transformer architectures, leverage of temporal redundancy specific to video generation, emphasis on visual quality preservation",
      "Summary": "Re-ttention proposes an ultra-sparse attention method for Diffusion Transformers in visual generation, reshaping attention scores based on prior softmax distributions to maintain quality while using only 3.1% of tokens during inference. Though focused on visual content rather than language models, the core attention optimization principles are highly relevant to inference efficiency and GPU utilization in transformer systems."
    }
  },
  {
    "id": "knPz7gtjPW",
    "title": "Superposition Yields Robust Neural Scaling",
    "abstract": "The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.",
    "authors": [
      "~Yizhou_Liu1",
      "~Ziming_Liu2",
      "~Jeff_Gore1"
    ],
    "pdf": "/pdf/58f6788f2c9231a4e1f19cf31d1e6accd0fbd43f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Investigates neural scaling laws fundamental to LLM scaling; analyzes representation superposition affecting model efficiency; examines relationship between model size and loss impacting training decisions; studies performance characteristics affecting GPU utilization and scalability; validates findings on open-sourced LLMs with practical implications; discusses Chinchilla scaling laws critical for training optimization.",
      "Irrelevant Aspects": "Focuses more on theoretical understanding rather than specific optimization techniques; doesn't directly address inference optimization; no mention of specific GPU utilization techniques or hardware optimizations; appears more focused on explaining scaling laws rather than proposing new optimization methods.",
      "Summary": "The paper investigates representation superposition as a key factor in neural scaling laws, demonstrating how it affects the relationship between model size and loss in LLMs. While primarily theoretical, its insights into when scaling laws improve or break down could inform training optimization decisions, though it doesn't directly propose specific techniques for improving GPU utilization or inference efficiency."
    }
  },
  {
    "id": "29FRqmVQK8",
    "title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty",
    "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty",
    "authors": [
      "~Zhewei_Kang1",
      "~Xuandong_Zhao1",
      "~Dawn_Song1"
    ],
    "pdf": "/pdf/6cc10792a30efaf0a61ec8bc79161b410fbddef1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Test-time computation optimization, scalability with increasing sample sizes, efficient inference without computationally expensive reward models, performance improvement through better selection mechanisms, practical and efficient approach to Best-of-N selection",
      "Irrelevant Aspects": "Does not focus on training optimization, lacks specific GPU utilization discussions, limited mention of latency measurements, no hardware-specific optimizations highlighted",
      "Summary": "This paper presents self-certainty, a novel and efficient metric for Best-of-N selection in LLMs that improves reasoning performance without the computational overhead of reward models. It offers a scalable approach to test-time computation optimization, effectively handling increasing sample sizes. The method complements chain-of-thought and generalizes to open-ended tasks. While highly relevant to inference optimization and scalability aspects of my research, it doesn't specifically address GPU utilization, training optimization, or hardware-specific performance aspects."
    }
  },
  {
    "id": "dH8mKmvADv",
    "title": "Learning in Compact Spaces with Approximately Normalized Transformer",
    "abstract": "The successful training of deep neural networks requires addressing challenges such as overfitting, numerical instabilities leading to divergence, and increasing variance in the residual stream. A common solution is to apply regularization and normalization techniques that usually require tuning additional hyperparameters. An alternative is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic, approximate normalization via simple scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. Additionally, instead of applying strict normalization for the parameters, we constrain their norms. These modifications remove the need for weight decay and learning rate warm-up as well, but do not increase the total number of normalization layers. Our experiments with transformer architectures show up to 40% faster convergence compared to GPT models with QK normalization, with only 3% additional runtime cost. When deriving scaling laws, we found that our method enables training with larger batch sizes while preserving the favorable scaling characteristics of classic GPT architectures.",
    "authors": [
      "~Jörg_K.H._Franke1",
      "~Urs_Spiegelhalter1",
      "~Marianna_Nezhurina1",
      "~Jenia_Jitsev1",
      "~Frank_Hutter1",
      "~Michael_Hefenbrock1"
    ],
    "pdf": "/pdf/162411a97e05bbb815f8b5409e8c2599f9ea6419.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for transformer architectures, showing 40% faster convergence compared to GPT models with QK normalization. It enables training with larger batch sizes while preserving favorable scaling characteristics, which directly impacts GPU utilization and scalability. The proposed approximate normalization technique removes the need for weight decay and learning rate warm-up without increasing normalization layers, contributing to more efficient training pipelines. The method only adds 3% runtime cost while maintaining performance.",
      "Irrelevant Aspects": "The paper focuses primarily on training optimization and doesn't address inference optimization techniques. There's limited discussion of specific GPU utilization techniques, distributed training strategies, or model compression methods for inference optimization. The research doesn't cover latency-specific optimization for deployment scenarios.",
      "Summary": "This paper presents a novel approximate normalization technique for transformer models that significantly improves training efficiency by achieving 40% faster convergence with minimal computational overhead. The method enables training with larger batch sizes while maintaining scaling characteristics, making it highly relevant for large language model training optimization and GPU utilization. While it doesn't address inference optimization directly, the training improvements it offers are substantial for the overall ML system efficiency."
    }
  },
  {
    "id": "NFM8F5cV0V",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "abstract": "We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training on the base model focusing on scalability, simplicity and accessibility.\nThrough extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both benchmark performance and response length, replicating the scaling phenomenon observed in DeepSeek-R1-Zero.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance across AIME2024, MATH500, and GPQA Diamond, while demonstrating remarkable efficiency—requiring only 1/10 of the training steps compared to the DeepSeek-R1-Zero pipeline. \nWe validate that this recipe generalizes well across diverse training domains and different model families without algorithmic modifications. \nMoreover, our analysis not only covers training dynamics and ablation for critical design choices, but also quantitatively show how the learned critic in Reasoner-Zero training effectively identifies and devalues repetitive response patterns, yielding more robust advantage estimations and enhancing training stability. \nEmbracing the principles of open-source, we release our source code, parameter settings, training data, and model weights across various sizes, fostering reproducibility and encouraging further exploration of the properties of related models.",
    "authors": [
      "~Jingcheng_Hu1",
      "~Yinmin_Zhang1",
      "~Qi_Han3",
      "~Daxin_Jiang2",
      "~Xiangyu_Zhang1",
      "~Heung-Yeung_Shum1"
    ],
    "pdf": "/pdf/afe1d28b1ce36a43b6b9635f485c7af91a6f8ffb.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization through minimalist approach using vanilla PPO with GAE, achieving 10x efficiency improvement over previous methods. Focus on scalability of RL training for LLMs. Open-source implementation including code, parameters, training data, and model weights. Demonstrates better GPU utilization through reduced training steps. Training dynamics analysis and critic functionality for improved training stability. Generalization across different model families without algorithmic modifications.",
      "Irrelevant Aspects": "Specific focus on reasoning-oriented models rather than general LLM training. Limited discussion of hardware-specific optimizations. Minimal direct analysis of throughput and latency metrics. Emphasis on reasoning task performance benchmarks rather than system-level metrics.",
      "Summary": "Open-Reasoner-Zero presents an open-source implementation for scaling RL training of reasoning models using a minimalist vanilla PPO approach. The method achieves superior performance with 10x fewer training steps than DeepSeek-R1-Zero, indicating significant efficiency improvements. The paper focuses on scalability, simplicity, and accessibility, providing all code, parameters, data, and weights publicly. The approach generalizes across different domains and model families without algorithmic modifications, while training stability is enhanced through a learned critic that identifies repetitive patterns."
    }
  },
  {
    "id": "wpkmEV57YT",
    "title": "Learning to Focus: Causal Attention Distillation via Gradient‐Guided Token Pruning",
    "abstract": "Large language models (LLMs) have demonstrated significant improvements in contextual understanding. However, their ability to attend to truly critical information during long-context reasoning and generation still falls behind the pace. Specifically, our preliminary experiments reveal that certain distracting patterns can misdirect the model’s attention during inference, and removing these patterns substantially improves reasoning accuracy and generation quality. We attribute this phenomenon to spurious correlations in the training data, which obstruct the model’s capacity to infer authentic causal instruction–response relationships. This phenomenon may induce redundant reasoning processes, potentially resulting in significant inference overhead and, more critically, the generation of erroneous or suboptimal responses. To mitigate this, we introduce a two-stage framework called Learning to Focus (LeaF) leveraging intervention-based inference to disentangle confounding factors. In the first stage, LeaF employs gradient-based comparisons with an advanced teacher to automatically identify confounding tokens based on causal relationships in the training corpus. Then, in the second stage, it prunes these tokens during distillation to enact intervention, aligning the student’s attention with the teacher’s focus distribution on truly critical context tokens. Experimental results demonstrate that LeaF not only achieves an absolute improvement in various mathematical reasoning, code generation and multi-hop question answering benchmarks but also effectively suppresses attention to confounding tokens during inference, yielding a more interpretable and reliable reasoning model.",
    "authors": [
      "~Yiju_Guo1",
      "~Wenkai_Yang1",
      "~Zexu_Sun1",
      "~Ning_Ding5",
      "~Zhiyuan_Liu1",
      "~Yankai_Lin1"
    ],
    "pdf": "/pdf/d3824bda73a525773efe78f8716af085acd1fe56.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization through token pruning, which reduces computational overhead and potentially improves throughput. The gradient-guided pruning approach could lead to better GPU utilization by eliminating unnecessary computations. The distillation framework aims to create more efficient student models with comparable performance to larger teacher models. The paper specifically mentions reducing 'significant inference overhead' and improving reasoning efficiency.",
      "Irrelevant Aspects": "The primary focus is on improving model accuracy and reasoning quality through causal attention alignment rather than pure system optimization. There's no detailed discussion of GPU utilization metrics, multi-GPU scaling strategies, or specific throughput/latency measurements. The paper doesn't address memory optimization techniques or batching strategies that are common in inference optimization research.",
      "Summary": "LeaF introduces a two-stage framework for improving LLM efficiency through gradient-guided token pruning and attention distillation. By identifying and removing confounding tokens during training, the method aims to create more focused and efficient models that require less computational overhead during inference. While the paper emphasizes improvements in reasoning quality, it also claims to reduce inference costs, making it moderately relevant to LLM optimization research, though it lacks detailed system-level performance metrics."
    }
  },
  {
    "id": "dEi1S731lk",
    "title": "Parallel Scaling Law for Language Models",
    "abstract": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce another and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $\\mathcal O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning. Our code and 67 trained model checkpoints are publicly available at https://github.com/QwenLM/ParScale and https://huggingface.co/ParScale.",
    "authors": [
      "~Mouxiang_Chen1",
      "~Binyuan_Hui1",
      "~Zeyu_Cui1",
      "~Jiaxi_Yang1",
      "~Dayiheng_Liu1",
      "~Jianling_Sun2",
      "~Junyang_Lin1",
      "~Zhongxin_Liu1"
    ],
    "pdf": "/pdf/03133be3ee8ca0d3932468b715d293424793cbcc.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper introduces parallel scaling (ParScale) for language models, which directly addresses training and inference optimization goals. It demonstrates significant improvements in memory (up to 22× less memory increase) and latency (6× less latency increase) compared to traditional scaling approaches. The method focuses on better GPU utilization through parallel computation, which aligns with the research interest in scalability, throughput, and latency optimization. The approach is applicable to any model structure and can even enhance pre-trained models through post-training, making it practically valuable.",
      "Irrelevant Aspects": "The paper focuses more on the conceptual framework of scaling laws rather than specific hardware optimizations or GPU architectural considerations. It doesn't appear to delve deeply into the specific parallel computing techniques at the implementation level that might be relevant to systems optimization. The research seems more focused on the machine learning theoretical aspects rather than the systems engineering aspects.",
      "Summary": "This paper presents a novel scaling paradigm called parallel scaling (ParScale) that improves efficiency by increasing parallel computation during both training and inference of language models. It demonstrates significant improvements in memory usage and latency compared to traditional scaling approaches, making it highly relevant to the optimization of large language models. The method is broadly applicable and can even enhance pre-trained models, offering practical value for deploying powerful models in resource-constrained environments."
    }
  },
  {
    "id": "fSFgcEVDT2",
    "title": "Irrational Complex Rotations Empower Low-bit Optimizers",
    "abstract": "In this paper, we propose a novel optimizer state compression algorithm, namely \\textbf{$\\pi$-Quant}, which leverages the properties of irrational numbers (\\eg $\\pi$) for memory-efficient training. \nThe core idea is based on our mathematical findings, which show that a pair of parameters can be represented by a single rotation angle using the complex rotation scheme.\nBuilding on this insight, we map the parameters into a complex space and perform quantization using the corresponding rotation angles. \nTo efficiently integrate it into optimization process, we develop an efficient system of geometric equations that computes the precise rotation angles with linear complexity.\nWe evaluate $\\pi$-Quant on  a wide range of tasks. Our experiments show that it can reduce the bit-width of parameters to 3.32-bit, achieving a 41.8\\% decrease in GPU memory usage, all while maintaining full accuracy. \\textcolor{blue}{We have submitted the code in supplementary materials}.",
    "authors": [
      "~Zhen_Tian1",
      "~Xin_Zhao10",
      "~Ji-Rong_Wen1"
    ],
    "pdf": "/pdf/f789055267d15df264150a6b6282280064d0e2aa.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper proposes a novel optimizer state compression algorithm (π-Quant) that directly addresses memory efficiency during training, a critical aspect of large language model training. It reduces parameter bit-width to 3.32-bit while maintaining full accuracy, and claims to decrease GPU memory usage by 41.8%, which could enable training larger models or using larger batch sizes. The algorithm has linear complexity, suggesting good scalability potential for large models.",
      "Irrelevant Aspects": "The paper focuses exclusively on training optimization rather than inference optimization, which is only half of my research focus. The abstract doesn't explicitly address throughput or latency improvements, which are key metrics in my research. There's no specific mention of applications to language models or NLP tasks, which are central to my interest in LLMs.",
      "Summary": "π-Quant introduces an innovative approach to optimizer state compression using complex rotation schemes and properties of irrational numbers to represent parameters more efficiently. By mapping parameters to complex space and quantizing using rotation angles, it achieves significant memory reduction (41.8%) while maintaining accuracy with only 3.32-bit parameters. While promising for training optimization and memory efficiency, the paper doesn't address inference optimization or explicitly target language model applications, limiting its overall relevance to my research portfolio."
    }
  },
  {
    "id": "9e2H9DhKPa",
    "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
    "abstract": "Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation—especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model’s weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points.",
    "authors": [
      "~Tianteng_Gu1",
      "~Bei_Liu3",
      "~Bo_Xiao7",
      "~Ke_Zeng1",
      "~Jiacheng_Liu3",
      "~Yanmin_Qian1"
    ],
    "pdf": "/pdf/8067a0744f87926b879a84ebfeb80307d7571bb0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on LLM optimization through pruning, which is directly relevant to LLM compression and efficiency. It addresses semi-structured sparsity (2:4), which is particularly important for GPU hardware optimization. The method works on large models like LLaMA3-70B, demonstrating scalability. By reducing model size while maintaining performance, it can potentially improve GPU memory utilization and enable larger models to be deployed. The technique is model-agnostic and can be integrated with existing pruning methods, making it practical for optimization pipelines.",
      "Irrelevant Aspects": "The paper primarily focuses on maintaining model quality (perplexity and zero-shot accuracy) after pruning rather than directly measuring GPU utilization, throughput, or latency improvements. There's limited discussion of inference speed benefits or how the technique specifically impacts runtime efficiency. The research focuses on the pre-deployment optimization phase rather than runtime inference optimization techniques or memory management during active inference.",
      "Summary": "DenoiseRotator introduces a novel approach to enhance pruning robustness in LLMs by concentrating parameter importance through orthogonal transformations before pruning. The method significantly reduces performance degradation under semi-structured sparsity constraints, with demonstrated improvements on large models like LLaMA3-70B. While the paper shows promise for improving model compression efficiency, it lacks direct measurements of GPU utilization, throughput, or latency improvements that would be most relevant to optimization-focused research."
    }
  },
  {
    "id": "X9diEuva9R",
    "title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "abstract": "Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL  system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77x training speedup compared to synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
    "authors": [
      "~Wei_Fu1",
      "~Jiaxuan_Gao1",
      "~Xujie_Shen3",
      "~Chen_Zhu12",
      "~Zhiyu_Mei1",
      "~Chuyi_He1",
      "~Shusheng_Xu1",
      "~Guo_Wei1",
      "~Jun_Mei2",
      "~WANG_JIASHU1",
      "~Tongkai_Yang1",
      "~Binhang_Yuan1",
      "~Yi_Wu1"
    ],
    "pdf": "/pdf/1b2829a8cfd93ca52bca9bf2c38c826016159024.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing large language model training through an asynchronous RL system that improves GPU utilization, scalability, and throughput. It presents system-level optimizations for better GPU utilization and shows significant training speedups (up to 2.77x). The decoupling of generation from training is a novel architectural optimization directly relevant to my interests in LLM training efficiency.",
      "Irrelevant Aspects": "The paper specifically focuses on reinforcement learning for LLMs rather than covering all training paradigms. It's limited to language reasoning tasks rather than general LLM applications. There's no explicit discussion of inference optimization, which is part of my research interest.",
      "Summary": "AREAL presents an asynchronous RL system for language reasoning that decouples generation from training to improve GPU utilization. While highly relevant to my focus on LLM training optimization, GPU utilization, and throughput improvement, it's limited to the RL paradigm and doesn't address inference optimization."
    }
  },
  {
    "id": "Orpf8yDjdj",
    "title": "The Curse of Depth in Large Language Models",
    "abstract": "In this paper, we re-introduce the Curse of Depth, a concept that re-introduces, explains, and addresses the recent observation in modern Large Language Models (LLMs) where deeper layers are much less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs, such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 7B, demonstrate that \\ours significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.",
    "authors": [
      "~Wenfang_Sun2",
      "~Xinyuan_Song2",
      "~Pengxiang_Li2",
      "~Lu_Yin1",
      "~Yefeng_Zheng3",
      "~Shiwei_Liu2"
    ],
    "pdf": "/pdf/3862c46cebc7ae5935feb3f6e925547563e2ffa6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses fundamental training optimization in LLMs by identifying why deep layers become ineffective and proposing LayerNorm Scaling to improve their contribution. This directly relates to training efficiency and could have downstream effects on model performance and efficiency. The solution is validated across different model sizes (130M to 7B), showing scalability potential.",
      "Irrelevant Aspects": "The paper doesn't directly address inference optimization, GPU utilization, throughput, or latency, which are key aspects of my research focus. It focuses on architectural modification rather than system-level optimizations and doesn't discuss deployment considerations.",
      "Summary": "This paper identifies and addresses the 'Curse of Depth' in LLMs, where deeper layers become ineffective due to Pre-Layer Normalization. The proposed LayerNorm Scaling technique improves training performance by enabling deeper layers to contribute more effectively. While highly relevant to training optimization, it doesn't directly address inference optimization or system-level efficiency concerns."
    }
  },
  {
    "id": "LVDRJE4xQ2",
    "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
    "abstract": "Training high-performing Small Language Models (SLMs) remains computationally expensive, even with knowledge distillation and pruning from larger teacher models. \nExisting approaches often face three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs).\nTo address these challenges, we introduce \\textbf{Low-Rank Clone (LRC)}, an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models.\nLRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher.\nThis unified design maximizes knowledge transfer while removing the need for explicit alignment modules.\nExtensive experiments with open-source teachers such as Llama-3.2-3B-Instruct and Qwen2.5-3B/7B-Instruct show that LRC matches or surpasses the performance of state-of-the-art models trained on trillions of tokens--using only 20B tokens, achieving over \\textbf{1,000$\\times$} greater training efficiency. \nOur codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/JitaiHao/LRC-4B-Base.",
    "authors": [
      "~Jitai_Hao1",
      "~Qiang_Huang3",
      "~Hao_Liu20",
      "~Xinyan_Xiao1",
      "~Zhaochun_Ren1",
      "~Jun_Yu1"
    ],
    "pdf": "/pdf/3d174b71d13bf8e1d728563aa95bdc04a181edef.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on training optimization for Small Language Models through knowledge distillation, which directly relates to improving GPU utilization and training efficiency. The Low-Rank Clone (LRC) method claims to achieve over 1,000× greater training efficiency, which is highly relevant to the goal of better resource utilization. The method of compressing teacher weights and aligning student activations has implications for creating more efficient models that could potentially improve inference throughput and reduce latency.",
      "Irrelevant Aspects": "The paper appears to focus primarily on training optimization rather than inference optimization techniques. It doesn't seem to directly address specific inference acceleration methods or real-time deployment optimizations. The abstract doesn't mention hardware-specific optimizations or inference speed benchmarks that would be most relevant to latency-critical applications.",
      "Summary": "This paper presents Low-Rank Clone (LRC), a knowledge distillation method for efficiently training Small Language Models that achieves significant training efficiency gains (1,000×). While highly relevant to training optimization and resource utilization aspects of the research interest, it has limited direct focus on inference optimization techniques specifically. The method produces smaller models that could indirectly benefit inference efficiency, but this appears to be a secondary effect rather than the paper's primary focus."
    }
  },
  {
    "id": "Vj48eXaQDM",
    "title": "Learned Prefix Caching for Efficient LLM Inference",
    "abstract": "Prefix caching is a key technique for reducing Large Language Model (LLM) inference costs. However, the prevalent least-recently-used (LRU) eviction algorithm has a large gap to the optimal algorithm. This paper introduces LPC, the first learned method to perform LLM prefix cache eviction. LPC leverages conversational content analysis to provide predictive guidance for eviction, determining which conversations are likely to continue. These insights, combined with last access timestamps, inform more effective cache management. Extensive evaluations across three real-world datasets demonstrate that LPC achieves 18-47% reductions in required cache sizes for equivalent hit ratios and has an 11% improvement in LLM prefilling throughput in an emulated environment.",
    "authors": [
      "~Dongsheng_Yang2",
      "~Austin_Li1",
      "~Kai_Li8",
      "~Wyatt_Lloyd1"
    ],
    "pdf": "/pdf/a340edd38ffafcfd1843a7f71d85464d9fb3e3df.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "LLM inference efficiency, cache management optimization, GPU utilization improvement, throughput enhancement, resource efficiency",
      "Irrelevant Aspects": "No focus on training optimization, no direct latency measurements, evaluated in emulated environment rather than actual hardware",
      "Summary": "Presents LPC, a learned cache eviction algorithm for LLM prefix caching that improves upon LRU by predicting conversation continuation likelihood. Demonstrates 18-47% reductions in required cache sizes and 11% improvement in LLM prefilling throughput across three real-world datasets."
    }
  },
  {
    "id": "6ICFqmixlS",
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "abstract": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B o1-like models across various math benchmarks, and achieve performance on par with the teacher model QwQ-32B-Preview that produces the seed data.",
    "authors": [
      "~Wenkai_Yang1",
      "~Shuming_Ma1",
      "~Yankai_Lin1",
      "~Furu_Wei1"
    ],
    "pdf": "/pdf/a85740fc42f5b0086b031316724925f3d39daa30.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Test-time compute optimization, performance optimization through optimal CoT lengths, resource efficiency by avoiding excessive computation, scaling strategies for reasoning capabilities",
      "Irrelevant Aspects": "Limited focus on technical GPU utilization details, focuses more on reasoning effectiveness than computational efficiency metrics, lacks explicit discussion of GPU memory management or parallel processing",
      "Summary": "The paper explores optimal test-time compute scaling for LLM reasoning, specifically focusing on finding optimal Chain of Thought lengths to improve reasoning performance. While it addresses important aspects of compute optimization and efficiency, it appears to focus more on the effectiveness of reasoning rather than the technical details of GPU utilization and scalability from a systems perspective."
    }
  },
  {
    "id": "7qq1UeCYL6",
    "title": "Degrees of Freedom for Linear Attention: Distilling Softmax Attention with Optimal Feature Efficiency",
    "abstract": "Linear attention has attracted interest as a computationally efficient approximation to softmax attention, especially for long sequences. Recent studies has explored distilling softmax attention in pre-trained Transformers into linear attention. However, a critical challenge remains: *how to choose the feature dimension that governs the approximation quality*. Existing methods fix this dimension uniformly across all attention layers, overlooking the diverse roles and complexities of them. In this paper, we propose a principled method to automatically determine the feature dimension in linear attention using the concept of statistical *degrees of freedom*, which represent the effective dimensionality of the inputs. We provide a theoretical bound on the approximation error and show that the dimension chosen by our method achieves smaller errors under a fixed computational budget. Furthermore, we introduce an efficient layerwise training strategy to learn nonlinear features tailored to each layer. Experiments on multiple pre-trained transformers demonstrate that our method improves the performance of distilled models compared to baselines without increasing the inference cost. Our findings also provide insight into how the complexity of the attention mechanism evolves across layers.",
    "authors": [
      "~Naoki_Nishikawa1",
      "~Rei_Higuchi1",
      "~Taiji_Suzuki1"
    ],
    "pdf": "/pdf/b2181d8e38b9e7fb7151c847a3bf35b6f4351b6d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses optimization of linear attention mechanisms, which is directly relevant to computational efficiency in Transformer models. It proposes a method to determine optimal feature dimensions for approximation, which impacts computational budget and GPU utilization. The layerwise optimization approach for attention mechanisms could lead to better model performance without increasing inference costs. The distillation technique discussed can reduce model complexity while maintaining performance, which is valuable for inference optimization.",
      "Irrelevant Aspects": "The paper appears to focus primarily on the mathematical approximation of attention mechanisms rather than system-level implementation details. It lacks discussion of specific GPU utilization techniques, memory management strategies, or distributed training considerations. There's limited focus on latency measurements or throughput optimization in real deployment scenarios. The work doesn't address quantization or other hardware-specific optimizations.",
      "Summary": "This paper presents a method to optimize linear attention approximation in Transformers by automatically determining feature dimensions based on degrees of freedom. While the approach offers computational benefits and could reduce inference costs, it's more focused on the mathematical aspects of attention approximation rather than system-level optimization techniques that would maximize GPU utilization."
    }
  },
  {
    "id": "20JDhbJqn3",
    "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
    "abstract": "Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce \\method{}, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in \\method{}, reducing computational overhead and improving throughput by 26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that \\method{} outperforms state-of-the-art methods by up to 8.7\\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem. \\emph{The code is available at \\href{https://github.com/NVIDIA/kvpress}{link}.}",
    "authors": [
      "~Xiang_Liu10",
      "~Zhenheng_Tang2",
      "~Peijie_Dong1",
      "~Zeyu_Li3",
      "~Liuyue1",
      "~Bo_Li33",
      "~Xuming_Hu1",
      "~Xiaowen_Chu2"
    ],
    "pdf": "/pdf/49b5b8e7b6f5c878d028cbba29caf728aa81ffe8.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses GPU memory optimization during LLM inference, focusing on KV cache compression which is a critical bottleneck. It improves throughput by 26.5% while maintaining or improving precision. The semantic-aware approach to preserving context integrity while aggressively compressing memory is highly relevant to GPU utilization optimization. The layer-wise index reuse technique contributes to better scalability. The focus on maintaining performance under compression ratios aligns with efficient resource utilization goals.",
      "Irrelevant Aspects": "The paper is solely focused on inference optimization rather than training optimization. While it addresses GPU memory, it doesn't explicitly discuss latency measurements. The semantic chunking approach, while novel, may have limited applicability across different model architectures. The evaluation focuses on specific benchmarks without comprehensive analysis across diverse use cases.",
      "Summary": "ChunkKV presents a semantic-aware approach to KV cache compression for long-context LLM inference. By treating semantic chunks rather than individual tokens as compression units, it preserves contextual integrity while achieving 26.5% throughput improvement and up to 8.7% precision gains over state-of-the-art methods. The layer-wise index reuse technique reduces computational overhead, making it a significant contribution to GPU memory optimization and efficient inference scaling."
    }
  },
  {
    "id": "uwL0vbeEVn",
    "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications",
    "abstract": "Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 3.9$\\times$, outperforming state-of-the-art methods -- 2.2$\\times$ faster than model-based approaches like EAGLE-2/3 and 1.6$\\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced.",
    "authors": [
      "~Gabriele_Oliaro1",
      "~Zhihao_Jia2",
      "~Daniel_F_Campos1",
      "~Aurick_Qiao1"
    ],
    "pdf": "/pdf/eab308d22c40e44f8fd67ac67876864621ddace9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative decoding techniques to reduce latency in LLM inference, directly addressing my interest in inference optimization. It introduces a novel caching mechanism using suffix trees to improve GPU utilization and throughput. The method claims significant speedups (up to 3.9x) compared to state-of-the-art approaches like EAGLE-2/3 and Token Recycling. It specifically targets emerging AI workloads with repetitive inference patterns, which represents an important optimization scenario. The technique adaptively adjusts speculation length based on acceptance likelihood, demonstrating a sophisticated approach to balancing computation and performance gains.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization with no discussion of training optimization. It targets a specific niche of workloads (agentic applications with highly predictable sequences) rather than general LLM scenarios. The abstract doesn't mention scalability across multiple GPUs or distributed systems. The implementation details regarding hardware-specific optimizations are not discussed. The technique appears to be most effective only when certain workload characteristics are present.",
      "Summary": "SuffixDecoding introduces a novel speculative decoding method that leverages suffix trees to cache long token sequences from prompts and previous outputs. Designed specifically for emerging AI applications like LLM-based agents that generate repetitive inference requests, the technique adaptively speculates more tokens when acceptance likelihood is high. The paper claims performance improvements of up to 3.9x on agentic benchmarks, outperforming both model-based (EAGLE-2/3) and model-free (Token Recycling) approaches. While focused on a specific application domain, the technique offers valuable insights into optimizing LLM inference for specialized workloads."
    }
  },
  {
    "id": "MNduv07wAu",
    "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts",
    "abstract": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space.Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves a two-stage self-distillation process: first distilling natural language CoT into latent-space continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior performance of our method.\nFor example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20× and reducing token generation by 91.0\\% on average.",
    "authors": [
      "~Xiaoqiang_Wang3",
      "~Suyuchen_Wang1",
      "~Yun_Zhu5",
      "~Bang_Liu1"
    ],
    "pdf": "/pdf/d239a496aa58524a3d724289b589b295e3d663a1.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization in LLMs with claimed 20× acceleration and 91% token reduction. It introduces dynamic shortcuts in model architecture that adaptively allocate computation across reasoning steps, which directly relates to GPU utilization and computational resource optimization. The method operates in latent space to avoid unnecessary token decoding, improving efficiency. The two-stage self-distillation training process represents an optimized training approach. These innovations have significant implications for system scalability, throughput improvement, and latency reduction - all core areas of my research focus.",
      "Irrelevant Aspects": "The specific reasoning benchmarks used (like GSM8K) are not central to my system optimization focus. The psychological framing around 'System-1' and 'System-2' reasoning is more about cognitive modeling than system performance. Some aspects of the paper may focus more on reasoning quality maintenance rather than pure system optimization metrics.",
      "Summary": "This paper is highly relevant to my research interests as it addresses critical challenges in LLM inference efficiency through architectural innovations. The proposed System-1.5 Reasoning framework introduces dynamic shortcuts that optimize computational allocation across reasoning steps, achieving significant throughput improvements (20× acceleration) and latency reduction (91% fewer tokens) while maintaining reasoning performance. The work aligns closely with my focus on GPU utilization, scalability, and system optimization for large language models."
    }
  },
  {
    "id": "yOs12gdsaL",
    "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference",
    "abstract": "Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through'' input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively improves performance with the reduced costs associated with commercial API calls compared to prompt compressing methods. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.",
    "authors": [
      "~Weizhi_Fei1",
      "~Xueyan_Niu1",
      "~XIE_GUOQING1",
      "~Yingqing_Liu1",
      "~Bo_Bai2",
      "~Wei_Han6"
    ],
    "pdf": "/pdf/c92d4ba71ed258b69cbc3420e9b3873b1d9c5b21.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for long-context LLMs, addressing computational costs and performance. It proposes EHPC, a training-free method that leverages specific attention heads to select important tokens, reducing the amount of processing needed during inference. The approach achieves state-of-the-art results in prompt compression and long-context inference acceleration, potentially improving throughput and reducing latency. It's a novel technique that doesn't require additional training, making it immediately applicable to existing models.",
      "Irrelevant Aspects": "The paper does not address training optimization, which is part of my research interest. It doesn't explicitly discuss GPU utilization or scalability optimizations in detail. The method is focused specifically on prompt compression rather than broader inference optimization techniques.",
      "Summary": "This paper introduces EHPC (Evaluator Head-based Prompt Compression), a method to accelerate long-context LLM inference by identifying and utilizing specific attention heads that can select the most significant tokens from inputs. By enabling models to process only important tokens during the pre-filling stage, it reduces computational costs while maintaining performance. The method is training-free and achieves state-of-the-art results on relevant benchmarks, making it a valuable contribution to inference optimization for LLMs, though it doesn't address training optimization or detailed GPU utilization strategies."
    }
  },
  {
    "id": "EaTRrceoU9",
    "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
    "abstract": "Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy---for example, preferring less desirable responses---posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose Hölder-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. Hölder-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, applied to Anthropic HH-RLHF dataset, it reveals substantial noise levels and removing these mislabels significantly improves alignment performance across methods. The code is available at https://github.com/ma921/HolderDPO.",
    "authors": [
      "~Masahiro_Fujisawa1",
      "~Masaki_Adachi1",
      "~Michael_A_Osborne1"
    ],
    "pdf": "/pdf/c56c099f1d58ea117fdd4dcbc3908c7126f6249a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization through robust alignment methods, Handling noisy human feedback efficiently, Gradient-free metric for dataset valuation, Scalable human feedback valuation without manual verification, Code implementation for practical application",
      "Irrelevant Aspects": "No focus on inference optimization, No direct discussion of GPU utilization, No consideration of throughput and latency improvements, No discussion of model architecture optimizations",
      "Summary": "The paper introduces Hölder-DPO, a robust alignment method for language models that handles noisy human feedback through a provable redescending property. It enables scalable valuation of human feedback and identifies mislabels without manual verification or clean validation datasets. While the paper makes significant contributions to training optimization (especially the alignment phase) and scalability of feedback processing, it doesn't directly address inference optimization, GPU utilization, throughput, or latency, which are central to my research interests."
    }
  },
  {
    "id": "1cjLvtFOmL",
    "title": "Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving",
    "abstract": "When serving a single base LLM with several different LoRA adapters simultaneously, the adapters cannot simply be merged with the base model’s weights as the adapter swapping would create overhead and requests using different adapters could not be batched. Rather, the LoRA computations have to be separated from the base LLM computations, and in a multi-device setup the LoRA adapters can be sharded in a way that is well aligned with the base model’s tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA sharding strategy encounters some communication overhead, which may be small in theory, but can be large in practice. In this paper, we propose to constrain certain LoRA factors to be block-diagonal, which allows for an alternative way of sharding LoRA adapters that does not require any additional communication for the LoRA computations. We demonstrate in extensive experiments that our block-diagonal LoRA approach is similarly parameter efficient as standard LoRA (i.e., for a similar number of parameters it achieves similar downstream performance) and that it leads to significant end-to-end speed-up over S-LoRA. For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x) end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x) the number of adapter parameters for Llama-3.1-8B.",
    "authors": [
      "~Xinyu_Wang9",
      "~Jonas_M._Kübler1",
      "~Kailash_Budhathoki1",
      "~Yida_Wang1",
      "~Matthäus_Kleindessner2"
    ],
    "pdf": "/pdf/f6434f0bd078dcfc3e94f895210a57883e8f50f9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs, specifically addressing GPU utilization and scalability in multi-GPU setups for serving multiple LoRA adapters. It directly tackles communication overhead in tensor parallel execution, which is critical for achieving high throughput and low latency in distributed LLM serving. The paper provides practical performance improvements on real hardware (A100 GPUs) with large models (Llama-3.1-8B and 70B), demonstrating significant end-to-end speed-ups.",
      "Irrelevant Aspects": "The paper focuses solely on inference serving rather than training optimization, which is part of my research interest. It also specifically addresses LoRA optimization rather than a broader range of LLM optimization techniques.",
      "Summary": "This paper introduces Block-Diagonal LoRA, a technique to eliminate communication overhead in tensor parallel LoRA serving. By constraining certain LoRA factors to be block-diagonal, the authors enable an alternative sharding approach that avoids additional communication for LoRA computations. The approach maintains parameter efficiency while achieving significant end-to-end speed-ups over S-LoRA, with results showing up to 1.79x speed-up for Llama-3.1-70B and 1.63x speed-up for Llama-3.1-8B on eight A100 GPUs."
    }
  },
  {
    "id": "MJvwM5dBZM",
    "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
    "abstract": "Recent models such as OpenAI o1 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks by generating extended Chain-of-Thought (CoT) traces. While longer reasoning helps with thorough exploration of solution paths for complex problems, it also often leads to inefficient and redundant outputs—a phenomenon commonly described as $\\textit{overthinking}$.  In this paper, we propose $\\texttt{ShorterBetter}$, a simple yet effective reinforcement learning method that enables reasoning models to learn their own optimal CoT lengths without manual supervision. We define the $\\textit{Sample Optimal Length}$ (SOL) as the length of the shortest correct response among multiple generations, which serves as a dynamic reward signal to guide the model toward efficient reasoning. Applied to DeepSeek-Distill-Qwen-1.5B/7B as base models, $\\texttt{ShorterBetter}$ achieves 50\\%-80\\% reduction in output lengths in both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our reasoning trace analysis shows that $\\texttt{ShorterBetter}$ refines the structure of the reasoning traces by reducing unnecessary repetition, excessive self-verification, and over-exploration of alternatives.",
    "authors": [
      "~Jingyang_Yi1",
      "~Justin_Wang1",
      "~Sida_Li1"
    ],
    "pdf": "/pdf/bde69f28fc3bc8b96d74cd959ffd7e961aa0e70e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models by reducing output lengths while maintaining accuracy, which impacts latency and throughput. It proposes a reinforcement learning method to optimize model behavior, relevant to training optimization. The approach achieves 50-80% reduction in output lengths, significantly improving GPU utilization and computational costs during inference. The method works on different model sizes (1.5B and 7B parameters), showing scalability considerations. The approach reduces unnecessary computation, which is crucial for scalable inference and better resource utilization.",
      "Irrelevant Aspects": "The paper focuses primarily on reasoning efficiency rather than hardware-specific optimizations. It doesn't appear to discuss how this optimization specifically affects GPU utilization at the system level. The paper doesn't seem to address distributed inference or model parallelism, important for scalability. It may not discuss throughput improvements in terms of requests per second or concurrent processing capabilities. The method doesn't appear to address model compression, quantization, or other memory optimization techniques.",
      "Summary": "ShorterBetter is highly relevant to my research interests as it addresses inference optimization for LLMs by finding optimal reasoning lengths. The paper proposes a reinforcement learning method that reduces output tokens by 50-80% while maintaining accuracy, directly impacting inference efficiency, latency, and GPU utilization. While it focuses on reducing computational redundancy in reasoning traces, it doesn't deeply explore hardware-specific optimizations or distributed inference strategies that would complement the approach at scale."
    }
  },
  {
    "id": "zefDc9oi5T",
    "title": "RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models",
    "abstract": "Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of fine-tuning large models by updating a low-dimensional subspace of the pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal convergence and noticeable performance degradation, due to inconsistent and imbalanced weight updates induced by its nonunique low-rank factorizations. To overcome these limitations, this article identifies the optimal low-rank factorization per step that minimizes an upper bound on the loss. The resultant refactored low-rank adaptation (RefLoRA) method promotes a flatter loss landscape, along with consistent and balanced weight updates, thus speeding up stable convergence. Extensive experiments evaluate RefLoRA on natural language understanding, and commonsense reasoning tasks with popular large language models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical tests corroborate that RefLoRA converges faster, outperforms various benchmarks, and enjoys negligible computational overhead compared to state-of-the-art LoRA variants.",
    "authors": [
      "~Yilang_Zhang2",
      "~Bingcong_Li1",
      "~Georgios_B._Giannakis1"
    ],
    "pdf": "/pdf/a07e246283f59dd645e5fa3d3f1c2a038878ba10.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization of large language models through efficient fine-tuning methods. RefLoRA reduces computational and memory overhead during fine-tuning, which relates to better GPU utilization and resource efficiency. The faster convergence with minimal computational overhead aligns with research goals of optimizing LLM training systems. The focus on popular large models (LLaMA series) makes it highly relevant to current LLM research.",
      "Irrelevant Aspects": "The paper primarily focuses on fine-tuning rather than both training and inference optimization. There's limited discussion of hardware-specific optimizations or GPU-level improvements. The abstract doesn't explicitly address throughput or latency metrics, which are key performance indicators for my research. The paper appears to be algorithmic rather than system-level optimization.",
      "Summary": "RefLoRA presents an improved method for low-rank adaptation during fine-tuning of large language models. It addresses limitations of standard LoRA by optimizing the low-rank factorization process, resulting in faster convergence, better performance, and more balanced weight updates. While the paper focuses on training optimization rather than inference, its contributions to reducing computational overhead during fine-tuning make it valuable for research in LLM optimization systems. The method was validated on multiple popular LLMs, showing consistent improvements with minimal additional computational cost."
    }
  },
  {
    "id": "TxedB8hI5O",
    "title": "Scaling RL to Long Videos",
    "abstract": "We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames). Code and models are available at https://github.com/NVlabs/Long-RL",
    "authors": [
      "~Yukang_Chen1",
      "~Wei_Huang36",
      "~Baifeng_Shi1",
      "~Qinghao_Hu3",
      "~Hanrong_Ye1",
      "~Ligeng_Zhu1",
      "~Zhijian_Liu1",
      "~Pavlo_Molchanov1",
      "~Jan_Kautz1",
      "~XIAOJUAN_QI2",
      "~Sifei_Liu2",
      "~Hongxu_Yin2",
      "~Yao_Lu13",
      "~Song_Han5"
    ],
    "pdf": "/pdf/086f9eed5c2d342130b7d5c3c1f80a2cc8f3594f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces Multi-modal Reinforcement Sequence Parallelism (MR-SP), a specialized training infrastructure for long video RL that incorporates sequence parallelism and a vLLM-based engine. It demonstrates system optimizations including cached video embeddings for efficient rollout and prefilling, achieving up to 2.1x speedup in long video RL training. The system supports processing up to 8,192 video frames and can handle hour-long videos on a single A100 node (8 GPUs), showcasing scalability and efficient GPU utilization. The framework's support for various modalities and models demonstrates flexibility in optimization approaches.",
      "Irrelevant Aspects": "The paper focuses heavily on the application aspects including the LongVideo-Reason dataset creation, the specific performance metrics on video benchmarks (e.g., 65.1% accuracy on VideoMME), and the detailed descriptions of the two-stage training pipeline (CoT-SFT and RL). These aspects are more about the application and algorithm performance rather than system optimization. The diversity of video domains (sports, games, vlogs) is application-specific and not directly related to system efficiency.",
      "Summary": "The paper presents a system optimization framework for scaling reinforcement learning to long videos, with a focus on improving training efficiency through specialized parallelization techniques (MR-SP) and memory optimizations. While heavily focused on the application domain of video understanding, it provides valuable contributions to the field of large-scale ML system optimization, particularly in handling long sequence data efficiently. The reported speedup in training and ability to process hour-long videos on a single node demonstrate significant advancements in GPU utilization and scalability."
    }
  },
  {
    "id": "vvEVQrm9M8",
    "title": "DUO: No Compromise to Accuracy Degradation",
    "abstract": "Distributed training often suffers from high communication overhead due to large-scale gradient synchronization. Although gradient compression—particularly at 4-bit or even lower precision—significantly reduces transfer volume, it typically results in sacrifice in precision and degradation of the final model accuracy. \n\nIn this work, we introduce DUO, a distributed training framework designed to mitigate accuracy degradation incurred by gradient compression without involving additional overhead. DUO achieves this by inserting an additional high-precision gradient synchronization step into a previously computation-only phase, so that its communication is fully hidden by computation.\n\nWe provide a comprehensive theoretical proof of convergence for DUO and validate its effectiveness through extensive pre-training experiments on GPT models. Our results indicate that DUO effectively restores accuracy when using 4-bit gradient compression, achieving performance comparable to uncompressed training. Remarkably, DUO maintains minimal accuracy degradation even under extreme compression scenarios, including 1-bit gradients or complete omission of the low-precision gradient communication step (0-bit transmission).",
    "authors": [
      "~Jinda_Jia2",
      "~Cong_Xie1",
      "~Hanlin_Lu1",
      "~Fanjiang_Ye1",
      "~Hao_Feng7",
      "~Daoce_Wang1",
      "~Haibin_Lin1",
      "~Zhi_Zhang4",
      "~Xin_Liu51"
    ],
    "pdf": "/pdf/503971f94e4a666bde3e0ec78b4fc192a393d885.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Distributed training optimization, gradient compression, communication overhead reduction, 4-bit and 1-bit gradient compression, computation-communication overlap, GPT model training, accuracy preservation, scalability, convergence proof",
      "Irrelevant Aspects": "None significant - the paper appears well-aligned with expertise in ML systems optimization",
      "Summary": "DUO presents a distributed training framework that mitigates accuracy degradation from gradient compression by overlapping high-precision gradient synchronization with computation. It enables extreme compression (4-bit to 0-bit) while maintaining accuracy comparable to uncompressed training, directly addressing the communication bottleneck in large language model training through theoretical convergence guarantees and GPT model validation."
    }
  },
  {
    "id": "Rx6m16By6l",
    "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning",
    "abstract": "Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \\textbf{Di}vergence-driven \\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. Our code is released at \\url{https://github.com/Skilteee/DiZO}.",
    "authors": [
      "~Qitao_Tan1",
      "~Jun_Liu17",
      "~Zheng_Zhan3",
      "~Caiwen_Ding1",
      "~Yanzhi_Wang3",
      "~Xiaolong_Ma2",
      "~Jaewoo_Lee2",
      "~Jin_Lu1",
      "~Geng_Yuan1"
    ],
    "pdf": "/pdf/dd451c5626987f15f36450fc926ede1387861407.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Focuses on LLM training optimization, directly addressing memory efficiency during fine-tuning",
        "Demonstrates up to 48% reduction in training GPU hours, directly impacting GPU utilization",
        "Explicitly maintains throughput while optimizing, which is a key metric in my research",
        "Shows scalability across different LLM architectures including OPT and Llama series",
        "Addresses resource-constrained scenarios, which relates to making ML systems more efficient"
      ],
      "Irrelevant Aspects": [
        "Primarily focuses on zeroth-order optimization rather than more common first-order methods",
        "Does not address inference optimization, which is half of my research focus",
        "No explicit discussion of latency optimization, which is one of my key metrics",
        "Limited discussion on distributed training aspects that would impact large-scale scalability"
      ],
      "Summary": "This paper introduces DiZO (Divergence-driven Zeroth-Order optimization), a novel approach for LLM fine-tuning that significantly improves upon traditional zeroth-order methods. By incorporating layer-wise divergence analysis and projections, DiZO achieves better convergence speed and accuracy while maintaining memory efficiency. The method demonstrates up to 48% reduction in training GPU hours without sacrificing throughput, and shows competitive or superior performance on RoBERTa, OPT, and Llama models compared to existing ZO baselines and even some first-order methods. While highly relevant for training optimization and GPU utilization, it does not address inference optimization or latency concerns."
    }
  },
  {
    "id": "xNJenVNmzL",
    "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts",
    "abstract": "Modern foundation models are trained on diverse datasets to enhance generalization across tasks and domains. A central challenge in this process is determining how to effectively mix and sample data from multiple sources. This naturally leads to a multi-task learning (MTL) perspective. While prior work in MTL has emphasized mitigating gradient conflicts, we observe that large-scale pretraining scenarios—such as multilingual or multi-domain training—often exhibit little to no gradient conflict. Motivated by this observation, we propose $\\textbf{PiKE}$ ($\\textbf{P}$ositive gradient $\\textbf{i}$nteraction-based $\\textbf{K}$-task weights $\\textbf{E}$stimator), an adaptive data mixing algorithm that dynamically adjusts sampling weights during training. PiKE exploits non-conflicting gradient interactions to minimize a near-tight upper bound on the average loss decrease at each step, while incurring negligible computational overhead. We provide theoretical convergence guarantees and  show that PiKE  outperforms static and non-adaptive mixing baselines. Furthermore, we extend PiKE to promote balanced learning across tasks. Extensive experiments on large-scale language model pretraining confirm that PiKE achieves faster convergence and improved downstream performance compared to existing approaches.",
    "authors": [
      "~Zeman_Li1",
      "~Yuan_Deng1",
      "~Peilin_Zhong1",
      "~Meisam_Razaviyayn1",
      "~Vahab_Mirrokni2"
    ],
    "pdf": "/pdf/138d67f3b4689dc9559171f7e3bfdbc74d2d00eb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on large-scale language model pretraining optimization, addresses training efficiency with faster convergence, designed with negligible computational overhead for better GPU utilization, directly applicable to multi-task learning scenarios in large language models, demonstrates performance improvements in foundation model pretraining",
      "Irrelevant Aspects": "Primarily focuses on data mixing strategy rather than computational optimization aspects, limited discussion on specific GPU utilization techniques, lacks emphasis on hardware scalability and parallelization strategies, minimal coverage of inference optimization techniques",
      "Summary": "PiKE presents an adaptive data mixing algorithm for large-scale multi-task learning in language models that dynamically adjusts sampling weights during training. It exploits low gradient conflict scenarios common in large-scale pretraining to minimize loss with minimal computational overhead. While it contributes to training optimization and convergence speed, it focuses more on data strategy rather than system-level optimizations or hardware-specific efficiency gains."
    }
  },
  {
    "id": "OcMpSh79aE",
    "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
    "abstract": "Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight, activation, and error (output gradient) outlier values that make lower-precision optimization difficult. To address this, we present HALO, a new quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLaMa models, HALO achieves near-full-precision-equivalent results during fine-tuning  on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameter-efficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in INT8 and FP6 precision, while delivering performance benefits.",
    "authors": [
      "~Saleh_Ashkboos1",
      "~Mahdi_Nikdan1",
      "~Soroush_Tabesh1",
      "~Roberto_L._Castro1",
      "~Torsten_Hoefler1",
      "~Dan_Alistarh7"
    ],
    "pdf": "/pdf/a4a2a052fc20fb6537deb7232ea3c6579f9da438.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM training optimization through quantization techniques, which is crucial for improving GPU utilization and memory efficiency. It introduces HALO, a quantization-aware training approach for Transformers that enables low-precision training while maintaining accuracy. The paper demonstrates performance improvements (1.41x end-to-end speedup), which relates to throughput optimization. It includes FSDP integration for low-precision communication, addressing scalability across multiple GPUs. The approach supports both standard and parameter-efficient fine-tuning (PEFT) methods. It specifically works with LLaMa models and mentions custom kernel implementations, both relevant to GPU utilization optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on training rather than inference optimization. There's limited discussion of latency optimization, which is mentioned in the research interest. The paper doesn't address model architecture optimizations that might be relevant to the research scope. There's no clear mention of energy efficiency, which could be relevant to GPU utilization concerns.",
      "Summary": "HALO presents a novel quantization-aware training approach for LLMs that uses Hadamard rotations to mitigate outliers during low-precision training. It enables accurate INT8 and FP6 precision training while maintaining performance, with demonstrated speedups of up to 1.41x for full fine-tuning on RTX 4090 GPUs. The method integrates with FSDP for low-precision communication and supports both standard and PEFT approaches, making it the first practical approach to fully quantized LLM fine-tuning that maintains accuracy while delivering performance benefits."
    }
  },
  {
    "id": "Hs3FrjwyVZ",
    "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL",
    "abstract": "Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities—enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity.\nBuilding on R1-style distilled models, we observe that inserting a simple ellipsis (\"...\") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks.\nExperiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy–efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4\\% while reducing token usage by 52\\% on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.\n\nProject Page: https://github.com/ScienceOne-AI/AutoThink.",
    "authors": [
      "~Songjun_Tu1",
      "~Jiahao_Lin4",
      "~Qichao_Zhang3",
      "~Xiangyu_Tian1",
      "~Linjing_Li1",
      "~Xiangyuan_Lan4",
      "~Dongbin_Zhao1"
    ],
    "pdf": "/pdf/dc89a6b0221240f7e120c7bd39c116d57711eec4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses key aspects of my research interests in LLM optimization: 1) Inference efficiency by reducing computational overhead and latency, 2) GPU utilization through 52% token usage reduction, 3) Scalability through its adaptive reasoning paradigm, 4) Throughput improvements by avoiding over-thinking on simple problems, 5) Multi-stage RL framework for model optimization, 6) Dynamic resource allocation based on problem complexity",
      "Irrelevant Aspects": "Limited focus on R1-style models rather than general architectures, emphasis on mathematical benchmarks rather than broader efficiency metrics, reliance on prompt engineering (ellipsis insertion), lack of hardware-specific optimization details, no mention of system-level implementation or GPU-specific optimizations, narrow focus on reasoning tasks rather than general LLM applications",
      "Summary": "This paper presents AutoThink, a multi-stage RL framework that enables LLMs to adaptively decide when to use explicit reasoning versus direct responses. It significantly reduces computational overhead (52% token reduction) while improving accuracy (6.4%), addressing key efficiency and scalability concerns in LLM deployment. The approach optimizes inference by avoiding unnecessary reasoning steps on simple problems, directly impacting latency and throughput. While focused on R1-style models and reasoning tasks, it offers valuable insights into adaptive computation strategies for resource-efficient LLM deployment."
    }
  },
  {
    "id": "1qKUVyymXs",
    "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped Sharded LLM Training",
    "abstract": "Training LLMs relies on distributed implementations using multiple GPUs to compute gradients in parallel with sharded optimizers. However, synchronizing gradients in data parallel setups introduces communication overhead that grows with the number of workers, limiting parallelization efficiency. Local optimization algorithms reduce communications but incur high memory costs as they prevent optimizer state sharding, hindering scalability. To address this, we propose $\\textbf{AC}$cumulate while $\\textbf{CO}$mmunicate ($\\texttt{ACCO}$), a memory-efficient optimization algorithm for distributed LLM training. By synchronizing delayed gradients while computing new ones, $\\texttt{ACCO}$ reduces GPU idle time and supports heterogeneous hardware. To mitigate the convergence issues caused by delayed updates, we introduce a novel technique ensuring training dynamics align with standard distributed optimization. Compared to ZeRO-1, our approach is significantly faster and scales effectively across heterogeneous hardware.",
    "authors": [
      "~Adel_Nabli1",
      "~Louis_Fournier1",
      "~Pierre_ERBACHER1",
      "~Louis_Serrano1",
      "~Eugene_Belilovsky1",
      "~Edouard_Oyallon1"
    ],
    "pdf": "/pdf/f848af968d1c36e105b6d643f30262765a17fdf9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper addresses distributed training of LLMs, improves GPU utilization by reducing communication overhead during gradient synchronization, enhances scalability across multiple workers, focuses on memory efficiency, shows performance improvements over ZeRO-1, and supports heterogeneous hardware.",
      "Irrelevant Aspects": "The paper doesn't address inference optimization and focuses primarily on the training phase rather than both training and inference.",
      "Summary": "The paper presents ACCO, a memory-efficient optimization algorithm for distributed LLM training that synchronizes delayed gradients while computing new ones. It reduces GPU idle time, supports heterogeneous hardware, and introduces a technique to mitigate convergence issues. Compared to ZeRO-1, ACCO shows significant performance improvements and scales effectively across heterogeneous hardware."
    }
  },
  {
    "id": "EpgMSwJY8t",
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
    "abstract": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a *memory-efficient* and *exact* BP method called **StreamBP**, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by $2.8-5.5 \\times$ larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
    "authors": [
      "~Qijun_Luo3",
      "~Mengqi_Li5",
      "~Lei_Zhao8",
      "~Xiao_Li5"
    ],
    "pdf": "/pdf/29adc0353825da13026b2601d3a98fd39d9f3906.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses memory optimization during backpropagation for LLMs, which is a critical bottleneck in training. It proposes a method to reduce activation memory usage while maintaining exact backpropagation, enabling longer sequence training. The work focuses on training optimization with claims of 2.8-5.5× longer sequence support, faster BP speed, and less computational FLOPs compared to gradient checkpointing. It also includes distributed implementation for multi-GPU training, addressing scalability concerns. The method integrates easily with existing transformer training pipelines and works with common objectives like SFT, GRPO, and DPO.",
      "Irrelevant Aspects": "The paper focuses exclusively on training optimization without addressing inference optimization techniques. There is limited discussion about latency improvements, which is part of my expertise focus. The work doesn't cover interaction with other optimization techniques like mixed-precision training or hardware-specific optimizations. The distributed aspect appears less comprehensive than the single-GPU optimization.",
      "Summary": "StreamBP presents a memory-efficient backpropagation method that decomposes the chain rule along the sequence dimension to significantly reduce activation memory requirements. It enables training on longer sequences (2.8-5.5× longer than checkpointing) while using comparable or less time, improving GPU utilization and training throughput for LLMs. The method is exact, easily integrable, and has a distributed implementation, but focuses primarily on training rather than inference optimization."
    }
  },
  {
    "id": "zEj1FSYCRn",
    "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
    "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seam- lessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model’s performance on open benchmarks—without any training or healing steps, resulting in minimal computational overhead. We provide an open- source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
    "authors": [
      "~Dmitriy_Shopkhoev1",
      "~Ammar_Ali1",
      "~Magauiya_Zhussip1",
      "~Valentin_Malykh1",
      "~Stamatios_Lefkimmiatis1",
      "~Nikos_Komodakis1",
      "~Sergey_Zagoruyko1"
    ],
    "pdf": "/pdf/334128876882158c871dd4bf17ccf8f6555a3cfd.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on depth pruning of transformer models for inference optimization, specifically applied to large language models (LLMs). It aims to reduce computational requirements and overhead without additional training, which directly impacts GPU utilization, throughput, and latency. The technique achieves significant pruning (up to 25%) while maintaining high performance (90% of original), and provides an open-source implementation for practical applications.",
      "Irrelevant Aspects": "The approach has a narrower focus on depth pruning and block linearization rather than broader optimization techniques. It doesn't directly address GPU utilization specifically or distributed computing/scalability aspects in depth. As a training-free approach, it doesn't address training optimization, which is part of my research interest.",
      "Summary": "ReplaceMe presents a training-free depth pruning method for transformer models that replaces blocks with linear operations, achieving significant model compression (25%) while maintaining most performance (90%). The technique reduces computational overhead without requiring additional training, making it highly relevant to inference optimization for LLMs. While valuable for reducing latency and improving resource utilization, it has a narrower focus than my complete research interests, particularly around training optimization and scalability aspects."
    }
  },
  {
    "id": "UFqqqJZwM7",
    "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Train Faster",
    "abstract": "The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4\\% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82\\%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: \\url{https://github.com/bazyagami/LearningWithRevision}.",
    "authors": [
      "~Shriram_M_S1",
      "~Xinyue_Hao1",
      "~Shihao_Hou3",
      "~Yang_Lu5",
      "~Laura_Sevilla-Lara3",
      "~Anurag_Arnab1",
      "~Shreyank_N_Gowda1"
    ],
    "pdf": "/pdf/cf490de9cf7081c52fef93ce7371b23fde6e7f19.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training efficiency by reducing epochs to 12.4% of baseline, which could significantly improve GPU utilization and training throughput. The approach requires no changes to model architecture or optimizer, making it potentially applicable to large language models without modifying their structure. This aligns with my interest in training optimization for better resource utilization.",
      "Irrelevant Aspects": "The paper doesn't specifically address large language models, which are my primary focus. It also doesn't discuss inference optimization, distributed training across multiple GPUs/nodes, or latency optimization - all key aspects of my research. The approach seems more suited to single-node training scenarios rather than the distributed systems needed for modern LLMs.",
      "Summary": "This paper introduces 'Progressive Data Dropout,' a training optimization technique that reduces epochs while improving accuracy. It shows promise for enhancing GPU utilization during training by eliminating redundant data processing cycles. While applicable to training optimization, it lacks specific connections to large language models and doesn't address inference optimization or distributed system scalability, which limits its direct relevance to my core research interests."
    }
  },
  {
    "id": "9Hweh7Wx8R",
    "title": "Efficient and Generalizable Mixed-Precision Quantization via Topological Entropy",
    "abstract": "Network quantization effectively reduces both memory footprints and inference time of deep neural networks, enabling their deployment on resource-constrained devices. To fully utilize the multiple bit-width arithmetic operations of the hardware, mixed-precision quantization (MPQ) is developed to assign different bit-widths to each layer. However, the quantization policy obtained by existing MPQ methods struggles to achieve the objectives of efficiency and generalization simultaneously. In this paper, we propose an efficient and generalizable MPQ based on topological entropy (TE) (GMPQ-TE). Specifically, TE, derived from \\textit{topological data analysis}, effectively measures the quantization sensitivity of each layer by using the minibatch of data with the same label. Furthermore, we observe that TE remains consistent across various datasets and shows a strong correlation with both quantized model accuracy and bit-width. Thus, MPQ is formulated as a single-pass linear programming problem, obtaining a generalizable quantization policy in a few seconds (11s on MobileNet-V2). Extensive experiments show that the quantization policy obtained on CIFAR-10 can generalize to ImageNet and PASCAL VOC. GMPQ-TE achieves a competitive accuracy-complexity trade-off compared to state-of-the-art MPQ methods.",
    "authors": [
      "~Nan_Li6",
      "~Yonghui_Su1",
      "~Lianbo_Ma1"
    ],
    "pdf": "/pdf/a262420c37ba5124240001c0b55312fc6a2691d5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Mixed-precision quantization directly addresses inference optimization for better GPU utilization and scalability. The linear programming formulation achieving results in seconds (11s on MobileNet-V2) suggests potential for reducing quantization overhead. The approach's focus on efficiency, throughput improvement, and generalizability across datasets aligns with key optimization objectives for LLM deployment. The paper's quantization sensitivity measurement technique could potentially be adapted for transformer architectures.",
      "Irrelevant Aspects": "The paper exclusively evaluates on computer vision models (MobileNet-V2) and datasets (CIFAR-10, ImageNet, PASCAL VOC) with no mention of language models or transformer architectures. The topological entropy approach may face unique challenges when applied to attention mechanisms prevalent in LLMs. The research doesn't address training-specific optimizations, focusing primarily on inference deployment rather than the full training lifecycle.",
      "Summary": "This paper presents a novel mixed-precision quantization method using topological entropy to efficiently assign bit-widths to neural network layers. While demonstrating strong results in computer vision contexts with minimal computation time, its direct applicability to large language models remains uncertain. The quantization techniques are relevant to inference optimization goals, but the computer vision focus and lack of transformer-specific considerations limit its immediate relevance to LLM research."
    }
  },
  {
    "id": "cECo8tetzF",
    "title": "Restoring Pruned Large Language Models via Lost Component Compensation",
    "abstract": "Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models.",
    "authors": [
      "~Zijian_Feng2",
      "~Hanzhang_Zhou1",
      "~Zixiao_Zhu2",
      "~Tianjiao_Li2",
      "~Chua_Jia_Jim_Deryl2",
      "~Mak_Lee_Onn1",
      "~Gee_Wah_Ng1",
      "~Kezhi_Mao1"
    ],
    "pdf": "/pdf/1d315915657111b82a395d0523efc4d6ef6f4561.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Model pruning for inference efficiency, performance recovery after optimization, attention mechanism optimization, various pruning schemes (structured, semi-structured, unstructured), sparsity preservation, inference efficiency optimization",
      "Irrelevant Aspects": "Parameter-Efficient Fine-tuning (PEFT) techniques like LoRA, specific task performance recovery details, fine-tuning methodologies",
      "Summary": "RestoreLCC proposes a novel approach to recover performance in pruned LLMs through attention activation analysis and component compensation, directly addressing the critical trade-off between model efficiency and performance in LLM deployment optimization."
    }
  },
  {
    "id": "y2eWc6jrlu",
    "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation",
    "abstract": "As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate a single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces a next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-clean-token-prediction in the embedding space. We employ a probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images.",
    "authors": [
      "~Yao_Teng1",
      "~Fu-Yun_Wang1",
      "~Xian_Liu1",
      "~Zhekai_Chen1",
      "~Han_Shi1",
      "~Yu_Wang3",
      "~Zhenguo_Li1",
      "~Weiyang_Liu1",
      "~Difan_Zou1",
      "~Xihui_Liu1"
    ],
    "pdf": "/pdf/687ac997036b688b5040f3693447ac71e61256b1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference optimization for autoregressive models, focusing on reducing the number of model forward passes and enabling parallel token generation. This aligns with my interest in improving throughput and latency. The approach uses a system-level optimization technique (Jacobi-Denoising) that could potentially be adapted to improve GPU utilization in other autoregressive models.",
      "Irrelevant Aspects": "The paper focuses specifically on text-to-image models rather than general large language models. There's no mention of training optimization, only inference. The technique is specialized for visual generation tasks and doesn't explicitly discuss GPU utilization or scalability metrics, though these would be natural consequences of the parallelization approach.",
      "Summary": "This paper presents Speculative Jacobi-Denoising Decoding (SJD2), a method to accelerate autoregressive text-to-image generation by enabling parallel token generation through a denoising process. While highly relevant to inference optimization, its specialization for text-to-image models limits its direct applicability to general LLM systems. The approach shows promise for reducing forward passes and potentially improving GPU utilization, making it moderately relevant to my research interests."
    }
  },
  {
    "id": "BcKYVmh3yH",
    "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding",
    "abstract": "Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of-$N$ (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling:\n(1) Generating $N$ full samples consumes substantial GPU memory, reducing inference capacity under limited resources.\n(2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs.\nAlthough some studies have explored efficiency improvements, none have addressed both challenges at once.\nTo address this gap, we propose **Self-Truncation Best-of-$N$ (ST-BoN)**, a decoding method that avoids fully generating all $N$ samples and eliminates the need for reward models. It leverages early sampling consistency in the model’s internal states to identify the most promising path and truncate suboptimal ones.\nIn terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%.\nIn terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points.",
    "authors": [
      "~Yiming_Wang13",
      "~Pei_Zhang7",
      "~Siyuan_Huang10",
      "~Baosong_Yang1",
      "~Zhuosheng_Zhang1",
      "~Fei_Huang2",
      "~Rui_Wang10"
    ],
    "pdf": "/pdf/a95df403a5a9fd31e6e3524083ea4c3f8facd1c0.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models, focusing on Best-of-N sampling techniques that improve GPU utilization and reduce latency. The proposed Self-Truncation Best-of-N (ST-BoN) method reduces dynamic GPU memory usage by over 80% and inference latency by 50%, achieving better cost-performance trade-offs by saving 70-80% computational cost while maintaining performance. It eliminates the need for reward models, further reducing memory and latency overhead.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization techniques. It doesn't cover distributed training strategies, training pipeline efficiency, or model training data optimization aspects.",
      "Summary": "This paper is highly relevant to my research interests in LLM inference optimization, GPU utilization, and latency reduction. The ST-BoN method represents a significant advance in efficient test-time scaling for LLMs, directly addressing the critical challenge of balancing computational costs with performance during inference. While it doesn't cover training optimization, its contributions to inference efficiency align perfectly with my expertise in optimizing LLM deployment."
    }
  },
  {
    "id": "021PIPyOU1",
    "title": "ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation",
    "abstract": "Diffusion models have demonstrated exceptional capabilities in generating high-fidelity images. However, their iterative denoising process results in significant computational overhead during inference, limiting their practical deployment in resource-constrained environments. \nExisting acceleration methods often adopt uniform strategies that fail to capture the temporal variations during diffusion generation, while the commonly adopted sequential $\\textit{pruning-then-fine-tuning strategy}$ suffers from sub-optimality due to the misalignment between pruning decisions made on pretrained weights and the model’s final parameters. To address these limitations, we introduce $\\textbf{ALTER}$: $\\textbf{A}$ll-in-One $\\textbf{L}$ayer Pruning and $\\textbf{T}$emporal $\\textbf{E}$xpoert $\\textbf{R}$outing, a unified framework that transforms diffusion models into a mixture of efficient temporal experts.\nALTER achieves a single-stage optimization that unifies layer pruning, expert routing, and model fine-tuning by employing a trainable hypernetwork, which dynamically generates layer pruning decisions and manages timestep routing to specialized, pruned expert sub-networks throughout the ongoing fine-tuning of the UNet. This unified co-optimization strategy enables significant efficiency gains while preserving high generative quality. Specifically, ALTER achieves same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model while utilizing only 25.9\\% of its total MACs with just 20 inference steps and delivering a 3.64$\\times$ speedup through 35\\% sparsity.",
    "authors": [
      "~Xiaomeng_Yang2",
      "~Lei_Lu4",
      "~Qihui_Fan1",
      "~Changdi_Yang1",
      "~Juyi_Lin1",
      "~Yanzhi_Wang3",
      "~Xuan_Zhang1",
      "~Shangqian_Gao1"
    ],
    "pdf": "/pdf/ca150bb4f8f17fc601502832f57cd1fb21ab7b68.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization techniques, computational efficiency improvements (3.64× speedup), resource utilization strategies (25.9% of MACs), unified optimization approach combining pruning and routing, model compression for deployment in resource-constrained environments",
      "Irrelevant Aspects": "Focus on diffusion models rather than language models, UNet architecture instead of transformer-based LLMs, limited discussion of GPU-specific optimizations, primarily addresses image generation rather than text processing, no emphasis on training optimization",
      "Summary": "ALTER introduces a unified framework for efficient diffusion model inference through layer pruning and temporal expert routing. While achieving significant computational savings (3.64× speedup with 35% sparsity), the paper focuses on image generation models rather than language models. The optimization techniques for inference efficiency and resource utilization are relevant to my expertise, though the architectural differences limit direct applicability to LLMs."
    }
  },
  {
    "id": "LiQH1MOCMs",
    "title": "Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy",
    "abstract": "In this work, we first revisit the sampling issues in current autoregressive (AR) image generation models and identify that image tokens, unlike text tokens, exhibit lower information density and non-uniform spatial distribution. Accordingly, we present an entropy-informed decoding strategy that facilitates higher autoregressive generation quality with faster synthesis speed. Specifically, the proposed method introduces two main innovations: 1) dynamic temperature control guided by spatial entropy of token distributions, enhancing the balance between content diversity, alignment accuracy, and structural coherence in both mask-based and scale-wise models, without extra computational overhead, and 2) entropy-aware acceptance rules in speculative decoding, achieving near-lossless generation at about 85% of the inference cost of conventional acceleration methods. Extensive experiments across multiple benchmarks using diverse AR image generation models demonstrate the effectiveness and generalizability of our approach in enhancing both generation quality and sampling speed.",
    "authors": [
      "~Xiaoxiao_Ma3",
      "~Feng_Zhao6",
      "~Pengyang_Ling1",
      "~Haibo_Qiu1",
      "~Zhixiang_Wei1",
      "~Hu_Yu2",
      "~Jie_Huang4",
      "~Zhixiong_Zeng1",
      "~Lin_Ma2"
    ],
    "pdf": "/pdf/0ad8a552febbccc573ef7dd169e022140d5de61c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for autoregressive models, which directly aligns with my expertise in LLM optimization. It addresses sampling speed improvements, speculative decoding techniques, and achieving better efficiency without extra computational overhead - all key concerns in my field of GPU utilization and scalability. The paper's approach to reducing inference costs (85% of conventional methods) and improving throughput is highly relevant to my research on optimization techniques.",
      "Irrelevant Aspects": "The paper specifically targets image generation rather than text/language processing, which represents a significant domain difference. The unique characteristics of image tokens (lower information density and non-uniform spatial distribution) may not generalize to text tokens. The benchmarks and model architectures discussed appear to be specific to image generation tasks, limiting direct applicability to language models.",
      "Summary": "This paper presents entropy-informed decoding strategies to improve autoregressive image generation models, focusing on dynamic temperature control and entropy-aware acceptance rules in speculative decoding. While image generation is a different domain, the optimization techniques for improving sampling efficiency, reducing inference costs, and enhancing speed without additional overhead offer potentially transferable insights for LLM inference optimization. The paper's approach to balancing quality and performance metrics aligns well with my research interests in maximizing GPU utilization and scalability."
    }
  },
  {
    "id": "tM5mjMfFmS",
    "title": "DuSA: Fast and Accurate Dual-Stage Sparse Attention Mechanism Accelerating Both Training and Inference",
    "abstract": "This paper proposes the Dual-Stage Sparse Attention (DuSA) mechanism for attention acceleration of transformers. In the first stage, DuSA performs intrablock sparse attention to aggregate local inductive biases. In the second stage, DuSA performs interblock sparse attention to obtain long-range dependencies. Both stages have low computational complexity and can be further accelerated by memory acceleration attention mechanisms directly, which makes DuSA faster than some extremely fast attention mechanisms. The dual-stage sparse attention design provides a lower error in approximating vanilla scaled-dot product attention than the basic single-stage sparse attention mechanisms and further advances the basic sparse attention mechanisms to match or even outperform vanilla scaled-dot product attention. Even in some plug and play situations, DuSA can still maintain low performance loss. DuSA can be used in both training and inference acceleration. DuSA achieves leading performance in different benchmarks: long range arena, image classification, semantic segmentation, object detection, text to video generation, and long context understanding, and accelerates models of different sizes.",
    "authors": [
      "~Chong_Wu4",
      "~Jiawang_Cao1",
      "~Renjie_Xu1",
      "~Zhuoheng_Ran1",
      "~Maolin_Che2",
      "~Wenbo_Zhu4",
      "~Hong_Yan2"
    ],
    "pdf": "/pdf/a296fb31589d28d7e2ad3700c5867b2aa12a7f81.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper proposes a dual-stage sparse attention mechanism that directly accelerates transformer models in both training and inference phases. It focuses on computational efficiency and memory optimization, which are critical for GPU utilization. The mechanism is designed to work with models of different sizes, addressing scalability concerns. The paper claims faster performance than existing attention mechanisms while maintaining accuracy, which relates to higher throughput and lower latency goals. DuSA's design for long-range dependencies and low computational complexity is particularly relevant to large language model optimization.",
      "Irrelevant Aspects": "The paper covers various application domains beyond language models (image classification, semantic segmentation, object detection, text-to-video generation), which dilutes its specific focus on LLMs. There's no explicit mention of distributed training strategies, quantization techniques, or other advanced LLM-specific optimization methods. The paper doesn't appear to address hardware-specific optimizations or the interaction between the sparse attention mechanism and specific GPU architectures.",
      "Summary": "DuSA presents a novel sparse attention mechanism that divides attention computation into two efficient stages: intrablock sparse attention for local context and interblock sparse attention for long-range dependencies. This approach maintains accuracy while significantly reducing computational complexity, enabling faster training and inference of transformer models. The method demonstrates strong performance across multiple benchmarks and model sizes, making it a promising technique for optimizing LLM systems with better resource utilization, improved throughput, and reduced latency."
    }
  },
  {
    "id": "hzBqQZK2iV",
    "title": "Uni-LoRA: One Vector is All You Need",
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space R^D, can be reconstructed through a projection from a subspace R^d, with d << D. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, P ∈ R^{D×d}. \nMost existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM -- making Uni-LoRA both a unified framework and a “one-vector-only” solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.",
    "authors": [
      "~Kaiyang_Li1",
      "~Shaobo_Han1",
      "~Qing_Su1",
      "~Wei_Li8",
      "~Zhipeng_Cai1",
      "~Shihao_Ji1"
    ],
    "pdf": "/pdf/d6394e5a1af214d98d759f8e5a95139a93fbbde9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-efficient fine-tuning for LLMs, memory optimization during training, reduction in trainable parameters which can improve GPU utilization, introduction of efficient projection matrix that enables global parameter sharing",
      "Irrelevant Aspects": "Limited focus on inference optimization, lack of system-level implementation details, primary focus on theoretical framework rather than practical system performance metrics",
      "Summary": "Uni-LoRA presents a unified framework for LoRA variants that requires only a single trainable vector to reconstruct LoRA parameters for entire LLMs, potentially improving parameter efficiency during fine-tuning but with limited direct insights into inference optimization or system-level performance metrics."
    }
  },
  {
    "id": "XyCDB1Uiqa",
    "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead",
    "abstract": "The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss.\nWhile significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities.\nMotivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam.\nWe evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization.",
    "authors": [
      "~Marlon_Becker1",
      "~Frederick_Altrock1",
      "~Benjamin_Risse1"
    ],
    "pdf": "/pdf/22751c624b5d460d5378616b7e9c8cbc0dfcfd5c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization for deep neural networks, computational efficiency improvements over existing methods, reduction in computational overhead, optimization algorithm innovation, memory demand considerations, potential for better GPU utilization",
      "Irrelevant Aspects": "No specific focus on large language models, lack of inference optimization discussion, no explicit mention of GPU scalability, throughput, or latency metrics in the abstract",
      "Summary": "Momentum-SAM (MSAM) is proposed as an optimization algorithm that achieves the benefits of Sharpness Aware Minimization (finding flat minima for better generalization) without the computational overhead. By perturbing parameters in the direction of accumulated momentum (inspired by Nesterov Accelerated Gradient), MSAM aims to improve generalization while maintaining computational efficiency similar to SGD or Adam, making it potentially valuable for training large models."
    }
  },
  {
    "id": "m4sQcq5UK5",
    "title": "Gatekeeper: Improving Model Cascades Through Confidence Tuning",
    "abstract": "Large-scale machine learning models deliver strong performance across a wide range of tasks but come with significant computational and resource constraints. To mitigate these challenges, local smaller models are often deployed alongside larger models, relying on routing and deferral mechanisms to offload complex tasks. However, existing approaches inadequately balance the capabilities of these models, often resulting in unnecessary deferrals or sub-optimal resource usage. In this work, we introduce a novel loss function called Gatekeeper for calibrating smaller models in cascade setups. Our approach fine-tunes the smaller model to confidently handle tasks it can perform correctly while deferring complex tasks to the larger model. Moreover, it incorporates a mechanism for managing the trade-off between model performance and deferral accuracy and is broadly applicable across various tasks and domains without any architectural changes. We evaluated our method on encoder-only, decoder-only, and encoder-decoder architectures. Experiments across image classification, language modeling, and vision-language tasks show that our approach substantially improves deferral performance.",
    "authors": [
      "~Stephan_Rabanser1",
      "~Nathalie_Rauschmayr1",
      "~Achin_Kulshrestha1",
      "~Petra_Poklukar1",
      "~Wittawat_Jitkrittum1",
      "~Sean_Augenstein1",
      "~Congchao_Wang1",
      "~Federico_Tombari1"
    ],
    "pdf": "/pdf/55ad53ad9827ad9b34e26f318ac4c05d3dbd78aa.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational and resource constraints of large-scale models through model cascades, which relates to my focus on inference optimization and resource utilization. The Gatekeeper approach aims to optimize the routing between smaller and larger models, which can improve throughput and reduce latency by using smaller models when appropriate. The method is tested on decoder-only architectures used in LLMs and language modeling tasks. The deferral mechanism directly contributes to better resource management and could potentially improve GPU utilization by reserving larger models for only the most complex tasks.",
      "Irrelevant Aspects": "The paper includes evaluation on image classification and vision-language tasks which fall outside my LLM focus. The abstract doesn't explicitly address GPU utilization metrics or scalability considerations in depth. The approach focuses more on model routing and deferral rather than the specific training optimizations and inference acceleration techniques I primarily research. There's limited discussion of memory optimization techniques or distributed computing strategies specific to LLM training/inference.",
      "Summary": "This paper introduces Gatekeeper, a novel loss function for calibrating smaller models in cascade setups with larger models. It aims to optimize the routing between models by improving the confidence calibration of smaller models, enabling them to handle appropriate tasks while deferring complex ones to larger models. The approach is architecture-agnostic and improves deferral performance across multiple tasks including language modeling. While relevant to inference optimization and resource management, it doesn't directly address the specific training optimization and GPU utilization techniques that are the primary focus of my research."
    }
  },
  {
    "id": "zNLlglSOwD",
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
    "abstract": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at \\url{https://github.com/yuezhouhu/adaspec}.",
    "authors": [
      "~Yuezhou_Hu1",
      "~Jiaxin_Guo4",
      "~Xinyu_Feng1",
      "~Tuo_Zhao2"
    ],
    "pdf": "/pdf/4169ec79fb39e606daf2863e8f46df0a76009e66.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative decoding, a technique to accelerate LLM inference, which directly relates to inference optimization and latency reduction. It introduces AdaSPEC, a novel method that improves token acceptance rates through selective knowledge distillation, leading to better throughput. The approach specifically addresses the alignment between draft and target models in speculative decoding, which is critical for efficient inference systems. The paper demonstrates consistent performance improvements across multiple tasks and model configurations, which suggests practical value for deployment scenarios.",
      "Irrelevant Aspects": "The paper doesn't directly address GPU utilization optimization or scalability aspects of LLM systems. There's limited discussion of memory optimization techniques or parallelization strategies. The focus is primarily on the algorithmic improvement of speculative decoding rather than system-level optimizations that would affect GPU efficiency. There's no explicit discussion of quantization, pruning, or other compression techniques that could impact memory footprint.",
      "Summary": "AdaSPEC introduces a selective knowledge distillation approach to improve speculative decoding for large language models. By identifying and filtering difficult-to-fit tokens during distillation, the method creates better-aligned draft models that achieve higher token acceptance rates (up to 15% improvement). While the paper doesn't explicitly address GPU utilization or scalability, the inference acceleration through improved speculative decoding directly contributes to lower latency and higher throughput, which are key aspects of efficient LLM deployment."
    }
  },
  {
    "id": "8nOMhDFpkU",
    "title": "Traversal Verification for Speculative Tree Decoding",
    "abstract": "Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results on various models and multiple tasks demonstrate that our method consistently improves acceptance length and throughput over token-level verification.",
    "authors": [
      "~Yepeng_Weng1",
      "~Qiao_Hu2",
      "~Xujie_Chen1",
      "~Li_Liu17",
      "~Dianwen_Mei2",
      "~Huishi_Qiu2",
      "~Jiang_Tian1",
      "~zhongchao_shi1"
    ],
    "pdf": "/pdf/7857ef7b5c58a4e8c2e23bbb2c89cc3ef23dedb1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative decoding for accelerating large language models, which directly relates to inference optimization. It introduces a novel verification mechanism that claims to improve acceptance rates and throughput, which aligns with my research interests in increasing LLM inference efficiency. The approach claims to provide substantial acceleration gains while maintaining lossless inference, which is valuable for practical deployment. The paper tests on various models and tasks, demonstrating consistent improvements.",
      "Irrelevant Aspects": "The paper doesn't address training optimization, which is another key area of my research focus. There's no explicit discussion of GPU utilization or scalability aspects. While latency implications exist due to throughput improvements, they aren't directly addressed. The paper is specific to speculative decoding verification rather than broader optimization strategies.",
      "Summary": "This paper introduces 'Traversal Verification,' a novel verification method for speculative decoding in LLMs that improves acceptance rates and throughput through a leaf-to-root traversal approach. It directly addresses inference acceleration, a core aspect of my research, but doesn't cover training optimization or explicitly discuss GPU utilization. The technique maintains theoretical correctness while providing performance gains, making it highly relevant to my focus on inference efficiency."
    }
  },
  {
    "id": "VBx4yMNtjt",
    "title": "Scaling Law with Learning Rate Annealing",
    "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation accounts for two main effects: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Unlike previous studies that only fit losses at final steps, our formulation captures the entire training curve, allowing for parameter fitting using losses from any training step. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step under any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures and can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical insights into empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while democratizing scaling laws, and it is helpful to guide both research and industrial participants in refining training strategies for further LLMs.",
    "authors": [
      "~Howe_Tissue1",
      "~Venus_Wang1",
      "~Lu_Wang26"
    ],
    "pdf": "/pdf/6bd9ed2c31ee01c7daf58ebc1bc8e2ec6d4e6d2c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization for LLMs, scaling laws, learning rate scheduling, computational efficiency reduction, training dynamics understanding, applicability across architectures and hyperparameters",
      "Irrelevant Aspects": "No focus on inference optimization, no direct addressing of GPU utilization, no discussion of latency reduction, limited throughput optimization discussion",
      "Summary": "This paper introduces a novel scaling law for neural language models that accounts for learning rate annealing throughout the training process. It proposes a formulation that captures the entire training curve rather than just final losses, enabling prediction of loss at any step under any learning rate scheduler with significantly reduced computational cost. The research provides valuable insights into LLM training dynamics that could inform more efficient training strategies, which is directly relevant to training optimization. While it doesn't address inference optimization or latency reduction, its focus on training efficiency and scalability makes it highly relevant to my research interests in LLM training optimization, though slightly less comprehensive since it doesn't cover the full training-to-inference pipeline."
    }
  },
  {
    "id": "A2pmNL7L1E",
    "title": "Depth-Width Tradeoffs for Transformers on Graph Tasks",
    "abstract": "Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems.  This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. \nWe empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware.",
    "authors": [
      "~Gilad_Yehudai2",
      "~Clayton_Sanford1",
      "~Maya_Bechler-Speicher1",
      "~Orr_Fischer1",
      "~Ran_Gilad-Bachrach2",
      "~Amir_Globerson1"
    ],
    "pdf": "/pdf/6eaaa5be6a3149a0b867a8770aed003688ae2d76.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates depth-width trade-offs in transformer models, which directly relates to model architecture optimization for better performance. The finding that wider but shallower models can achieve faster training and inference times due to better hardware parallelization is highly relevant to my interests in GPU utilization, throughput, and latency optimization. The empirical investigation of these trade-offs provides practical insights for model design decisions that impact efficiency.",
      "Irrelevant Aspects": "The paper's focus on graph-based tasks rather than general language model applications reduces its direct relevance. The theoretical analysis of minimal transformer sizes for specific algorithmic problems is less directly applicable to practical training and inference optimization. The paper doesn't explore specific training optimization techniques or detailed GPU utilization strategies.",
      "Summary": "This paper explores depth-width trade-offs in transformers for graph tasks, finding that wider but shallower models can match the performance of deeper models while offering faster training and inference times due to better hardware parallelization. While the focus on graph tasks and theoretical aspects reduces direct relevance, the empirical insights on architectural efficiency trade-offs provide valuable information for optimizing transformer models for better GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "SVHerutWxp",
    "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
    "abstract": "This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO). GRPO, while effective, incurs high training costs due to the need to sample multiple completions for each question. Our experiment and theoretical analysis reveal that the number of completions impacts model accuracy yet increases training time multiplicatively, and not all completions contribute equally to policy training---their contribution depends on their relative advantage. To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates. Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency. Experiments show that CPPO achieves up to $7.98\\times$ speedup on GSM8K and $3.48\\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO. We release our code at https://github.com/lzhxmu/CPPO.",
    "authors": [
      "~ZhiHang_Lin1",
      "~Mingbao_Lin1",
      "~Yuan_Xie10",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/dc4f449811faf7b76a974cc460c639c5a895a718.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization, GPU utilization improvement, throughput enhancement, resource efficiency, acceleration of LLM training, dynamic completion allocation strategy for maximizing GPU usage",
      "Irrelevant Aspects": "Specific focus on Group Relative Policy Optimization rather than broader LLM optimization, no mention of inference optimization techniques, lack of direct latency analysis",
      "Summary": "CPPO introduces a method to accelerate training of GRPO-based reasoning models by pruning low-advantage completions and implementing dynamic completion allocation. The approach achieves significant speedups (up to 7.98×) while preserving accuracy, directly addressing training efficiency and GPU utilization optimization, though it doesn't cover inference optimization or latency improvements."
    }
  },
  {
    "id": "cxb5EsQHW3",
    "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity—that is, how frequently the verifier is invoked during generation, \nbeyond verifying only the final output or individual generation steps.\nTo this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter $g$. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting $g$ can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
    "authors": [
      "~Hao_Mark_Chen1",
      "~Guanxi_Lu1",
      "~Yasuyuki_Okoshi1",
      "~Zhiwen_Mo1",
      "~Masato_Motomura1",
      "~Hongxiang_Fan1"
    ],
    "pdf": "/pdf/007919ac50b98a7b9200159433b2b419d1b7327b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on compute-efficient test-time scaling (TTS) for LLMs, addressing GPU utilization and inference optimization. Introduces Variable Granularity Search (VG-Search) to balance verification frequency with computational cost. Demonstrates significant FLOPs reduction (52%) while improving accuracy, directly impacting throughput and efficiency. Addresses the performance-computation trade-off critical for optimized inference systems. The adaptive strategies for dynamic granularity selection align with scalable system optimization goals.",
      "Irrelevant Aspects": "Primarily focuses on test-time/inference optimization rather than training optimization. Lacks explicit discussion of GPU-specific optimizations or multi-GPU distributed systems. Limited focus on latency metrics, though FLOPs reduction would indirectly improve latency. No mention of memory optimization techniques during inference. Doesn't address model architecture changes or quantization strategies that could further enhance efficiency.",
      "Summary": "This paper presents a novel approach to optimize LLM inference efficiency through Variable Granularity Search (VG-Search), which generalizes beam search and Best-of-N sampling by tuning verification granularity during generation. The method achieves accuracy improvements (3.1% over Beam Search, 3.6% over Best-of-N) while reducing computational costs by over 52%. The adaptive strategies for dynamic granularity selection are particularly relevant for GPU utilization and scalable inference systems, though the focus remains on algorithmic optimization rather than hardware-specific implementations."
    }
  },
  {
    "id": "lwIQC4MVJZ",
    "title": "Efficient Large Language Model Inference with Neural Block Linearization",
    "abstract": "The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce *Neural Block Linearization* (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in *DeepSeek-R1-Distill-Llama-8B* increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.",
    "authors": [
      "~Mete_Erdogan1",
      "~Francesco_Tonin1",
      "~Volkan_Cevher1"
    ],
    "pdf": "/pdf/f20ae3e2f2904be154a733cdd3e06fbcfc68213f.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focus on accelerating transformer inference, addressing self-attention computational bottlenecks, practical approach without requiring model retraining, demonstrated 32% speed improvement with minimal accuracy loss, applicable to large-scale models (8B parameters), theoretical framework with error bounds",
      "Irrelevant Aspects": "Limited discussion on hardware-specific optimizations, minimal focus on distributed inference across multiple GPUs, heavy mathematical theory with less practical implementation guidance",
      "Summary": "This paper introduces Neural Block Linearization (NBL), a framework that replaces self-attention layers with linear approximations to accelerate LLM inference. Using theoretical error bounds to guide substitution, NBL achieves significant speed improvements (32% on an 8B model) with minimal accuracy degradation without requiring fine-tuning. The work directly addresses key inference optimization challenges and offers a practical approach for improving GPU utilization and throughput in production environments."
    }
  },
  {
    "id": "LkqK19tqgx",
    "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding",
    "abstract": "We propose TPP-SD, a novel approach that accelerates Transformer temporal point process (TPP) sampling by adapting speculative decoding (SD) techniques from language models. By identifying the structural similarities between thinning algorithms for TPPs and speculative decoding for language models, we develop an efficient sampling framework that leverages a smaller draft model to generate multiple candidate events, which are then verified by the larger target model. TPP-SD maintains the same output distribution as autoregressive sampling while achieving significant acceleration. Experiments on both synthetic and real datasets demonstrate that our approach produces samples from identical distributions as standard methods, but with 2-6$\\times$ speedup. Our ablation studies analyze the impact of hyperparameters such as draft length and draft model size on sampling efficiency. TPP-SD bridges the gap between powerful Transformer TPP models and the practical need for rapid sequence generation.",
    "authors": [
      "~Shukai_Gong1",
      "~YIYANG_FU1",
      "~Fengyuan_Ran1",
      "~Quyu_Kong1",
      "~Feng_Zhou9"
    ],
    "pdf": "/pdf/fe36aa837f3c952ae94cacfdc0a5d79ff0f60999.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on speculative decoding, an inference optimization technique directly relevant to my interest in large language model inference optimization. It achieves 2-6× speedup, addressing the need for higher throughput and lower latency. The approach uses a smaller draft model with a larger target model, which is a technique for improving GPU utilization and scalability. It bridges the gap between powerful Transformer models and practical deployment needs, which aligns with my research on making large models efficient and scalable.",
      "Irrelevant Aspects": "The paper applies to temporal point processes (TPPs) rather than traditional language models, which is a specialized application domain. The evaluation focuses on event sequences in continuous time rather than typical NLP tasks. While the techniques might be transferable, the thinning algorithms mentioned are specific to TPP sampling, not general to language model optimization.",
      "Summary": "TPP-SD adapts speculative decoding from language models to accelerate Transformer temporal point process sampling, achieving 2-6× speedup while maintaining output distribution quality. The approach uses a smaller draft model to generate candidate events verified by a larger target model. While the application domain (temporal point processes) is somewhat specialized, the techniques and insights on inference optimization, throughput improvement, and the draft/target model framework are highly relevant to large language model optimization research."
    }
  },
  {
    "id": "RmqWt1btxQ",
    "title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness",
    "abstract": "Large Language Model (LLM) agents are capable of task execution across various domains by autonomously interacting with environments and refining LLM responses based on feedback.\nHowever, existing model serving systems are not optimized for the unique demands of serving agents. Compared to classic model serving, agent serving has different characteristics:\npredictable request pattern, increasing quality requirement, and unique prompt formatting. We identify a key problem for agent serving: LLM serving systems lack session-awareness. They neither perform effective KV cache management nor precisely select the cheapest yet competent model in each round.\nThis leads to a cost-quality tradeoff, and we identify an opportunity to surpass it in an agent serving system.\n\nTo this end, we introduce AgServe for AGile AGent SERVing.\nAgServe features a session-aware server that boosts KV cache reuse via Estimated-Time-of-Arrival-based eviction and in-place positional embedding calibration, a quality-aware client that performs session-aware model cascading through real-time quality assessment, and a dynamic resource scheduler that maximizes GPU utilization. \nWith AgServe, we allow agents to select and upgrade models during the session lifetime, and to achieve similar quality at much lower costs, effectively transcending the tradeoff. Extensive experiments on real testbeds demonstrate that AgServe (1) achieves comparable response quality to GPT-4o at a 16.5\\% cost. (2) delivers 1.8$\\times$ improvement in quality relative to the tradeoff curve.",
    "authors": [
      "~Yanyu_Ren1",
      "~Li_Chen24",
      "~Dan_Li14",
      "~Xizheng_Wang1",
      "~Zhiyuan_Wu1",
      "~Yukai_Miao1",
      "~Yu_Bai5"
    ],
    "pdf": "/pdf/27eca12a3d907e412808e51efe4a0b94f9e7353a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on LLM serving optimization, particularly for agent systems. It addresses KV cache management and model selection to improve efficiency. It introduces session-awareness to boost KV cache reuse and in-place positional embedding calibration. The system includes a dynamic resource scheduler to maximize GPU utilization and performs session-aware model cascading for cost reduction. The approach achieves comparable quality to GPT-4o at only 16.5% of the cost and delivers 1.8x quality improvement relative to the tradeoff curve.",
      "Irrelevant Aspects": "The paper focuses specifically on agent serving rather than general LLM serving. It doesn't address training optimization, only inference optimization. There's no explicit mention of scalability to large systems or distributed serving. Throughput and latency metrics aren't mentioned in the abstract. The solution is tailored to the characteristics of agent serving (predictable request patterns, increasing quality requirements), which may not generalize to all LLM serving scenarios.",
      "Summary": "AgServe introduces a session-aware approach to LLM agent serving that optimizes KV cache management and model selection. It implements Estimated-Time-of-Arrival-based cache eviction and in-place positional embedding calibration to boost cache reuse. The system includes a quality-aware client for session-aware model cascading and a dynamic resource scheduler to maximize GPU utilization. The result is a system that can transcend the traditional cost-quality tradeoff in agent serving, achieving GPT-4o level quality at only 16.5% of the cost while providing 1.8x better quality than existing solutions on the tradeoff curve."
    }
  },
  {
    "id": "Xh3AIqtOHM",
    "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
    "abstract": "Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our controlled study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long-context tasks. Through extensive experiments, we pretrain 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines.",
    "authors": [
      "~Tongyao_Zhu2",
      "~Qian_Liu2",
      "~Haonan_Wang1",
      "~Shiqi_Chen3",
      "~Xiangming_Gu1",
      "~Tianyu_Pang1",
      "~Min-Yen_Kan1"
    ],
    "pdf": "/pdf/66cbd2bd5a19c2cc0d7e00af667a4bff9512a722.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses LLM training optimization with a focus on improving efficiency and performance. The proposed SkyLadder approach achieves up to 22% faster training speeds, which is highly relevant to my research on training optimization. It also works with different model sizes (1B and 3B parameters), relating to scalability concerns. The context window scheduling approach represents a novel technique for optimizing resource utilization during pretraining.",
      "Irrelevant Aspects": "The paper does not address inference optimization, which is a key part of my research interests. It lacks explicit discussion of GPU utilization metrics, throughput, or latency measurements. The focus is primarily on model performance rather than system-level optimizations or infrastructure considerations.",
      "Summary": "SkyLadder presents a context window scheduling approach that transitions from short to long contexts during pretraining, achieving both performance gains (up to 3.7%) and significant training speed improvements (up to 22%). While highly relevant to training optimization aspects of my research, it doesn't cover inference optimization or detailed system-level performance metrics that would make it more comprehensive for my interests."
    }
  },
  {
    "id": "LbNL8xGai2",
    "title": "S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning",
    "abstract": "Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S’MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Conceptually, S’MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S’MoRE emulates the capacity of numerous experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S’MoRE’s residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S’MoRE improves structural flexibility of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S’MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation. Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.",
    "authors": [
      "~Hanqing_Zeng1",
      "~Yinglong_Xia1",
      "~Zhuokai_Zhao1",
      "~Chuan_Jiang1",
      "~Qiang_Zhang29",
      "~Jiayi_Liu1",
      "~Qunshu_Zhang2",
      "~Lizhu_Zhang2",
      "~Xiangjun_Fan1",
      "~Benyu_Zhang1"
    ],
    "pdf": "/pdf/6a52d860cb672fec3ef5dfb8132b7346892361d0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-efficient fine-tuning of LLMs, LoRA (low-rank adaptations), Mixture-of-Experts (MoE), hierarchical low-rank decomposition, efficient LLM adaptation, structural flexibility under parameter budgets, improved GPU utilization during training",
      "Irrelevant Aspects": "Limited focus on inference optimization, no explicit mention of throughput or latency, theoretical connection to Graph Neural Networks, primary focus on model architecture rather than system optimization",
      "Summary": "S'MoRE presents a novel approach combining LoRA's efficiency with MoE's flexibility through hierarchical low-rank decomposition of expert weights. While highly relevant to training optimization and parameter efficiency (affecting GPU utilization), it has less direct focus on inference optimization, throughput, or latency concerns. The paper's contribution to parameter-efficient fine-tuning aligns well with training optimization aspects of the research interest, making it moderately to highly relevant overall."
    }
  },
  {
    "id": "d1dL1ymD6N",
    "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
    "abstract": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large language models (LLMs), with its effectiveness influenced by two key factors: rank selection and weight initialization. While numerous LoRA variants have been proposed to improve performance by addressing one of these aspects, they often compromise usability or computational efficiency. In this paper, we analyze and identify the core limitations of existing approaches and propose a novel framework—**GoRA** (**G**radient-driven Adaptive L**o**w **R**ank **A**daptation)—that simultaneously adapts both the rank and initialization strategy within a unified framework. GoRA leverages gradient information during training to dynamically assign optimal ranks and initialize low-rank adapter weights in an adaptive manner. To our knowledge, GoRA is the first method that not only addresses the limitations of prior approaches—which often focus on either rank selection or initialization in isolation—but also unifies both aspects within a single framework, enabling more effective and efficient adaptation. Extensive experiments across various architectures and modalities show that GoRA consistently outperforms existing LoRA-based methods while preserving the efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning, GoRA achieves a 5.13-point improvement over standard LoRA and even outperforms full fine-tuning by 2.05 points under high-rank settings. Code is available at: https://github.com/hhnqqq/MyTransformers.",
    "authors": [
      "~haonan_he5",
      "~Peng_Ye4",
      "~Yuchen_Ren1",
      "~yuan_yuan23",
      "~LuyangZhou1",
      "~ShucunJu1",
      "~lei_chen41"
    ],
    "pdf": "/pdf/8fcd7715235c728bec5bdc382aecf6a74347bcfa.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Efficient fine-tuning of large language models, Parameter-efficient training methods, LoRA optimization, Gradient-based adaptation algorithms, Maintaining computational efficiency while improving performance, Applied to large models (Llama3.1-8B)",
      "Irrelevant Aspects": "No explicit focus on GPU utilization metrics, Limited discussion of distributed training optimization, Minimal coverage of inference optimization techniques, Lack of system-level implementation details, No throughput or latency measurements",
      "Summary": "GoRA introduces an enhancement to LoRA that simultaneously adapts both rank selection and weight initialization using gradient information during training. While it offers significant improvements in parameter-efficient fine-tuning and maintains computational efficiency, the paper focuses primarily on algorithmic improvements rather than system-level optimizations. The work is relevant for training optimization but lacks explicit coverage of GPU utilization, scalability considerations, and inference performance that would be of primary interest to my research focus."
    }
  },
  {
    "id": "8JwMjKDppz",
    "title": "SNAP: Low-Latency Test-Time Adaptation with Sparse Updates",
    "abstract": "Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1\\% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12\\%, while keeping the accuracy drop below 3.3\\%, even across adaptation rates ranging from 1\\% to 50\\%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.",
    "authors": [
      "~Hyeongheon_Cha1",
      "~Dong_Min_Kim1",
      "~Hye_Won_Chung2",
      "~Taesik_Gong1",
      "~Sung-Ju_Lee1"
    ],
    "pdf": "/pdf/43685888501b39aa03306ac4409f563aa5b0773d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Low-latency adaptation techniques, sparse updates for efficiency, 93.12% latency reduction, memory management strategies during inference, batch normalization optimization, computational cost reduction methods, edge deployment optimizations for ML models",
      "Irrelevant Aspects": "Focus on computer vision tasks rather than language models, test-time adaptation rather than training optimization, extreme resource constraints of edge devices vs typical GPU optimization, distribution shift handling in non-linguistic contexts",
      "Summary": "SNAP introduces a sparse test-time adaptation framework that significantly reduces latency (up to 93.12%) while maintaining accuracy by using only 1-50% of data for adaptation. It employs Class and Domain Representative Memory and Inference-only Batch-aware Memory Normalization to efficiently adjust models for distribution shifts. While not specifically targeting LLMs, its techniques for sparse updates and inference-time normalization have potential relevance to model optimization challenges."
    }
  },
  {
    "id": "T62TYoF8R3",
    "title": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion",
    "abstract": "Diffusion generative models have become the standard for producing high-quality, coherent video content, yet their slow inference speeds and high computational demands hinder practical deployment. Although both quantization and sparsity can independently accelerate inference while maintaining generation quality, naively combining these techniques in existing training-free approaches leads to significant performance degradation, as they fail to achieve proper joint optimization.\nWe introduce FPSAttention, a novel training-aware co-design of FP8 quantization and Sparsity for video generation, with a focus on the 3D bi-directional attention mechanism. Our approach features three key innovations: 1) A unified 3D tile-wise granularity that simultaneously supports both quantization and sparsity. 2) A denoising step-aware strategy that adapts to the noise schedule, addressing the strong correlation between quantization/sparsity errors and denoising steps. 3) A native, hardware-friendly kernel that leverages FlashAttention and is implemented with optimized Hopper architecture features, enabling highly efficient execution.\nTrained on Wan2.1's 1.3B and 14B models and evaluated on the vBench benchmark, FPSAttention achieves a 7.09$\\times$ kernel speedup for attention operations and a 4.96$\\times$ end-to-end speedup for video generation compared to the BF16 baseline at 720p resolution—without sacrificing generation quality.",
    "authors": [
      "~Akide_Liu1",
      "~Zeyu_Zhang11",
      "~Zhexin_Li1",
      "~Xuehai_Bai1",
      "~Yuanjie_Xing1",
      "~Yizeng_Han1",
      "~Jiasheng_Tang1",
      "~Jichao_Wu1",
      "~Mingyang_Yang2",
      "~Weihua_Chen1",
      "~Jiahao_He6",
      "~Yuanyu_He1",
      "~Fan_Wang6",
      "~Gholamreza_Haffari2",
      "~Bohan_Zhuang1"
    ],
    "pdf": "/pdf/5bdf83b483c23f38d53963d34767e208f9fd9e99.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization, GPU utilization, throughput and latency improvements, FP8 quantization techniques, hardware-aware kernel optimization, scalability across model sizes",
      "Irrelevant Aspects": "Focus on video diffusion models rather than large language models, different modality (visual vs. text), diffusion model-specific optimizations",
      "Summary": "FPSAttention presents a training-aware co-design of FP8 quantization and sparsity specifically optimized for video diffusion models' 3D attention mechanisms. The approach achieves significant speedups (7.09× for attention kernels, 4.96× end-to-end) while maintaining generation quality, through innovations in tile-wise granularity, denoising step-aware strategies, and hardware-friendly kernel implementation. While focused on video generation rather than text, the optimization techniques for quantization, sparsity, and GPU utilization are highly relevant to LLM inference optimization."
    }
  },
  {
    "id": "DpeJYRFRQY",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
    "abstract": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token router that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6×, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8× wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency.",
    "authors": [
      "~Tianyu_Fu3",
      "~Yi_Ge4",
      "~Yichen_You1",
      "~Enshu_Liu1",
      "~Zhihang_Yuan1",
      "~Guohao_Dai4",
      "~Shengen_Yan1",
      "~Huazhong_Yang2",
      "~Yu_Wang3"
    ],
    "pdf": "/pdf/14de671242c3654992d79919aba5d312e05f7347.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces an innovative token routing mechanism between small and large language models to optimize inference efficiency. It addresses the core challenge of balancing computational cost with reasoning capabilities in LLMs. The R2R approach demonstrates significant speedup (2.8×) while maintaining comparable performance to larger models, which directly relates to improving GPU utilization and throughput. The paper advances the Pareto frontier of test-time scaling efficiency, which is central to my research interest in inference optimization. The automatic data generation pipeline and token-level routing strategy represent novel approaches to model resource allocation.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization with less emphasis on training optimization techniques. It doesn't explicitly address GPU utilization metrics or distributed computing scalability. The evaluation appears limited to specific model families (DeepSeek) and doesn't explore the approach's applicability across different model architectures. There's no discussion of memory optimization techniques or quantization strategies that could further enhance efficiency.",
      "Summary": "R2R presents a token-level routing approach that strategically uses a lightweight router to determine when to switch between small (1.5B) and large (32B) language models during inference. The key insight is that only a small fraction of tokens genuinely diverge in reasoning paths between models, allowing most generation to be handled by the smaller, more efficient model. This approach achieves comparable performance to the large model with 2.8× speedup, representing a significant advancement in efficient inference for large language models."
    }
  },
  {
    "id": "6BHDre6WQW",
    "title": "Hierachical Balance Packing: Towards Efficient Supervised Fine-tuning for Long-Context LLM",
    "abstract": "Training Long-Context Large Language Models (LLMs) is challenging, as hybrid training with long-context and short-context data often leads to workload imbalances. Existing works mainly use data packing to alleviate this issue, but fail to consider imbalanced attention computation and wasted communication overhead. This paper proposes Hierarchical Balance Packing (HBP), which designs a novel batch-construction method and training recipe to address those inefficiencies. In particular, the HBP constructs multi-level data packing groups, each optimized with a distinct packing length. It assigns training samples to their optimal groups and configures each group with the most effective settings, including sequential parallelism degree and gradient checkpointing configuration. To effectively utilize multi-level groups of data, we design a dynamic training pipeline specifically tailored to HBP, including curriculum learning, adaptive sequential parallelism, and stable loss. Our extensive experiments demonstrate that our method significantly reduces training time over multiple datasets and open-source models while maintaining strong performance. For the largest DeepSeek-V2 (236B) MoE model, our method speeds up the training by 2.4$\\times$ with competitive performance. Codes will be released at https://github.com/ModelTC/HBP.",
    "authors": [
      "~Yongqiang_Yao1",
      "~Jingru_Tan1",
      "~Kaihuan_Liang1",
      "~Feizhao_Zhang1",
      "~Jiahao_Hu3",
      "~Shuo_Wu2",
      "~Yazhe_Niu1",
      "~Ruihao_Gong1",
      "~Dahua_Lin1",
      "~Ningyi_Xu1"
    ],
    "pdf": "/pdf/3d55471711afe8f6378c0c3608b71d8ba929d827.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training optimization for large language models, GPU utilization improvement, addressing workload imbalances, throughput enhancement (2.4× speedup), efficient batch construction methods, attention computation optimization, communication overhead reduction, sequential parallelism strategies, gradient checkpointing configuration, dynamic training pipeline design, scalability to large models (236B MoE)",
      "Irrelevant Aspects": "Focus specifically on long-context LLMs rather than general LLM optimization, limited to supervised fine-tuning phase rather than pre-training, no inference optimization techniques, specialized balance packing approach with limited application scope",
      "Summary": "This paper presents Hierarchical Balance Packing (HBP), a method to optimize training for long-context LLMs by addressing workload imbalances through multi-level data packing groups and dynamic training pipelines. It achieves significant speedups (2.4× for 236B models) while maintaining performance, directly addressing GPU utilization and throughput concerns relevant to LLM training optimization."
    }
  },
  {
    "id": "65llKR17s4",
    "title": "Scaling Up Parameter Generation: A Recurrent Diffusion Approach",
    "abstract": "Parameter generation has long struggled to match the scale of today's large vision and language models, curbing its broader utility. In this paper, we introduce Recurrent Diffusion for Large-Scale Parameter Generation (RPG), a novel framework that generates full neural network parameters—up to hundreds of millions—on a single GPU. Our approach first partitions a network's parameters into non-overlapping 'tokens', each corresponding to a distinct portion of the model. A recurrent mechanism then learns the inter-token relationships, producing 'prototypes' which serve as conditions for a diffusion process that ultimately synthesizes the full parameters. Across a spectrum of architectures and tasks—including ResNets, ConvNeXts and ViTs on ImageNet-1K and COCO, and even LoRA-based LLMs—RPG achieves performance on par with fully trained networks while avoiding excessive memory overhead. Notably, it generalizes beyond its training set to generate valid parameters for previously unseen tasks, highlighting its flexibility in dynamic and open-ended scenarios. By overcoming the longstanding memory and scalability barriers, RPG serves as a critical advance in 'AI generating AI', potentially enabling efficient weight generation at scales previously deemed infeasible.",
    "authors": [
      "~Kai_Wang8",
      "~Dongwen_Tang1",
      "~Wangbo_Zhao1",
      "~Konstantin_Schürholt1",
      "~Zhangyang_Wang1",
      "~Yang_You1"
    ],
    "pdf": "/pdf/d7296047e517ff09e4497884e89f850e18696255.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses GPU utilization by generating hundreds of millions of parameters on a single GPU, focuses on memory optimization by avoiding excessive memory overhead, tackles scalability barriers in parameter generation, applies to LoRA-based LLMs, and offers a novel approach to efficient parameter creation.",
      "Irrelevant Aspects": "The paper primarily focuses on vision models rather than language models, uses diffusion models rather than traditional optimization techniques, emphasizes parameter generation rather than optimizing training processes, has limited discussion of inference optimization, and doesn't address latency concerns directly.",
      "Summary": "RPG introduces a recurrent diffusion approach for generating neural network parameters at scale with efficient GPU utilization and memory management. While it presents innovative solutions for parameter scalability and applies to LoRA-based LLMs, its primary focus on vision models and diffusion-based approach places it somewhat outside my core research on traditional LLM training and inference optimization. The paper offers valuable insights on GPU efficiency but is less directly relevant to inference optimization and latency reduction."
    }
  },
  {
    "id": "PMSNd8xTHp",
    "title": "ParetoQ: Improving Scaling Laws in Extremely Low-bit LLM Quantization",
    "abstract": "The optimal bit-width for achieving the best trade-off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others propose that 1.58-bit offers superior results. However, the lack of a cohesive framework for different bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quantization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3-bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the representations change drastically. By optimizing training schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2-bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantization offers promising potential for memory reduction and speedup.",
    "authors": [
      "~Zechun_Liu1",
      "~Changsheng_Zhao2",
      "~Hanxian_Huang1",
      "~Sijia_Chen8",
      "~Jing_Zhang55",
      "~Jiawei_Zhao2",
      "~Scott_Roy3",
      "~Lisa_Jin1",
      "~Yunyang_Xiong2",
      "~Yangyang_Shi1",
      "~Lin_Xiao1",
      "~Yuandong_Tian1",
      "~Bilge_Soran1",
      "~Raghuraman_Krishnamoorthi1",
      "~Tijmen_Blankevoort1",
      "~Vikas_Chandra2"
    ],
    "pdf": "/pdf/d60552949e4c9d184b694025d9111adfcb953fac.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM quantization which is crucial for inference optimization. It focuses on extreme low-bit quantization (1-4 bits) which significantly impacts memory usage and can improve GPU utilization. The work considers hardware constraints when evaluating different bit-widths, which is directly relevant to implementation on actual systems. The paper identifies optimal bit-widths based on the size-accuracy trade-off, helping inform system design decisions for better throughput and lower latency. The finding that 2-bit quantization offers promising potential for memory reduction and speedup is particularly valuable for optimizing inference systems.",
      "Irrelevant Aspects": "The paper doesn't explicitly focus on GPU utilization metrics or provide detailed analysis of how quantization techniques affect actual GPU performance. There's limited discussion about distributed training or scaling across multiple GPUs. The work appears to concentrate primarily on the quantization techniques rather than the detailed systems implementation aspects. The paper doesn't provide comprehensive measurements of inference latency improvements in production environments.",
      "Summary": "ParetoQ presents a unified framework for extremely low-bit LLM quantization (1-4 bits), identifying a learning transition between 2-3 bits that affects how models behave during quantization. The authors optimize training schemes and quantization functions to achieve impressive results, including a ternary 600M-parameter model outperforming a previous 3B-parameter ternary model. The paper concludes that 2-bit quantization offers the best potential for memory reduction and speedup considering hardware constraints. This research is highly relevant to LLM inference optimization, offering techniques that can significantly improve GPU utilization, throughput, and reduce latency, though it could benefit from more explicit analysis of actual system performance metrics."
    }
  },
  {
    "id": "nxnBaaRLnz",
    "title": "Don’t Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models",
    "abstract": "While recent success of large reasoning models (LRMs) significantly advanced LLMs' reasoning capability by optimizing the final answer accuracy using reinforcement learning, they may also drastically increase the output length due to *overthinking*—characterized by unnecessarily complex reasoning paths that waste computation and potentially degrade the performance.\nWe hypothesize that such inefficiencies stem from LRMs' limited capability to dynamically select the proper modular reasoning strategies, termed *thinking patterns* at the right position. To investigate this hypothesis, we propose a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns, systematically identifying and promoting beneficial patterns that improve the answer while removing detrimental ones. Empirical analysis confirms that our optimized thinking paths yield more concise yet sufficiently informative trajectories, enhancing reasoning efficiency by reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Moreover, a non-trivial portion of originally incorrect responses are transformed into correct ones, achieving a 15.6% accuracy improvement with reduced length.\nMotivated by the improvement brought by the optimized thinking paths, we apply a preference optimization technique supported by a pairwise dataset contrasting suboptimal and optimal reasoning paths. Experimental evaluations across multiple mathematical reasoning benchmarks reveal that our method notably reduces computational overhead while simultaneously improving reasoning accuracy, achieving up to a 12% accuracy improvement and reducing token usage from approximately 5,000 to 3,000 tokens.",
    "authors": [
      "~Sohyun_An1",
      "~Ruochen_Wang2",
      "~Tianyi_Zhou2",
      "~Cho-Jui_Hsieh1"
    ],
    "pdf": "/pdf/1b9aa688f3d241f228fbd9b8694bdffb938d1d5f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses computational efficiency by reducing attention FLOPs by up to 47%, which directly impacts GPU utilization and throughput. It reduces token usage from approximately 5,000 to 3,000 tokens, directly affecting latency and computational cost. The approach maintains or improves accuracy while reducing computation, a key optimization goal. The paper introduces a dynamic optimization framework for reasoning patterns, which could inform better inference strategies. The preference optimization technique using pairwise datasets relates to training optimization.",
      "Irrelevant Aspects": "The paper focuses more on reasoning quality optimization rather than system-level performance optimization. There's limited discussion about hardware-specific optimizations or detailed GPU utilization. The paper doesn't appear to address scalability across multiple GPUs or distributed systems. The focus is on reasoning models specifically rather than general LLM optimization.",
      "Summary": "The paper presents a method to optimize thinking dynamics in large reasoning models by identifying and promoting efficient reasoning patterns while removing detrimental ones. This approach reduces computational overhead (attention FLOPs reduced by up to 47%, token usage reduced from ~5,000 to ~3,000) while maintaining or improving accuracy (up to 15.6% improvement for incorrect responses becoming correct). The method involves a preference optimization technique using pairwise datasets contrasting suboptimal and optimal reasoning paths. While the paper doesn't focus on hardware-specific optimizations or scalability across multiple GPUs, it directly addresses key metrics of interest: computational efficiency, throughput, and latency in both training and inference contexts."
    }
  },
  {
    "id": "lqywifxoo1",
    "title": "Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection",
    "abstract": "State Space Models (SSMs) offer remarkable performance gains in efficient sequence modeling, with constant per-step inference-time computation and memory complexity. Recent advances, such as Mamba, further enhance SSMs with input-dependent gating and hardware-aware implementations, positioning them as strong alternatives to Transformers for long sequence modeling. However, efficiently scaling the expressive power of SSMs, particularly with Mixture of Experts (MoE), remains challenging, as naive integration attempts often falter or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel approach that scales SSM parameters using sparse mixtures of linear projection experts. By sharing routing decisions between projection layers and lightweight sub-modules within Mamba across experts, RoM leverages synergies among linear projection experts for effective and efficient sparse scaling of Mamba layers. At a scale of 1.3B active parameters (10B total) and 16K training sequence length, RoM achieves language modeling performance equivalent to a dense Mamba model requiring over 2.3$\\times$ more active parameters, and demonstrates consistent perplexity across context lengths. Experimental results further show RoM effectively scales hybrid language models, yielding a 23% FLOPS saving compared to dense Mamba scaling for similar performance. We release our training codebase at https://github.com/zhanzheng8585/Routing-Mamba.",
    "authors": [
      "~Zheng_Zhan3",
      "~Liliang_Ren1",
      "~Shuohang_Wang1",
      "~Liyuan_Liu3",
      "~Yang_Liu50",
      "~Yeyun_Gong2",
      "~Yanzhi_Wang3",
      "~yelong_shen1"
    ],
    "pdf": "/pdf/189421ae12f7679f6560b0b200b7f59631788726.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses scaling and efficiency improvements for State Space Models (SSMs), specifically Mamba, using sparse mixtures of experts. It mentions constant per-step inference-time computation and memory complexity, which is directly relevant to inference optimization. The approach achieves significant parameter efficiency (2.3× fewer active parameters for equivalent performance) and 23% FLOPS savings, which relates to computational efficiency and better resource utilization.",
      "Irrelevant Aspects": "The paper doesn't specifically focus on GPU utilization optimization techniques, throughput measurements, or latency analysis. While it mentions hardware-aware implementations, it doesn't provide detailed hardware optimization strategies. The focus is primarily on model architecture rather than system-level optimizations that would be directly applicable to GPU utilization and scalability at the infrastructure level.",
      "Summary": "Routing Mamba (RoM) introduces a novel approach to efficiently scale State Space Models using sparse mixtures of linear projection experts. The method shares routing decisions between projection layers and lightweight sub-modules within Mamba across experts. At 1.3B active parameters (10B total), RoM matches the performance of dense Mamba models requiring over 2.3× more active parameters and achieves 23% FLOPS savings. While this represents significant model efficiency improvements, the paper doesn't directly address GPU utilization, throughput, or latency optimization at the system level."
    }
  },
  {
    "id": "Zfgvo65gxm",
    "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making",
    "abstract": "We explore the use of expert-guided bandit learning, which we refer to as online mixture-of-experts (OMoE). In this setting, given a context, a candidate committee of experts must determine how to aggregate their outputs to achieve optimal results in terms of aggregate accuracy. We propose two algorithms to address this problem. The first algorithm combines aggregate voting with UCB-driven successive elimination, efficiently pruning suboptimal exploration actions. The second algorithm employs an online weighted-majority-voting mechanism, leveraging the respective voting power of each expert proportional to their predictive power. We derive theoretical guarantees for the regret properties in the bandit setting under ideal circumstances, and empirical results are provided accordingly. As a modern study on applications, these methods are applied to the online fine-tuning of a set of expert large language models (LLMs), where after each response, the generative LLM dynamically reweighs its set of experts and/or selects the optimal committee of experts to generate the most accurate response. Our results introduce new methodologies and no-regret guarantees for combining multiple experts to improve on the performance of the an aggregate model overall.",
    "authors": [
      "~Larkin_Liu1",
      "~Jalal_Etesami2"
    ],
    "pdf": "/pdf/45fa03110976e0eb7eb39277399403215c451cc2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on mixture of experts (MoE) approaches which is a key technique for scaling large language models while maintaining computational efficiency. It discusses dynamic selection and reweighting of expert LLMs, which relates to inference optimization. The paper addresses online fine-tuning of expert LLMs, which connects to training optimization. The approach involves selecting 'optimal committees of experts' to generate responses, which can improve efficiency by not requiring all experts to be active simultaneously. The methods could potentially improve GPU utilization by selectively activating only the most relevant experts for a given task.",
      "Irrelevant Aspects": "The paper seems to focus more on the bandit learning framework and theoretical regret guarantees rather than hardware-specific optimization techniques. There's no explicit mention of GPU utilization, throughput measurements, or latency reduction in the abstract. The focus appears to be on decision-making accuracy rather than computational efficiency. The paper doesn't seem to address the specific challenges of distributed training or inference serving infrastructure.",
      "Summary": "The paper presents two algorithms for online mixture-of-experts (OMoE) that combine aggregate voting with UCB-driven elimination and weighted-majority-voting mechanisms. These methods are applied to online fine-tuning of expert LLMs, dynamically selecting and reweighting experts to optimize response generation. While the paper addresses expert model approaches relevant to LLM scalability, it focuses more on theoretical regret guarantees in a bandit setting rather than explicit GPU utilization, throughput, or latency optimization. The expert selection mechanisms could potentially lead to more efficient inference by activating fewer experts, but this aspect is not the primary focus of the paper."
    }
  },
  {
    "id": "X2r9D46kvI",
    "title": "🎧MOSPA: Human Motion Generation Driven by Spatial Audio",
    "abstract": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive \"Spatial Audio-Driven Human Motion\" (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human \"MOtion generation driven by SPatial Audio,\" termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation.git",
    "authors": [
      "~Shuyang_Xu1",
      "~Zhiyang_Dou1",
      "~Mingyi_Shi1",
      "~Liang_Pan3",
      "~Leo_Ho1",
      "~Jingbo_Wang3",
      "~Yuan_Liu3",
      "~Cheng_Lin1",
      "~Yuexin_Ma2",
      "~Wenping_Wang1",
      "~Taku_Komura2"
    ],
    "pdf": "/pdf/24dfaa6d844839043ba8cf990bdb9c5dff7928a0.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper uses a diffusion-based generative framework, which shares some technical foundations with models used in NLP. However, there are no specific optimizations discussed that would be applicable to large language model training or inference.",
      "Irrelevant Aspects": "The paper focuses on human motion generation from spatial audio, which is completely different from language modeling. It does not address GPU utilization, scalability, throughput, or latency optimizations for large language models. The application domain is character animation rather than natural language processing.",
      "Summary": "MOSPA introduces a diffusion model for generating human motion from spatial audio inputs. While technically competent in its domain of audio-driven motion synthesis, the paper has virtually no relevance to optimizing large language models for better GPU utilization, scalability, throughput, or latency - the core focus of my research."
    }
  },
  {
    "id": "ODCHMTXKHO",
    "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism",
    "abstract": "Diffusion models have emerged as a powerful class of generative models across various modalities, including image, video, and audio synthesis. However, their deployment is often limited by significant inference latency, primarily due to the inherently sequential nature of the denoising process. While existing parallelization strategies attempt to accelerate inference by distributing computation across multiple devices, they typically incur high communication overhead, hindering deployment on commercial hardware. To address this challenge, we propose $\\textbf{ParaStep}$, a novel parallelization method based on a reuse-then-predict mechanism that parallelizes diffusion inference by exploiting similarity between adjacent denoising steps. Unlike prior approaches that rely on layer-wise or stage-wise communication, ParaStep employs lightweight, step-wise communication, substantially reducing overhead. ParaStep achieves end-to-end speedups of up to $\\textbf{3.88}$$\\times$ on SVD, $\\textbf{2.43}$$\\times$ on CogVideoX-2b, and $\\textbf{6.56}$$\\times$ on AudioLDM2-large, while maintaining generation quality. These results highlight ParaStep as a scalable and communication-efficient solution for accelerating diffusion inference, particularly in bandwidth-constrained environments.",
    "authors": [
      "~Kunyun_Wang1",
      "~Bohan_Li13",
      "~Kai_Yu3",
      "~Minyi_Guo1",
      "~Jieru_Zhao1"
    ],
    "pdf": "/pdf/350669db30af0742c65752b726dddd2d1197f97f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for large generative models, addressing latency reduction, parallelization across multiple devices, communication overhead reduction, and throughput improvements (speedups of 3.88×, 2.43×, and 6.56× on different models). It's designed for commercial hardware and focuses on GPU utilization and scalability.",
      "Irrelevant Aspects": "The paper focuses on diffusion models rather than large language models specifically. It doesn't discuss training optimization, only inference. The applications mentioned (image, video, and audio synthesis) are different from text generation which is the typical application of large language models.",
      "Summary": "This paper is relevant to my research interests as it focuses on inference optimization for large generative models, addressing parallelization, communication efficiency, and throughput improvements. While it doesn't specifically address large language models, the techniques could potentially be transferable. The paper's focus on reducing communication overhead and improving GPU utilization aligns well with my research interests in scalable ML systems."
    }
  },
  {
    "id": "RGUcF6pIZN",
    "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
    "abstract": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as ``fuzzy'' speculation. After each drafting-and-verification iteration, the draft model’s key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7\\%. The code is available at https://github.com/Yize-Wu/EasySpec.",
    "authors": [
      "~Yize_Wu1",
      "~KE_GAO3",
      "~Ling_Li6",
      "~Yanjun_Wu1"
    ],
    "pdf": "/pdf/400622f254de773d7e2d32482c818cbf668c53a3.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on inference optimization through speculative decoding, addresses multi-GPU utilization issues, reduces GPU idling during drafting stages, improves throughput with reported speedups of up to 4.17x, reduces inference latency in multi-GPU systems, enhances scalability for LLM deployment",
      "Irrelevant Aspects": "Does not address training optimization, the method is training-free, focuses specifically on the inference phase rather than the complete ML pipeline",
      "Summary": "EasySpec introduces a layer-parallel speculation strategy that optimizes multi-GPU utilization during LLM inference. It breaks inter-layer dependencies in draft models, enabling simultaneous execution across multiple devices. The method achieves significant speedups (up to 4.17x) while preserving the original distributions of base LLMs, with the drafting stage accelerated by up to 1.62x with minimal accuracy loss. As a training-free, plug-in method, it offers a practical approach to improve inference efficiency without requiring model retraining."
    }
  },
  {
    "id": "WZQXaTNYEB",
    "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search",
    "abstract": "We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6× generation throughput speedup and 6.1× prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.",
    "authors": [
      "~Yuxian_Gu1",
      "~Qinghao_Hu3",
      "~Haocheng_Xi1",
      "~Junyu_Chen4",
      "~Shang_Yang1",
      "~Song_Han5",
      "~Han_Cai1"
    ],
    "pdf": "/pdf/662161ea85e41febf6822bfc610424e868557c1e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Post Neural Architecture Search methodology for efficient model design, Hybrid-architecture combining different attention mechanisms, Significant generation throughput speedup (53.6×) and prefilling speedup (6.1×), Hardware-aware hyperparameter search component, Maintaining accuracy while improving efficiency",
      "Irrelevant Aspects": "Limited focus on training optimization techniques, Not much discussion of multi-GPU or distributed training/inference, Limited analysis of GPU memory utilization, Less emphasis on scalability across multiple nodes",
      "Summary": "The paper presents Jet-Nemotron, a hybrid-architecture language model designed through Post Neural Architecture Search that achieves significant performance improvements while maintaining comparable accuracy to leading models. It focuses on optimizing attention block designs for better inference efficiency, directly addressing throughput and latency optimization goals, though with less emphasis on training optimization or distributed computing aspects."
    }
  },
  {
    "id": "h3gaIBwemp",
    "title": "Neural Attention Search",
    "abstract": "We present Neural Attention Search (NAtS), an end-to-end learnable sparse transformer that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. To this end, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache and the inference costs for the transformer-based models while maintaining the models' performance.",
    "authors": [
      "~Difan_Deng1",
      "~Marius_Lindauer1"
    ],
    "pdf": "/pdf/4470965d2dee209dc3998812c1e5b2cb0024d29e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "End-to-end learnable sparse transformer for inference optimization, automatic token importance evaluation, KV cache reduction, inference cost reduction, applicable to large language models, joint learning of token types and architecture weights, fine-tuning existing LLMs, maintaining model performance while reducing computational costs",
      "Irrelevant Aspects": "Focuses primarily on inference optimization rather than training optimization, lacks GPU kernel-level optimization details, doesn't cover distributed training aspects, limited discussion on hardware-specific optimizations",
      "Summary": "NAtS presents a learnable sparse attention mechanism that categorizes tokens into three types (Global, Local, Sliding Window) and learns to drop unimportant tokens during inference. This approach directly addresses key concerns in LLM inference optimization by reducing KV cache and computational costs while maintaining performance. The method's applicability to both training from scratch and fine-tuning existing models makes it highly relevant for practical deployment scenarios seeking improved GPU utilization and lower latency."
    }
  },
  {
    "id": "JFygzwx8SJ",
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\\times$ and FlashAttention decoding latency by approximately $2\\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios.",
    "authors": [
      "~Jang-Hyun_Kim1",
      "~Jinuk_Kim1",
      "~Sangwoo_Kwon1",
      "~Jae_W._Lee1",
      "~Sangdoo_Yun1",
      "~Hyun_Oh_Song1"
    ],
    "pdf": "/pdf/31fbad5b02e0ec167a202d1b5481e2b478a8dcfc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses KV cache optimization which is a critical bottleneck in LLM inference. It achieves 3-4x reduction in memory usage and 2x improvement in decoding latency, directly improving GPU utilization and throughput. The query-agnostic approach enables cache reuse across different queries, enhancing system efficiency. The method scales to 170K tokens and works across multiple model architectures (LLaMA3.1, Qwen2.5, Gemma3), addressing scalability concerns.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization. It doesn't cover hardware-specific optimizations or custom kernel implementations that could further improve GPU efficiency. The approach also doesn't discuss quantization techniques beyond the importance scoring mechanism.",
      "Summary": "KVzip introduces a query-agnostic KV cache compression method that uses the underlying LLM to reconstruct contexts and evict less important KV pairs. It demonstrates significant memory savings (3-4x) and latency improvements (2x) with minimal performance degradation across various LLMs and long-context scenarios, making it highly relevant for inference optimization and efficient GPU utilization in production systems."
    }
  },
  {
    "id": "qILtlupwrR",
    "title": "Computation and Memory-Efficient Model Compression with  Gradient Reweighting",
    "abstract": "Pruning is a commonly employed technique for deep neural networks (DNNs) aiming at compressing the model size to reduce computational and memory costs during inference. In contrast to conventional neural networks, large language models (LLMs) pose a unique challenge regarding pruning efficiency due to their substantial computational and memory demands. Existing methods, particularly optimization-based ones, often require considerable computational resources in gradient estimation because they cannot effectively leverage weight sparsity of the intermediate pruned network to lower compuation and memory costs in each iteration. The fundamental challenge  lies in the need to frequently instantiate intermediate pruned sub-models to achieve these savings, a task that becomes infeasible even for moderately sized neural networks.  To this end, this paper proposes a novel pruning method for DNNs that is both computationally and memory-efficient. Our key idea is to develop an effective reweighting mechanism that enables us to estimate the gradient of the pruned network in current iteration via reweigting the gradient estimated on an outdated intermediate sub-model instantiated at an earlier stage, thereby significantly reducing model instantiation frequency. We further develop a series of techniques, e.g., clipping and preconditioning matrix, to reduce the variance of gradient estimation and stabilize the optimization process. We conducted extensive experimental validation across various domains. Our approach achieves 50\\% sparsity and a 1.58$\\times$ speedup in forward pass on Llama2-7B model with only 6 GB of memory usage, outperforming state-of-the-art methods with respect to both perplexity and zero-shot performance. As a by-product, our method is highly suited for  distributed sparse training  and can achieve a 2 $\\times$ speedup over the dense distributed baselines.",
    "authors": [
      "~Zhiwei_Li11",
      "~Yuesen_Liao1",
      "~Binrui_Wu1",
      "~Yuquan_Zhou1",
      "~Xupeng_Shi1",
      "~Dongsheng_Jiang2",
      "~Yin_Li7",
      "~WEIZHONG_ZHANG2"
    ],
    "pdf": "/pdf/92754dce25ddf4827f44c09c5afb26a84249c340.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on pruning techniques for large language models, directly addressing computational and memory efficiency during inference. It specifically targets LLMs like Llama2-7B and demonstrates 1.58× speedup with only 6GB memory usage. The method reduces model instantiation frequency and leverages weight sparsity to lower computation and memory costs. It also achieves 2× speedup in distributed sparse training over dense baselines, which is relevant for scalability.",
      "Irrelevant Aspects": "The paper focuses specifically on pruning as an optimization technique rather than covering the broader spectrum of LLM optimization methods. It doesn't address techniques like quantization, knowledge distillation, or other inference optimization approaches beyond pruning.",
      "Summary": "This paper presents a gradient reweighting method for efficient pruning of large language models, achieving significant memory savings and computational speedups during both training and inference. The approach reduces the frequency of model instantiation while maintaining performance, making it particularly relevant for LLM optimization on resource-constrained systems."
    }
  },
  {
    "id": "zWHKKspghT",
    "title": "Mozart: Modularized and Efficient MoE Training on 3.5D Wafer-Scale Chiplet Architectures",
    "abstract": "Mixture-of-Experts (MoE) architecture offers enhanced efficiency for Large Language Models (LLMs) with modularized computation, yet its inherent sparsity poses significant hardware deployment challenges, including memory locality issues, communication overhead, and inefficient computing resource utilization. Inspired by the modular organization of the human brain, we propose $\\texttt{Mozart}$, a novel algorithm-hardware co-design framework tailored for efficient training of MoE-based LLMs on 3.5D wafer-scale chiplet architectures. On the algorithm side, $\\texttt{Mozart}$ exploits the inherent modularity of chiplets and introduces: \n($1$) an expert allocation strategy that enables efficient on-package all-to-all communication, and ($2$) a fine-grained scheduling mechanism that improves communication-computation overlap through streaming tokens and experts. On the architecture side, $\\texttt{Mozart}$ adaptively co-locates heterogeneous modules on specialized chiplets with a 2.5D NoP-Tree topology and hierarchical memory structure.\nEvaluation across three popular MoE models demonstrates significant efficiency gains, enabling more effective parallelization and resource utilization for large-scale modularized MoE-LLMs.",
    "authors": [
      "~Shuqing_Luo1",
      "~Ye_Han4",
      "~Pingzhi_Li1",
      "~Jiayin_Qin1",
      "~Jie_Peng4",
      "~Yang_Katie_Zhao1",
      "~Yu_Cao10",
      "~Tianlong_Chen1"
    ],
    "pdf": "/pdf/eb51e3e44d378b7b6e4b0112caf84db1a38248d8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM training optimization with focus on Mixture-of-Experts (MoE) architecture, which is highly relevant to large language model optimization. It tackles critical system challenges including memory locality, communication overhead, and resource utilization. The algorithm-hardware co-design approach, expert allocation strategy, and fine-grained scheduling mechanism for computation overlap align well with optimization goals. The framework claims to enable more effective parallelization and resource utilization for large-scale MoE-LLMs, which directly addresses scalability and throughput concerns.",
      "Irrelevant Aspects": "The paper focuses specifically on 3.5D wafer-scale chiplet architectures rather than more general GPU systems that are more commonly used. There appears to be less emphasis on inference optimization, with the primary focus on training. The highly specialized hardware considerations may limit the broader applicability of some techniques to more standard ML infrastructure.",
      "Summary": "Mozart presents an algorithm-hardware co-design framework for optimizing MoE-based LLM training on specialized wafer-scale chiplet architectures. It introduces expert allocation strategies and fine-grained scheduling mechanisms to address memory locality, communication overhead, and resource utilization challenges. While focused on specialized hardware, the approaches to optimizing MoE training, improving parallelization, and enhancing resource utilization are highly relevant to LLM optimization research."
    }
  },
  {
    "id": "tQZK5frjVU",
    "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
    "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge.\nIn this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.",
    "authors": [
      "~Xinran_Gu2",
      "~Kaifeng_Lyu2",
      "~Jiazheng_Li5",
      "~Jingzhao_Zhang2"
    ],
    "pdf": "/pdf/95bd21460976cad8c6926b5c9ab1243abb9dfdbf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization through data mixing strategies, which directly impacts GPU utilization and training efficiency. It explores how model size affects optimal data mixing, which has implications for scalability. The concept of 'phase transitions' in knowledge acquisition could help identify optimal training configurations that maximize throughput. The capacity allocation framework might provide insights into better GPU memory utilization. These findings could reduce wasted computation during training.",
      "Irrelevant Aspects": "The paper does not address inference optimization techniques, which is a key component of the research interest. It doesn't focus specifically on latency reduction or provide practical GPU optimization strategies. The research appears more theoretical than implementation-focused. There's no connection to inference-time optimizations such as quantization or distillation.",
      "Summary": "This paper investigates how data mixing between web-scraped and knowledge-dense sources affects LLM training efficiency, revealing 'phase transitions' in knowledge acquisition based on mixing ratios and model size. It frames this as a capacity allocation problem with discontinuous optimal allocation points. While highly relevant to training optimization and scalability, it does not address inference optimization aspects of the research interest."
    }
  },
  {
    "id": "DBybUx7ARy",
    "title": "Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Expansion",
    "abstract": "Finetuning large language models (LLMs) is a resource-intensive task for researchers in academia, with memory constraints posing a key bottleneck. A classic optimization method, block coordinate descent (BCD), significantly reduces memory cost by segmenting the trainable parameters into multiple blocks and optimizing one active block at a time while freezing the others. However, we identify that blindly applying BCD to train LLMs can be inefficient for two reasons. First, optimizing only the active block requires backpropagating through multiple deeper yet inactive blocks, resulting in wasteful computations. Second, the frozen blocks, when they are not quite close to optimality, can narrow the optimization landscape, potentially misguiding the training of the active block. To address these issues simultaneously, we propose integrating BCD with *landscape expansion*, which unfreezes the inactive blocks and updates them in a cost-efficient manner during the same backpropagation as the update to the active block. Experiments on 8B and 70B models demonstrate that our proposed method surpasses memory-efficient baselines and matches Adam's downstream performance while requiring only 24 GB of memory for the 8B model and 300 GB for the 70B model.",
    "authors": [
      "~Qijun_Luo3",
      "~Yifei_Shen1",
      "~Liangzu_Peng2",
      "~Dongsheng_Li2",
      "~Xiao_Li5"
    ],
    "pdf": "/pdf/980d28f2eee0e69c82f8b49bd7958df3b79f1509.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses memory optimization for LLM finetuning, which is crucial for GPU utilization and scalability. It proposes a method that matches Adam's performance while requiring significantly less memory (24GB for 8B model, 300GB for 70B model). The technique reduces wasteful computations during backpropagation, potentially improving throughput. It addresses optimization efficiency which is core to training optimization research.",
      "Irrelevant Aspects": "The paper focuses specifically on finetuning rather than pre-training or inference optimization. It doesn't address inference optimization or latency considerations explicitly. The landscape expansion technique is a specific optimization method rather than a general framework. The method may be specific to certain model architectures or training scenarios.",
      "Summary": "This paper presents an optimization technique for LLM finetuning that enhances block coordinate descent (BCD) with landscape expansion to address memory constraints and computational inefficiencies. The key innovation is updating inactive blocks cost-efficiently during the same backpropagation as the active block update. The approach matches Adam's performance while requiring significantly less memory, making LLM finetuning more accessible for researchers with limited resources."
    }
  },
  {
    "id": "a95Vd41o1u",
    "title": "STree: Speculative Tree Decoding for Hybrid State Space Models",
    "abstract": "Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be find at: https://github.com/wyc1997/stree.",
    "authors": [
      "~Yangchao_Wu1",
      "~Zongyue_Qin1",
      "~Alex_Wong2",
      "~Stefano_Soatto3"
    ],
    "pdf": "/pdf/03c4f094562f11882a8ad0419ce691c83d698a85.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization for large language models using speculative tree decoding techniques. It directly addresses GPU utilization through leveraging hardware concurrency to enable multiple token generation steps in a single forward pass. The approach improves throughput and reduces latency, which aligns with the research interest. The paper introduces the first scalable algorithm for tree-based speculative decoding in state-space models and hybrid architectures, addressing scalability concerns. It also mentions a hardware-aware implementation, showing attention to optimization for specific hardware architectures.",
      "Irrelevant Aspects": "The paper doesn't address training optimization, focusing primarily on inference. While it mentions hardware concurrency, it doesn't explicitly focus on GPU utilization metrics or detailed GPU-specific optimizations. The research is limited to SSMs and hybrid models rather than covering broader Transformer architectures.",
      "Summary": "This paper introduces STree, a speculative tree decoding algorithm for hybrid State Space Models that aims to improve inference efficiency by enabling multiple token generation steps in a single forward pass. It's the first scalable algorithm for tree-based speculative decoding in SSMs, exploiting the structure of accumulated state transition matrices. The paper includes a hardware-aware implementation and demonstrates performance improvements over vanilla speculative decoding with SSMs across three benchmarks."
    }
  },
  {
    "id": "v6vBK4t8vB",
    "title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training",
    "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning~(PEFT) methods have been proposed to address these challenges by freezing most model parameters and training only a small subset. While PEFT is efficient, it may not outperform full fine-tuning when high task-specific performance is required.\nZeroth-Order (ZO) methods offer an alternative for fine-tuning the entire pre-trained model by approximating gradients using only the forward pass, thus eliminating the computational burden of back-propagation,\n% in first-order methods, \nbut they converge painfully slowly and are very sensitive to the choice of task prompts.\nWe bridge these worlds with Bilevel‑ZOFO, a penalty‑based bilevel formulation that treats adapter parameters as a lower‑level learner coupled to an upper‑level ZO optimizer of the full backbone. This double-loop optimization strategy only requires the gradient of the PEFT model and the forward pass of the base model. We provide theoretical convergence guarantees for Bilevel ZOFO. Empirically, we demonstrate that Bilevel-ZOFO significantly outperforms existing ZO methods, achieves 2–4$\\times$ faster training, and reduces sensitivity to prompts. Bilevel-ZOFO also outperforms FO PEFT methods while maintaining similar memory efficiency. Additionally, we show its strong potential for meta learning.",
    "authors": [
      "~Reza_Shirkavand1",
      "~Peiran_Yu1",
      "~Qi_He6",
      "~Heng_Huang1"
    ],
    "pdf": "/pdf/8ec3d66bc96dab3092b445848f32c95d84e46176.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization for LLMs, computational efficiency by eliminating back-propagation, Parameter-Efficient Fine-Tuning (PEFT) methods, performance vs. efficiency trade-offs, 2-4x faster training claims, memory efficiency improvements, meta-learning applications",
      "Irrelevant Aspects": "Specific prompt sensitivity issues, detailed theoretical convergence guarantees, fine details of bilevel mathematical formulation, limited discussion on inference optimization",
      "Summary": "The paper proposes Bilevel ZOFO, a new optimization method for LLM fine-tuning that combines Zeroth-Order methods with PEFT approaches. It addresses computational challenges by eliminating back-propagation while maintaining performance. The method claims 2-4x faster training with similar memory efficiency to PEFT, potentially improving GPU utilization and throughput. While highly relevant to training optimization aspects of my research, it has limited discussion on inference optimization, which is another key area of interest."
    }
  },
  {
    "id": "YdggdEL41C",
    "title": "Vision-centric Token Compression in Large Language Model",
    "abstract": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters.\nThis dual expansion send compute and memory costs skyrocketing, making $\\textit{token compression}$ indispensable.\nWe introduce Vision Centric Token Compression ($\\textbf{Vist}$), a $\\textit{slow–fast}$ compression framework that mirrors human reading: \nthe $\\textit{fast}$ path renders distant tokens into images, letting a $\\textbf{frozen, lightweight vision encoder}$ skim the low-salience context; \nthe $\\textit{slow}$ path feeds the proximal window into the LLM for fine-grained reasoning.\nA Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions—just as skilled reader gloss over function words.\nOn eleven in-context learning benchmarks, $\\textbf{Vist}$ achieves the same accuracy with 2.3$\\times$ fewer tokens, cutting FLOPs by 16\\% and memory by 50\\%.\nThis method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by $\\textbf{7.6}$\\% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
    "authors": [
      "~Ling_Xing1",
      "~Alex_Jinpeng_Wang1",
      "~Rui_Yan5",
      "~Xiangbo_Shu1",
      "~Jinhui_Tang1"
    ],
    "pdf": "/pdf/4b6503e82dcc8dce34f5860c08697a359fe881cb.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on token compression which directly addresses LLM inference optimization by reducing computational and memory requirements. It achieves 2.3× token reduction with maintained accuracy, 16% FLOPs reduction, and 50% memory savings, which are highly relevant to GPU utilization, throughput, and latency optimization. The approach addresses scalability challenges for expanding context windows in large LLMs.",
      "Irrelevant Aspects": "The vision-centric approach using image representation of tokens is somewhat tangential to pure LLM optimization. The specific benchmark results and human reading metaphor are less relevant than the core optimization techniques. The training methodology aspects are less central to my inference optimization focus.",
      "Summary": "Vist presents a vision-centric token compression framework for LLMs that reduces computational costs while maintaining accuracy. By converting distant tokens to images processed by a frozen vision encoder, it achieves significant efficiency gains. The method outperforms text encoder-based approaches, making it highly relevant for LLM inference optimization, though the vision aspect adds complexity beyond standard text-only optimization approaches."
    }
  },
  {
    "id": "7aSBAw7tJf",
    "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?",
    "abstract": "Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. To resolve this, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes. \nFirst, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. In light of this, we propose a \\textit{norm-based scaling} method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to achieve this goal.\nMoreover, we find that there are potential loss spikes during training. To address this, we further put forward a norm-growth limiter to smooth the gradient.\nExtensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore. Notably, for pre-training LLaMA 7B, our Fira uses $8\\times$ smaller memory of optimizer states than Galore, yet outperforms it by a large margin.",
    "authors": [
      "~Xi_Chen65",
      "~Kaituo_Feng1",
      "~Changsheng_Li4",
      "~Xunhao_Lai1",
      "~Xiangyu_Yue1",
      "~Ye_Yuan15",
      "~Guoren_Wang1"
    ],
    "pdf": "/pdf/623f06afa08d7e3bfb893ecd92b1f91f5238d10f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Memory optimization for LLM training",
        "Comparison with established methods like LoRA and GaLore",
        "Training efficiency for large models (7B parameters)",
        "Technical innovations in optimizer scaling and gradient handling",
        "Claims of superior performance with reduced memory requirements",
        "Addresses both pre-training and fine-tuning of LLMs"
      ],
      "Irrelevant Aspects": [
        "No mention of inference optimization",
        "No direct discussion of throughput or latency metrics",
        "Limited information about computational overhead",
        "No mention of distributed training across multiple GPUs/nodes"
      ],
      "Summary": "Fira is a training framework that achieves memory-efficient low-rank training while preserving full-rank weight updates. It leverages the observation that adaptive optimizers' scaling impacts remain consistent across low-rank and full-rank training, introducing a norm-based scaling method and a norm-growth limiter. The paper claims 8x smaller memory usage for optimizer states compared to GaLore while maintaining or improving training quality for LLM pre-training and fine-tuning."
    }
  },
  {
    "id": "QZYZ0Xm58q",
    "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive Token Caching",
    "abstract": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal reasoning capabilities, enabling direct action generation from visual perception and language instructions in an end-to-end manner. However, their substantial computational cost poses a challenge for real-time robotic control, where rapid decision-making is essential. This paper introduces VLA-Cache, a training-free inference acceleration method that reduces computational overhead by adaptively caching and reusing static visual tokens across frames. Exploiting the temporal continuity in robotic manipulation, VLA-Cache identifies minimally changed tokens between adjacent frames and reuses their cached key-value representations, thereby circumventing redundant computations. Additionally, to maintain action precision, VLA-Cache selectively re-computes task-relevant tokens that are environmentally sensitive, ensuring the fidelity of critical visual information. To further optimize efficiency, we introduce a layer adaptive token reusing strategy that dynamically adjusts the reuse ratio based on attention concentration across decoder layers, prioritizing critical tokens for recomputation. Extensive experiments on two simulation platforms (LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache achieves up to 1.7× speedup in CUDA latency and a 15\\% increase in control frequency, with negligible loss on task success rate. The code and videos can be found at our project page: https://vla-cache.github.io.",
    "authors": [
      "~Siyu_Xu1",
      "~Yunke_Wang1",
      "~Chenghao_Xia1",
      "~Dihao_Zhu1",
      "~Tao_Huang5",
      "~Chang_Xu4"
    ],
    "pdf": "/pdf/ab187b19e4f174f9a6c3f7d82d52c8f6f1abfafb.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization for transformer models, GPU utilization improvement, latency reduction (1.7× speedup), throughput enhancement (15% increase in control frequency), adaptive token caching strategy, selective recomputation of critical tokens, layer-wise optimization based on attention patterns, training-free approach applicable at inference time, reduction of redundant computations",
      "Irrelevant Aspects": "Focus on Vision-Language-Action models rather than pure language models, application domain specific to robotic manipulation, evaluation on specialized simulation platforms (LIBERO and SIMPLER) rather than standard NLP benchmarks, limited scope for training phase optimization, specialized for visual token processing rather than general token optimization",
      "Summary": "VLA-Cache introduces a training-free inference acceleration method for Vision-Language-Action models that achieves 1.7× speedup by adaptively caching and reusing static visual tokens across frames. The method exploits temporal continuity in robotic manipulation to identify minimally changed tokens between adjacent frames, reusing their cached key-value representations while selectively re-computing task-relevant tokens. A layer adaptive strategy dynamically adjusts token reuse based on attention concentration. The approach demonstrates improved GPU utilization and higher throughput with negligible impact on task success rates."
    }
  },
  {
    "id": "5fKXeqtBuB",
    "title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles",
    "abstract": "Real-world image super-resolution (RealSR) aims to enhance the visual quality of in-the-wild images, such as those captured by mobile phones. While existing methods leveraging large generative models demonstrate impressive results, the high computational cost and latency make them impractical for edge deployment. In this paper, we introduce PocketSR, an ultra-lightweight, single-step model that brings generative modeling capabilities to RealSR while maintaining high fidelity. To achieve this, we design LiteED, a highly efficient alternative to the original computationally intensive VAE in SD, reducing parameters by 97.5\\% while preserving high-quality encoding and decoding. Additionally, we propose online annealing pruning for the U-Net, which progressively shifts generative priors from heavy modules to lightweight counterparts, ensuring effective knowledge transfer and further optimizing efficiency. To mitigate the loss of prior knowledge during pruning, we incorporate a multi-layer feature distillation loss. Through an in-depth analysis of each design component, we provide valuable insights for future research. PocketSR, with a model size of 146M parameters, processes 4K images in just 0.8 seconds, achieving a remarkable speedup over previous methods. Notably, it delivers performance on par with state-of-the-art single-step and even multi-step RealSR models, making it a highly practical solution for edge-device applications.",
    "authors": [
      "~Haoze_Sun2",
      "~Linfeng_Jiang1",
      "~Fan_Li26",
      "~Renjing_Pei1",
      "~Zhixin_Wang6",
      "~Yong_Guo1",
      "~Jiaqi_Xu1",
      "~Haoyu_Chen2",
      "~Jin_Han1",
      "~Fenglong_Song1",
      "~Yujiu_Yang2",
      "~Wenbo_Li6"
    ],
    "pdf": "/pdf/f4e7e56c587c6b1430af40012051b0c3d0dab56c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for edge deployment, designing ultra-lightweight models with reduced parameters and improved efficiency. It proposes techniques like LiteED (97.5% parameter reduction), online annealing pruning, and multi-layer feature distillation loss. These approaches directly address reducing computational cost, improving speed (0.8 seconds for 4K images), and optimizing model size (146M parameters) - all core aspects of my research interests in optimization.",
      "Irrelevant Aspects": "The paper applies to image super-resolution rather than language models. It focuses on mobile/edge deployment rather than large-scale data center GPU utilization. The generative modeling capabilities discussed are for images, not text processing. While optimization techniques are relevant, the specific application domain is outside my primary focus.",
      "Summary": "PocketSR presents efficient model optimization techniques for image super-resolution on mobile devices. While not directly focused on language models, it offers valuable insights on model compression, pruning, and distillation techniques that can inform my research on optimization of large models. The paper demonstrates significant performance improvements while maintaining quality, which aligns with my interests in achieving better throughput and lower latency."
    }
  },
  {
    "id": "s3IT4Qo7bm",
    "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval",
    "abstract": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla decoding.",
    "authors": [
      "~Wenhao_Li20",
      "~Yuxin_Zhang3",
      "~Gen_Luo1",
      "~Haiyuan_Wan1",
      "~ZiYang_Gong1",
      "~Fei_Chao1",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/ed67f98841d797ac112893481f9f76623cdb124a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on LLM inference optimization, KV cache reduction which is a key bottleneck in LLM inference, improved GPU utilization through specialized CUDA kernels, reported 3× higher throughput than vanilla decoding, efficient token retrieval (512K tokens in under 100μs), lightweight training framework optimized for GPU memory constraints, and scalability to large context windows",
      "Irrelevant Aspects": "Specific focus on hashing-based retrieval rather than broader optimization techniques, attention mechanism variant rather than general optimization framework, limited discussion on distributed training/inference across multiple GPUs, no clear mention of quantization or other compression techniques",
      "Summary": "This paper introduces Spotlight Attention, a novel method to reduce the KV cache burden in LLMs for inference acceleration. It uses non-linear hashing to optimize embedding distribution of queries and keys, improving retrieval precision while shortening hash codes by at least 5× compared to linear hashing. The implementation includes specialized CUDA kernels achieving hashing retrieval for 512K tokens in under 100μs on a single A100 GPU, with end-to-end throughput up to 3× higher than vanilla decoding. The training framework is lightweight and can run on 16GB GPUs in 8 hours."
    }
  },
  {
    "id": "iZk78dZ1Ap",
    "title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws",
    "abstract": "Scaling laws are typically fit using a family of models with a narrow range of frozen hyperparameter choices. \nIn this work we study scaling laws using multiple architectural shapes and hyperparameter choices, highlighting their impact on resulting prescriptions.\nAs a primary artifact of our research, we release the Gemstones: an open-source scaling law dataset, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters and diverse architectural shapes; including ablations over learning rate and cooldown.\nOur checkpoints enable more complex studies of scaling, such as analyzing the relationship between width and depth.\nBy examining our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting.",
    "authors": [
      "~Sean_Michael_McLeish1",
      "~John_Kirchenbauer1",
      "~David_Yu_Miller1",
      "~Siddharth_Singh7",
      "~Abhinav_Bhatele1",
      "~Micah_Goldblum1",
      "~Ashwinee_Panda1",
      "~Tom_Goldstein1"
    ],
    "pdf": "/pdf/95127d65e12116da8d728fa548c14863b0f180b5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on scaling laws for language models, directly related to optimizing training of LLMs. It studies impact of architectural shapes and hyperparameters on scaling laws, central to GPU utilization and scalability. Includes transformers up to 2 billion parameters, relevant to large language models. Examines width-depth relationship in transformers, crucial for model optimization. Analyzes how experimental design affects scaling law prescriptions, relevant to determining optimal training strategies.",
      "Irrelevant Aspects": "Doesn't focus specifically on inference optimization, one of my key interests. No explicit mention of GPU utilization techniques or specific hardware optimizations. Focuses more on empirical study of scaling laws rather than practical implementation details for optimization. No clear mention of techniques for reducing latency during inference or maximizing throughput.",
      "Summary": "This paper is highly relevant to training optimization of LLMs, particularly in understanding how different architectural choices affect scaling laws. It provides valuable empirical data through the Gemstones dataset of over 4000 checkpoints with diverse architectural shapes. The findings that scaling law prescriptions can be highly sensitive to experimental design are important for developing optimal training strategies that maximize GPU utilization and scalability. However, the paper doesn't address inference optimization aspects as much."
    }
  },
  {
    "id": "yPXOfBoQL7",
    "title": "FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic",
    "abstract": "Low-bit floating-point (FP) formats, such as FP8, provide significant acceleration and memory savings in model training thanks to native hardware support on modern GPUs and NPUs. However, we analyze that FP8 quantization offers speedup primarily for large-dimensional matrix multiplications, while inherent quantization overheads diminish speedup when applied to low-rank adaptation (LoRA), which uses small-dimensional matrices for efficient fine-tuning of large language models (LLMs). To address this limitation, we propose FALQON, a novel framework that eliminates the quantization overhead from separate LoRA computational paths by directly merging LoRA adapters into an FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the forward and backward computations for merged adapters to significantly reduce quantization overhead, and introduce a row-wise proxy update mechanism that efficiently integrates substantial updates into the quantized backbone. Experimental evaluations demonstrate that FALQON achieves approximately a 3$\\times$ training speedup over existing quantized LoRA methods with a similar level of accuracy, providing a practical solution for efficient large-scale model fine-tuning. Moreover, FALQON’s end-to-end FP8 workflow removes the need for post-training quantization, facilitating efficient deployment. Code is available at https://github.com/iamkanghyunchoi/falqon.",
    "authors": [
      "~Kanghyun_Choi1",
      "~Hyeyoon_Lee1",
      "~SunJong_Park1",
      "~Dain_Kwon1",
      "~Jinho_Lee2"
    ],
    "pdf": "/pdf/5a683d332fb2a627ab2b9914a6fb47c792ba9c35.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models, focusing on improving GPU utilization and throughput (achieving 3× training speedup), which aligns with my research interest. It deals with quantization techniques (FP8), an important approach for optimizing LLM training and inference. The work discusses practical implementation aspects, including the elimination of post-training quantization, which relates to deployment efficiency. It addresses scalability issues in large-scale model fine-tuning.",
      "Irrelevant Aspects": "The paper is specifically focused on LoRA fine-tuning rather than full model training or inference optimization. The quantization approach is limited to FP8, not exploring other quantization methods. There's no detailed discussion of inference optimization specifically, only mention that the approach 'facilitates efficient deployment.'",
      "Summary": "FALQON proposes a novel framework to accelerate LoRA fine-tuning of LLMs using FP8 quantization. The key innovation is merging LoRA adapters directly into an FP8-quantized backbone during fine-tuning, reformulating computations, and introducing a row-wise proxy update mechanism. This approach eliminates quantization overhead from separate LoRA computational paths, achieving approximately 3× training speedover existing quantized LoRA methods while maintaining accuracy. The framework provides an end-to-end FP8 workflow that removes the need for post-training quantization, facilitating efficient deployment."
    }
  },
  {
    "id": "rYv42fDKQi",
    "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment",
    "abstract": "Test-time adaptation (TTA) enhances the zero-shot robustness under distribution shifts by leveraging unlabeled test data during inference. Despite notable advances, several challenges still limit its broader applicability. First, most methods rely on backpropagation or iterative optimization, which limits scalability and hinders real-time deployment. Second, they lack explicit modeling of class-conditional feature distributions. This modeling is crucial for producing reliable decision boundaries and calibrated predictions, but it remains underexplored due to the lack of both source data and supervision at test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and backPropagation-free Test-time adaptation method. We reframe TTA as a probabilistic inference task by modeling class-conditional likelihoods using gradually updated class means and a shared covariance matrix. This enables closed-form, training-free inference. To correct potential likelihood bias, we introduce lightweight regularization guided by CLIP priors and a historical knowledge bank. ADAPT requires no source data, no gradient updates, and no full access to target data, supporting both online and transductive settings. Extensive experiments across diverse benchmarks demonstrate that our method achieves state-of-the-art performance under a wide range of distribution shifts with superior scalability and robustness.",
    "authors": [
      "~Youjia_Zhang3",
      "~Youngeun_Kim1",
      "~Young-Geun_Choi1",
      "~Hongyeob_Kim1",
      "~Huiling_Liu1",
      "~Sungeun_Hong2"
    ],
    "pdf": "/pdf/66f67be0809d947c24e38cf89235a80f90f3468d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses test-time adaptation without backpropagation, which is directly relevant to inference optimization. Its focus on closed-form, training-free inference aligns with reducing computational overhead during inference. The emphasis on scalability and real-time deployment connects to my research on improving throughput and reducing latency. The method's gradient-free approach could potentially improve GPU utilization by avoiding expensive backward passes. The paper addresses both online and transductive settings, which relates to different deployment scenarios in production systems.",
      "Irrelevant Aspects": "The paper doesn't specifically focus on large language models, which is my primary area of expertise. There's no explicit discussion of GPU utilization optimization techniques. The work focuses on distribution shift adaptation rather than general inference optimization for large models. The paper doesn't address training optimization, which is another key part of my research focus. The method seems more oriented towards computer vision tasks (with CLIP references) rather than natural language processing tasks.",
      "Summary": "This paper presents ADAPT, a backpropagation-free test-time adaptation method that reframes TTA as a probabilistic inference task using Gaussian distributions. While not directly targeting LLMs, its gradient-free approach and focus on scalable, real-time inference make it moderately relevant to my research interests in inference optimization. The method's closed-form solution could potentially improve inference efficiency, though its applications to language models remain unclear."
    }
  },
  {
    "id": "KGt0F2yjBz",
    "title": "Angles Don’t Lie: Unlocking Training‑Efficient RL Through the Model’s Own Signals",
    "abstract": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed *angle concentration* that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. \nBy leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5$\\times$ acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data.",
    "authors": [
      "~Qinsi_Wang2",
      "~Jinghan_Ke2",
      "~Hancheng_Ye1",
      "~Yueqian_Lin2",
      "~Yuzhe_Fu1",
      "~Jianyi_Zhang1",
      "~Kurt_Keutzer1",
      "~Chenfeng_Xu1",
      "~Yiran_Chen1"
    ],
    "pdf": "/pdf/989737f7297e9d1317a70bd84674046b1ef2d6f1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training efficiency for LLMs, tackles sample inefficiency in Reinforcement Fine-tuning, considers varying model scales (scalability), reports 2.5× acceleration in training efficiency (GPU utilization), achieves data efficiency (better performance with half data), and uses gradient-driven approaches for impactful updates.",
      "Irrelevant Aspects": "The paper focuses solely on training efficiency without addressing inference optimization, doesn't consider low latency improvements during inference, lacks specific GPU utilization implementation details, and doesn't mention hardware-specific optimizations.",
      "Summary": "This paper presents GAIN-RL, a framework that leverages a model-inherent signal called 'angle concentration' to dynamically select training data during Reinforcement Fine-tuning of LLMs. The method correlates angular distribution of token hidden states with resulting gradients, enabling more efficient training by selecting data that provides consistently impactful gradient updates. It achieves 2.5× acceleration in training efficiency and better performance with half the data across mathematical and coding tasks and various model scales. While highly relevant to training optimization, it doesn't address inference optimization aspects."
    }
  },
  {
    "id": "R6m6bNnmWm",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
    "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens.\nHowever, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution.\nTherefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink.\nIt starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks.\nWe adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreoever, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio.\nExtensive experiments demonstrate the superiority, efficiency, and effectiveness of our method.\nAll our code and data are open-sourced.",
    "authors": [
      "~Senqiao_Yang1",
      "~Junyi_Li12",
      "~Xin_Lai1",
      "~Jinming_Wu1",
      "~Wei_Li78",
      "~Zejun_MA1",
      "~Bei_Yu2",
      "~Hengshuang_Zhao2",
      "~Jiaya_Jia1"
    ],
    "pdf": "/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Efficient token processing, dynamic resource allocation, reinforcement learning for optimization, GPU utilization improvement, throughput enhancement, memory efficiency, intelligent compression strategy, performance-efficiency trade-offs, scalable architecture design",
      "Irrelevant Aspects": "Specific vision task analysis (OCR vs. general VQA), vision-language domain focus rather than pure language models, visual token specifics, LLM-as-Judge implementation details, image processing techniques",
      "Summary": "VisionThink presents a reinforcement learning approach to dynamically optimize visual token processing in vision-language models. By starting with downsampled images and intelligently deciding when higher resolution is needed, it achieves significant computational savings while maintaining performance. This directly addresses resource utilization, a key concern for my research on ML system optimization. The RL-based decision-making framework and token compression strategies offer valuable insights for optimizing GPU utilization and throughput, though the vision-specific aspects make it slightly less relevant than pure language model optimization work."
    }
  },
  {
    "id": "D6PwC6Xogv",
    "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners",
    "abstract": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling Fine-Tuning (RFT), is an integral part of the training pipeline of self-improving reasoning Language Models (LMs). \nThe self-improving mechanism often employs random observation (data) sampling. \nHowever, this results in trained observation imbalance; inefficiently over-training on solved examples while under-training on challenging ones. \nIn response, we introduce Adaptive STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting balanced training across observations, and (2) Adaptive Sampling for Curriculum: dynamically adjusting data difficulty to match the model's evolving strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all instances (6/6) and reduces training FLOPs by an average of 58.6\\% against an extensive list of baselines. These improvements in performance and efficiency generalize to different pre-trained LMs and larger models, paving the way for more efficient and effective self-improving LMs.",
    "authors": [
      "~Woosung_Koh1",
      "~Wonbeen_Oh1",
      "~Jaein_Jang1",
      "~MinHyung_Lee1",
      "~Hyeongjin_Kim2",
      "~Ah_Yeon_Kim1",
      "~Joonkee_Kim1",
      "~Junghyun_Lee1",
      "~Taehyeon_Kim1",
      "~Se-Young_Yun1"
    ],
    "pdf": "/pdf/4abd53ac71c0b8910baa3a191e85753fc51b834e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization for large language models, efficient training with 58.6% FLOP reduction, scalability to larger models, training pipeline optimization through adaptive data sampling.",
      "Irrelevant Aspects": "Limited focus on Self-Taught Reasoners specifically, no direct mention of inference optimization, no explicit GPU utilization discussion, no throughput or latency metrics.",
      "Summary": "The paper presents AdaSTaR, an adaptive data sampling algorithm for training self-improving reasoning language models. It claims significant training efficiency improvements (58.6% FLOP reduction) while maintaining or improving accuracy across benchmarks. While highly relevant to training optimization, it doesn't address the full scope of the research interest, particularly inference optimization aspects."
    }
  },
  {
    "id": "7L4NvUtZY3",
    "title": "FlashBias: Fast Computation of Attention with Bias",
    "abstract": "Attention with bias, which extends standard attention by introducing prior knowledge as an additive bias matrix to the query-key scores, has been widely deployed in vision, language, protein-folding and other advanced scientific models, underscoring its status as a key evolution of this foundational module. However, introducing bias terms creates a severe efficiency bottleneck in attention computation. It disrupts the tightly fused memory-compute pipeline that underlies the speed of accelerators like FlashAttention, thereby stripping away most of their performance gains and leaving biased attention computationally expensive. Surprisingly, despite its common usage, targeted efficiency optimization for attention with bias remains absent, which seriously hinders its application in complex tasks. Diving into the computation of FlashAttention, we prove that its optimal efficiency is determined by the rank of the attention weight matrix. Inspired by this theoretical result, this paper presents FlashBias based on the low-rank compressed sensing theory, which can provide fast-exact computation for many widely used attention biases and a fast-accurate approximation for biases in general formalizations. FlashBias can fully take advantage of the extremely optimized matrix multiplication operation in modern GPUs, achieving 1.5$\\times$ speedup for Pairformer in AlphaFold 3, and over 2$\\times$ speedup for attention with bias in vision and language models without loss of accuracy. Code is available at this repository: https://github.com/thuml/FlashBias.",
    "authors": [
      "~Haixu_Wu1",
      "~Minghao_Guo1",
      "~Yuezhou_Ma1",
      "~Yuanxu_Sun1",
      "~Jianmin_Wang1",
      "~Wojciech_Matusik2",
      "~Mingsheng_Long5"
    ],
    "pdf": "/pdf/cf182076c5bccd2ad8c6482bfffe7da2298ddcc6.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses attention computation optimization, which is fundamental to LLM performance. It focuses on improving GPU utilization and provides measurable speedups (1.5×-2×) without accuracy loss. The work on optimizing matrix multiplication operations in GPUs is highly relevant for both LLM training and inference. FlashBias specifically addresses language models among its applications, making it directly pertinent to my research focus.",
      "Irrelevant Aspects": "While the paper has applications in vision and protein-folding models, these are less directly relevant to my LLM focus. The abstract doesn't clearly distinguish between training and inference optimizations, so the specific benefits for each phase aren't fully clear.",
      "Summary": "FlashBias presents a novel approach to optimize attention mechanisms with bias terms, which are widely used in advanced models but create computational bottlenecks. By leveraging theoretical insights about the rank of attention weight matrices and low-rank compressed sensing theory, FlashBias enables efficient computation while maintaining accuracy. The solution achieves significant speedups (1.5×-2×) for attention with bias in various models, including language models, making it highly relevant to LLM optimization research."
    }
  },
  {
    "id": "894Yo61h1P",
    "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning",
    "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers.\nWhile this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and reduce throughput of token generation, limiting the practical deployment of such models.\nWe propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths.\nRPC periodically compresses the KV cache by retaining cache entries that receive high importance score, which are computed using a selector window composed of recently generated queries.\nExperiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark.\nOur findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
    "authors": [
      "~Jiwon_Song1",
      "~Dongwon_Jo1",
      "~Yulhwa_Kim1",
      "~Jae-Joon_Kim2"
    ],
    "pdf": "/pdf/8e675a892e87d2f3f98f3a4c08cb1e44bbf0f712.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization for LLMs, KV cache compression, Throughput improvement (1.60×), Memory usage reduction, GPU utilization enhancement, Scalability for reasoning LLMs, Training-free method",
      "Irrelevant Aspects": "No training optimization focus, Specific to reasoning models only, Limited evaluation (single benchmark mentioned)",
      "Summary": "The paper introduces Reasoning Path Compression (RPC), a training-free method to accelerate inference for reasoning-focused LLMs by exploiting semantic sparsity in reasoning paths. RPC periodically compresses the KV cache based on importance scores, achieving 1.60× throughput improvement with minimal accuracy drop (1.2%). This directly addresses key concerns in LLM deployment efficiency, memory usage, and throughput, making it highly relevant to inference optimization research."
    }
  },
  {
    "id": "sbmYVM4zRr",
    "title": "Chain-of-Model Learning for Language Model",
    "abstract": "In this paper, we propose a novel learning paradigm, termed *Chain-of-Model* (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style. thereby introducing great scaling efficiency in model training and inference flexibility in deployment.We introduce the concept of *Chain-of-Representation* (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains). In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise *Chain-of-Language-Model* (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a *KV sharing* mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models.",
    "authors": [
      "~Xiaohua_Wang2",
      "~Kaitao_Song1",
      "~Xu_Tan1",
      "~Huiqiang_Jiang2",
      "~Chengruidong_Zhang1",
      "~Yongliang_Shen1",
      "~Cen_LU1",
      "~Zihao_Li9",
      "~Zifan_Song1",
      "~Caihua_Shan1",
      "~Yansen_Wang2",
      "~Kan_Ren1",
      "~Xiaoqing_Zheng2",
      "~Tao_Qin1",
      "~Yuqing_Yang1",
      "~Dongsheng_Li2",
      "~Lili_Qiu3"
    ],
    "pdf": "/pdf/f468d111a7d3d8513acd9d5d6cfebffac9fc381a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces a novel approach that offers great scaling efficiency in model training and inference flexibility. It enables progressive scaling of model size and provides multiple sub-models at varying sizes for elastic inference. The KV sharing mechanism in CoLM-Air specifically addresses inference optimization. The approach allows for seamless LM switching and prefilling acceleration, which directly impacts training and inference efficiency. The framework offers progressive scaling to improve training efficiency while maintaining comparable performance to standard Transformers.",
      "Irrelevant Aspects": "The paper focuses on introducing a new learning paradigm rather than optimizing existing architectures. There is no explicit discussion of GPU utilization or throughput metrics. The paper doesn't directly address latency metrics in detail.",
      "Summary": "This paper proposes the Chain-of-Model (CoM) paradigm that introduces causal relationships between hidden states in each layer, enabling more efficient training and flexible inference. The approach allows for progressive scaling of model size and elastic inference through varying chain numbers. The KV sharing mechanism in CoLM-Air further enhances efficiency by computing keys and values once and sharing them across chains. While not explicitly addressing GPU utilization or latency metrics, the techniques presented offer promising avenues for optimizing both training and inference of language models, making it highly relevant to my research interests in large language model optimization."
    }
  },
  {
    "id": "oUghNi5XWc",
    "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs",
    "abstract": "Multimodal Large Language Models (MLLMs) typically process a large number of visual tokens, leading to considerable computational overhead, even though many of these tokens are redundant. Existing visual token pruning methods primarily focus on selecting the most salient tokens based on attention scores, resulting in the semantic incompleteness of the selected tokens. In this paper, we propose a novel visual token pruning strategy, called **S**aliency-**C**overage **O**riented token **P**runing for **E**fficient MLLMs (SCOPE), to jointly model both the saliency and coverage of the selected visual tokens to better preserve semantic completeness. Specifically, we introduce a set-coverage for a given set of selected tokens, computed based on the token relationships. We then define a token-coverage gain for each unselected token, quantifying how much additional coverage would be obtained by including it. By integrating the saliency score into the token-coverage gain, we propose our SCOPE score and iteratively select the token with the highest SCOPE score. We conduct extensive experiments on multiple vision-language understanding benchmarks using the LLaVA-1.5 and LLaVA-Next models. Experimental results demonstrate that our method consistently outperforms prior approaches.",
    "authors": [
      "~Jinhong_Deng1",
      "~Wen_Li2",
      "~Joey_Tianyi_Zhou1",
      "~Yang_He2"
    ],
    "pdf": "/pdf/c831447b06dbadc03fcdd6afd8a908e5b220eb4c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses computational efficiency in multimodal LLMs, focusing on token pruning to reduce computational overhead, which can improve GPU utilization and throughput. It works with widely used models like LLaVA and aims to maintain performance while improving efficiency, which is central to inference optimization.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU utilization metrics, scalability aspects of training or distributed inference, or system-level optimizations. It focuses more on the algorithmic approach rather than implementation details that would affect real-world deployment efficiency.",
      "Summary": "The paper presents SCOPE, a token pruning method for multimodal LLMs that considers both saliency and coverage when selecting visual tokens to reduce computational overhead while maintaining semantic completeness. It demonstrates effectiveness on LLaVA models across various vision-language benchmarks. While contributing to inference optimization, it focuses more on the algorithmic approach rather than system-level optimizations."
    }
  },
  {
    "id": "l6C6Pw30Gl",
    "title": "Mixture of Inputs: Text Generation Beyond Discrete Token Sampling",
    "abstract": "In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution’s rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.",
    "authors": [
      "~Yufan_Zhuang1",
      "~Liyuan_Liu3",
      "~Chandan_Singh1",
      "~Jingbo_Shang2",
      "~Jianfeng_Gao1"
    ],
    "pdf": "/pdf/ffbd9e60685bc4a0005eada53244f25de6aea675.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents Mixture of Inputs (MoI), a training-free inference optimization method that improves text generation quality without additional computational overhead. It works across multiple model architectures (QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B) and enhances reasoning capabilities while maintaining efficiency. The approach preserves token distribution information that would normally be discarded in standard autoregressive generation, potentially improving model performance without additional training costs.",
      "Irrelevant Aspects": "The paper focuses primarily on generation quality improvements rather than system-level optimizations like GPU utilization, distributed processing, or detailed throughput/latency measurements. As a training-free method, it doesn't address training optimization techniques which are part of my research focus. The abstract doesn't provide details about computational complexity analysis or memory efficiency gains during inference.",
      "Summary": "Mixture of Inputs (MoI) proposes a novel inference optimization technique that enhances autoregressive generation by preserving and utilizing token distribution information that is typically discarded after sampling. This training-free method blends discrete tokens with probability distributions using Bayesian estimation, creating continuous posterior expectations as model inputs. The approach consistently improves performance across multiple large models (32B-49B parameters) on reasoning and generation tasks with negligible computational overhead, making it relevant for inference optimization research, though it lacks detailed system performance metrics."
    }
  },
  {
    "id": "JrZY7ilKLs",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "abstract": "While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.",
    "authors": [
      "~Zeju_Qiu1",
      "~Simon_Buchholz1",
      "~Tim_Z._Xiao1",
      "~Maximilian_Dax1",
      "~Bernhard_Schölkopf1",
      "~Weiyang_Liu1"
    ],
    "pdf": "/pdf/c0636d840d7b5e6a3b0ad84dec8a95d0ea5ceda1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models, proposes a novel reparameterization technique (POET) using orthogonal equivalence transformation, mentions developing efficient approximations for scalability in large-scale neural networks, and claims to preserve spectral properties for stable optimization with improved generalization.",
      "Irrelevant Aspects": "The paper does not explicitly discuss inference optimization, GPU utilization techniques, latency improvements, or hardware-specific optimizations. It focuses on mathematical reparameterization rather than system-level performance optimizations.",
      "Summary": "The paper introduces POET, a reparameterization method for LLM training that uses orthogonal equivalence transformation to optimize neurons. It reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, claiming to preserve spectral properties for stable optimization and better generalization. The authors developed efficient approximations to make POET scalable for large neural networks. The paper focuses primarily on mathematical training optimizations rather than system-level performance improvements."
    }
  },
  {
    "id": "s4LnWgjacg",
    "title": "The Primacy of Magnitude in Low-Rank Adaptation",
    "abstract": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive “Noise \\& Zeros” scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven “Basis \\& Basis” initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence.\nWe prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. \n(ii) Spectral initialization succeeds via magnitude amplification.\n We demystify that the presumed knowledge-driven benefit of spectral component essentially arises from the boost in the weight update magnitude.\n(iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks.",
    "authors": [
      "~Zicheng_Zhang3",
      "~Haoran_Li17",
      "~Yifeng_Zhang4",
      "~Guoqiang_Gong1",
      "~Jiaxing_Wang1",
      "~pengzhang_liu1",
      "~Qixia_Jiang3",
      "~Junxing_Hu1"
    ],
    "pdf": "/pdf/121809dfba3d2c5a29cd17820f8bd69ccf63b4cc.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on Low-Rank Adaptation (LoRA) optimization, which is directly related to parameter-efficient fine-tuning of large language models. It addresses computational and storage overhead concerns, which are critical for GPU utilization. The proposed LoRAM initialization method aims to improve performance without sacrificing efficiency, which could enhance throughput during model adaptation. The analysis of weight update magnitude provides insights into optimization mechanisms for training.",
      "Irrelevant Aspects": "The paper primarily focuses on training/fine-tuning optimization rather than inference optimization, which is a core aspect of my research interest. There's limited discussion of GPU utilization during inference or latency-specific optimizations. The theoretical foundations of magnitude in weight updates, while academically interesting, may not provide practical implementation guidance for production scalability.",
      "Summary": "This paper analyzes performance drivers in LoRA and proposes LoRAM, a new initialization scheme that maintains parameter efficiency while potentially improving training performance. It focuses on magnitude-driven initialization rather than spectral methods, claiming to match performance without computational overhead. While relevant to training optimization and parameter efficiency, it has limited direct applicability to inference optimization or production-scale GPU utilization concerns."
    }
  },
  {
    "id": "0D3ja9s17M",
    "title": "FlexSelect: Flexible Token Selection for Efficient  Long Video Understanding",
    "abstract": "Long-form video understanding poses a significant challenge for video large language models (VideoLLMs) due to prohibitively high computational and memory demands. \nIn this paper, We propose $\\textbf{FlexSelect}$, a flexible and efficient token selection strategy for processing long videos.\nFlexSelect identifies and retains the most semantically relevant content by leveraging cross-modal attention patterns from a reference transformer layer.\nIt comprises two key components: (1) $\\textbf{a training-free token ranking pipeline}$ that leverages faithful cross-modal attention weights to estimate each video token’s importance, and (2) $\\textbf{a rank-supervised lightweight selector}$ that is trained to replicate these rankings and filter redundant tokens.\nThis generic approach can be seamlessly integrated into various VideoLLM architectures, such as LLaVA-Video, InternVL and Qwen-VL, serving as a plug-and-play module to extend their temporal context length. Empirically, FlexSelect delivers strong gains across multiple long-video benchmarks – including VideoMME, MLVU, LongVB, and LVBench. Morever, it achieves significant speed-ups ($\\textit{e.g.,}$ up to 9 $\\times$ on a LLaVA-Video-7B model), highlighting FlexSelect’s promise for efficient long-form video understanding. Project page: https://flexselect.github.io",
    "authors": [
      "~Yunzhuzhang1",
      "~Yu_Lu11",
      "~Tianyi_Wang12",
      "~Fengyun_Rao2",
      "~Yi_Yang22",
      "~Linchao_Zhu1"
    ],
    "pdf": "/pdf/d3bab82ef67e7141dd196deca2e83f109f4db49b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "FlexSelect addresses inference optimization by developing a token selection strategy that reduces computational and memory demands for VideoLLMs, achieving significant speed-ups (up to 9×). The approach improves GPU utilization and throughput through a lightweight selector that filters redundant tokens. The method is architecture-agnostic and can be integrated as a plug-and-play module into various VideoLLMs, demonstrating good scalability. The paper leverages cross-modal attention patterns from a reference transformer layer, which is relevant to transformer optimization techniques.",
      "Irrelevant Aspects": "The paper focuses specifically on video understanding, a specialized application domain rather than general LLM optimization. It primarily addresses cross-modal attention between video and text rather than text-only models. The paper emphasizes inference optimization with limited discussion of training optimization techniques. Without the full paper, it's unclear how much architectural optimization of the underlying transformer models is covered.",
      "Summary": "FlexSelect introduces a token selection strategy to improve efficiency in processing long videos with VideoLLMs. The approach identifies semantically relevant content while filtering redundant tokens, significantly reducing computational and memory demands. It achieves up to 9× speed-up while maintaining performance across benchmarks. The method consists of a training-free token ranking pipeline and a lightweight selector that can be integrated into various VideoLLM architectures as a plug-and-play module."
    }
  },
  {
    "id": "5xwyxupsLL",
    "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
    "abstract": "This paper presents PipeFusion, an innovative parallel methodology to tackle the high latency issues associated with generating high-resolution images using diffusion transformers (DiTs) models.  PipeFusion partitions images into patches and the model layers across multiple GPUs. It employs a patch-level pipeline parallel strategy to orchestrate communication and computation efficiently.  By capitalizing on the high similarity between inputs from successive diffusion steps, PipeFusion reuses one-step stale feature maps to provide context for the current pipeline step. This approach notably reduces communication costs compared to existing DiTs inference parallelism, including tensor parallel, sequence parallel and DistriFusion. PipeFusion enhances memory efficiency through parameter distribution across devices, ideal for large DiTs like Flux.1. Experimental results demonstrate that PipeFusion achieves state-of-the-art performance on 8$\\times$L40 PCIe GPUs for Pixart, Stable-Diffusion 3, and Flux.1 models. Our Source code is available at \\url{https://github.com/xdit-project/xDiT}.",
    "authors": [
      "~Jiarui_Fang2",
      "~Jinzhe_Pan1",
      "~Aoyu_Li2",
      "~Xibo_Sun1",
      "~WANG_Jiannan1"
    ],
    "pdf": "/pdf/47201721f3487d801bf9a26fcf23e16bcefe4735.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization for large transformer models, pipeline parallelism across multiple GPUs, communication reduction strategies, memory efficiency improvements, GPU utilization optimization, throughput and latency improvements, scalability solutions for large models",
      "Irrelevant Aspects": "Focus on diffusion transformers for image generation rather than language models, patch-level partitioning specific to 2D image data rather than sequential text, stale feature map reuse based on diffusion step similarity rather than token sequence patterns",
      "Summary": "PipeFusion introduces patch-level pipeline parallelism for diffusion transformer inference, partitioning both images and model layers across GPUs. It reduces communication overhead through innovative feature reuse strategies and achieves better memory efficiency. While focused on image generation, its parallelization techniques for large transformer inference have significant relevance to LLM inference optimization challenges."
    }
  },
  {
    "id": "VrYCLQ5inI",
    "title": "Faster Video Diffusion with Trainable Sparse Attention",
    "abstract": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions.  We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in diffusion loss. Retrofitting the open-source Wan2.1-1.3B model speeds up attention time by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with comparable quality, while for the 14B model, end-to-end generation time is reduced from 1274s to 576s.\nFurthermore, we introduce a preliminary study of Sparse-Distill, the first method to enable  sparse attention and distillation concurrently, achieving 50.9x speed up for Wan-1.3B while maintaining quality.\nThese results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.  Code is available at https://github.com/hao-ai-lab/FastVideo.",
    "authors": [
      "~Peiyuan_Zhang2",
      "~Yongqi_Chen3",
      "~Haofeng_Huang3",
      "~Will_Lin1",
      "~Zhengzhong_Liu1",
      "~Ion_Stoica1",
      "~Eric_Xing1",
      "~Hao_Zhang2"
    ],
    "pdf": "/pdf/ad00c4c39f2be3b485a83607471ed25cd7cceff4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Hardware-efficient sparse attention mechanisms that optimize transformer architectures, end-to-end differentiable kernels, training/inference acceleration techniques, scaling laws for large models, significant reduction in FLOPs (2.53×) without quality loss, attention speedup (6×) and end-to-end generation time improvements (from 31s to 18s for 1.3B model and 1274s to 576s for 14B model), integration with distillation techniques (Sparse-Distill achieving 50.9x speedup), practical implementation without requiring post-hoc profiling, and block computing layouts for hardware efficiency.",
      "Irrelevant Aspects": "Primary focus on video diffusion models rather than language models, 3D attention computation specific to video processing, application domain of video generation rather than text generation or understanding, and domain-specific quality metrics for visual content.",
      "Summary": "This paper presents VSA (trainable sparse attention) for video diffusion transformers that significantly improves computational efficiency while maintaining quality. It introduces a two-stage approach with coarse token pooling and fine attention computation only on critical tokens. The technique achieves 85% of FlashAttention3's MFU, reduces training FLOPs by 2.53×, speeds up attention by 6×, and reduces end-to-end generation time by 40-55%. The authors also introduce Sparse-Distill combining sparse attention with distillation for 50.9× speedup. While focused on video diffusion, the sparse attention principles and optimization techniques are highly relevant to LLM efficiency research."
    }
  },
  {
    "id": "kXieirlPjF",
    "title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving",
    "abstract": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards  enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.",
    "authors": [
      "~Xinji_Mai1",
      "~Haotian_Xu1",
      "~Xing_W1",
      "~Weinong_Wang1",
      "~Yingying_Zhang2",
      "~Wenqiang_Zhang1"
    ],
    "pdf": "/pdf/6d8a6cfbc785966d9381495a786eea81de48d681.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates scaling laws in RL training for LLMs, which directly relates to scalability concerns and resource utilization. It explores RL optimization techniques for LLMs. The mention of a 'robust framework' with 'decoupled code execution environment' could provide insights into efficient implementations and GPU utilization. The validation across standard RL algorithms and frameworks might offer insights into which approaches scale better. The released code implementation might contain optimizations relevant to GPU utilization and throughput.",
      "Irrelevant Aspects": "The focus on mathematical problem solving as the application domain is not directly related to optimization concerns. The emphasis on tool use emergence is more about capabilities than efficiency. There is no explicit mention of inference optimization, throughput, or latency optimization, which are central to my research interests.",
      "Summary": "This paper investigates scaling laws for RL training of LLMs to spontaneously use code execution for mathematical reasoning. It addresses several aspects relevant to my research interests, particularly scaling laws for RL training, implementation frameworks, and optimization techniques. While it doesn't directly focus on inference optimization or latency, its insights into training scalability and implementation efficiency are valuable. The paper's code release and framework descriptions could contain relevant optimization techniques for GPU utilization."
    }
  },
  {
    "id": "dbaYQyruY2",
    "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis",
    "abstract": "Reward modeling, crucial for aligning large language models (LLMs) with human preferences, is often bottlenecked by the high cost of preference data. Existing textual data synthesis methods are computationally expensive. We propose a novel framework LENS for synthesizing preference data directly in the LLM's latent embedding space. Our method employs a Variational Autoencoder (VAE) to learn a structured latent representation of response embeddings. By performing controlled perturbations in this latent space and decoding back to the embedding space, we efficiently generate diverse, semantically consistent synthetic preference pairs, bypassing costly text generation and annotation. We provide theoretical guarantees that our synthesized pairs approximately preserve original preference ordering and improve reward model generalization. Empirically, our latent-space synthesis significantly outperforms text-based augmentation on standard benchmarks, achieving superior results while being 18× faster in generation and using a 16,000× smaller model. Our work offers a scalable and effective alternative for enhancing reward modeling through efficient data augmentation. Code is publicly available at https://github.com/deeplearning-wisc/lens.",
    "authors": [
      "~Leitian_Tao1",
      "~Xuefeng_Du1",
      "~Sharon_Li1"
    ],
    "pdf": "/pdf/12e779439c922ba09d37907c04e017559b3a804c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization for LLMs by proposing an efficient method for synthesizing preference data in latent space, achieving 18× faster generation with a 16,000× smaller model. This directly relates to GPU utilization and scalability improvements. The method bypasses costly text generation, reducing computational requirements and increasing throughput in reward model training. Working directly in latent space improves efficiency, making it a relevant contribution to training optimization.",
      "Irrelevant Aspects": "The paper focuses specifically on reward modeling, which is a subset of LLM training. It doesn't address inference optimization, which is a significant part of my research interest. The paper lacks details on GPU utilization metrics or performance profiling. There's limited discussion on integration with broader training pipelines and no coverage of deployment considerations or production environment optimizations.",
      "Summary": "The paper proposes LENS, a framework for synthesizing preference data in LLM's latent embedding space using a VAE. It achieves significant computational efficiency improvements (18× faster generation, 16,000× smaller model) while maintaining quality of synthetic preference pairs. This directly contributes to training optimization and resource efficiency but doesn't address inference optimization, limiting its overall relevance to my broader research interests."
    }
  },
  {
    "id": "LWuhOoHpo5",
    "title": "OSKAR: Omnimodal Self-supervised Knowledge Abstraction and Representation",
    "abstract": "We present OSKAR, the first multimodal foundation model based on bootstrapped latent feature prediction. Unlike generative or contrastive methods, it avoids memorizing unnecessary details (e.g., pixels), and does not require negative pairs, large memory banks, or hand-crafted augmentations. We propose a novel pretraining strategy: given masked tokens from multiple modalities, predict a subset of missing tokens per modality, supervised by momentum-updated uni-modal target encoders. This design efficiently utilizes the model capacity in learning high-level representations while retaining modality-specific information. Further, we propose a scalable design which decouples the compute cost from the number of modalities using a fixed representative token budget—in both input and target tokens—and introduces a parameter-efficient cross-attention predictor that grounds each prediction in the full multimodal context. We instantiate OSKAR on video, skeleton, and text modalities. Extensive experimental results show that OSKAR's unified pretrained encoder outperforms models with specialized architectures  of similar size in action recognition (rgb, skeleton, frozen, low-shot) and localization, video-text retrieval, and video question answering. Project website: https://multimodal-oskar.github.io",
    "authors": [
      "~Mohamed_O_Abdelfattah2",
      "~Kaouther_Messaoud1",
      "~Alexandre_Alahi3"
    ],
    "pdf": "/pdf/156a5f26f94fda2e26c366236306318bb1073b80.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces a scalable multimodal architecture with efficient utilization of model capacity and parameter-efficient cross-attention predictors. These aspects directly relate to GPU utilization and scalability goals. The fixed representative token budget approach decouples computational cost from the number of modalities, which could improve throughput during training. The avoidance of negative pairs and large memory banks suggests memory optimization strategies relevant to efficient model training.",
      "Irrelevant Aspects": "The paper focuses primarily on multimodal representation learning rather than specifically addressing large language models. It doesn't explicitly discuss inference optimization techniques, latency measurements, or detailed GPU utilization metrics. The novelty seems centered on the representation learning methodology rather than computational optimizations for training or inference efficiency.",
      "Summary": "OSKAR presents a multimodal foundation model with several efficiency-oriented design choices including a fixed token budget and parameter-efficient prediction mechanisms. While these elements could positively impact GPU utilization and throughput, the paper's focus remains on representation quality rather than computational performance optimization. The approach offers insights into scalable multimodal architectures but doesn't directly address the core concerns of inference optimization and latency reduction central to the specified research interest."
    }
  },
  {
    "id": "jDxFD45kkc",
    "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
    "abstract": "We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.",
    "authors": [
      "~Kai_Lion1",
      "~Liang_Zhang6",
      "~Bingcong_Li1",
      "~Niao_He3"
    ],
    "pdf": "/pdf/5d59c2749e51962a3ba416becb8c785e85050de4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses low-rank adaptation (LoRA), which is directly relevant to parameter-efficient fine-tuning of large language models. It proposes a new parameterization (PoLAR) that improves upon standard LoRA by better utilizing the allocated subspace, potentially leading to better GPU utilization during training. The method shows faster convergence rates, which relates to training optimization. The paper tests on models ranging from 350M to 27B parameters, covering the scale of large models I'm interested in.",
      "Irrelevant Aspects": "The paper focuses primarily on training optimization rather than inference optimization. It emphasizes mathematical aspects of parameterization more than specific GPU utilization techniques or implementation details for hardware optimization.",
      "Summary": "PoLAR introduces a polar-decomposition-based parameterization for low-rank adaptation that improves subspace utilization and convergence rates in large model fine-tuning. It factorizes low-rank updates into direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix, showing consistent performance gains across multiple benchmarks and model sizes, making it relevant for efficient training of large language models."
    }
  },
  {
    "id": "JwnAItQF9v",
    "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding",
    "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment.\nThe training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features.\nExperiments on LLaMA, Vicuna, Qwen and Mixtral models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 8\\% and a speedup ratio exceeding 7\\%, outperforming current speculative decoding state-of-the-art methods. Our code and GRIFFIN's draft models will be released publicly in https://github.com/hsj576/GRIFFIN.",
    "authors": [
      "~Shijing_Hu1",
      "~Jingyang_Li4",
      "~Xingyu_Xie1",
      "~Zhihui_Lu2",
      "~Kim-chuan_Toh1",
      "~Pan_Zhou3"
    ],
    "pdf": "/pdf/2f7db559d934562ae3db43319f9b4b3c7a26c928.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Speculative decoding for LLM inference acceleration",
        "Token alignment between training and decoding phases",
        "Training strategy for optimizing draft models",
        "Speedup improvements (>7%) and acceptance length improvements (>8%)",
        "Performance evaluation on multiple models (LLaMA, Vicuna, Qwen, Mixtral)",
        "Focus on inference efficiency and throughput"
      ],
      "Irrelevant Aspects": [
        "Limited discussion on GPU utilization metrics",
        "Minimal focus on broader scalability considerations",
        "No analysis of memory optimization during inference",
        "Limited discussion on energy efficiency",
        "Primarily focused on a specific optimization technique rather than comprehensive system optimization"
      ],
      "Summary": "GRIFFIN introduces a novel framework for improving speculative decoding in LLMs by addressing token misalignment between training and decoding phases. The paper proposes a token-alignable training strategy with a loss masking mechanism and a token-alignable draft model. Experiments on multiple models demonstrate significant improvements in acceptance length and speedup ratio. While relevant to LLM inference optimization, the paper focuses primarily on a specific technique without broader system-level optimization considerations or detailed GPU utilization analysis."
    }
  },
  {
    "id": "jqDtzUQkmu",
    "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression",
    "abstract": "Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy—adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4$\\times$L20, it achieves $3.0\\times$ speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving $6.7\\times$ speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at https://github.com/Cobalt-27/CompactFusion",
    "authors": [
      "~Jiajun_Luo3",
      "~Yicheng_Xiao4",
      "~Jianru_Xu1",
      "~Yangxiu_You1",
      "~Rongwei_Lu1",
      "~Chen_Tang3",
      "~Jingyan_Jiang2",
      "~Zhi_Wang5"
    ],
    "pdf": "/pdf/8dbaf1c51b51ec8da3008b3c369caf02e48d1b39.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parallel inference optimization, GPU utilization and scalability, latency reduction, throughput improvement, communication overhead reduction in distributed systems",
      "Irrelevant Aspects": "Focus on diffusion models rather than LLMs, exploitation of temporal redundancy specific to diffusion model inference",
      "Summary": "This paper presents CompactFusion, a compression framework that reduces communication overhead in parallel diffusion model serving by transmitting only compressed residuals between steps. While the domain is diffusion models rather than LLMs, the parallel inference optimization techniques and focus on communication reduction are highly relevant to LLM serving optimization. The impressive 3.0× and 6.7× speedups demonstrate the effectiveness of the approach for improving GPU utilization and scalability. The residual compression technique could potentially inspire similar approaches for LLM inference systems where communication overhead is also a bottleneck."
    }
  },
  {
    "id": "JxxKj8pow1",
    "title": "Scaling Speculative Decoding with Lookahead Reasoning",
    "abstract": "Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level specualtive decoding (SD) helps, but its benefit is capped, \nbecause the chance that an entire $\\gamma$-token guess is correct falls exponentially as $\\gamma$ grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling -- making the speedup modest and hardware-agnostic. We raise this ceiling with lookahead reasoning, which exploits a second, step-level layer of parallelism. \nOur key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching.  In lookahead reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show lookahead reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, lookahead reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github.com/hao-ai-lab/LookaheadReasoning",
    "authors": [
      "~Yichao_Fu1",
      "~Rui_Ge1",
      "~Zelei_Shao2",
      "~Zhijie_Deng1",
      "~Hao_Zhang2"
    ],
    "pdf": "/pdf/6fa865a343372d69355a7146653c669f566a1277.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models, focusing on improving throughput and reducing latency. It introduces a technique (lookahead reasoning) that exploits multiple levels of parallelism for better GPU utilization. The approach specifically tackles the bottleneck of decoding long chains of thoughts in reasoning models, which is a significant challenge in efficient LLM deployment. The paper demonstrates empirical speedup improvements (1.4x to 2.1x) while preserving answer quality, indicating effective performance optimization. The method scales well with additional GPU throughput, suggesting good scalability properties.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization, which is part of my expertise. It targets reasoning models specifically rather than providing general LLM optimization techniques. There's no discussion of memory optimization strategies or hardware-specific optimizations that could complement the algorithmic improvements. The paper doesn't explore the interaction between model size/architecture and the proposed optimization technique.",
      "Summary": "This paper introduces 'lookahead reasoning' to enhance speculative decoding for reasoning models with long chains of thoughts. The key innovation is adding a second layer of step-level parallelism to the existing token-level parallelism, allowing a draft model to propose future steps that the target model can expand in batched passes. This approach raises the algorithmic ceiling of speculative decoding speedup, demonstrating improvements from 1.4x to 2.1x across benchmarks while maintaining answer quality. The technique shows better scaling with GPU throughput, addressing a crucial need for efficient LLM inference."
    }
  },
  {
    "id": "nn51ewu5k2",
    "title": "Efficiently Scaling LLM Reasoning Programs with Certaindex",
    "abstract": "Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50\\% compute savings and 3.3$\\times$ higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git",
    "authors": [
      "~Yichao_Fu1",
      "~Junda_Chen1",
      "~Siqi_Zhu1",
      "~Zheyu_Fu1",
      "~Zhongdongming_Dai1",
      "~Yonghao_Zhuang1",
      "~Yian_Ma1",
      "~Aurick_Qiao1",
      "~Tajana_Rosing1",
      "~Ion_Stoica1",
      "~Hao_Zhang2"
    ],
    "pdf": "/pdf/4ca50f6ea693aeda7b95d22f1929d6a5d49cf4ff.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs through early termination of reasoning programs when answers stabilize. It demonstrates compute savings of up to 50% and throughput improvements of 3.3×, which directly relates to GPU utilization and efficiency. The Certaindex metric enables dynamic resource allocation and scheduling in LLM serving systems. The authors built a reasoning-aware serving system (Dynasor) that integrates this optimization technique.",
      "Irrelevant Aspects": "The paper does not address training optimization or traditional distributed training challenges. It doesn't explicitly discuss latency improvements, only throughput. GPU utilization is not directly analyzed, though compute savings would indirectly impact it. The work focuses on test-time reasoning rather than training-time efficiency.",
      "Summary": "Certaindex introduces a metric to detect when LLM reasoning programs have stabilized their answers, enabling early exit from unnecessary computation. This approach optimizes inference by reducing wasted token generation, leading to significant compute savings and throughput improvements when integrated into their Dynasor serving system. The technique represents an important advance in efficient LLM inference optimization."
    }
  },
  {
    "id": "iZC5xoQQkX",
    "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data",
    "abstract": "LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.",
    "authors": [
      "~Asterios_Tsiourvas1",
      "~Wei_Sun7",
      "~Georgia_Perakis1"
    ],
    "pdf": "/pdf/783fb09e1b38c81b870d1cb9784b315e5c6cb686.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs by developing a routing system that selects the most appropriate model for each query, which can improve throughput and reduce latency. It focuses on optimizing system performance across multiple models, which relates to GPU utilization and scalability. The approach learns from observational data rather than requiring evaluation on all models, which is more scalable. The framework aims to balance accuracy and cost metrics, which is crucial for efficient LLM deployment.",
      "Irrelevant Aspects": "The paper does not address training optimization techniques for LLMs or technical GPU utilization strategies. It focuses on system-level routing rather than model architecture improvements or low-level GPU operations. The paper does not cover distributed training techniques or memory optimization methods during training, which are central to my research interests.",
      "Summary": "This paper presents a causal framework for LLM routing that minimizes decision-making regret from observational data. It introduces surrogate objectives for efficient optimization and handles heterogeneous cost preferences. While relevant to inference optimization and system-level efficiency, it does not address the core training optimization techniques or low-level GPU utilization strategies that are the primary focus of my research."
    }
  },
  {
    "id": "dIHSZTx9Lu",
    "title": "Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access",
    "abstract": "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages.\nTo overcome this limitation, we propose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information.\nThe core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths.\nTo make HSA efficient, we further introduce a hardware-aligned kernel design.\nBy combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.",
    "authors": [
      "~Xiang_Hu2",
      "~Jiaqi_Leng3",
      "~Jun_Zhao5",
      "~Kewei_Tu1",
      "~Wei_Wu1"
    ],
    "pdf": "/pdf/88b14a796c7f0fd04942537f430aa88b63bf87c4.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Hardware-aligned kernel design, Linear computational and space complexity, Hierarchical sparse attention for reduced computational load, Nearly constant memory footprint for long contexts, Faster training and inference for long sequences, Scalability to extremely long contexts (64 million)",
      "Irrelevant Aspects": "Focus on RNNs rather than Transformers (as primary approach), Passkey retrieval as a specific evaluation task, Implementation specific to Mamba model",
      "Summary": "The paper introduces Hierarchical Sparse Attention (HSA), a novel mechanism enhancing RNNs with long-range random access while maintaining efficiency advantages. The approach divides inputs into chunks, selects top-k chunks, and hierarchically aggregates information. With a hardware-aligned kernel design, it improves efficiency. Combined with Mamba as RAMba, it achieves perfect accuracy in passkey retrieval across 64 million contexts with nearly constant memory footprint. This work is highly relevant to LLM training and inference optimization, addressing GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "22CqLfjiVl",
    "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs",
    "abstract": "Large language models (LLMs) excel at complex tasks thanks to advances in their reasoning abilities. However, existing methods overlook the trade-off between reasoning effectiveness and efficiency, often encouraging unnecessarily long reasoning chains and wasting tokens. To address this, we propose Learning to Think (L2T), an information-theoretic reinforcement fine-tuning framework for LLMs to make the models achieve optimal reasoning with fewer tokens. Specifically, L2T treats each query-response interaction as a hierarchical session of multiple episodes and proposes a universal dense process reward, i.e., quantifies the episode-wise information gain in parameters, requiring no extra annotations or task-specific evaluators. We propose a method to quickly estimate this reward based on PAC-Bayes bounds and the Fisher information matrix. Theoretical analyses show that it significantly reduces computational complexity with high estimation accuracy. By immediately rewarding each episode's contribution and penalizing excessive updates, L2T optimizes the model via reinforcement learning to maximize the use of each episode and achieve effective updates. Empirical results on various reasoning benchmarks and base models demonstrate the advantage of L2T across different tasks, boosting both reasoning effectiveness and efficiency.",
    "authors": [
      "~Jingyao_Wang1",
      "~Wenwen_Qiang1",
      "~Zeen_Song1",
      "~Changwen_Zheng1",
      "~Hui_Xiong1"
    ],
    "pdf": "/pdf/de9fc962acefe20dd0d80073eadeb19263afeb06.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimizing LLM reasoning efficiency by reducing unnecessary token usage, which relates to my interest in resource utilization. It introduces an information-theoretic reinforcement fine-tuning approach that claims to reduce computational complexity, which is relevant to GPU utilization and scalability. The framework aims to maximize effectiveness while minimizing resource expenditure, which aligns with goals of higher throughput. The paper addresses training optimization through RL techniques, which is directly relevant to my focus on training optimization for LLMs.",
      "Irrelevant Aspects": "The paper doesn't specifically address inference optimization techniques like quantization, distillation, or hardware-specific optimizations. There's no explicit focus on distributed systems or multi-GPU architectures that would be relevant to scalability concerns. The abstract doesn't mention latency reduction specifically, which is a key aspect of my research interests. The paper appears to focus more on algorithmic efficiency rather than system-level or hardware-level optimizations.",
      "Summary": "This paper introduces L2T, an information-theoretic reinforcement fine-tuning framework for LLMs that optimizes the trade-off between reasoning effectiveness and efficiency. It proposes a dense process reward based on information gain in parameters, estimated using PAC-Bayes bounds and Fisher information matrix. The approach aims to reduce wasted tokens and computational resources while maintaining reasoning performance, making it moderately relevant to optimization of LLM systems, though it focuses more on algorithmic efficiency rather than system-level or hardware optimizations."
    }
  },
  {
    "id": "Vvb27TQzO9",
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "abstract": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). \nIn this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term~\\emph{few-shot expert localization}, with only a few in-domain demonstrations, the model consistently activates a sparse and stable subset of experts on tasks within the same domain.\nBuilding on this observation, we propose a simple yet effective pruning framework, \\textbf{EASY-EP}, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts.\nEASY-EP comprises two key components: \\textbf{output-aware expert importance assessment} and \\textbf{expert-level token contribution estimation}. The former evaluates the importance of each expert for the current token by considering the gating scores and L2 norm of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities before and after routed experts.\nExperiments on DeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget as the full model, with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP.",
    "authors": [
      "~zican_Dong1",
      "~Han_Peng3",
      "~Peiyu_Liu1",
      "~Xin_Zhao10",
      "~Dong_Wu4",
      "~Feng_Xiao10",
      "~Zhifeng_Wang5"
    ],
    "pdf": "/pdf/88eff2489325614e2c6772b85611951f693ed088.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses key challenges in large language model optimization, particularly focusing on inference efficiency of Mixture-of-Experts (MoE) architectures. It presents a novel pruning technique (EASY-EP) that significantly reduces memory overhead while maintaining performance, achieving 2.99x throughput improvement with only half the experts. This directly relates to GPU utilization optimization and scalability of very large models like DeepSeek-R1 (671B). The method's ability to identify and retain only relevant experts for specific domains represents a practical approach to resource-efficient inference.",
      "Irrelevant Aspects": "The paper's emphasis on few-shot demonstrations for expert localization is more of a methodology detail rather than a core optimization technique. The analysis of domain specialization, while providing valuable insights, is secondary to the actual optimization methods developed. The work focuses primarily on inference optimization rather than training optimization, which is only tangentially related to the stated interests.",
      "Summary": "This paper presents a highly relevant optimization technique for large MoE models through domain-specific expert pruning. The EASY-EP framework addresses memory and computational constraints while maintaining model performance, directly supporting the goals of better GPU utilization, scalability, and higher throughput. The claimed 2.99x throughput improvement with half the experts on massive models like DeepSeek-R1 demonstrates significant practical value for efficient LLM deployment."
    }
  },
  {
    "id": "B9Aj2aWLKi",
    "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "abstract": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization across various models and plugins, and improved support for multi-GPU parallel inference. Models and codes will be available.",
    "authors": [
      "~Songhua_Liu2",
      "~Zhenxiong_Tan1",
      "~Xinchao_Wang1"
    ],
    "pdf": "/pdf/610999169a00584f7cbd2c1ec6a2251da1c36cde.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses key optimization challenges for transformer models, specifically reducing the quadratic complexity of attention mechanisms to linear complexity. It achieves 6.3x acceleration for high-resolution generation and reduces attention computations by 99.5%, demonstrating significant inference optimization. The improvements in multi-GPU parallel inference support and the efficient attention mechanism principles are directly applicable to large language model optimization. The distillation approach to maintain performance while improving efficiency is also relevant to my research interests.",
      "Irrelevant Aspects": "The paper focuses specifically on Diffusion Transformers for image generation rather than language models. The evaluation metrics and specific implementation details are tailored to visual tasks rather than NLP applications. While the optimization principles transfer, the immediate application domain differs from my primary focus on language models.",
      "Summary": "CLEAR presents a method to optimize Diffusion Transformers by replacing quadratic attention with a linear complexity alternative. It introduces a convolution-like local attention strategy that limits feature interactions to local windows, achieving 6.3x speedup for 8K-resolution image generation while maintaining quality. The approach requires minimal fine-tuning and shows favorable properties including zero-shot generalization across models and improved multi-GPU inference support."
    }
  },
  {
    "id": "X8oEu4Gs3W",
    "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
    "abstract": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial–temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs.\nIn this paper, we introduce the novel task of **Extreme Short Token Reduction**, which aims to represent entire videos using a minimal set of discrete tokens.  We propose **VQToken**, a neural discrete token representation framework that\n(i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and \n(ii) preserves spatial–temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry.\nOn the Extreme Short Token Reduction task, our VQToken compresses sequences to just **0.07\\%** of their original length while incurring only a **0.66\\%** drop in accuracy on NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. \nWe further introduce the **Token Information Density** (**TokDense**) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, way fewer tokens counts, and enables efficient video large language models in resource-constrained environments.",
    "authors": [
      "~Haichao_Zhang7",
      "~Yun_Fu1"
    ],
    "pdf": "/pdf/a40769a7cbdb383c92f583c36410de2950277c2d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Extreme token reduction to 0.07% of original size directly impacts GPU utilization and latency, vector quantization techniques for compact representations applicable to general LLM optimization, resource-constrained environment focus aligns with scalability goals, theoretical complexity reduction strategies, new Token Information Density (TokDense) metric for evaluating efficiency",
      "Irrelevant Aspects": "Heavy focus on video-specific processing rather than general LLM optimization, spatial-temporal preservation techniques tailored to video data, evaluation limited to video-specific benchmarks rather than general LLM performance metrics, minimal discussion of training optimization focus, lacks direct GPU utilization metrics",
      "Summary": "VQToken introduces extreme token reduction specifically for video LLMs, achieving compression to just 0.07% of original token length with minimal performance impact. While video-specific, the vector quantization approach and token efficiency concepts offer valuable insights for general LLM optimization. The method demonstrates significant computational efficiency gains through token reduction, directly relevant to improving GPU utilization and reducing latency, though adaptation beyond video processing would be required for broader application to LLM optimization research."
    }
  },
  {
    "id": "SELYlDHZk2",
    "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models",
    "abstract": "Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability.  We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features.\nWe apply our method to a standard VLA model CogACT, yielding a $1.93\\times$ inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.",
    "authors": [
      "~Yantai_Yang1",
      "~Yuhao_Wang30",
      "~Zichen_Wen1",
      "~Luo_Zhongwei2",
      "~Chang_Zou2",
      "~Zhipeng_Zhang2",
      "~Chuan_Wen1",
      "~Linfeng_Zhang2"
    ],
    "pdf": "/pdf/0817b5f3709b35da9c8167416c077f27b525ad17.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference acceleration for large models without requiring retraining, which aligns with my research interest in inference optimization for better GPU utilization and scalability. It presents a structured approach to reduce computational demands and improve throughput through systematic elimination of redundancies. The three-pronged strategy addresses multiple optimization points across the model pipeline, resulting in significant speedup (1.93×) and FLOPs reduction (to 28.9%). The approach is training-free, making it practical for deployment, and demonstrates techniques that could potentially be adapted to other large model architectures.",
      "Irrelevant Aspects": "The paper is specifically focused on Vision-Language-Action (VLA) models rather than general language models, limiting direct applicability to my primary interest. The diffusion-based action head optimization is highly specific to this architecture type. The benchmark and evaluation are in the embodied intelligence domain (SIMPLER benchmark) rather than general NLP tasks. The visual processing optimization component may not translate to language-only models.",
      "Summary": "EfficientVLA presents a training-free framework for accelerating Vision-Language-Action models through systematic redundancy elimination. The approach combines language module pruning, task-aware visual token selection, and temporal redundancy caching in the diffusion-based action head. Applied to CogACT, it achieves 1.93× speedup while reducing FLOPs to 28.9% with minimal performance degradation. While the techniques are specific to VLA models, the optimization strategies for inference acceleration offer valuable insights applicable to large model optimization in general."
    }
  },
  {
    "id": "UogChes7e9",
    "title": "Computational Budget Should Be Considered in Data Selection",
    "abstract": "Data selection improves computational efficiency by choosing informative subsets of training samples. \nHowever, existing methods ignore the compute budget, treating data selection and importance evaluation independently of compute budget constraints. Yet empirical studies show no algorithm can consistently outperform others (or even random selection) across varying budgets. We therefore argue that compute budget must be integral to data-selection strategies, since different budgets impose distinct requirements on data quantity, quality, and distribution for effective training. \nTo this end, we propose a novel Computational budget-Aware Data Selection (CADS) method and naturally formulate it into a bilevel optimization framework, where the inner loop trains the model within the constraints of the computational budget on some selected subset of training data, while the outer loop optimizes data selection based on model evaluation. \nOur technical contributions lie\nin addressing two main challenges in solving this bilevel optimization problem: the expensive Hessian matrix estimation for outer-loop gradients and the computational burden of achieving inner-loop optimality during iterations. \nTo solve the first issue, we propose a probabilistic reparameterization strategy and compute the gradient using a Hessian-free policy gradient estimator. To address the second challenge, we transform the inner optimization problem into a penalty term in the outer objective, further discovering that we only need to estimate the minimum of a one-dimensional loss to calculate the gradient, significantly improving efficiency. To accommodate different data selection granularities, we present two complementary CADS variants: an example-level version (CADS-E) offering fine-grained control and a source-level version (CADS-S) aggregating samples into source groups for scalable, efficient selection without sacrificing effectiveness.\nExtensive experiments show that our method achieves performance gains of up to 14.42\\% over baselines in vision and language benchmarks. \nAdditionally, CADS achieves a 3-20× speedup compared to conventional bilevel implementations, with acceleration correlating positively with compute budget size.",
    "authors": [
      "~Weilin_Wan2",
      "~WEIZHONG_ZHANG2",
      "~Cheng_Jin2"
    ],
    "pdf": "/pdf/c8a9138dc6684dfac70eb23a57c1b3aab423742f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization through intelligent data selection, computational efficiency improvements, scalable solutions for large models, performance gains in language benchmarks, 3-20× speedup in training, GPU utilization optimization through better data selection, addressing computational budget constraints in ML training",
      "Irrelevant Aspects": "Focus on vision tasks rather than language models, lack of inference optimization techniques, absence of low-level GPU utilization strategies, no specific focus on latency optimization, primarily training-focused rather than end-to-end system optimization",
      "Summary": "The paper introduces CADS, a computational budget-aware data selection method that formulates data selection as a bilevel optimization problem. It addresses computational constraints in training by selecting informative subsets of data, achieving both performance improvements (up to 14.42%) and significant speedups (3-20×). The work is highly relevant to training optimization for large language models, focusing on computational efficiency and scalability, though it doesn't address inference optimization or low-level GPU techniques."
    }
  },
  {
    "id": "wCbOKbZ7kf",
    "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning",
    "abstract": "Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to efficiently assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of cross-domain reasoning benchmarks, SpecReason achieves 1.4-3.0$\\times$ speedup over vanilla LRM inference while improving accuracy by 0.4-9.0%. Compared to speculative decoding without SpecReason, their combination yields an additional 8.8-58.0% latency reduction. We open-source SpecReason at \\url{https://anonymous.4open.science/r/specreason/}.",
    "authors": [
      "~Rui_Pan9",
      "~Yinwei_Dai1",
      "~Zhihao_Zhang2",
      "~Gabriele_Oliaro1",
      "~Zhihao_Jia2",
      "~Ravi_Netravali2"
    ],
    "pdf": "/pdf/7c62b5e4703772251c504c1cdc203fa7a96cd873.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for Large Reasoning Models (LRMs), which is a core area of my expertise. It focuses on reducing inference latency through a novel approach called SpecReason, which uses a lightweight model for intermediate reasoning steps while reserving the base model for assessment. This technique achieves 1.4-3.0× speedup over vanilla LRM inference and can be combined with existing methods like speculative decoding for additional latency reduction. The approach has potential implications for GPU utilization, scalability, and throughput optimization.",
      "Irrelevant Aspects": "The paper does not address training optimization, which is another important aspect of my expertise. It also doesn't discuss hardware-specific optimizations, memory optimization strategies, or batch processing techniques, which are all relevant to improving GPU utilization and throughput. The focus is specifically on reasoning models and their unique characteristics rather than general optimization techniques for all large language models.",
      "Summary": "SpecReason introduces a system for accelerating Large Reasoning Models by using a lightweight model to handle intermediate reasoning steps speculatively, while preserving the base model for assessment and correction. The approach leverages the semantic flexibility of reasoning tokens rather than requiring token-level equivalence. It achieves significant speedups while maintaining or improving accuracy, and can be combined with existing optimization techniques. The paper is highly relevant to my expertise in inference optimization, though it doesn't cover training optimization or all aspects of GPU utilization."
    }
  },
  {
    "id": "TcVCu2PKb9",
    "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup",
    "abstract": "Modern large-language models often face communication bottlenecks on current hardware rather than computational limitations. \n*Multi-head latent attention (MLA)* addresses this by compressing the key-value cache using low-rank matrices, while the Absorb operation prevents the KV cache from reverting to its original size, significantly boosting both training and inference speed.\nDespite the success of DeepSeek V2/V3/R1, most model providers have heavily invested in optimizing GQA-based models and, therefore, lack strong incentives to retrain MLA-based models from scratch.\nThis paper demonstrates that MLA provides superior expressive power compared to GQA with the same KV cache overhead, thereby offering a rationale for transitioning from GQA to MLA.\nIn addition, we introduce TransMLA, a framework that seamlessly converts any GQA-based pre-trained model (e.g., LLaMA, Qwen, Gemma, Mistral/Mixtral) into an MLA-based model. \nFor the first time, our method enables *direct conversion of these models into a format compatible with DeepSeek's codebase*, allowing them to fully leverage the existing, highly-optimized support for the DeepSeek architecture within inference engines like vLLM and SGlang.\nBy compressing 93\\% of the KV cache in LLaMA-2-7B, we achieve a **10x speedup** with an 8K context length while maintaining meaningful output. \nMoreover, the model requires only **6B tokens** for fine-tuning to recover comparable performance across multiple benchmarks.\nTransMLA provides a practical path for migrating GQA-based models to the MLA structure, and when combined with DeepSeek’s advanced optimizations—such as FP8 quantization and Multi-Token Prediction—further inference acceleration can be achieved.",
    "authors": [
      "~Fanxu_Meng1",
      "~Pingzhi_Tang1",
      "~Zengwei_Yao1",
      "~Xing_Sun1",
      "~Muhan_Zhang1"
    ],
    "pdf": "/pdf/15d1bbe6b7effe0eddb887b962c88b630785366a.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through MLA (Multi-head latent attention) which compresses KV cache using low-rank matrices, addressing communication bottlenecks rather than computational limitations. It demonstrates 10x speedup with 8K context length while compressing 93% of KV cache. It enables compatibility with optimized inference engines like vLLM and SGlang, and leverages DeepSeek's optimizations including FP8 quantization. This aligns perfectly with research interests in GPU utilization, scalability, throughput, and latency optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on model conversion (GQA to MLA) rather than developing fundamentally new optimization techniques. It doesn't explore novel training optimization methods but rather applies existing MLA to convert pre-trained models.",
      "Summary": "TransMLA introduces a framework to convert GQA-based pre-trained models into MLA-based models compatible with DeepSeek's optimized codebase. By compressing 93% of KV cache, it achieves 10x speedup with 8K context while maintaining performance after minimal fine-tuning. This work offers a practical migration path to leverage existing MLA optimizations, directly addressing communication bottlenecks in LLMs for improved inference performance."
    }
  },
  {
    "id": "Gppo2JImHs",
    "title": "dKV-Cache: The Cache for Diffusion Language Models",
    "abstract": "Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models (ARs). However, diffusion language models have long been constrained by slow inference. A core challenge is that their non‑autoregressive architecture and bidirectional attention preclude the key–value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, **d**elayed **KV-Cache**, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step‑by‑step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under‑utilise contextual information during inference. (2) dKV-Cache‑Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10$\\times$ speedup in inference, largely narrowing the gap between ARs and DLMs.  We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code‑generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.",
    "authors": [
      "~Xinyin_Ma1",
      "~Runpeng_Yu2",
      "~Gongfan_Fang2",
      "~Xinchao_Wang1"
    ],
    "pdf": "/pdf/a867ec3b42ee935d1312c7d4231f276f7382aea8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization in language models, proposing a novel caching mechanism that achieves 2-10x speedup. It focuses on improving GPU utilization and throughput through better attention handling. The work introduces a practical solution that can be applied to existing models without retraining, which is valuable for deployment scenarios. The technique addresses the fundamental challenge of attention computation efficiency in large language models.",
      "Irrelevant Aspects": "The paper specifically focuses on Diffusion Language Models (DLMs) rather than the more widely deployed autoregressive models. It does not address training optimization, only inference improvements. The proposed solution is specialized for DLMs' unique bidirectional attention pattern rather than offering a general optimization applicable to all LLM architectures.",
      "Summary": "dKV-Cache introduces a delayed caching mechanism specifically designed for Diffusion Language Models, which have been constrained by slow inference due to their non-autoregressive nature. The approach leverages observation of token representation dynamics throughout the diffusion process to implement strategic caching. The authors present two variants: one with near-lossless acceleration that improves long sequence performance, and another more aggressive approach with higher speedups but some performance degradation. The work achieves significant inference speedup (2-10x), narrowing the performance gap between AR models and DLMs, and can be implemented without retraining existing models."
    }
  },
  {
    "id": "CA1xVSvn72",
    "title": "Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their extensive parameter scales pose significant challenges for practical deployment. Unstructured pruning has emerged as an effective model compression strategy with minimal performance loss, which introduces fine-grained sparsity for weight parameters. While existing methods employ a layer-wise pruning strategy to avoid the complexity of global pruning for billion-scale LLMs, they require appropriate sparsity allocation for the layer-wise pruning objectives and often lead to suboptimal solutions for the overall model. In this paper, we propose Lua-LLM ($\\textbf{L}$earning $\\textbf{u}$nstructured-sparsity $\\textbf{a}$llocation in LLMs), a learning-based global pruning framework that explores the optimal unstructured sparsity allocation. Unlike existing pruning methods, which primarily focus on allocating per-layer sparsity, Lua-LLM achieves flexible allocation for both layer-wise and intra-layer sparsity. Furthermore, Lua-LLM leverages a soft Top-K operator to approximate the importance-based mask selection mechanism, enabling efficient binary mask learning. Experimental results on LLaMA and OPT families demonstrate significant performance improvements over existing methods.",
    "authors": [
      "~Mingge_Lu1",
      "~Jingwei_Sun3",
      "~Junqing_Lin1",
      "~Zechun_Zhou1",
      "~Guangzhong_Sun1"
    ],
    "pdf": "/pdf/21279c0520792d04bed63335a2db748bcc57ae5d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses LLM compression through unstructured pruning, proposes a learning-based global pruning framework for flexible sparsity allocation, and demonstrates performance improvements on widely-used LLM families (LLaMA and OPT). These aspects directly relate to improving GPU utilization, throughput, and latency in LLM systems. The learning-based approach to sparsity allocation represents an advanced optimization technique.",
      "Irrelevant Aspects": "The abstract doesn't provide specific details on improvements in GPU utilization, throughput, or latency metrics. There's limited focus on training optimization, and scalability is only mentioned in the context of handling billion-scale models rather than distributed systems.",
      "Summary": "Lua-LLM presents a learning-based approach to unstructured sparsity allocation in LLMs, which is highly relevant to LLM optimization for better GPU utilization and efficiency. The framework offers flexible sparsity allocation and efficient mask learning, addressing key challenges in deploying large-scale models. While the paper aligns well with inference optimization goals, it lacks specific metrics on the performance indicators that are central to my research focus."
    }
  },
  {
    "id": "bHCB8H90aZ",
    "title": "Each Complexity Deserves a Pruning Policy",
    "abstract": "The established redundancy in visual tokens within large vision–language models (LVLMs) allows for pruning to effectively reduce their substantial computational demands. Empirical evidence from previous works indicates that visual tokens in later decoder stages receive less attention than shallow layers. Then, previous methods typically employ heuristics layer-specific pruning strategies where, although the number of tokens removed may differ across decoder layers, the overall pruning schedule is fixed and applied uniformly to all input samples and tasks, failing to align token elimination with the model’s holistic reasoning trajectory. Cognitive science indicates that human visual processing often begins with broad exploration to accumulate evidence before narrowing focus as the target becomes distinct. Our experiments reveal an analogous pattern in LVLMs. This observation strongly suggests that neither a fixed pruning schedule nor a heuristics layer-wise strategy can optimally accommodate the diverse complexities inherent in different inputs. To overcome this limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), which is a training-free, plug-and-play framework that tailors pruning policies to varying sample and task complexities. Specifically, AutoPrune quantifies the mutual information between visual and textual tokens, and then projects this signal to a budget-constrained logistic retention curve. Each such logistic curve, defined by its unique shape, is shown to effectively correspond with the specific complexity of different tasks, and can easily guarantee adherence to a pre-defined computational constraints. We evaluate AutoPrune not only on standard vision-language tasks but also on Vision-Language-Action (VLA) models for autonomous driving. Notably, when applied to LLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference FLOPs by 76.8%, but still retaining 96.7% of the original accuracy averaged over all tasks. This corresponds to a 9.1% improvement over the recent work PDrop (CVPR'2025), demonstrating the effectivenes.  Code is available at https://github.com/AutoLab-SAI-SJTU/AutoPrune.",
    "authors": [
      "~Hanshi_Wang2",
      "~yuhao_xu4",
      "~Zekun_Xu4",
      "~Jin_Gao1",
      "~Yufan_Liu1",
      "~Weiming_Hu1",
      "~Ke_Wang3",
      "~Zhipeng_Zhang2"
    ],
    "pdf": "/pdf/9d60bd42e7fc9d510ccac1a6b17d3241e1b96849.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization through pruning, which directly relates to GPU utilization and efficiency. It demonstrates substantial FLOPs reduction (76.8%) while maintaining high accuracy (96.7%). The approach is training-free and plug-and-play, suggesting practical applicability. It addresses the challenge of adapting pruning strategies to different complexity levels, which is crucial for optimizing large models across diverse tasks. The results on a 7B parameter model show relevance to current large-scale systems.",
      "Irrelevant Aspects": "The paper specifically targets vision-language models (LVLMs) rather than pure language models, requiring adaptation of techniques for text-only systems. It focuses on mutual information between visual and textual tokens, which is specific to multimodal architectures. Some evaluation is on Vision-Language-Action models for autonomous driving, which is outside my core research interest.",
      "Summary": "This paper presents AutoPrune, a complexity-adaptive pruning framework for LVLMs that dynamically adjusts token retention based on task complexity. While focused on multimodal systems, its approach to inference optimization, GPU utilization, and maintaining performance under computational constraints is highly relevant to LLM optimization research. The method shows significant improvements over existing techniques and offers insights into adaptive pruning that could inform text-only model optimization strategies."
    }
  },
  {
    "id": "2xS4VtpApy",
    "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
    "abstract": "Video Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. \nExisting pruning techniques fail to effectively exploit the spatiotemporal redundancy present in video data. \nTo bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. \nLeveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential spatial and temporal information.\nOur method significantly reduces computational overhead while maintaining temporal and visual integrity. \nExtensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision, LLaVA-Video, Qwen2-VL, and Qwen2.5-VL.\nNotably, on LLaVA-OneVision-7B, FastVID effectively prunes $\\textbf{90.3\\%}$ of video tokens, reduces FLOPs to $\\textbf{8.3\\%}$, and accelerates the prefilling stage by $\\textbf{7.1}\\times$, while maintaining $\\textbf{98.0\\%}$ of the original accuracy. \nThe code is available at https://github.com/LunarShen/FastVID.",
    "authors": [
      "~Leqi_Shen1",
      "~Guoqiang_Gong1",
      "~Tao_He2",
      "~Yifeng_Zhang4",
      "~pengzhang_liu1",
      "~Sicheng_Zhao1",
      "~Guiguang_Ding1"
    ],
    "pdf": "/pdf/0f034cec09015c31c9ffe918643cb9066d29da68.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "Inference optimization for large language models",
        "Token pruning techniques to reduce computational overhead",
        "Maintaining accuracy while improving performance",
        "Addressing scalability challenges for deployment",
        "Focus on reducing FLOPs and accelerating prefilling",
        "GPU utilization improvements (implied by computational efficiency)",
        "Throughput and latency improvements (mentioned as acceleration)",
        "Deployment optimization for practical use"
      ],
      "Irrelevant Aspects": [
        "Focus is specifically on video data rather than general LLMs",
        "No mention of training optimization techniques",
        "Limited discussion on GPU-specific optimizations",
        "Doesn't appear to address distributed systems aspects"
      ],
      "Summary": "FastVID introduces a dynamic density pruning method for Video Large Language Models that significantly reduces computational overhead while maintaining accuracy. The paper addresses the spatiotemporal redundancy in video data through temporal context partitioning and density-based token pruning. It achieves impressive results on multiple Video LLMs, including a 90.3% reduction in video tokens, FLOPs reduced to 8.3% of the original, and 7.1x acceleration in the prefilling stage while maintaining 98% of the original accuracy. This work is highly relevant to inference optimization for large models, focusing on improving efficiency, throughput, and reducing latency in video understanding tasks."
    }
  },
  {
    "id": "Y1bt0YIS6Y",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "abstract": "Large Reasoning Models (LRMs) have garnered considerable attention for their ability to tackle complex tasks through the Chain-of-Thought (CoT) approach. However, their tendency toward overthinking results in unnecessarily lengthy reasoning chains, dramatically increasing the inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning.",
    "authors": [
      "~Zigeng_Chen1",
      "~Xinyin_Ma1",
      "~Gongfan_Fang2",
      "~Ruonan_Yu1",
      "~Xinchao_Wang1"
    ],
    "pdf": "/pdf/21faeda3ee514a2282f383933827be95d5eab10c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on making reasoning models efficient by reducing reasoning chain lengths, which directly addresses inference optimization. It achieves significant token reductions (from 3790 to 2125 on MATH500), reducing computational requirements and improving throughput. The approach maintains or improves accuracy while reducing inference costs, which aligns with GPU utilization optimization. The method of suppressing 'overthinking' in models to make them more efficient is relevant to the user's interest in optimization.",
      "Irrelevant Aspects": "The paper doesn't address detailed GPU utilization metrics or specific GPU architecture optimizations. It doesn't focus on scalability in terms of distributed computing or model parallelism. The research seems more focused on the methodology of improving reasoning efficiency rather than infrastructure-level optimizations.",
      "Summary": "VeriThinker introduces an approach to make Large Reasoning Models more efficient by compressing Chain-of-Thought reasoning through a novel verification task. Instead of directly fine-tuning models with concise data, it trains models to verify CoT solutions, making them more discerning about subsequent reflection steps. The method significantly reduces reasoning tokens (up to 44% reduction in some cases) while maintaining or slightly improving accuracy, directly addressing inference optimization and computational efficiency."
    }
  },
  {
    "id": "Fg9HufTI0K",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification",
    "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment. Existing sparsification strategies—such as Mixture-of-Depths, layer skipping, and early exit—fall short by neglecting the semantic coupling across vision-language-action modalities, and focusing narrowly on intra-LLM computation while overlooking end-to-end coherence from perception to control. To address these challenges, we propose **CogVLA**, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) **Encoder-FiLM based Aggregation Routing (EFA-Routing)** injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, **LLM-FiLM based Pruning Routing (LFP-Routing)** introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce **V‑L‑A Coupled Attention (CAtten)**, which combines causal vision-language attention with bidirectional action parallel decoding.\nExtensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4\\% and 70.0\\%, respectively, while reducing training costs by 2.5$\\times$ and decreasing inference latency by 2.8$\\times$ compared to OpenVLA.",
    "authors": [
      "~Wei_Li120",
      "~Renshan_Zhang1",
      "~Rui_Shao1",
      "~Jie_He2",
      "~Liqiang_Nie2"
    ],
    "pdf": "/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training cost reduction by 2.5x, inference latency reduction by 2.8x, sparsification techniques including Encoder-FiLM based Aggregation Routing and LLM-FiLM based Pruning Routing, token-level sparsity optimization, computational efficiency improvements, addressing scalability limitations of large models, token pruning for better GPU utilization, efficient multi-modal processing from perception to control",
      "Irrelevant Aspects": "Robotics-specific application domain, LIBERO benchmark evaluation, human cognitive alignment inspiration, V-L-A Coupled Attention mechanism details",
      "Summary": "CogVLA presents highly relevant optimization techniques for large multi-modal models, specifically focusing on reducing computational overhead through novel routing and sparsification strategies. The paper demonstrates significant improvements in both training efficiency (2.5x cost reduction) and inference performance (2.8x latency reduction), which directly align with research interests in GPU utilization and model scalability. While applied to Vision-Language-Action models for robotics, the optimization techniques are broadly applicable to large language model systems."
    }
  },
  {
    "id": "lMU2kaMANl",
    "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
    "abstract": "We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34% compute efficiency improvements over the prior state-of-the-art. All experiments were run on Cerebras CS-3 systems. A minimal implementation is available at https://github.com/EleutherAI/nanoGPT-mup/tree/completep.",
    "authors": [
      "~Nolan_Simran_Dey1",
      "~Bin_Claire_Zhang1",
      "~Lorenzo_Noci1",
      "~Mufan_Li1",
      "~Blake_Bordelon1",
      "~Shane_Bergsma1",
      "~Cengiz_Pehlevan2",
      "~Boris_Hanin1",
      "~Joel_Hestness2"
    ],
    "pdf": "/pdf/684cef7ea0e3b34421adafdea187da1e5d24eb4f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on LLM training optimization with 12-34% compute efficiency improvements, addresses model scalability through different parameterizations, enables wider range of model width/depth ratios for different hardware settings, claims to unlock better throughput through compute efficiency improvements, discusses hyperparameter transfer across model depth changes, and addresses efficient use of computational resources during training",
      "Irrelevant Aspects": "Experiments were run on Cerebras CS-3 systems rather than GPUs, focuses more on theoretical aspects of parameterization than specific GPU implementation details, does not directly address inference optimization or latency reduction, and has minimal focus on actual GPU utilization metrics",
      "Summary": "This paper introduces CompleteP, a parameterization method for deep transformers that achieves 12-34% compute efficiency improvements during LLM training. It addresses the 'lazy learning' regime where layers don't fully leverage nonlinearity and depth. The method enables better hyperparameter transfer across model depth changes and supports more flexible model architectures suited for different hardware. While highly relevant to training optimization, it focuses on Cerebras systems rather than GPUs and doesn't directly address inference latency concerns."
    }
  },
  {
    "id": "4n7IifN7yr",
    "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
    "abstract": "Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53× faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5\\% improvement in semantic correctness with 18.1\\% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0\\% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.",
    "authors": [
      "~Zhenyu_Yang6",
      "~Kairui_Zhang1",
      "~Yuhang_Hu3",
      "~Bing_Wang20",
      "~Shengsheng_Qian1",
      "~Bin_Wen3",
      "~Fan_Yang30",
      "~Tingting_Gao1",
      "~Weiming_Dong1",
      "~Changsheng_Xu1"
    ],
    "pdf": "/pdf/d5d3569553a33d7568c9cba9ab9ba6de8fbb66ba.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization techniques including streaming key-value cache achieving 1.53× faster inference, memory-aware acceleration through peak-end memory compression, and response-silence decoding framework for optimal timing via single forward pass verification. It addresses scalability with online inference on 10+ minute videos and improves FPS by 12.0% across tasks. The paper also discusses GPU resource optimization, throughput, and latency reduction with 18.1% reduced timing difference compared to existing models.",
      "Irrelevant Aspects": "The paper primarily focuses on video understanding rather than pure language models. The application domain (live streaming assistance) is specific rather than general-purpose LLM optimization. The OmniStar dataset construction is more about benchmarking resources than optimization techniques.",
      "Summary": "LiveStar presents relevant inference optimization techniques for large models, particularly streaming key-value caches and memory compression strategies that achieve significant speed improvements (1.53× faster) and better GPU utilization. While focused on Video-LLMs rather than pure LLMs, the optimization approaches for real-time processing, memory management, and reducing latency are highly applicable to my research interests in LLM inference optimization."
    }
  },
  {
    "id": "WBEknRZBpT",
    "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
    "abstract": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT.\nIn this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.",
    "authors": [
      "~Hongtao_Xu3",
      "~Wenting_Shen2",
      "~Yuanxin_Wei1",
      "~Ang_Wang1",
      "~Guo_Runfan1",
      "~Tianxing_Wang3",
      "~Yong_Li11",
      "~Mingzhen_Li1",
      "~Weile_Jia1"
    ],
    "pdf": "/pdf/a7327b197b07cb1df3b5e408eecd14e1276acc6d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on training optimization for large language models, specifically addressing efficiency in long-context supervised fine-tuning; addresses heterogeneous data distribution challenges; implements on DeepSpeed (a distributed training system); aims to improve GPU utilization through dynamic data scheduling; reports significant performance improvements (3.76x average, up to 7.54x); formulates scheduling as optimization problem; uses lightweight scheduling algorithm for near-zero cost online scheduling.",
      "Irrelevant Aspects": "Exclusively focuses on training optimization, not inference optimization; no explicit mention of latency improvements; limited to Long-SFT scenario rather than general LLM training/inference; appears not to discuss hardware-level GPU architecture optimizations.",
      "Summary": "The paper presents Skrull, a dynamic data scheduler designed to improve efficiency in long-context supervised fine-tuning of large language models. It addresses challenges posed by heterogeneous data distributions (mixed long and short sequences) by formulating scheduling as a joint optimization problem. Implemented on DeepSpeed, Skrull reportedly achieves significant speedups over DeepSpeed alone in real-world Long-SFT scenarios through a scheduling algorithm that balances computational requirements of different sequence lengths."
    }
  },
  {
    "id": "JcEqp4aPmb",
    "title": "InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation",
    "abstract": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long-duration video synthesis via straightforward temporal autoregression. Through extensive experiments, InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10$\\times$ faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.",
    "authors": [
      "~Jinlai_Liu3",
      "~Jian_Han3",
      "~Bin_Yan1",
      "~Wuhui1",
      "~Fengda_Zhu1",
      "~Xing_Wang14",
      "~Yi_Jiang2",
      "~BINGYUE_PENG1",
      "~Zehuan_Yuan1"
    ],
    "pdf": "/pdf/ac89202213ac8daf2cb3e1cd8f713e5cb5b96a56.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on autoregressive modeling which is directly relevant to LLM expertise. It emphasizes generation efficiency (10x faster than diffusion methods) which relates to throughput and latency optimization. The discrete approach may have parallels with language modeling in LLMs. The unified architecture design could provide insights into efficient model structures.",
      "Irrelevant Aspects": "The primary domain is visual generation rather than language processing. The abstract doesn't mention specific GPU utilization techniques or scalability solutions. There's no detailed discussion of memory optimization, model parallelism, or other technical optimization aspects. The focus is more on the application results rather than the underlying training/inference optimizations.",
      "Summary": "InfinityStar presents a unified autoregressive framework for visual generation with impressive efficiency claims (10x faster than diffusion methods). While the autoregressive approach and efficiency focus are relevant to my expertise in ML optimization, the paper's emphasis on visual generation rather than language processing and lack of technical optimization details limit its direct relevance to my specific research interests in LLM training/inference optimization and GPU utilization."
    }
  },
  {
    "id": "FDAI0PY9Qp",
    "title": "AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated excellent performance in video understanding but suffer from degraded effectiveness when processing long videos due to fixed-length contexts and weaknesses in modeling long-term dependencies. Retrieval-Augmented Generation (RAG) technology can mitigate these limitations through dynamic knowledge expansion, but existing RAG schemes for video understanding employ fixed retrieval paradigms that use uniform structures regardless of input query difficulty. This introduces redundant computational overhead and latency (*e.g.*, complex graph traversal operations) for simple queries (*e.g.*, frame-level object recognition) while potentially causing critical information loss due to insufficient retrieval granularity for multi-hop reasoning. Such single-step retrieval mechanisms severely constrain the model's balance between resource efficiency and cognitive depth. \nTo address this, we first propose a novel AdaVideoRAG framework for long-video understanding, which uses a lightweight intent classifier to dynamically and adaptively allocate appropriate retrieval schemes, ranging from the simplest to the most sophisticated, for different video understanding tasks based on query complexity. We introduce an Omni-Knowledge Indexing module to extract valuable information from multi-modal signals for context modeling and build corresponding databases, *i.e.*, a text base from clip captions, ASR, and OCR; a visual base; and a graph for deep semantic understanding. This enables hierarchical knowledge access, integration, and generation from naive retrieval to graph retrieval, achieving an optimal balance between resource consumption and video understanding capabilities.  \nFinally, we construct the HiVU benchmark for deep understanding evaluation. Extensive experiments show that our framework enhances the overall efficiency and accuracy of Video-QA for long videos and can be seamlessly integrated with existing MLLMs via lightweight API calls, establishing a new paradigm for adaptive retrieval augmentation in video analysis.",
    "authors": [
      "~Zhucun_Xue3",
      "~Jiangning_Zhang1",
      "~Xurong_Xie1",
      "~yuxuan_cai3",
      "~Yong_Liu11",
      "~Xiangtai_Li1",
      "~Dacheng_Tao1"
    ],
    "pdf": "/pdf/9bc7379fdd250fabf83c21b5f4eccf9c62d83552.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for multimodal systems through adaptive resource allocation based on query complexity, directly addressing computational efficiency and latency reduction. It introduces a hierarchical retrieval approach that balances resource consumption with model performance, which is relevant to optimization research. The lightweight design and API integration for existing MLLMs also align with optimization goals for better system efficiency.",
      "Irrelevant Aspects": "The paper is primarily focused on video understanding applications rather than general LLM optimization. It doesn't address training optimization techniques, GPU utilization strategies, or distributed training systems. The specialized domain (long video understanding) limits the direct applicability to broader ML systems optimization.",
      "Summary": "AdaVideoRAG presents an adaptive retrieval-augmented framework for efficient long video understanding that dynamically adjusts computational resources based on query complexity. While it offers valuable insights on inference optimization and resource efficiency, its specialized focus on video processing limits its direct relevance to general ML systems optimization. The paper's approach to balancing efficiency and accuracy through adaptive resource allocation is noteworthy for inference optimization research."
    }
  },
  {
    "id": "lMxuq0GNeC",
    "title": "SmartCache: Context-aware Semantic Cache for Efficient Multi-turn LLM Inference",
    "abstract": "Large Language Models (LLMs) for multi-turn conversations suffer from inefficiency: semantically similar queries across different user sessions trigger redundant computation and duplicate memory-intensive Key-Value (KV) caches. Existing optimizations such as prefix caching overlook semantic similarities, while typical semantic caches either ignore conversational context or are not integrated with low-level KV cache management.\nWe propose SmartCache, a system-algorithm co-design framework that tackles this inefficiency by exploiting semantic query similarity across sessions. SmartCache leverages a Semantic Forest structure to hierarchically index conversational turns, enabling efficient retrieval and reuse of responses only when both the semantic query and conversational context match.\nTo maintain accuracy during topic shifts, it leverages internal LLM attention scores—computed during standard prefill—to dynamically detect context changes with minimal computational overhead. Importantly, this semantic understanding is co-designed alongside the memory system: a novel two-level mapping enables transparent cross-session KV cache sharing for semantically equivalent states, complemented by a semantics-aware eviction policy that significantly improves memory utilization. This holistic approach significantly reduces redundant computations and optimizes GPU memory utilization. \nThe evaluation demonstrates SmartCache's effectiveness across multiple benchmarks. On the CoQA and SQuAD datasets, SmartCache reduces KV cache memory usage by up to $59.1\\%$ compared to prefix caching and $56.0\\%$ over semantic caching, while cutting Time-to-First-Token (TTFT) by $78.0\\%$ and $71.7\\%$, respectively. It improves answer quality metrics, achieving $39.9\\%$ higher F1 and $39.1\\%$ higher ROUGE-L for Qwen-2.5-1.5B on CoQA. The Semantic-aware Tiered Eviction Policy (STEP) outperforms LRU/LFU by $29.9\\%$ in reuse distance under skewed workloads.",
    "authors": [
      "~Chengye_YU1",
      "~Tianyu_Wang19",
      "~Zili_Shao1",
      "~Song_Jiang2"
    ],
    "pdf": "/pdf/5bc13f5689dfb66b132abd36782eb71e1da88f36.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on LLM inference optimization with significant reductions in KV cache memory usage (up to 59.1%); Implements two-level mapping for cross-session KV cache sharing; Reduces Time-to-First-Token (TTFT) by up to 78%; Improves GPU memory utilization through semantics-aware eviction policy (STEP); Addresses computation redundancy across multiple user sessions; Demonstrates improved throughput and efficiency metrics; System-algorithm co-design approach for optimization",
      "Irrelevant Aspects": "Does not address training optimization; Limited discussion on scaling to extremely large models; Minimal focus on hardware-level optimizations beyond memory management; Doesn't cover distributed inference approaches in detail",
      "Summary": "SmartCache presents a context-aware semantic caching system that optimizes LLM inference efficiency by detecting and reusing semantically similar queries across different user sessions. The paper introduces a Semantic Forest structure, leverages attention scores for context change detection, and implements a two-level mapping for KV cache sharing. It demonstrates significant improvements in memory usage (up to 59.1% reduction) and latency (up to 78% TTFT reduction), while also improving answer quality metrics. This work is highly relevant to LLM inference optimization, GPU utilization, and throughput improvements, though it doesn't address training optimization aspects."
    }
  },
  {
    "id": "ezOfR26pGQ",
    "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning",
    "abstract": "Large reasoning models (LRMs) have demonstrated strong performance on complex reasoning tasks, but often suffer from overthinking, generating redundant content regardless of task difficulty. Inspired by the dual process theory in cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a reinforcement learning framework that enables LRMs to achieve efficient reasoning through adaptive cognitive allocation and dynamic system switch.\nACPO incorporates two key components: (1) introducing system-aware reasoning tokens to explicitly represent the thinking modes thereby making the model's cognitive process transparent, and (2) integrating online difficulty estimation and token length budget to guide adaptive system switch and reasoning during reinforcement learning. \nTo this end, we propose a two-stage training strategy. The first stage begins with supervised fine-tuning to cold start the model, enabling it to generate reasoning paths with explicit thinking modes. In the second stage, we apply ACPO to further enhance adaptive system switch for difficulty-aware reasoning.\nExperimental results demonstrate that ACPO effectively reduces redundant reasoning while adaptively adjusting cognitive allocation based on task complexity, achieving efficient hybrid reasoning.",
    "authors": [
      "~Xiaoxue_Cheng1",
      "~Junyi_Li4",
      "~Zhenduo_Zhang1",
      "~Xinyu_Tang2",
      "~Xin_Zhao10",
      "~XinYu_KONG1",
      "~Zhiqiang_Zhang4"
    ],
    "pdf": "/pdf/1d54ebca47124f3efdafcb5b44f5c7243ca94f82.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses efficiency in large language model reasoning, which aligns with inference optimization goals. By reducing redundant reasoning through adaptive cognitive allocation, it could improve GPU utilization and throughput. The dynamic system switching approach could contribute to lower latency for simpler tasks. The reinforcement learning framework for optimizing model behavior during inference is relevant to LLM optimization.",
      "Irrelevant Aspects": "The paper focuses more on cognitive efficiency rather than computational optimizations. There's no explicit mention of GPU utilization techniques, scalability across distributed systems, memory optimization, or parallelization strategies. The framework seems primarily concerned with reasoning quality rather than raw performance metrics like tokens per second.",
      "Summary": "This paper proposes ACPO, a reinforcement learning framework that enables large reasoning models to adaptively allocate cognitive resources based on task complexity. By introducing system-aware reasoning tokens and implementing dynamic switching between thinking modes, the approach aims to reduce unnecessary computation in LLMs. While relevant to inference efficiency, the paper focuses more on reasoning process optimization than on technical aspects of GPU utilization, scalability, or latency optimization at the system level."
    }
  },
  {
    "id": "xkGxogC2mF",
    "title": "Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch",
    "abstract": "We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation.  While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch—a process nearly as costly as pretraining itself.\n\nOur key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding.  Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux <1 A100 day.  Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality.  This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.",
    "authors": [
      "~Xu_Cai1",
      "~Yang_Wu12",
      "~Qianli_Chen3",
      "~Haoran_Wu13",
      "~Lichuan_Xiang1",
      "~Hongkai_Wen1"
    ],
    "pdf": "/pdf/bb6661f039bb34b57158cc6e2db47ed9febeb477.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents an ultra-efficient post-training method for optimizing large-scale flow matching diffusion models, directly addressing training optimization and inference efficiency. The approach enables efficient few-step sampling which would improve throughput and reduce latency. The claim of training a 3-step Flux model in less than 1 A100 day demonstrates excellent GPU utilization. The method scales to billion-parameter models and introduces a novel velocity field self-distillation technique that operates without costly retraining, representing a significant advance in model optimization.",
      "Irrelevant Aspects": "The research focuses specifically on diffusion models rather than language models, which is the primary focus of my research. The paper doesn't address LLM-specific challenges like attention mechanism optimization or efficient handling of long text sequences. The applications appear to be centered on image generation rather than text generation or understanding.",
      "Summary": "This paper presents a highly relevant optimization technique for large generative models, specifically focusing on making diffusion models more efficient through a novel post-training distillation method. While the domain focus differs from my primary interest in language models, the techniques for efficient training, resource utilization, and inference optimization have significant potential transfer value to LLM research, making it substantially relevant to my work on model optimization, scalability, and performance improvements."
    }
  },
  {
    "id": "4EkEL77k6O",
    "title": "Compress Large Language Models via  Collaboration Between Learning and Matrix Approximation",
    "abstract": "Sparse and low-rank matrix composite approximation has emerged as a promising paradigm for compressing large language models (LLMs), offering a more flexible pruning structure than conventional methods based solely on sparse matrices. The significant variation in weight redundancy across layers, along with the differing rank and sparsity structures of weight matrices, makes identifying the globally optimal pruning structure extremely challenging. Existing methods often depend on uniform or manually designed heuristic rules to allocate weight sparsity across layers, subsequently compressing each matrix using matrix approximation techniques. Given the above theoretical difficulty in global compression of LLMs and the limited computational and data resources available compared to the training phase, we argue that a collaboration between learning and matrix approximation is essential for effective compression. In this paper, we propose a novel LLM compression framework based on generalized bilevel optimization that naturally formulates an effective collaborative mechanism. Specifically, the outer loop frames the weight allocation task as a probabilistic optimization problem, enabling the automatic learning of both layer-wise sparsities and matrix-wise retained ranks, while the inner loop solves the corresponding sparsity and rank-constrained model compression problem via matrix approximation. Our main technical contributions include two key innovations for efficiently solving this bilevel optimization problem. First,  we  introduce a truncated Gaussian prior-based probabilistic parameterization integrated with a policy gradient estimator, which avoids expensive backpropagation and stabilizes the optimization process. Second, we design an adapted QR-based matrix approximation algorithm that significantly accelerates inner loop computations. Extensive experiments on Phi-3 and the LLama-2/3 family demonstrate the effectiveness of our method. Notably, it maintains over 95\\% zero-shot accuracy under 50\\% sparsity and achieves up to 2× inference speedup.",
    "authors": [
      "~Yuesen_Liao1",
      "~Zhiwei_Li11",
      "~Binrui_Wu1",
      "~Zihao_Cheng2",
      "~Su_Zhao2",
      "~Shuai_Chen18",
      "~WEIZHONG_ZHANG2"
    ],
    "pdf": "/pdf/334cbc6da780d9a5e021d78140cd02e165f9c643.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "LLM compression techniques directly addressing inference optimization",
        "Claims of 2× inference speedup - directly relevant to performance goals",
        "Sparsity and low-rank matrix approximation methods for model efficiency",
        "Probabilistic optimization approach for automatic compression ratio allocation",
        "Practical results on Llama-2/3 family models",
        "Balance between model accuracy (maintaining 95% zero-shot accuracy) and computational efficiency",
        "GPU utilization and throughput optimization through model compression"
      ],
      "Irrelevant Aspects": [
        "Limited focus on system-level implementation details of compression",
        "Minimal discussion on distributed inference or scaling across multiple GPUs",
        "Primarily focused on inference optimization rather than training optimization",
        "Limited hardware-specific optimization details beyond general GPU utilization",
        "Not addressing memory management techniques beyond compression",
        "Limited comparison with other state-of-the-art inference optimization techniques"
      ],
      "Summary": "This paper presents a novel LLM compression framework using a bilevel optimization approach that combines learning and matrix approximation. It introduces a probabilistic parameterization with policy gradient estimation for automatic sparsity allocation and an adapted QR-based algorithm for inner loop computations. The method maintains over 95% zero-shot accuracy under 50% sparsity and achieves up to 2× inference speedup on models like Phi-3 and Llama-2/3. The work is highly relevant to inference optimization goals but lacks detailed system-level implementation insights and distributed optimization considerations."
    }
  },
  {
    "id": "LCFYj0R2rH",
    "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades",
    "abstract": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: \"How can we efficiently leverage existing LoRA weights to adapt to newer model versions?\" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.",
    "authors": [
      "~Yanan_Li5",
      "~Fanxu_Meng1",
      "~Muhan_Zhang1",
      "~Shiai_Zhu1",
      "~Shangguang_Wang1",
      "~Mengwei_Xu1"
    ],
    "pdf": "/pdf/f13d757e8c39cd68d8d3c0678a4e7e3dc4d9b25a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Efficient LoRA adaptation across model versions, 5.5GB memory reduction, 78.23% computational time reduction, parameter-efficient fine-tuning methodology, practical solution for frequently updated LLMs",
      "Irrelevant Aspects": "Limited focus on distributed system optimization, minimal discussion of GPU utilization strategies, primarily addresses weight transfer rather than inference optimization techniques",
      "Summary": "LoRASuite presents an efficient method for transferring LoRA weights between LLM versions, offering significant memory and computational savings that align with training optimization goals. While not comprehensive across all system optimization aspects, its practical approach to parameter efficiency and resource reduction makes it valuable for GPU utilization and scalability research."
    }
  },
  {
    "id": "LfcfwlLCHM",
    "title": "DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization",
    "abstract": "Quantization plays a crucial role in accelerating the inference of large-scale models, and rotational matrices have been shown to effectively improve quantization performance by smoothing outliers. However, end-to-end fine-tuning of rotational optimization algorithms incurs high computational costs and is prone to overfitting. To address this challenge, we propose an efficient distribution-aware rotational calibration method, DartQuant, which reduces the complexity of rotational optimization by constraining the distribution of the activations after rotation. This approach also effectively reduces reliance on task-specific losses, thereby mitigating the risk of overfitting. Additionally, we introduce the QR-Orth optimization scheme, which replaces expensive alternating optimization with a more efficient solution. In a variety of model quantization experiments, DartQuant demonstrates superior performance. Compared to existing methods, it achieves 47$\\times$ acceleration and 10$\\times$ memory savings for rotational optimization on a 70B model. Furthermore, it is the first to successfully complete rotational calibration for a 70B model on a single 3090 GPU, making quantization of large language models feasible in resource-constrained environments.",
    "authors": [
      "~Yuantian_Shao1",
      "~Yuanteng_Chen1",
      "~Peisong_Wang1",
      "~Jianlin_Yu1",
      "~Jing_Lin6",
      "~Yiwu_Yao1",
      "~Zhihui_Wei1",
      "~Jian_Cheng7"
    ],
    "pdf": "/pdf/558f2c2f758af90ca5cc88dc8c7a55449762ca20.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on quantization for accelerating inference of large models; achieves 47× acceleration and 10× memory savings; enables 70B model calibration on a single 3090 GPU; directly addresses GPU utilization improvements; makes large language models more feasible in resource-constrained environments; reduces computational costs of rotational optimization",
      "Irrelevant Aspects": "Doesn't focus on training optimization; limited discussion of latency metrics; method is specific to quantization rather than general optimization techniques",
      "Summary": "DartQuant introduces an efficient distribution-aware rotational calibration method for LLM quantization that significantly reduces computational costs while maintaining performance. The approach achieves 47× acceleration and 10× memory savings for rotational optimization, making it possible to perform calibration for a 70B model on a single 3090 GPU. This work has high relevance for LLM inference optimization, GPU utilization, and scalability in resource-constrained environments."
    }
  },
  {
    "id": "fTkBZLxBzV",
    "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "abstract": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet still require a separate optimization run for every downstream dataset. We introduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned parameter generator that eliminates per-task training by mapping a handful of unlabeled task prompts directly to LoRA weight updates. A lightweight text encoder distills each prompt batch into  condition embeddings, which are then transformed by a cascaded hyper-convolutional decoder into the full set of LoRA matrices. Once trained in a diverse collection of\nprompt-checkpoint pairs, DnD produces task-specific parameters in seconds, yielding i) up to\n\\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains up to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen common-sense reasoning, math, coding, and multimodal benchmarks, and iii) robust cross-domain generalization improving \\textbf{40\\%} performance without access to the target data or labels. Our results demonstrate that prompt-conditioned parameter generation is a viable alternative to gradient-based adaptation for rapidly specializing LLMs.\nWe open source \\href{https://jerryliang24.github.io/DnD}{our project} in support of future research.",
    "authors": [
      "~Zhiyuan_Liang1",
      "~Dongwen_Tang1",
      "~Yuhao_Zhou4",
      "~Xuanlei_Zhao1",
      "~Mingjia_Shi1",
      "~Wangbo_Zhao1",
      "~Zekai_Li2",
      "~Peihao_Wang1",
      "~Konstantin_Schürholt1",
      "~Damian_Borth1",
      "~Michael_M._Bronstein1",
      "~Yang_You1",
      "~Zhangyang_Wang1",
      "~Kai_Wang8"
    ],
    "pdf": "/pdf/aba6e3723f2c22ae934c3f7b6a1b734942cedbed.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Directly addresses training optimization by eliminating per-task fine-tuning, dramatically reduces computational overhead (12,000×), produces task-specific parameters in seconds (low latency), enables rapid specialization of LLMs improving deployment flexibility, uses parameter-efficient LoRA method, could lead to more efficient GPU utilization across multiple tasks, shows strong performance improvements over baseline methods",
      "Irrelevant Aspects": "Primary focus is on specialization/adaptation rather than core inference optimization, doesn't directly address inference runtime optimizations (quantization, pruning, etc.), limited discussion of actual GPU utilization metrics during the generation process",
      "Summary": "The paper introduces a novel approach for rapidly customizing LLMs by generating LoRA weights directly from prompts without per-task training. This highly relevant work addresses training optimization, reduces overhead dramatically (12,000×), and enables parameter generation in seconds, directly supporting goals of better GPU utilization, scalability, higher throughput, and lower latency in LLM operations. The method shows strong performance gains across multiple benchmarks and enables zero-shot adaptation to new domains."
    }
  },
  {
    "id": "NFxA2Von7y",
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression",
    "abstract": "Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.",
    "authors": [
      "~Kunjun_Li1",
      "~Zigeng_Chen1",
      "~Cheng-Yen_Yang2",
      "~Jenq-Neng_Hwang1"
    ],
    "pdf": "/pdf/841de5e7edb1f7c7e50df5a45f067dcee2605672.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory optimization during inference, KV cache compression which is critical for transformer models, computational efficiency improvements, scalability solutions, attention pattern optimization - all techniques directly applicable to LLM optimization despite being demonstrated on visual models",
      "Irrelevant Aspects": "Focus on visual autoregressive models rather than language models, text-to-image generation application rather than text generation, the specific next-scale prediction approach is particular to visual models",
      "Summary": "This paper presents ScaleKV, a KV cache compression framework for Visual Autoregressive models that reduces memory usage to 10% while preserving quality. Though focused on visual models rather than language models, the core optimization techniques around KV cache management and attention pattern analysis are highly relevant to LLM inference optimization, addressing critical challenges in memory usage and computational efficiency that directly impact GPU utilization and scalability."
    }
  },
  {
    "id": "ariVQf0KZx",
    "title": "Thinkless: LLM Learns When to Think",
    "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, \\<short\\> for concise responses and \\<think\\> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing the collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at \\url{https://github.com/VainF/Thinkless}",
    "authors": [
      "~Gongfan_Fang2",
      "~Xinyin_Ma1",
      "~Xinchao_Wang1"
    ],
    "pdf": "/pdf/69d5f7c0fe7ed54d42a4a1908d50a94bc062b721.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs by adaptively selecting between short-form and long-form reasoning. This approach can significantly reduce computational overhead during inference by 50-90%, which translates to higher throughput, lower latency, and better GPU utilization. The reinforcement learning framework with decoupled objectives (DeGRPO) provides insights into training optimization that could stabilize the learning process. The adaptive selection mechanism enables more efficient resource allocation based on problem complexity, which is crucial for scalability in production systems.",
      "Irrelevant Aspects": "The paper does not focus on system-level optimizations such as model parallelism, tensor parallelism, or hardware-specific implementations. It doesn't address memory management techniques, quantization methods, or kernel-level optimizations that are often critical for maximizing GPU utilization. The research is primarily at the algorithmic level rather than the systems engineering level.",
      "Summary": "Thinkless introduces a framework that teaches LLMs when to use complex reasoning versus simple responses, potentially reducing computational costs by 50-90%. This is highly relevant to inference optimization as it directly impacts GPU utilization, throughput, and latency. The DeGRPO algorithm for training this capability offers insights into stabilizing reinforcement learning for LLMs. While the paper doesn't address lower-level system optimizations, its adaptive approach to reasoning selection represents a significant efficiency improvement for deployed LLM systems."
    }
  },
  {
    "id": "mjhCFB3HLQ",
    "title": "Understanding Data Influence in Reinforcement Finetuning",
    "abstract": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning and generalization capabilities of large language models, but its success heavily relies on the quality of the training data. While data selection has been extensively studied in supervised learning, its role in reinforcement learning, particularly during the RFT stage, remains largely underexplored. In this work, we introduce RFT-Inf, the first influence estimator designed for data in reinforcement learning. RFT-Inf quantifies the importance of each training example by measuring how its removal affects the final training reward, offering a direct estimate of its contribution to model learning. \nTo ensure scalability, we propose a first-order approximation of the RFT-Inf score by backtracking through the optimization process and applying temporal differentiation to the sample-wise influence term, along with a first-order Taylor approximation to adjacent time steps. \nThis yields a lightweight, gradient-based estimator that evaluates the alignment between an individual sample’s gradient and the average gradient direction of all training samples, where a higher degree of alignment implies greater training utility. Extensive experiments demonstrate that RFT-Inf consistently improves reward performance and accelerates convergence in reinforcement fine-tuning.",
    "authors": [
      "~Haoru_Tan1",
      "~Xiuzhe_Wu1",
      "~Sitong_Wu1",
      "~Shaofeng_Zhang1",
      "~Yanfeng_Chen1",
      "~Xingwu_Sun2",
      "~Jeanne_Shen1",
      "~XIAOJUAN_QI2"
    ],
    "pdf": "/pdf/638998b7b6d1f0d77f8814e5ec18032081462b68.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on reinforcement fine-tuning of large language models, which is directly related to training optimization. It addresses data quality and selection, which impacts training efficiency and model performance. The proposed RFT-Inf method is designed to be scalable, which aligns with the interest in scalable systems. The paper mentions accelerating convergence in reinforcement fine-tuning, which is related to training efficiency and can improve GPU utilization. The method involves gradient-based estimation and optimization process analysis, which is relevant to training optimization.",
      "Irrelevant Aspects": "The paper doesn't directly address GPU utilization optimization. There's no explicit discussion of inference optimization or latency reduction. The focus is on data influence rather than system-level optimizations. The paper doesn't appear to discuss distributed training or hardware-specific optimizations. There's no explicit mention of throughput improvements at the system level.",
      "Summary": "This paper introduces RFT-Inf, a method to quantify the importance of training examples in reinforcement fine-tuning of large language models. It measures how each example's removal affects the final training reward, providing an estimate of its contribution to model learning. The approach uses a first-order approximation to ensure scalability, evaluating the alignment between individual sample gradients and the average gradient direction. The method is shown to improve reward performance and accelerate convergence."
    }
  },
  {
    "id": "w2xl15ZbD3",
    "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
    "abstract": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). \nLoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank.\nIn standard LoRA, all input tokens share the same weights and undergo an identical input-output projection.\nThis limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens.\nTo address this limitation, we propose **Token-wise Projected Low-Rank Adaptation (TopLoRA)**, which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner.\nFormally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$.\nNotably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections).\nExtensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. \nThe code is available at https://github.com/Leopold1423/toplora-neurips25.",
    "authors": [
      "~Shiwei_Li3",
      "~Xiandi_Luo1",
      "~Haozhao_Wang1",
      "~Xing_Tang2",
      "~Ziqiang_Cui1",
      "~Dugang_Liu1",
      "~Yuhua_Li2",
      "~xiuqiang_He3",
      "~Ruixuan_Li1"
    ],
    "pdf": "/pdf/05b3a11a599e2e994b8865d4d30748696784862d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parameter-efficient fine-tuning (PEFT) which directly impacts GPU utilization during LLM training. Token-specific weight adjustments that improve adaptation without increasing rank, suggesting better resource utilization. The method maintains computational efficiency while enhancing model performance. Demonstrates scalability across multiple models and datasets.",
      "Irrelevant Aspects": "No explicit focus on GPU hardware utilization patterns or specific optimization for different GPU architectures. Limited information about inference-time performance improvements. Doesn't directly address throughput or latency metrics in the abstract.",
      "Summary": "TopLoRA introduces a token-wise approach to low-rank adaptation for LLMs that maintains computational efficiency while improving performance. This method enhances the fine-tuning process without increasing rank, which can contribute to better GPU utilization during training. While the paper doesn't explicitly address all aspects of my research interests, it presents a significant advancement in efficient fine-tuning that has important implications for training optimization."
    }
  },
  {
    "id": "hYovE4nHTt",
    "title": "Radial Attention: $\\mathcal{O}(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
    "abstract": "Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\\mathcal{O}(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that \\method maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9× speedup over the original dense attention. With minimal tuning, it enables video generation up to 4× longer while reducing training costs by up to 4.4× compared to direct fine-tuning and accelerating inference by up to 3.7× compared to dense attention inference. Code is released at https://github.com/mit-han-lab/radial-attention.",
    "authors": [
      "~Xingyang_Li1",
      "~Muyang_Li2",
      "~Tianle_Cai1",
      "~Haocheng_Xi1",
      "~Shuo_Yang22",
      "~Yujun_Lin1",
      "~Lvmin_Zhang2",
      "~Songlin_Yang1",
      "~Jinbo_Hu1",
      "~Kelly_Peng1",
      "~Maneesh_Agrawala2",
      "~Ion_Stoica1",
      "~Kurt_Keutzer1",
      "~Song_Han5"
    ],
    "pdf": "/pdf/e6225275c6e2b1f231eaf342a2faa0e8f6bca52f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Computational complexity reduction from O(n²) to O(n log n), GPU utilization improvements, training cost reduction by 4.4×, inference acceleration by up to 3.7×, sparse attention mechanisms for large models, scalability improvements for models up to 14B parameters, efficient fine-tuning approaches",
      "Irrelevant Aspects": "Domain focus on video generation rather than language models, spatiotemporal data processing vs. sequential text data, specific application to diffusion models",
      "Summary": "This paper introduces Radial Attention, a sparse attention mechanism that significantly reduces computational costs for long video generation in diffusion models. By exploiting the 'Spatiotemporal Energy Decay' phenomenon, the method achieves O(n log n) complexity compared to standard O(n²) dense attention, providing 1.9× speedup in training and up to 3.7× in inference. It enables generation of videos up to 4× longer while reducing training costs by 4.4×. Although focused on video models, the attention optimization techniques are highly relevant to LLM training and inference optimization for better GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "BLLixcuZgl",
    "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
    "abstract": "In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named **CDPruner**, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by **95\\%** and CUDA latency by **78\\%**, while maintaining **94\\%** of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.",
    "authors": [
      "~Qizhe_Zhang1",
      "~Mengzhen_Liu1",
      "~Lichen_Li1",
      "~Ming_Lu2",
      "~Yuan_Zhang20",
      "~Junwen_Pan1",
      "~Qi_She1",
      "~Shanghang_Zhang4"
    ],
    "pdf": "/pdf/e9d277877d779f8f4cd3d6b7530ec58f34561a54.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Inference optimization for MLLMs, 95% FLOPs reduction, 78% CUDA latency reduction, token pruning strategy, model-agnostic approach, training-free method, maintains 94% accuracy, directly impacts GPU utilization and throughput, determinantal point process for token selection",
      "Irrelevant Aspects": "Specific focus on multimodal aspects rather than general LLM optimization, no mention of training optimization, mathematical formulation details of DPP may be less relevant to implementation",
      "Summary": "This paper presents CDPruner, a novel visual token pruning method for multimodal large language models that maximizes conditional diversity through determinantal point process. The training-free, model-agnostic approach achieves significant computational savings while maintaining high accuracy. The research directly contributes to inference optimization, GPU utilization, scalability, throughput, and latency improvements for large language models, with a specific focus on the multimodal domain."
    }
  },
  {
    "id": "2hgHyoyVWj",
    "title": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full fine-tuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA’s representational capacity. To address this limitation, we propose \\ourmethod, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture \\emph{fixed} and \\emph{learnable} nonlinearities. This combination forms an {\\fontfamily{lmtt}\\selectfont \\textbf{MLP-like structure}} with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that \\ourmethod: (\\textbf{I}) not only matches or surpasses full fine-tuning performance with only $6.18\\%\\sim25\\%$ of LoRA’s parameters but also (\\textbf{II}) outperforms state-of-the-art PEFT methods by up to $10.88\\%$ in both NLP and CV tasks, and \\textbf{(III)} exhibits robust performance across various rank configurations.",
    "authors": [
      "~Haonan_Dong1",
      "~Wenhao_Zhu2",
      "~Guojie_Song1",
      "~Liang_Wang15"
    ],
    "pdf": "/pdf/5954ca0f431cb13403b1c993c66603e60c17ae9c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on parameter-efficient fine-tuning methods for large models, which is directly related to training optimization of LLMs. It proposes a method to reduce parameter overhead while maintaining performance, which can improve GPU utilization during fine-tuning. The method matches or surpasses full fine-tuning with only 6.18%-25% of LoRA's parameters, potentially enabling more efficient use of computational resources. The MLP-like structure with compressed rank may have implications for computational efficiency during both training and inference phases.",
      "Irrelevant Aspects": "The paper does not explicitly address inference optimization techniques such as quantization, pruning, or attention optimization. There is no detailed discussion of GPU utilization patterns or distributed training scalability. The focus is primarily on model architecture rather than system-level optimizations. Limited analysis of latency and throughput during inference is provided.",
      "Summary": "AuroRA presents a novel approach to enhance the Low-Rank Adaptation (LoRA) method by introducing an Adaptive Nonlinear Layer to address the low-rank bottleneck. The proposed method achieves comparable or better performance than full fine-tuning while using significantly fewer parameters. While the paper contributes to parameter-efficient fine-tuning for large models, it has limited direct relevance to inference optimization, GPU utilization, or system-level scalability aspects of large language model deployment."
    }
  },
  {
    "id": "PxximqJil4",
    "title": "StarTrail: Concentric Ring Sequence Parallelism for Efficient Near-Infinite-Context Transformer Model Training",
    "abstract": "Training Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overheads. To address this problem, we propose StarTrail, a multi-dimensional concentric distributed training system for long sequences, fostering an efficient communication paradigm and providing additional tuning flexibility for communication arrangements. Specifically, StarTrail introduces an extra parallel dimension and divides the peer-to-peer communication into sub-rings to substantially reduce communication volume and avoid bandwidth bottlenecks. Through comprehensive experiments across diverse hardware environments and on both Natural Language Processing (NLP) and Computer Vision (CV) tasks, we demonstrate that our approach significantly surpasses state-of-the-art methods that support Long sequence lengths, achieving performance improvements of up to 77.12% on GPT-style models and up to 114.33% on DiT (Diffusion Transformer) models without affecting the computations results.",
    "authors": [
      "~Ziming_Liu4",
      "~Shaoyu_Wang3",
      "~Shenggan_Cheng1",
      "~Zhongkai_Zhao1",
      "~Kai_Wang8",
      "~Xuanlei_Zhao1",
      "~James_Demmel1",
      "~Yang_You1"
    ],
    "pdf": "/pdf/3a84ca5724f0d5e35a94ee04fb8937f4de89d362.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for large Transformer models, focusing on efficiency and scalability for long sequences. It introduces a novel parallelization method called Concentric Ring Sequence Parallelism that reduces communication overhead and improves GPU utilization. The approach demonstrates significant performance improvements (up to 77.12% on GPT-style models), which directly relates to higher throughput. The method introduces an extra parallel dimension and optimizes peer-to-peer communication patterns to avoid bandwidth bottlenecks.",
      "Irrelevant Aspects": "The paper focuses exclusively on training optimization with no discussion of inference optimization techniques. It doesn't explicitly address latency improvements for inference scenarios. There's limited discussion on memory optimization strategies specifically.",
      "Summary": "This paper is highly relevant to research on large language model training optimization, presenting a novel approach to parallelization that significantly improves efficiency and scalability for training Transformer models on long sequences. The Concentric Ring Sequence Parallelism method reduces communication overhead and introduces additional parallel dimensions, resulting in substantial performance gains. While it doesn't address inference optimization, the techniques could potentially be adapted for inference scenarios."
    }
  },
  {
    "id": "yGOytgjurF",
    "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems",
    "abstract": "Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose **KVCOMM**, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples—termed *anchors*—that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi- agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8× speedup compared to the standard prefill pipeline, reducing TTFT from ∼430ms to ∼55ms. Code is available at https://github.com/FastMAS/KVCOMM.",
    "authors": [
      "~Hancheng_Ye1",
      "~Zhengqi_Gao1",
      "~Mingyuan_Ma1",
      "~Qinsi_Wang2",
      "~Yuzhe_Fu1",
      "~Ming-Yu_Chung1",
      "~Yueqian_Lin2",
      "~Zhijian_Liu1",
      "~Jianyi_Zhang1",
      "~Danyang_Zhuo1",
      "~Yiran_Chen1"
    ],
    "pdf": "/pdf/81561154949bf17e7f12ee6dc0485c10a2415686.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization through KV-cache reuse, reports significant performance improvements (7.8× speedup, reducing TTFT from ~430ms to ~55ms), likely improves GPU utilization by reducing redundant computation, focuses on crucial KV-cache management for LLMs, provides a training-free solution applicable to existing systems, addresses scalability in multi-agent systems, explicitly reduces Time To First Token (TTFT), and includes practical open-source implementation.",
      "Irrelevant Aspects": "Focuses primarily on multi-agent systems rather than single-agent optimization techniques, doesn't address training optimization (only inference), and is specialized for scenarios where agents share significant context.",
      "Summary": "This paper introduces KVCOMM, a training-free framework for optimizing LLM inference in multi-agent systems through efficient KV-cache reuse across agents. It addresses redundant computation when agents process overlapping contexts by aligning cache offsets using an anchor pool mechanism. The approach achieves significant performance improvements (up to 7.8× speedup) without quality degradation, directly addressing key metrics of interest: throughput, latency, and resource utilization."
    }
  },
  {
    "id": "Tf9eoTIIjh",
    "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization",
    "abstract": "Post-training compression has been a widely employed approach to scale down large language model (LLM) and facilitate efficient inference. In various proposed compression methods, including pruning and quantization, calibration data plays a vital role by informing the weight importance and activation dynamic ranges. However, how calibration data impacts the LLM capability after compression is less explored. Few of the existing works, though recognizing the significance of this study, only investigate the language modeling or commonsense reasoning performance degradation from limited angles, like the data sources or sample amounts. More systematic research is still needed to examine the impacts on  different LLM capabilities in terms of compositional properties and domain correspondence of calibration data. In this work, we aim at bridging this gap and further analyze underlying influencing mechanisms from the activation pattern perspective. Especially, we explore the calibration data's impacts on high-level complex reasoning capabilities, like math problem solving and code generation. Delving into the underlying mechanism, we find that the representativeness and diversity in activation space more fundamentally determine the quality of calibration data. Finally, we propose a calibration data curation framework based on such observations and analysis, enhancing the performance of existing post-training compression methods on preserving critical LLM capabilities. Our code is provided in [Link](https://github.com/BokwaiHo/COLA.git).",
    "authors": [
      "~Bowei_He1",
      "~Lihao_Yin2",
      "~Huiling_Zhen1",
      "~Shuqi_LIU1",
      "~Han_Wu5",
      "~Xiaokun_Zhang1",
      "~Mingxuan_Yuan1",
      "~Chen_Ma3"
    ],
    "pdf": "/pdf/451c6f73dc2b0953cd5c2ad72ff3e1cd9bca4c41.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Focuses on post-training compression techniques (pruning and quantization) directly related to inference optimization",
        "Addresses efficient inference for LLMs, which is central to improving GPU utilization and reducing memory requirements",
        "Provides systematic analysis of how compression affects different LLM capabilities",
        "Proposes a calibration data curation framework that enhances existing compression methods",
        "Code availability enables practical implementation and experimentation with optimization techniques"
      ],
      "Irrelevant Aspects": [
        "Does not explicitly measure GPU utilization, throughput, or latency improvements",
        "Limited focus on scalability aspects of model optimization",
        "Primarily focuses on post-training compression rather than training optimization",
        "More focused on theoretical understanding of calibration data than practical performance metrics",
        "Narrow focus on calibration data rather than broader system optimization aspects"
      ],
      "Summary": "This paper investigates post-training compression methods for LLMs, specifically examining how calibration data impacts the preservation of model capabilities during pruning and quantization. It's highly relevant to inference optimization, as these compression techniques directly influence GPU utilization, memory efficiency, and ultimately throughput and latency. The paper's proposed calibration data curation framework could enhance existing optimization pipelines. While it doesn't explicitly measure the performance metrics central to my research interest, understanding how to maintain model quality during compression is crucial for practical deployment of optimized LLMs."
    }
  },
  {
    "id": "PjbpL4brUb",
    "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs",
    "abstract": "Large language models (LLMs) deliver strong performance but are difficult to deploy due to high memory and compute costs. While pruning reduces these demands, most methods ignore activation sparsity observed at runtime. We reinterpret activation sparsity as dynamic structured weight sparsity and propose DuoGPT, a unified framework that constructs dual-sparse (spMspV) workloads by combining unstructured weight pruning with activation sparsity. To preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with activation-aware calibration and introduce output residuals from the dense model as correction terms. We further optimize the solution for efficient GPU execution, enabling scalability to billion-parameter LLMs. Evaluations on LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured pruning methods by up to 9.17\\% accuracy at an iso-speedup of 1.39$\\times$ compared to the baseline dense model. Code is available at GitHub.",
    "authors": [
      "~Ruokai_Yin1",
      "~Yuhang_Li1",
      "~Donghyun_Lee5",
      "~Priyadarshini_Panda1"
    ],
    "pdf": "/pdf/6827bccec4b62c1789e879b8f3c71d807f2a2f02.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM optimization for reducing memory and compute costs, which directly relates to GPU utilization and throughput optimization. It addresses both weight pruning and activation sparsity, which are key techniques for improving inference efficiency. The paper claims to optimize for efficient GPU execution and scalability to billion-parameter models, achieving 1.39× speedup compared to baseline dense model. The training-free approach could imply lower optimization overhead, making it practical for deployment.",
      "Irrelevant Aspects": "The paper doesn't appear to address training optimization (only inference optimization). There's no mention of specific techniques related to model parallelism or distributed computing strategies. No discussion of quantization, which is another important technique for optimization. The 'training-free' aspect suggests it doesn't optimize the training process itself.",
      "Summary": "This paper presents DuoGPT, a unified framework for constructing dual-sparse workloads by combining unstructured weight pruning with activation sparsity in LLMs. The approach extends Optimal Brain Compression with activation-aware calibration and introduces output residuals as correction terms. The paper focuses on efficient GPU execution and scalability, demonstrating 1.39× speedup with accuracy preservation on LLaMA-2 and LLaMA-3 models. The work is highly relevant to LLM inference optimization but doesn't address training optimization or distributed computing strategies."
    }
  },
  {
    "id": "0VDmWjW456",
    "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
    "abstract": "Large language models (LLMs) have achieved notable progress. Despite their success, next-token prediction (NTP), the dominant method for LLM training and inference, is constrained in both contextual coverage and inference efficiency due to its inherently sequential process. To overcome these challenges, we propose leap multi-token prediction~(L-MTP), an innovative token prediction method that extends the capabilities of multi-token prediction (MTP) by introducing a leap-based mechanism. Unlike conventional MTP, which generates multiple tokens at adjacent positions, L-MTP strategically skips over intermediate tokens, predicting non-sequential ones in a single forward pass. This structured leap not only enhances the model's ability to capture long-range dependencies but also enables a decoding strategy specially optimized for non-sequential leap token generation, effectively accelerating inference. We theoretically demonstrate the benefit of L-MTP in improving inference efficiency. Experiments across diverse benchmarks validate its merit in boosting both LLM performance and inference speed. The source code is available at https://github.com/Xiaohao-Liu/L-MTP.",
    "authors": [
      "~Xiaohao_Liu1",
      "~Xiaobo_Xia1",
      "~Weixiang_Zhao1",
      "~Manyi_Zhang2",
      "~Xianzhi_Yu1",
      "~Xiu_Su1",
      "~Shuo_Yang5",
      "~See-Kiong_Ng1",
      "~Tat-Seng_Chua2"
    ],
    "pdf": "/pdf/03edf649a9f42cdf6d921cb59599e7130120540a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses inference optimization through multi-token prediction, directly targeting throughput improvement and latency reduction. The leap-based mechanism enables non-sequential token generation in a single forward pass, which could significantly improve GPU utilization by reducing the inherent sequential bottlenecks of next-token prediction. The approach focuses on accelerating inference while maintaining model performance, which is central to my research in LLM optimization.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU-specific optimizations, memory management techniques, quantization, or distributed system implementations. There's no mention of kernel-level optimizations or hardware-specific considerations that would be relevant to maximizing GPU utilization.",
      "Summary": "L-MTP introduces a leap-based multi-token prediction approach that predicts non-adjacent tokens in a single forward pass, aiming to improve both inference speed and model performance. This method has high relevance to my research interests in LLM inference optimization, particularly for improving throughput and reducing latency, though it lacks some GPU-specific optimization details."
    }
  },
  {
    "id": "M6zQNbCaLl",
    "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models",
    "abstract": "Large vision-language models (LVLMs) excel at multimodal understanding but suffer from high computational costs due to redundant vision tokens. Existing pruning methods typically rely on single-layer attention scores to rank and prune redundant visual tokens to solve this inefficiency. However, as the interaction between tokens and layers is complicated, this raises a basic question: Is such a simple single-layer criterion sufficient to identify redundancy? To answer this question, we rethink the emergence of redundant visual tokens from a fundamental perspective: information flow, which models the interaction between tokens and layers by capturing how information moves between tokens across layers. We find (1) the CLS token acts as an information relay, which can simplify the complicated flow analysis; (2) the redundancy emerges progressively and dynamically via layer-wise attention concentration; and (3) relying solely on attention scores from single layers can lead to contradictory redundancy identification. Based on this, we propose FlowCut, an information-flow-aware pruning framework, mitigating the insufficiency of the current criterion for identifying redundant tokens and better aligning with the model's inherent behaviors. Extensive experiments show FlowCut achieves superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2$\\times$ speed-up in the prefilling stage.  Our code is available at https://github.com/TungChintao/FlowCut.",
    "authors": [
      "~Jintao_Tong1",
      "~Wenwei_Jin1",
      "~Pengda_Qin2",
      "~Anqi_Li4",
      "~Yixiong_Zou1",
      "~Yuhong_Li3",
      "~Yuhua_Li2",
      "~Ruixuan_Li1"
    ],
    "pdf": "/pdf/f1b8572aa0ab2c78c41785f7d5e8291e97f4b115.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on inference optimization for large models through token pruning, achieving 3.2x speedup in prefilling stage; addresses computational efficiency in 7B parameter models; implements significant token reduction (88.9-94.4%) which improves GPU utilization; directly relates to reducing latency and improving throughput in vision-language models.",
      "Irrelevant Aspects": "Primarily focuses on vision-language models rather than pure text LLMs; specifically targets visual token pruning rather than general optimization; concentrates on prefilling stage optimization rather than complete inference pipeline or training optimization; lacks discussion of distributed systems scalability across multiple GPUs.",
      "Summary": "FlowCut presents an information-flow-aware pruning framework for efficient vision-language models, addressing redundant visual tokens through a novel approach that analyzes how information moves between tokens across layers. The method achieves significant computational savings (3.2x speedup) with minimal performance impact by eliminating 88.9-94.4% of visual tokens in LLaVA models. While the approach has clear relevance to inference optimization and efficiency, its specific focus on multimodal models and visual token pruning limits direct applicability to text-only LLM optimization."
    }
  },
  {
    "id": "6hvaQTKkpF",
    "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
    "abstract": "Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28× reduction in Time-To-First-Token (TTFT) and a 1.32× acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.",
    "authors": [
      "~Kele_Shao1",
      "~Keda_TAO1",
      "~Can_Qin1",
      "~Haoxuan_You1",
      "~Yang_Sui1",
      "~Huan_Wang3"
    ],
    "pdf": "/pdf/e81f13517f5b2cba6cdadb1a7c199704f89727d8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on reducing computational costs for video LLMs through token merging, addressing GPU utilization and scalability. It reports significant improvements in Time-To-First-Token (2.28× reduction) and decoding throughput (1.32× acceleration), directly relating to latency and throughput optimization. The approach reduces FLOPs to 6.9% of the original while maintaining 99.1% of performance, showcasing an excellent efficiency-performance trade-off. The dual approach of outer-LLM and inner-LLM token pruning represents novel inference optimization techniques specifically for video LLMs, which handle much larger inputs than standard LLMs.",
      "Irrelevant Aspects": "The paper focuses on a training-free framework, so it doesn't address training optimization aspects of LLMs. It lacks details on hardware-specific optimizations or detailed GPU utilization strategies. There's no mention of memory optimization techniques, distributed processing across multiple GPUs, or model architecture improvements. The scope is limited to token processing rather than broader system optimization techniques.",
      "Summary": "HoliTom introduces a holistic token merging framework for video LLMs that combines outer-LLM and inner-LLM pruning approaches. The method reduces visual tokens by over 90%, significantly decreasing computational burden while maintaining nearly identical performance. The paper demonstrates substantial improvements in key inference metrics including TTFT and throughput, making it highly relevant to LLM inference optimization research, though it doesn't address training optimization or distributed systems."
    }
  },
  {
    "id": "uwUkETPIJN",
    "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
    "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. \nOur code repository is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl/.",
    "authors": [
      "~Yifan_Sun8",
      "~Jingyan_Shen1",
      "~Yibin_Wang6",
      "~Tianyu_Chen3",
      "~Zhendong_Wang1",
      "~Mingyuan_Zhou1",
      "~Huan_Zhang1"
    ],
    "pdf": "/pdf/3348acd77f82a290d29ff3380719d7df4c40eee3.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses optimization of LLM training, specifically reinforcement learning fine-tuning which is resource-intensive. It introduces techniques (difficulty-targeted data selection and rollout replay) that reduce RL fine-tuning time by 23-62%, directly impacting GPU utilization and training efficiency. The work focuses on improving throughput and reducing computational requirements during the training phase.",
      "Irrelevant Aspects": "The paper primarily focuses on data efficiency rather than system-level optimizations. It doesn't address inference optimization, memory management techniques, distributed training architectures, or hardware-level optimizations. The approach is more algorithmic than system-focused, targeting data selection rather than computational architecture.",
      "Summary": "This paper presents techniques to improve data efficiency in LLM reinforcement fine-tuning, introducing adaptive difficulty estimation and rollout replay mechanisms to reduce computational requirements. While relevant to training optimization, it focuses on data selection strategies rather than comprehensive system optimizations for both training and inference phases."
    }
  },
  {
    "id": "TS128AgTRw",
    "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions",
    "abstract": "Aligning large language models with pointwise absolute rewards has so far required online, on-policy algorithms such as PPO and GRPO.\nIn contrast, simpler methods that can leverage offline or off-policy data, such as DPO and REBEL, are limited to learning from preference pairs or relative signals.\nTo bridge this gap, we introduce Quantile Reward Policy Optimization (QRPO), which learns from pointwise absolute rewards while preserving the simplicity and offline applicability of DPO-like methods.\nQRPO uses quantile rewards to enable regression to the closed-form solution of the KL-regularized RL objective.\nThis reward yields an analytically tractable partition function, removing the need for relative signals to cancel this term.\nMoreover, QRPO scales with increased compute to estimate quantile rewards, opening a new dimension for pre-computation scaling.\nEmpirically, QRPO consistently achieves top performance on chat and coding evaluations—reward model scores, AlpacaEval 2, and LeetCode—compared to DPO, REBEL, and SimPO across diverse datasets and 8B-scale models.\nFinally, we find that training with robust rewards instead of converting them to preferences induces less length bias.",
    "authors": [
      "~Simon_Matrenok1",
      "~Skander_Moalla1",
      "~Caglar_Gulcehre1"
    ],
    "pdf": "/pdf/2b158b043e79c9b894e23da28c8170a07013f88f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces QRPO, a new method for aligning large language models with pointwise absolute rewards, which is directly relevant to LLM training optimization. It preserves simplicity and offline applicability of DPO-like methods, which could impact training efficiency. QRPO scales with increased compute to estimate quantile rewards, relating to scalability aspects. The method works with 8B-scale models and removes the need for relative signals, potentially streamlining the training process.",
      "Irrelevant Aspects": "The focus is more on alignment methodology rather than specific optimization techniques for GPU utilization. There's no explicit discussion of inference optimization, latency, or throughput. The paper doesn't address memory optimization or specific GPU utilization techniques. The evaluation focuses on reward model scores, AlpacaEval 2, and LeetCode, rather than computational efficiency metrics.",
      "Summary": "QRPO introduces a novel approach to LLM alignment that bridges the gap between online on-policy methods like PPO and offline methods like DPO. By using quantile rewards, it enables regression to the closed-form solution of the KL-regularized RL objective while maintaining offline applicability. The method shows strong empirical performance on chat and coding evaluations and opens new dimensions for pre-computation scaling."
    }
  },
  {
    "id": "8Ounc8L4F7",
    "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices.  Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher information matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion.  (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints.  Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP\\# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy.  This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.",
    "authors": [
      "~Zukang_Xu1",
      "~Xing_Hu6",
      "~Qiang_Wu6",
      "~Dawei_Yang3"
    ],
    "pdf": "/pdf/cfc42b79aa8098ff234d95d98996d785c230d38f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Model compression for LLMs through vector quantization, inference optimization, low-bit quantization (2-4 bits), practical implementation with experiments on LLaMA-3 8B, theoretical foundation using Riemannian geometry, and performance improvements in perplexity and accuracy.",
      "Irrelevant Aspects": "Limited focus on training optimization, no explicit discussion of GPU utilization during training, lacks distributed training aspects, minimal hardware-specific optimizations, and no direct mention of throughput and latency measurements.",
      "Summary": "This paper presents RSAVQ, a novel vector quantization framework for extremely low-bit quantization of large language models. It uses Riemannian geometry and Fisher information matrix to improve quantization accuracy. The approach shows significant performance improvements on LLaMA-3 8B, making it highly relevant to inference optimization for LLMs. However, it primarily focuses on the quantization algorithm rather than system-level optimizations like GPU utilization or scalability across multiple GPUs during training."
    }
  },
  {
    "id": "1BAiQmAFsx",
    "title": "Walking the Tightrope: Autonomous Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning",
    "abstract": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large language models (MLLMs), especially for chest diagnosis: detrimental concept drift within chain-of-thought (CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where reasoning token distributions evolve unpredictably, thereby introducing significant biases in final predictions. To address this, we are pioneers in establishing the theoretical bridge between concept drift theory and RFT processes by formalizing CoT's autoregressive token streams as non-stationary distributions undergoing arbitrary temporal shifts. Leveraging this framework, we propose a novel autonomous counterfact-aware RFT that systematically decouples beneficial distribution adaptation from harmful concept drift through concept graph-empowered LLM experts generating counterfactual reasoning trajectories. Our solution, Counterfactual Preference Optimization (CPO), enables autonomous and stable RFT in non-stationary environments, particularly within the medical domain, through custom-tuning of counterfactual-aware preference alignment. Extensive experiments demonstrate our superior performance of robustness, generalization and coordination within RFT. Besides, we also contribute a large-scale dataset CXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual reasoning trajectories derived from MIMIC-CXR. Our code and data are public at: https://github.com/XiaoyuYoung/CPO.",
    "authors": [
      "~Xiaoyu_Yang2",
      "~Jie_Lu10",
      "~En_Yu3"
    ],
    "pdf": "/pdf/cef5e892fff85576a36e44702a9289193cfcc313.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization, reinforcement fine-tuning (RFT) techniques, handling concept drift during training, preference optimization methods",
      "Irrelevant Aspects": "Medical domain focus, multi-modal aspects, limited discussion of inference optimization, no explicit mention of GPU utilization or scalability metrics",
      "Summary": "The paper presents a novel approach called Counterfactual Preference Optimization (CPO) to address concept drift during reinforcement fine-tuning of multi-modal LLMs. While it offers valuable insights into training optimization techniques for handling non-stationary distributions, its focus on medical applications and multi-modal models limits its direct relevance to general LLM systems optimization. The approach doesn't appear to address inference optimization, GPU utilization, or scalability concerns that are central to my research interests."
    }
  },
  {
    "id": "XKnOA7MhCz",
    "title": "Pay Attention to Small Weights",
    "abstract": "Finetuning large pretrained neural networks is known to be resource-intensive, both in terms of memory and computational cost. To mitigate this, a common approach is to restrict training to a subset of the model parameters. By analyzing the relationship between gradients and weights during finetuning, we observe a notable pattern: large gradients are often associated with small-magnitude weights. This correlation is more pronounced in fine-tuning settings than in training from scratch. Motivated by this observation, we propose \\textsc{NanoAdam}, which dynamically updates only the small-magnitude weights during fine-tuning and offers several practical advantages: first, the criterion is \\emph{gradient-free}—the parameter subset can be determined without gradient computation; second, it preserves large-magnitude weights, which are likely to encode critical features learned during pre-training, thereby reducing the risk of catastrophic forgetting; thirdly, it permits the use of larger learning rates and consistently leads to better generalization performance in experiments. We demonstrate this for both NLP and vision tasks.",
    "authors": [
      "~Chao_Zhou11",
      "~Tom_Jacobs1",
      "~Advait_Gadhikar1",
      "~Rebekka_Burkholz1"
    ],
    "pdf": "/pdf/cc6824af23d341840ca714f26e3bde9780dd9a59.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses resource-intensive fine-tuning of large pretrained neural networks, which is central to my research on training optimization. NanoAdam's approach of updating only small-magnitude weights reduces computational and memory requirements during fine-tuning, which directly relates to improving GPU utilization and scalability. The method's gradient-free criterion for parameter selection is an interesting optimization strategy that could enhance training efficiency. Since it's demonstrated on NLP tasks, it has direct applicability to large language models, which is my primary focus.",
      "Irrelevant Aspects": "The paper focuses solely on training/fine-tuning optimization and does not address inference optimization, which is a significant part of my research interests. It doesn't specifically discuss latency considerations or inference-time performance improvements. The paper also mentions vision applications which are less relevant to my NLP/LLM focus.",
      "Summary": "This paper presents NanoAdam, a method that optimizes fine-tuning by only updating small-magnitude weights based on the observation that large gradients correlate with small weights during fine-tuning. The approach reduces computational and memory requirements while preserving critical knowledge from pre-training. While highly relevant to training optimization of large language models, it doesn't address inference optimization, which limits its overall relevance score to 8 out of 10."
    }
  },
  {
    "id": "AA0qGRIfPn",
    "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering",
    "abstract": "Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance. To address this, we derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging $\\epsilon$-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading. MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4\\% of performance for LLaVA-1.5-7B using only 11.1\\% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\\times$ with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks. The code will be made available soon.",
    "authors": [
      "~Yangfu_Li3",
      "~Hongjian_Zhan1",
      "~Tianyi_Chen13",
      "~Qi_Liu39",
      "~Yu-Jie_Xiong1",
      "~Yue_Lu3"
    ],
    "pdf": "/pdf/7ef7a4f75c3b6234de2b6c1469869fdfea4de55a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization through visual token pruning, directly addressing latency reduction (1.3-1.5× acceleration for LLaVA-Next-7B) and computational efficiency. It demonstrates significant resource efficiency by preserving 96.4% of performance using only 11.1% of visual tokens. The method claims linear scalability with respect to input tokens, which is relevant for handling larger models and inputs. These optimizations would lead to better GPU utilization and higher throughput during inference of multimodal models.",
      "Irrelevant Aspects": "The paper specifically targets multimodal models with visual components (vision-language models) rather than general large language models. It focuses on inference optimization rather than training optimization. Much of the content appears to be theoretical (closed-form error bounds, ε-covering theory) with less emphasis on practical GPU implementation details. The techniques are specialized for visual token processing, not applicable to text-only models.",
      "Summary": "This paper presents Multi-Objective Balanced Covering (MoB), a method for visual token pruning in multimodal LLMs that optimizes the trade-off between prompt alignment and visual preservation. It provides theoretical foundations through error bounds and covering theory, then reformulates pruning as a bi-objective covering problem. The method achieves significant computational reductions (88.9% fewer visual tokens) while maintaining high performance (96.4% of original) and provides 1.3-1.5× inference speedup with minimal quality loss. While highly relevant for multimodal inference optimization, it's specialized for vision-language models and focuses on inference rather than training optimization."
    }
  },
  {
    "id": "epZTfZF7JC",
    "title": "VITA-Audio: Fast Interleaved Audio-Text Token Generation for Efficient Large Speech-Language Model",
    "abstract": "With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "authors": [
      "~Zuwei_Long1",
      "~Yunhang_Shen1",
      "~Chaoyou_Fu1",
      "~Heting_Gao1",
      "~lijiang_Li1",
      "~Peixian_Chen2",
      "~Mengdan_Zhang2",
      "~Hang_Shao3",
      "~Jian_Li12",
      "~Jinlong_Peng1",
      "~Haoyu_Cao1",
      "~Ke_Li4",
      "~Rongrong_Ji5",
      "~Xing_Sun1"
    ],
    "pdf": "/pdf/9045b043695e1d38794557e2d092c84adae305ab.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization and latency reduction in large models, proposing techniques to generate multiple tokens in a single forward pass, achieving 3-5x inference speedup at 7B parameter scale. It discusses a progressive training strategy for model acceleration and specifically targets streaming scenarios with minimal latency.",
      "Irrelevant Aspects": "The focus is specifically on speech-language models rather than general text LLMs, which is a specialized domain. The paper doesn't explicitly discuss GPU utilization metrics. The audio-text multimodal nature requires different considerations than pure text models.",
      "Summary": "VITA-Audio presents techniques for efficient inference in large speech-language models, achieving significant speedups and latency reductions. While focused on the speech domain, its optimization approaches for generating multiple tokens in a single pass and progressive training strategy are highly relevant to my research in LLM inference optimization, throughput enhancement, and latency reduction."
    }
  },
  {
    "id": "gY9yOGYB48",
    "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
    "abstract": "Video diffusion transformers have achieved remarkable progress in high-quality video generation, but remain computationally expensive due to the quadratic complexity of attention over high-dimensional video sequences.\nRecent acceleration methods enhance the efficiency by exploiting the local sparsity of attention scores; yet they often struggle with accelerating the long-range computation.\nTo address this problem, we propose VORTA, an acceleration framework with two novel components: 1) a sparse attention mechanism that efficiently captures long-range dependencies, and 2) a routing strategy that adaptively replaces full 3D attention with specialized sparse attention variants.\nVORTA achieves an end-to-end speedup $1.76\\times$ without loss of quality on VBench.\nFurthermore, it can seamlessly integrate with various other acceleration methods, such as model caching and step distillation, reaching up to speedup $14.41\\times$ with negligible performance degradation.\nVORTA demonstrates its efficiency and enhances the practicality of video diffusion transformers in real-world settings.\nCodes and weights are available at https://github.com/wenhao728/VORTA.",
    "authors": [
      "~Wenhao_Sun1",
      "~Rong-Cheng_Tu1",
      "~Yifu_Ding2",
      "~Jingyi_Liao1",
      "~Zhao_Jin2",
      "~Shunyu_Liu1",
      "~Dacheng_Tao1"
    ],
    "pdf": "/pdf/a80330516d84c77f99886386221033b33e1aae25.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses computational efficiency in transformer models through sparse attention mechanisms and routing strategies, which are directly applicable to LLM optimization. It demonstrates significant speedup potential (1.76× to 14.41×) without quality degradation, which aligns with improving throughput and reducing latency. The techniques for handling quadratic attention complexity are transferable challenges between video and language domains.",
      "Irrelevant Aspects": "The primary focus is on video diffusion transformers rather than language models specifically. The paper doesn't directly address training optimization, focusing more on inference. The evaluation is on VBench (video benchmark) rather than language model benchmarks. 3D attention considerations are specific to video, not language models.",
      "Summary": "VORTA introduces a sparse attention framework with adaptive routing to accelerate video diffusion transformers. While focused on video rather than language, the techniques for attention optimization, sparse mechanisms, and routing strategies have high relevance to LLM optimization. The paper demonstrates significant speedup potential without quality degradation, which aligns with research goals of improving GPU utilization, scalability, throughput, and reducing latency in large models."
    }
  },
  {
    "id": "7pufO0SJAC",
    "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
    "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. However, while this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient optimization.",
    "authors": [
      "~Noam_Razin1",
      "~Zixuan_Wang4",
      "~Hubert_Strauss1",
      "~Stanley_Wei1",
      "~Jason_D._Lee1",
      "~Sanjeev_Arora1"
    ],
    "pdf": "/pdf/6d32b244e14e9b4d7958c2d36b3178dada088f56.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses optimization aspects of RLHF, which is a key training methodology for large language models. It discusses how reward model characteristics affect the optimization landscape of RLHF, directly impacting training efficiency. The research considers large-scale models (up to 8B parameters), focuses on training optimization efficiency beyond just model accuracy, and examines the rate of reward maximization, which relates to training throughput.",
      "Irrelevant Aspects": "The paper focuses primarily on reward model aspects of RLHF rather than broader system optimization. It doesn't address GPU utilization, memory efficiency, or distributed computing strategies. There's no mention of inference optimization, which is half of my research focus. The paper doesn't cover latency improvements or specific hardware optimization techniques.",
      "Summary": "This paper investigates the optimization dynamics of RLHF, particularly how reward models affect the optimization landscape and training efficiency. While it addresses important aspects of training optimization for large language models, it doesn't cover hardware utilization, distributed training strategies, or inference optimization, which are key components of my research interest."
    }
  },
  {
    "id": "7kQjbCQwtT",
    "title": "Discovering Important Experts for Mixture-of-Experts Models Pruning Through a Theoretical Perspective",
    "abstract": "Mixture-of-Experts (MoE) architectures enable efficient scaling of large language models but face prohibitive memory demands due to massive parameterization. Existing pruning methods rely on heuristic metrics or impractical enumeration of expert subsets, leading to suboptimal performance or scalability. In this paper, we propose Shapley-MoE, an efficient pruning method for MoE models inspired by cooperative game theory. By quantifying each expert’s contribution via Shapley value, our method identifies important experts without exhaustive combination evaluations. To overcome the NP-hard complexity of exact Shapley computation, we introduce a Monte Carlo sampling strategy for efficient approximation that reduces complexity to quadratic time. However, vanilla Monte Carlo sampling still faces issues of insufficient estimation accuracy and low sampling efficiency. To address these issues, we further propose two novel methods to improve sampling accuracy and efficiency: (1) Early Truncation, which early terminates unstable sampling steps caused by overly small expert subsets, and (2) Router-Guided Importance Sampling, which prioritize sampling important expert subsets using gating activation probabilities. Both theoretical and experimental analyses show that both methods can accelerate Shapley value estimation and improve accuracy. Extensive empirical evaluations demonstrate that our pruned MoE models outperform existing expert pruning methods. Notably, when applied to the Qwen2-57B-A14B model, our method reduces the number of experts by 25% with only a 0.92 increase in perplexity and over 96.4% of the average zero-shot accuracy is maintained.",
    "authors": [
      "~Weizhong_Huang1",
      "~Yuxin_Zhang3",
      "~Xiawu_Zheng1",
      "~Fei_Chao1",
      "~Rongrong_Ji5",
      "~Liujuan_Cao1"
    ],
    "pdf": "/pdf/fc4b6921cdab43d0d92b99143b7b10553f319bf5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses Mixture-of-Experts optimization, which is crucial for improving GPU utilization and scalability of large language models. It focuses on reducing memory demands and computational requirements through expert pruning, which can significantly improve inference efficiency. The method achieves substantial model compression (25% fewer experts) with minimal performance degradation, demonstrating effective trade-offs between model size and accuracy. The efficient algorithm with quadratic complexity is highly relevant for scalable LLM deployment.",
      "Irrelevant Aspects": "The heavy emphasis on cooperative game theory and Shapley values represents a mathematical approach rather than hardware-specific optimization. The theoretical foundations and sampling methodologies are less directly applicable to GPU utilization, throughput, or latency considerations compared to system-level optimizations. The paper appears to focus on algorithmic compression rather than implementation-specific optimizations.",
      "Summary": "Shapley-MoE presents a novel expert pruning method for Mixture-of-Experts models inspired by cooperative game theory. By using Shapley values to quantify expert contributions and implementing efficient sampling strategies (Early Truncation and Router-Guided Importance Sampling), the method achieves significant model compression with minimal performance loss. When applied to Qwen2-57B-A14B, it reduces experts by 25% with only a 0.92 perplexity increase while maintaining over 96.4% of zero-shot accuracy, offering promising improvements for efficient LLM deployment."
    }
  },
  {
    "id": "rJ5ky9C3ue",
    "title": "CrossSpectra: Exploiting Cross-Layer Smoothness for Parameter-Efficient Fine-Tuning",
    "abstract": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large foundation models without excessive storage cost. However, current approaches such as LoRA treat each layer’s adaptation independently, overlooking correlations across layers. This independence causes the number of trainable parameters to grow linearly with model depth. We provide theoretical and empirical evidence that skip connections in transformers create smooth gradient propagation across layers. This smoothness leads to weight adaptations that concentrate most of their energy in low-frequency spectral components, especially along the layer dimension. Empirical analysis confirms this effect, showing that most of adaptation energy lies in low frequencies. Building on this insight, we propose CrossSpectra, which parameterizes all attention-weight adaptations $(Q, K, V)$ across layers as a single 3D tensor and represents them with sparse spectral coefficients ($\\kappa_1, \\kappa_2$). Using $\\kappa_{1}$ non-zero coefficients within each layer’s frequency space and truncating to $\\kappa_{2}$ frequencies across layers, CrossSpectra requires $\\mathcal{O}(\\kappa_{1}\\kappa_{2})$ parameters instead of LoRA’s $\\mathcal{O}(Lrd)$, where $L$ is the number of layers and $r$ the rank. Across natural-language and vision benchmarks, \\methodname{} matches or surpasses baseline performance while using fewer parameters than LoRA, achieving only $0.36\\%$ of LoRA’s parameter count when fine-tuning LLaMA-7B on instruction-following tasks. These results show that exploiting the \\textbf{architectural smoothness of transformers} through spectral analysis yields major efficiency gains in PEFT.",
    "authors": [
      "~Yifei_Zhang6",
      "~Hao_Zhu2",
      "~Junhao_Dong1",
      "~Haoran_Shi7",
      "~Ziqiao_Meng1",
      "~Piotr_Koniusz1",
      "~Han_Yu1"
    ],
    "pdf": "/pdf/b02333f13922080005d5209331273da51a48baba.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parameter-efficient fine-tuning that reduces trainable parameters to 0.36% of LoRA's count; theoretical analysis of transformer architectural properties; spectral parameterization across layers that improves scalability; multi-domain effectiveness on both language and vision tasks; maintains or improves performance while reducing parameters",
      "Irrelevant Aspects": "Limited discussion of GPU utilization metrics; no direct analysis of inference latency improvements; minimal focus on hardware-specific optimizations; potential computational overhead from spectral analysis not addressed",
      "Summary": "CrossSpectra introduces a novel parameter-efficient fine-tuning method that exploits cross-layer smoothness in transformers through spectral analysis. By parameterizing weight adaptations as a 3D tensor and representing them with sparse spectral coefficients, it achieves significant parameter reduction from O(Lrd) to O(κ₁κ₂). The approach matches or exceeds baseline performance across multiple benchmarks while dramatically reducing parameter count, offering promising insights for training optimization of large foundation models."
    }
  }
]
