[
  {
    "id": "gx1wHnf5Vp",
    "title": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization",
    "abstract": "The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. \nExtensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more.\nAs a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs.\nAll experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE.",
    "authors": [
      "~Taishi_Nakamura1",
      "~Takuya_Akiba2",
      "~Kazuki_Fujii1",
      "~Yusuke_Oda1",
      "~Rio_Yokota1",
      "~Jun_Suzuki1"
    ],
    "pdf": "/pdf/161bbf8b9a237f60ba0aee05309543ab2c3f0b1b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on Mixture of Experts (MoE) architecture optimization, which directly relates to efficient training of large language models. It presents Drop-Upcycling, a method that enhances MoE model efficiency in knowledge acquisition. The paper demonstrates significant computational efficiency gains, with a 5.9B active parameter model achieving performance comparable to a 13B dense model using only 1/4 of training FLOPs. This directly addresses my interest in GPU utilization and scalability. The large-scale experiments conducted (hundreds of billions of tokens) align with my focus on training optimization for large-scale systems.",
      "Irrelevant Aspects": "The paper appears to focus primarily on training methodology rather than inference optimization, which is half of my research interest. It doesn't explicitly address GPU utilization at the systems level or provide detailed analysis of throughput and latency improvements. The focus is on the machine learning approach (partial re-initialization) rather than the systems engineering aspects of training and inference. The paper may not cover the low-level implementation details that would be most relevant for my research into GPU utilization and scalability.",
      "Summary": "Drop-Upcycling presents a novel approach for training Sparse Mixture of Experts models that combines pre-trained dense model knowledge with strategic weight re-initialization. The method promotes expert specialization and enhances knowledge acquisition efficiency. Through large-scale experiments, the authors demonstrate that their approach outperforms previous MoE construction methods when training on hundreds of billions of tokens or more, resulting in significant computational savings. Their 5.9B active parameter MoE model achieves comparable performance to a 13B dense model in the same family while requiring only 1/4 of the training FLOPs, making it highly relevant to training optimization research for large language models."
    }
  },
  {
    "id": "x83w6yGIWb",
    "title": "Beware of Calibration Data for Pruning Large Language Models",
    "abstract": "As large language models (LLMs) are widely applied across various fields, model\ncompression has become increasingly crucial for reducing costs and improving\ninference efficiency. Post-training pruning is a promising method that does not\nrequire resource-intensive iterative training and only needs a small amount of\ncalibration data to assess the importance of parameters. Recent research has enhanced post-training pruning from different aspects but few of them systematically\nexplore the effects of calibration data, and it is unclear if there exist better calibration data construction strategies. We fill this blank and surprisingly observe that\ncalibration data is also crucial to post-training pruning, especially for high sparsity. Through controlled experiments on important influence factors of calibration\ndata, including the pruning settings, the amount of data, and its similarity with\npre-training data, we observe that a small size of data is adequate, and more similar data to its pre-training stage can yield better performance. As pre-training data\nis usually inaccessible for advanced LLMs, we further provide a self-generating\ncalibration data synthesis strategy to construct feasible calibration data. Experimental results on recent strong open-source LLMs (e.g., DCLM, and LLaMA-3)\nshow that the proposed strategy can enhance the performance of strong pruning\nmethods (e.g., Wanda, DSnoT, OWL) by a large margin (up to 2.68%).",
    "authors": [
      "~Yixin_Ji2",
      "~Yang_Xiang6",
      "~Juntao_Li2",
      "~Qingrong_Xia1",
      "~Ping_Li16",
      "~Xinyu_Duan1",
      "~Zhefeng_Wang1",
      "~Min_Zhang9"
    ],
    "pdf": "/pdf/75e829c0ab0ccd4d7b93f8eabc80e93e243e1ee9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on post-training pruning of LLMs, which is a key technique for improving inference efficiency. It explores calibration data optimization for pruning methods, provides a self-generating calibration data synthesis strategy, and demonstrates improvements on strong pruning methods across recent LLMs. These aspects directly relate to inference optimization and performance enhancement of LLMs.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization metrics, training optimization, or detailed latency/throughput analysis. It focuses on post-training approaches rather than training-time optimizations. Hardware-specific considerations and deployment scenarios aren't extensively covered.",
      "Summary": "This paper is highly relevant to LLM inference optimization research. It investigates how calibration data affects post-training pruning performance, particularly at high sparsity levels. The authors find that small amounts of data similar to pre-training data work best, and they propose a self-generating calibration data synthesis strategy for cases where pre-training data is inaccessible. Experiments on LLaMA-3 and other models show their approach can enhance pruning methods by up to 2.68%, which directly impacts inference efficiency."
    }
  },
  {
    "id": "1Iu2Yte5N6",
    "title": "Rapid Selection and Ordering of In-Context Demonstrations via Prompt Embedding Clustering",
    "abstract": "While Large Language Models (LLMs) excel at in-context learning (ICL) using just a few demonstrations, their performances are sensitive to demonstration orders. The reasons behind this sensitivity remain poorly understood. In this paper, we investigate the prompt embedding space to bridge the gap between the order sensitivity of ICL with inner workings of decoder-only LLMs, uncovering the clustering property: prompts sharing the first and last demonstrations have closer embeddings, with first-demonstration clustering usually being stronger in practice. We explain this property through extensive theoretical analyses and empirical evidences. Our finding suggests that the positional encoding and the causal attention mask are key contributors to the clustering phenomenon. Leveraging this clustering insight, we introduce Cluster-based Search, a novel method that accelerates the selection and ordering of demonstrations in self-adaptive ICL settings. Our approach substantially decreases the time complexity from factorial to quadratic, saving 92% to nearly 100% execution time while maintaining comparable performance to exhaustive search.",
    "authors": [
      "~Kha_Pham1",
      "~Hung_Le1",
      "~Man_Ngo1",
      "~Truyen_Tran1"
    ],
    "pdf": "/pdf/5cec8d826e3c9e64289a5798fbe936ce23a51973.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Focuses on optimizing LLM inference efficiency for in-context learning",
        "Significantly reduces computational complexity from factorial to quadratic",
        "Claims 92% to nearly 100% execution time savings during inference",
        "Addresses performance optimization while maintaining model accuracy",
        "Presents a practical approach that directly improves inference speed"
      ],
      "Irrelevant Aspects": [
        "Does not explicitly address GPU utilization optimization",
        "Limited discussion of memory efficiency optimizations",
        "No focus on distributed computing scalability aspects",
        "Primarily concerned with prompt selection rather than system-level optimizations",
        "Does not address training optimization techniques"
      ],
      "Summary": "The paper introduces a novel clustering-based approach to optimize in-context learning demonstration selection, substantially reducing computational overhead and inference time. While it presents significant efficiency gains for the specific problem of demonstration ordering, it doesn't directly address the core GPU utilization and distributed scaling challenges that are central to large-scale LLM deployment optimization."
    }
  },
  {
    "id": "1qP3lsatCR",
    "title": "NetMoE: Accelerating MoE Training through Dynamic Sample Placement",
    "abstract": "Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency.\n\nIn this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop NetMoE, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of $1.67 \\times$ compared with current MoE training frameworks.",
    "authors": [
      "~Xinyi_Liu16",
      "~Yujie_Wang10",
      "~Fangcheng_Fu1",
      "~Xupeng_Miao1",
      "~Shenhan_Zhu1",
      "~Xiaonan_Nie1",
      "~Bin_CUI2"
    ],
    "pdf": "/pdf/b92193e3eb230e379ba2c07078799b70a54ecc40.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing Mixture of Experts (MoE) models which are critical for large language model training. It addresses All-to-All communication bottlenecks in distributed training, directly impacting GPU utilization and scalability. The approach improves training efficiency by 1.67× on 32 GPUs, demonstrating better resource utilization and throughput. The work targets system-level optimization of training processes, which aligns perfectly with my research interests in training optimization.",
      "Irrelevant Aspects": "The paper does not address inference optimization, focusing solely on training efficiency. There is no discussion about latency measurements or improvements. The optimization is specifically centered on communication patterns rather than other aspects of GPU utilization such as memory optimization or compute scheduling.",
      "Summary": "NetMoE presents a novel approach to accelerate MoE model training by rearranging training samples to minimize All-to-All communication overhead. The paper identifies that tokens from the same training sample exhibit locality in expert routing, and leverages this observation to optimize sample placement across GPUs. By formulating an integer programming problem to deduce optimal placement, they achieve significant efficiency improvements in distributed MoE training setups."
    }
  },
  {
    "id": "1eQT9OzfNQ",
    "title": "Long Context Compression with Activation Beacon",
    "abstract": "Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. \n1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts).\n2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. \n3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance.\n4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. \n\nExtensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, \nachieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache.",
    "authors": [
      "~Peitian_Zhang1",
      "~Zheng_Liu4",
      "~Shitao_Xiao1",
      "~Ninglu_Shao1",
      "~Qiwei_Ye1",
      "~Zhicheng_Dou1"
    ],
    "pdf": "/pdf/2cb117a00ef812283da61295a1d36584936a89ef.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses my research interests in LLM inference optimization, GPU utilization, memory efficiency, and scalability. It proposes Activation Beacon, a method that compresses activations (keys and values) to reduce computational and memory costs, achieving 2x inference acceleration and 8x memory reduction for KV cache. This directly relates to improving throughput, reducing latency, and enhancing GPU utilization. The method's flexibility in supporting various compression configurations is also valuable for practical deployment scenarios.",
      "Irrelevant Aspects": "The paper's focus on specific training data (plain texts and instructional data) and evaluation tasks (document understanding, few-shot learning, Needle-in-a-Haystack) are less directly relevant to my core research interests. Additionally, some technical details about the compression workflow, while important for understanding the method, may not directly impact my focus on optimization and efficiency.",
      "Summary": "This paper presents Activation Beacon, a plug-in module for transformer-based LLMs that compresses long contexts by directly compressing activations (keys and values) rather than using soft prompts. The method introduces progressive compression of fine-grained input units and is trained through compression-based auto-regression with randomly sampled compression ratios. Evaluations show that Activation Beacon maintains comparable performance to uncompressed baselines while achieving significant efficiency gains (2x inference acceleration and 8x memory reduction for KV cache). The paper is highly relevant to my research interests in LLM optimization, particularly for improving GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "UQJ7CDW8nb",
    "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
    "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.",
    "authors": [
      "~Shaolei_Zhang1",
      "~Qingkai_Fang1",
      "~Zhe_Yang7",
      "~Yang_Feng4"
    ],
    "pdf": "/pdf/efd2169a71f1800808f58038f0bf1023ce051103.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on efficient Large Multimodal Models with significant computational optimizations: 77% reduction in FLOPs, low-latency responses under 40ms, processing of 10,000+ video frames on 24GB GPU hardware, and extreme vision token compression (from 576 to 1 token) that directly impacts GPU utilization and throughput.",
      "Irrelevant Aspects": "Primarily focuses on multimodal models rather than general LLM optimization, limited discussion on training optimization, specific architectural changes for vision token processing rather than broader system optimizations.",
      "Summary": "LLaVA-Mini presents an innovative approach to optimize multimodal models by compressing vision tokens to a single token through modality pre-fusion. This yields substantial efficiency gains (77% FLOPs reduction) while maintaining performance, directly addressing key concerns of GPU utilization, throughput, and latency in LLM systems. Though specialized to vision-language tasks, its compression principles have broader relevance to LLM optimization."
    }
  },
  {
    "id": "5RZoYIT3u6",
    "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
    "abstract": "The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose *PruneNet*, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80\\% retention of its zero-shot performance with a 30\\% compression ratio, outperforming existing methods that retain only 75\\% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80\\% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.",
    "authors": [
      "~Ayan_Sengupta1",
      "~Siddhant_Chaudhary1",
      "~Tanmoy_Chakraborty2"
    ],
    "pdf": "/pdf/a4ec37f08d2849b2cd4d8f203d00b3bf1aa3f16a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses model compression for large language models, which is crucial for improving GPU utilization and inference efficiency. It proposes PruneNet, a method that eliminates the need for calibration datasets, making it more scalable. The approach can compress LLaMA-2-7B in just 15 minutes while preserving over 80% of zero-shot performance with a 30% compression ratio. This compression directly impacts memory requirements and computational needs during inference, potentially reducing latency and improving throughput. The method's focus on spectral structure preservation helps maintain model quality during optimization, which is essential for practical deployment.",
      "Irrelevant Aspects": "The paper focuses primarily on post-training compression rather than training optimization itself. It lacks specific details about GPU utilization techniques, throughput measurements, or latency improvements in the abstract. The policy learning aspect may not be directly relevant to practical deployment concerns. There's limited information about how the compression specifically impacts inference speed on different hardware architectures.",
      "Summary": "This paper presents PruneNet, a calibration-free model compression method that reformulates pruning as a policy learning process. It decouples pruning from model architecture and achieves efficient compression of LLaMA-2-7B in 15 minutes while maintaining 80% of zero-shot performance at 30% compression ratio. The method is highly relevant for LLM deployment optimization as it addresses memory and computational requirements that directly affect GPU utilization, throughput, and latency, though it focuses more on post-training optimization rather than training processes."
    }
  },
  {
    "id": "OL44KtasKc",
    "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
    "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of $O(N^2)$, compared to $O(N)$ for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer.\nIn response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1x and 2.7x, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models—including those for large language processing, image generation, and video generation. The code is available at https://github.com/thu-ml/SageAttention.",
    "authors": [
      "~Jintao_Zhang2",
      "~Jia_wei6",
      "~Pengle_Zhang1",
      "~Jun_Zhu2",
      "~Jianfei_Chen1"
    ],
    "pdf": "/pdf/e452ea42598e289378880ea077b682a961e4946d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focuses on attention optimization in transformers, which is crucial for LLMs; addresses inference acceleration through quantization; claims significant speed improvements (2.1x over FlashAttention2 and 2.7x over xformers); uses 8-bit quantization to improve GPU utilization; targets higher throughput and lower latency; applicable to large language processing models; described as plug-and-play for easy integration.",
      "Irrelevant Aspects": "Does not explicitly address training optimization; focuses specifically on attention mechanism rather than the entire LLM pipeline; limited discussion of specific hardware optimizations beyond quantization.",
      "Summary": "SageAttention proposes an efficient 8-bit quantization method for attention mechanisms in transformers, achieving significant speed improvements over existing methods while maintaining accuracy. The paper is highly relevant to LLM inference optimization, GPU utilization, and performance improvements, though it doesn't address training optimization aspects."
    }
  },
  {
    "id": "bc3sUsS6ck",
    "title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass",
    "abstract": "Large language models (LLMs) acquire substantial knowledge during pretraining but often need adaptation to new contexts, tasks, or domains, typically achieved through fine-tuning or prompting. However, fine-tuning incurs significant training costs, while prompting increases inference overhead. Inspired by fast weight memory, we introduce GenerativeAdapter, an effective and efficient adaptation method that encode test-time context into language model parameters with a single forward pass.\nGenerativeAdapter augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters.\nNotably, our generator is general-purpose, i.e., one generator can adapt the corresponding base model for all langauge processing scenarios.\nWe apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models across  knowledge acquisition from documents, learning from demonstrations, and personalization for users.\nIn StreamingQA, our approach is effective in injecting knowledge into the LM's parameters, achieving a 63.5\\% improvement in F1 score over the model with supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K tokens.\nIn the MetaICL in-context learning evaluation, our method achieves an average accuracy of $44.9$ across 26 tasks, outperforming the base model. \nOn MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to \nprompting with full conversation history.\nOverall, GenerativeAdapter provides a viable solution for adapting large LMs to evolving information and providing tailored user experience, while reducing training and inference costs relative to traditional fine-tuning and prompting techniques.",
    "authors": [
      "~Tong_Chen3",
      "~Hao_Fang2",
      "~Patrick_Xia1",
      "~Xiaodong_Liu1",
      "~Benjamin_Van_Durme2",
      "~Luke_Zettlemoyer1",
      "~Jianfeng_Gao1",
      "~Hao_Cheng4"
    ],
    "pdf": "/pdf/2511dfdd2011b77782b62c70d9e3d9ffa074e512.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents an efficient adaptation method for LLMs that reduces both training and inference costs, uses parameter-efficient adapters, achieves computation and memory cost reduction (4x), handles long contexts (up to 32K tokens), and demonstrates significant performance improvements (63.5% F1 score improvement). The single forward pass adaptation could improve inference speed and reduce latency.",
      "Irrelevant Aspects": "The paper focuses more on application effectiveness rather than deep technical details about GPU utilization optimizations. It doesn't extensively discuss hardware-specific optimizations, low-level implementation details, throughput and latency measurements, distributed training/inference strategies, or specific GPU memory management techniques.",
      "Summary": "The paper introduces GenerativeAdapter, a method for efficiently adapting large language models to new contexts by encoding test-time context into model parameters through a single forward pass using a lightweight adapter generator. The approach demonstrates effectiveness in various adaptation scenarios while reducing computational costs compared to traditional fine-tuning and prompting techniques."
    }
  },
  {
    "id": "OhauMUNW8T",
    "title": "Wavelet-based Positional Representation for Long Context",
    "abstract": "In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. \nThis is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences.\nWe analyzed conventional position encoding methods for long contexts and found the following characteristics.\n(1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using Haar-like wavelets. \nHowever, because it uses only a fixed scale parameter, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). \nThis limitation could explain why RoPE performs poorly in extrapolation.\n(2)\nPrevious research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes.\nHowever, it has limitations in capturing deep dependencies because it restricts the receptive field of the model.\nFrom these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field.\nExperimental results show that this new method improves the performance of the model in both short and long contexts. \nIn particular, our method allows extrapolation of position information without limiting the model's attention field.",
    "authors": [
      "~Yui_Oka1",
      "~Taku_Hasegawa1",
      "~Kyosuke_Nishida2",
      "~Kuniko_Saito1"
    ],
    "pdf": "/pdf/d5ed2fc8f4eb31801279781b07970b112229814c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses long context handling, a critical challenge in LLM deployment that directly impacts inference optimization. It analyzes limitations of RoPE, widely used in models like Llama, which is valuable for my work on inference efficiency. The proposed multi-scale wavelet-based approach that doesn't restrict attention fields could improve GPU utilization by enabling more efficient processing of longer sequences. Better position embeddings that support extrapolation without attention limitation could increase throughput and reduce latency for applications requiring long context processing.",
      "Irrelevant Aspects": "The paper focuses primarily on the mathematical representation of position embeddings rather than implementation details that directly affect inference speed or memory efficiency. There's no discussion of specific hardware optimizations, quantization techniques, or kernel implementations that impact GPU utilization. The work appears more theoretical regarding positional representations rather than focusing on practical deployment optimizations and performance metrics.",
      "Summary": "This paper introduces a wavelet-based positional representation method to improve how LLMs handle sequences longer than their training data. The authors analyze limitations in RoPE (interpreting it as a restricted wavelet transform) and ALiBi (functioning like windowed attention). Their approach captures multiple scales without limiting the model's attention field, potentially improving both short and long context performance. While the work has indirect relevance to inference optimization through better long context handling, it focuses more on theoretical aspects of position representation rather than practical implementation optimizations that directly affect GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "CS2JWaziYr",
    "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
    "abstract": "Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.",
    "authors": [
      "~Ranajoy_Sadhukhan1",
      "~Jian_Chen13",
      "~Zhuoming_Chen1",
      "~Vashisth_Tiwari1",
      "~Ruihang_Lai1",
      "~Jinyuan_Shi1",
      "~Ian_En-Hsu_Yen1",
      "~Avner_May1",
      "~Tianqi_Chen1",
      "~Beidi_Chen1"
    ],
    "pdf": "/pdf/ba6206117da0d8213b79a6a7a6dcc3cb6ec01dc6.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on achieving better GPU utilization through speculative decoding for long contexts. It tackles the critical tradeoff between latency and throughput, presenting techniques to improve scalability with batch sizes from 32 to 256. The research addresses KV cache management, which is essential for GPU memory optimization, and demonstrates practical speedup results (2.51x for LLaMA-3.1-8B). The work specifically targets reducing latency while increasing throughput, which aligns perfectly with the research interests.",
      "Irrelevant Aspects": "The paper does not address training optimization, focusing exclusively on inference. It is also narrowly focused on speculative decoding as a technique rather than exploring broader optimization approaches for LLM systems. The work is specific to long contexts and may not provide insights for all LLM serving scenarios.",
      "Summary": "MagicDec presents a highly relevant approach to LLM inference optimization by challenging the conventional limits of speculative decoding. It demonstrates how this technique can be effectively applied to larger batch sizes and longer sequences, addressing key bottlenecks through sparse KV cache management and optimal drafting strategies. The paper offers practical solutions for improving both GPU utilization and scalability in LLM serving systems, with verified performance improvements across different hardware configurations."
    }
  },
  {
    "id": "gU4ZgQNsOC",
    "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
    "abstract": "Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. \nFurthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.",
    "authors": [
      "~Daouda_Sow1",
      "~Herbert_Woisetschläger1",
      "~Saikiran_Bulusu1",
      "~Shiqiang_Wang1",
      "~Hans_Arno_Jacobsen1",
      "~Yingbin_Liang1"
    ],
    "pdf": "/pdf/c555e59407ab06c1d2bbfabea197b2ebbea4c16c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on improving LLM pretraining efficiency and effectiveness through dynamic sample reweighting. It directly addresses training optimization for large language models, particularly for 7B and 1.4B parameter models. The approach aims to achieve faster convergence and improved performance, which would positively impact training time, resource utilization, and throughput. The reweighting strategy allows models to focus on more informative samples at different training stages, potentially reducing wasted computation on less valuable data.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization, scalability, throughput, or latency metrics which are central to systems optimization. The theoretical framework for convergence analysis is more mathematical than systems-focused. The validation on smaller models and linear regression problems is less relevant to large-scale LLM systems optimization. The work appears more algorithmic than focused on implementation and systems engineering aspects.",
      "Summary": "This paper is highly relevant to training optimization for LLMs, introducing dynamic sample reweighting techniques that improve training efficiency and convergence speed. While it doesn't directly address GPU utilization or systems-level optimizations, the approach has significant implications for reducing computational waste and improving training throughput for large language models. The focus on 7B and 1.4B parameter models makes it particularly relevant to modern LLM training challenges, though it lacks explicit systems engineering considerations."
    }
  },
  {
    "id": "eENHKMTOfW",
    "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs",
    "abstract": "The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources to effectively explore the experiment space. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. The code used for the experiments can be found here: https://github.com/instructlab/training.\n\nKey insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, allowing for early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observe no significant difference in performance between phased (sequentially training on data divided into phases) and stacked (training on the entire dataset at once) strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets as well as model families and sizes, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive research environment for LLM development.",
    "authors": [
      "~Aldo_Pareja1",
      "~Nikhil_Shivakumar_Nayak1",
      "~Hao_Wang22",
      "~Krishnateja_Killamsetty1",
      "~Shivchander_Sudalairaj1",
      "~Wenlong_Zhao1",
      "~Seungwook_Han1",
      "~Abhishek_Bhandwaldar1",
      "~Guangxuan_Xu1",
      "~Kai_Xu4",
      "~Ligong_Han1",
      "~Luke_Inglis1",
      "~Akash_Srivastava1"
    ],
    "pdf": "/pdf/6062bafab9f3bb337fdcbbc211b6fec41a70900a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper explores supervised fine-tuning of LLMs with a focus on optimization strategies. It investigates batch sizes and learning rates, which are critical hyperparameters for GPU utilization and training efficiency. The study examines early-stage training dynamics that could lead to computational savings, explores training strategies (phased vs stacked), and focuses on small LLMs (3B-7B parameters), which are more accessible for resource-constrained environments. These aspects align with training optimization goals for better GPU utilization and scalability.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization, throughput, or latency optimization. There's no direct discussion of inference optimization. The focus is primarily on model capabilities (benchmarked by MMLU, MTBench, Open LLM Leaderboard) rather than computational efficiency metrics. The study centers on supervised fine-tuning rather than pre-training or other optimization techniques.",
      "Summary": "This paper provides a comprehensive study on supervised fine-tuning of small LLMs (3B-7B parameters), exploring various training configurations and strategies. While not directly targeting GPU utilization or inference optimization, it offers insights into training efficiency, including findings about batch sizes, learning rates, early training dynamics, and training strategies that could indirectly improve resource utilization and reduce computational costs."
    }
  },
  {
    "id": "uAtDga3q0r",
    "title": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters",
    "abstract": "Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\\sim$44\\% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in  modern Transformer architectures.",
    "authors": [
      "~Roberto_Garcia1",
      "~Jerry_Weihong_Liu1",
      "~Daniel_Sorvisto1",
      "~Sabri_Eyuboglu1"
    ],
    "pdf": "/pdf/8b7c7c6efc3f832655b5c2d2a9c5a4305d89d61d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference efficiency in LLMs by introducing RaNA adapters that reduce computational costs while maintaining model quality. It focuses on low-rank matrix decompositions and adaptive masking techniques that can be applied to both MLPs and attention modules in Transformers, which are core components of the models I'm optimizing. The claimed 44% reduction in FLOPs while improving perplexity and accuracy metrics is directly relevant to my goal of achieving better performance with fewer resources.",
      "Irrelevant Aspects": "The abstract doesn't explicitly discuss GPU utilization, scalability, throughput, or latency metrics, which are key performance indicators in my research focus. There's no mention of system-level optimization strategies or integration with existing inference frameworks. The paper appears to focus primarily on model architecture modifications rather than the broader machine learning system optimizations I'm interested in.",
      "Summary": "This paper introduces RaNA adapters, a technique to improve inference efficiency in LLMs by using adaptive rank allocation with low-rank matrix decompositions and masking. It addresses limitations of previous neuron-adaptive methods and can be applied to both MLPs and attention modules. The paper claims significant improvements in model quality while reducing FLOPs by ~44%, making it highly relevant to my research in inference optimization for large language models, though it lacks explicit discussion of system-level metrics like GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "TrKRpaOk8y",
    "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
    "abstract": "Training and serving long-context large language models (LLMs) incurs substantial overhead. \nTo address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. \nThis paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. \nThis leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. \nLongGen builds on three key insights: \n(1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. \n(2) It is essential for the model to have direct access to all tokens. \nA hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance.\n(3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K.\n\nWe evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. \nDuring training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. \nDuring inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.\nCompared to baselines that apply KV-cache reduction techniques to full-attention long-context LLMs, LongGen achieves substantially stronger performance not only on the Needle-in-a-Haystack retrieval task, but also on more challenging long-context reasoning tasks, including BABILong and RULER.",
    "authors": [
      "~Suyu_Ge1",
      "~Xihui_Lin1",
      "~Yunan_Zhang1",
      "~Jiawei_Han1",
      "~Hao_Peng4"
    ],
    "pdf": "/pdf/7ceb04c0d77c64ee96e21953799a34b2a15a7ddc.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Training optimization with 1.55x speedup and 36% reduction in wall-clock time for long-context LLMs, inference optimization with 62% KV cache reduction, 1.67x prefilling speedup, and 1.41x decoding speedup, GPU-friendly memory access patterns, efficient sparse attention mechanisms, scalability demonstrated across different model sizes (7B and 70B), lightweight training approach requiring only 5B tokens for context extension from 4K to 128K, hybrid architecture balancing efficiency and performance",
      "Irrelevant Aspects": "Detailed benchmark performance on specific tasks like Needle-in-Haystack, BABILong, and RULER beyond their relationship to model efficiency, some specific implementation details of attention mechanisms not directly related to optimization",
      "Summary": "LongGen is a highly relevant paper that integrates length extension with GPU-friendly KV cache reduction architecture, achieving significant efficiency improvements in both training (1.55x speedup) and inference (62% memory reduction, 1.67x prefilling and 1.41x decoding speedup). The work addresses core challenges in long-context LLM systems through hybrid architectures with sparse attention patterns optimized for GPU utilization, demonstrating effectiveness across model scales. The lightweight training approach requiring only 5B tokens for context extension makes it particularly valuable for resource-constrained environments."
    }
  },
  {
    "id": "9KxnxWOBA5",
    "title": "Towards Optimal Multi-draft Speculative Decoding",
    "abstract": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.",
    "authors": [
      "~Zhengmian_Hu1",
      "~Tong_Zheng1",
      "~Vignesh_Viswanathan1",
      "~Ziyi_Chen2",
      "~Ryan_A._Rossi2",
      "~Yihan_Wu1",
      "~Dinesh_Manocha3",
      "~Heng_Huang1"
    ],
    "pdf": "/pdf/fb6889feef0566ea8585022ca4f062c8b0239c0a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on autoregressive sampling as an efficiency bottleneck. It explores Multi-Draft Speculative Decoding (MDSD) to improve token generation efficiency, potentially increasing throughput and reducing latency. The research optimizes acceptance rates in speculative decoding and compares different sampling methods to identify optimal approaches. These optimizations could enhance GPU utilization, though not explicitly mentioned.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU utilization or scalability, though these would likely be impacted. It focuses on inference optimization without addressing training optimization. The emphasis is on theoretical bounds and optimal transport problems rather than practical implementation details or specific hardware optimizations.",
      "Summary": "This paper is highly relevant to my research interest in LLM optimization, particularly for inference efficiency. It addresses the autoregressive sampling bottleneck in LLMs through Multi-Draft Speculative Decoding optimization. While it doesn't explicitly cover all aspects of my research interest (like GPU utilization or training optimization), its focus on improving inference efficiency through algorithmic improvements directly aligns with my goals of achieving higher throughput and lower latency in LLM systems."
    }
  },
  {
    "id": "FxNNiUgtfa",
    "title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",
    "abstract": "Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \\emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \\emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. \n\nMore broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity.",
    "authors": [
      "~Zeyuan_Allen-Zhu1",
      "~Yuanzhi_Li1"
    ],
    "pdf": "/pdf/7a9985d2cec78e1746e3dc81372bed15021471b0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Scaling laws for model sizing, quantization effects (int8), sparsity constraints including MoE architectures, model architecture impacts on knowledge storage, training duration considerations",
      "Irrelevant Aspects": "Focus on information-theoretic measurement of knowledge rather than practical performance metrics, emphasis on factual knowledge representation as tuples rather than system-level optimizations",
      "Summary": "The paper investigates knowledge storage capacity scaling in language models, finding they store 2 bits per parameter even when quantized to int8. While it provides insights into how model architecture, quantization, and MoE affect knowledge capacity, its theoretical focus on measuring knowledge in bits rather than practical optimization techniques limits its direct applicability to my research on LLM training/inference optimization and GPU utilization."
    }
  },
  {
    "id": "FAfxvdv1Dy",
    "title": "STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning",
    "abstract": "Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have limitations: ❶ Overlooking valuable samples at high pruning rates, which degrades the coreset’s performance.\n❷ Requiring high time overhead during coreset selection to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a speculative coreset selection method. STAFF leverages a small model from the same family as the target LLM to efficiently estimate data scores and then verifies the scores on the target LLM to accurately identify and allocate more selection budget to important regions while maintaining coverage of easy regions. We evaluate STAFF on three LLMs and three downstream tasks and show that STAFF improves the performance of SOTA methods by up to 54.3% and reduces selection overhead by up to 70.5% at different pruning rates. Furthermore, we observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can even obtain better fine-tuning performance than the full dataset.",
    "authors": [
      "~Xiaoyu_Zhang8",
      "~Juan_Zhai1",
      "~Shiqing_Ma2",
      "~Chao_Shen2",
      "~Tianlin_Li2",
      "~Weipeng_Jiang1",
      "~Yang_Liu36"
    ],
    "pdf": "/pdf/89a576ef7fa4c2412ad068d7cdb1ebb5359a5320.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational efficiency in LLM fine-tuning, directly impacting GPU utilization and throughput. It proposes STAFF, a method to reduce fine-tuning overhead by up to 70.5% while maintaining performance. The approach of using a smaller proxy model for data scoring has implications for resource efficiency during training. The demonstrated ability to achieve better performance with pruned datasets (20%) could significantly improve training resource utilization.",
      "Irrelevant Aspects": "The paper focuses exclusively on data selection for fine-tuning rather than inference optimization techniques. No discussion of latency improvements during model serving. Lacks coverage of distributed training optimizations, quantization techniques, or architectural optimizations for inference. No mention of serving at scale or multi-GPU optimization strategies.",
      "Summary": "STAFF presents a coreset selection method to improve data efficiency during LLM fine-tuning, using a smaller proxy model for initial scoring followed by verification on the target model. The approach reduces computational overhead by up to 70.5% while maintaining or improving fine-tuning performance. While relevant to training optimization and resource utilization, the paper doesn't address inference optimization, which is also a key part of my research focus."
    }
  },
  {
    "id": "yzloNYH3QN",
    "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers",
    "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more  directly leverage such signals, we propose in-context re-ranking (ICR), a novel method  that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. We assume that more relevant documents should receive more attention weights when an LLM is processing the query tokens, and leverage such signals for re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two ($O(1)$) forward passes to re-rank $N$ documents, making it substantially more efficient than generative re-ranking methods that require at least $O(N)$ forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR's performance is specially strong on tasks that require more complex re-ranking signals, such as handling contextualization and contradiction between the query and passages, as well as information integration across multiple passages. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation.",
    "authors": [
      "~Shijie_Chen1",
      "~Bernal_Jimenez_Gutierrez1",
      "~Yu_Su2"
    ],
    "pdf": "/pdf/7d2a7da36c21349c367d4303418c208f5d9b4dbb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper proposes in-context re-ranking (ICR), a method that significantly improves inference efficiency by requiring only O(1) forward passes to re-rank N documents instead of O(N) like generative methods. It achieves over 60% latency reduction compared to RankGPT, demonstrating substantial inference optimization. The method leverages attention patterns in LLMs, which relates to understanding and optimizing core components of transformer architectures. The constant computational overhead regardless of document count suggests good scalability properties. The technique can be applied to any open-weight LLM without specialized training, showing generalizability across different models.",
      "Irrelevant Aspects": "The paper is primarily focused on information retrieval and re-ranking tasks rather than general LLM optimization. It doesn't address training optimization techniques. There's limited discussion of GPU-specific optimizations or hardware-level utilization improvements. The method is specialized for re-ranking applications, which may not generalize to all LLM inference scenarios. The paper doesn't cover broader aspects of machine learning system optimization like distributed training or serving infrastructure.",
      "Summary": "This paper presents an efficient inference optimization technique for LLMs applied to re-ranking tasks. The proposed ICR method leverages attention patterns to reduce computational overhead from O(N) to O(1) forward passes, cutting latency by over 60%. While highly relevant for inference efficiency, scalability, and latency reduction, it's somewhat specialized for re-ranking applications and doesn't address training optimization or general GPU utilization techniques. The method's application to any open-weight LLM without specialized training makes it a valuable contribution to inference optimization, but its narrow focus on information retrieval limits its broader applicability to ML system optimization."
    }
  },
  {
    "id": "gLa96FlWwn",
    "title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining",
    "abstract": "Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual *attribution* and causal *influence*. With increasing model size and training tokens, we find that influence more closely aligns with factual attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names. We release our prompt set and model outputs, along with a web-based visualization tool to explore influential examples for factual predictions, commonsense reasoning, arithmetic, and open-ended generation for an 8B-parameter LLM.",
    "authors": [
      "~Tyler_A._Chang1",
      "~Dheeraj_Rajagopal1",
      "~Tolga_Bolukbasi1",
      "~Lucas_Dixon1",
      "~Ian_Tenney1"
    ],
    "pdf": "/pdf/4d9f7737fab8eb180c95de6cf877ac18a83b8ac9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational scalability for large LLMs (8B parameters) and datasets (160B tokens). Techniques like optimizer state correction and task-specific Hessian approximation likely involve innovations in computational efficiency. The ability to perform TDA without subsampling suggests methods that handle large-scale computations efficiently.",
      "Irrelevant Aspects": "While addressing computational efficiency, the paper's primary focus remains on data attribution rather than direct optimization of training/inference pipelines. It doesn't explicitly measure or focus on throughput or latency improvements.",
      "Summary": "This paper develops scalable methods for Training Data Attribution in LLMs. While its primary focus is on attribution rather than optimization, the techniques developed for handling large-scale computations efficiently (optimizer state correction, Hessian approximation) have significant relevance to computational challenges in LLM training and inference, GPU utilization, and scalability."
    }
  },
  {
    "id": "8sSqNntaMr",
    "title": "RouteLLM: Learning to Route LLMs from Preference Data",
    "abstract": "Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a training framework for learning efficient router models that dynamically select between a stronger and weaker LLM during inference. Our framework leverages human preference data and employs data augmentation techniques to enhance performance. Evaluations on public benchmarks show that our approach can reduce costs by over 2 times without sacrificing response quality. Moreover, our routers exhibit strong generalization capabilities, maintaining performance even when routing between LLMs not included in training. This highlights the potential of our framework to deliver cost-effective, high-performance LLM solutions.",
    "authors": [
      "~Isaac_Ong1",
      "~Amjad_Almahairi1",
      "~Vincent_Wu1",
      "~Wei-Lin_Chiang1",
      "~Tianhao_Wu1",
      "~Joseph_E._Gonzalez1",
      "~M_Waleed_Kadous1",
      "~Ion_Stoica1"
    ],
    "pdf": "/pdf/900c2f5b71fd9d4dc34d8f299cce9a11336d30b4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization by routing between different LLMs, which directly relates to improving efficiency, reducing latency, and increasing throughput. It addresses cost efficiency by reducing inference costs by over 2 times without sacrificing quality. The approach enhances scalability by intelligently selecting appropriate models for different requests. The training framework for routers leverages human preference data, which relates to training optimization techniques. The generalization capabilities to route between unseen models demonstrate practical deployment value.",
      "Irrelevant Aspects": "The paper doesn't appear to address GPU utilization optimizations directly. There's limited focus on low-level hardware optimization techniques or memory management strategies. The paper doesn't seem to cover distributed training or inference scaling strategies across multiple GPUs. Details about specific quantization or pruning techniques aren't mentioned in the abstract. There's no indication of discussion about serving infrastructure or system-level optimizations.",
      "Summary": "RouteLLM presents a training framework for router models that dynamically select between stronger and weaker LLMs during inference. Using human preference data and data augmentation, the approach reduces costs by over 2 times while maintaining response quality. The routers generalize well to models not seen during training. This work is highly relevant to LLM inference optimization, cost efficiency, and scalability concerns, though it doesn't deeply address GPU utilization or low-level hardware optimization details."
    }
  },
  {
    "id": "4D0f16Vwc3",
    "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
    "abstract": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. \nTo address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead.  We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE’s continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.",
    "authors": [
      "~Ziteng_Wang3",
      "~Jun_Zhu2",
      "~Jianfei_Chen1"
    ],
    "pdf": "/pdf/f992c15598958e7e64fdac82d3d5285fe23b68d5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on Mixture-of-Experts (MoE) models which scale model capacity without increasing computation budget - directly relevant to efficient large language model training. It introduces a fully differentiable routing mechanism (ReLU routing) to replace TopK routing, addressing load balancing and efficient utilization of experts. The work demonstrates dynamic computation allocation across tokens and layers and shows superior scalability with increasing expert counts. The implementation is based on Megatron-LM, indicating practical relevance for large-scale training.",
      "Irrelevant Aspects": "The paper doesn't specifically address GPU utilization metrics, hardware-specific optimizations, or memory efficiency techniques beyond routing. There's no explicit focus on inference optimization techniques like quantization or pruning. While the approach may impact throughput and latency, these aren't the primary evaluation metrics or research questions in the paper.",
      "Summary": "ReMoE introduces a fully differentiable Mixture-of-Experts architecture using ReLU routing as a replacement for the standard TopK+Softmax approach. By making the routing mechanism differentiable, the method enables more continuous and effective expert selection while maintaining sparsity and load balancing. The paper demonstrates improved performance across various model configurations and better scalability with increasing expert counts, which could indirectly lead to better resource utilization in large language model training. However, the focus remains on the routing algorithm itself rather than comprehensive system-level optimizations for GPU utilization, throughput, or latency."
    }
  },
  {
    "id": "ISqx8giekS",
    "title": "LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid",
    "abstract": "Large language models (LLMs) have shown immense potential across various domains, but their high memory requirements and inference costs remain critical challenges for deployment. Post-training quantization (PTQ) has emerged as a promising technique to reduce memory requirements and decoding latency. However, recent accurate quantization methods often depend on specialized computations or custom data formats to achieve better model quality, which limits their compatibility with popular frameworks, as they require dedicated inference kernels tailored to specific hardware and software platforms, hindering wider adoption. Furthermore, many competitive methods have high resource requirements and computational overhead for quantizing models, making it challenging to scale them to hundreds of billions of parameters. In response to these challenges, we propose LeanQuant (Loss-error-aware network Quantization), a novel quantization method that is accurate, versatile, and scalable. In the existing popular iterative loss-error-based quantization framework, we identify a critical limitation in prior methods: the min-max affine quantization grid fails to preserve model quality due to outliers in inverse Hessian diagonals. To overcome this fundamental issue, we propose learning loss-error-aware grids, instead of using non-adaptive min-max affine grids. Our approach not only produces quantized models that are more accurate but also generalizes to a wider range of quantization types, including affine and non-uniform quantization, enhancing compatibility with more frameworks. Extensive experiments with recent LLMs demonstrate that LeanQuant is highly accurate, comparing favorably against competitive baselines in model quality, and scalable, achieving very accurate quantization of Llama-3.1 405B, one of the largest open-source LLMs to date, using two Quadro RTX 8000-48GB GPUs in 21 hours. Our code is available at https://github.com/LeanModels/LeanQuant.",
    "authors": [
      "~Tianyi_Zhang6",
      "~Anshumali_Shrivastava1"
    ],
    "pdf": "/pdf/0541474768ab68f9e69d70f30cf038ed31c98d54.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on post-training quantization (PTQ) for large language models, which is directly relevant to inference optimization. It addresses memory reduction and decoding latency improvement, aligning with the goal of better GPU utilization and lower latency. The method is demonstrated to scale to extremely large models like Llama-3.1 405B, showing good scalability. The approach emphasizes compatibility with popular frameworks, which is important for practical deployment of optimized models. The paper also mentions efficiency in the quantization process itself.",
      "Irrelevant Aspects": "The paper doesn't address training optimization, which is part of the research interest. While it reduces latency which would impact throughput, the paper doesn't explicitly measure or discuss throughput improvements. The focus is specifically on quantization techniques rather than broader optimization strategies for LLMs.",
      "Summary": "LeanQuant presents a novel quantization method for large language models that focuses on post-training quantization to reduce memory requirements and inference latency. The method addresses limitations in existing approaches by using loss-error-aware grids instead of min-max affine quantization grids. It demonstrates strong results on extremely large models (up to 405B parameters) and emphasizes compatibility with existing frameworks. The paper is highly relevant to research interests in inference optimization, GPU utilization, scalability, and latency reduction, though it doesn't cover training optimization aspects."
    }
  },
  {
    "id": "tkiZQlL04w",
    "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
    "abstract": "The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future queries. In this paper, we propose a novel compression technique for KV cache that preserves all token information. Our investigation reveals that: i) Most attention heads primarily focus on the local context; ii) Only a few heads, denoted as retrieval heads, can essentially pay attention to all input tokens. These key observations motivate us to use separate caching strategy for attention heads.Therefore, we propose RazorAttention, a training-free KV cache compression algorithm, which maintains a full cache for these crucial retrieval heads and discards the remote tokens in non-retrieval heads. Furthermore, we introduce a novel mechanism involving a “compensation token” to further recover the information in the dropped tokens. Extensive evaluations across a diverse set of large language models (LLMs) demonstrate that RazorAttention achieves a reduction in KV cache size by over 70% without noticeable impacts on performance. Additionally, RazorAttention is compatible with FlashAttention, rendering it an efficient and plug-and-play solution that enhances LLM inference efficiency without overhead or retraining of the original model.",
    "authors": [
      "~Hanlin_Tang2",
      "~Yang_Lin12",
      "~Jing_Lin6",
      "~Qingsen_Han1",
      "~Danning_Ke1",
      "~Shikuan_Hong1",
      "~Yiwu_Yao1",
      "~Gongyi_Wang1"
    ],
    "pdf": "/pdf/808d99b57ee82af73b93fe34b2eeaf04c4171bfd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Addresses critical KV cache memory/computational demands in LLMs, novel compression technique preserving all token information, training-free approach reduces KV cache size by over 70%, maintains performance while improving GPU utilization, compatible with FlashAttention, focuses on inference optimization, enhances throughput and reduces latency",
      "Irrelevant Aspects": "Limited focus on distributed systems implementation, doesn't address training optimization specifically, minimal discussion of hardware-specific optimizations beyond FlashAttention compatibility",
      "Summary": "RazorAttention introduces a breakthrough approach to LLM inference optimization by identifying 'retrieval heads' that attend to all tokens versus heads focusing on local context. By applying differentiated caching strategies and introducing 'compensation tokens' for dropped information, it achieves significant KV cache reduction (70%+) without performance loss. The training-free method integrates seamlessly with FlashAttention, offering a practical solution for improving GPU utilization and throughput in long-context LLMs."
    }
  },
  {
    "id": "u3TL0qxLWf",
    "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators",
    "abstract": "Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.",
    "authors": [
      "~Rasoul_Shafipour3",
      "~David_Harrison1",
      "~Maxwell_Horton1",
      "~JEFFREY_MARKER1",
      "~Houman_Bedayat1",
      "~Sachin_Mehta1",
      "~Mohammad_Rastegari2",
      "~Mahyar_Najibi1",
      "~Saman_Naderiparizi1"
    ],
    "pdf": "/pdf/41191f7e092e8e93212cbbe0a945ae7a663bd5bd.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Post-training compression method for LLMs that reduces memory access and trades compute for fewer memory accesses, addressing memory-bound inference tasks. Shows speed-up potential (up to 4x) and maintains performance at 3-4 bit precision on large models like Llama3 70B. Data-free approach simplifies deployment optimization.",
      "Irrelevant Aspects": "Focuses on post-training compression rather than training optimization. Implementation tested primarily on FPGA rather than GPUs, with limited details on GPU utilization. Uses Linear Feedback Shift Registers (LFSR) which is a more specialized technique.",
      "Summary": "SeedLM introduces a novel compression approach for LLM weights that uses pseudo-random generator seeds instead of traditional quantization. The method reduces memory bandwidth requirements during inference by generating weight matrices on-the-fly, showing promising speed-ups on compressed models. While the compression technique and memory-compute trade-offs are relevant to LLM optimization, the paper focuses more on FPGA implementation and post-training compression rather than GPU-specific optimization or training efficiency."
    }
  },
  {
    "id": "1qq1QJKM5q",
    "title": "More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing",
    "abstract": "The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability.\nIn this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning per update step and improved out-of-sample generalization. \nWe demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures.",
    "authors": [
      "~Sagi_Shaier1",
      "~Francisco_Pereira1",
      "~Katharina_von_der_Wense1",
      "~Lawrence_Hunter1",
      "~Matt_Jones1"
    ],
    "pdf": "/pdf/1930fa1914b975fe3a5863354004db86a057c964.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses sparse neural networks and Mixture of Experts models, which are crucial for scaling large language models efficiently. It introduces COMET, a method that replaces trainable gating functions with fixed routing, potentially reducing computational overhead during inference. The approach claims to reduce redundant computation, which could improve throughput. The paper specifically mentions applications to language modeling, directly relevant to LLMs. The concept of conditionally overlapping experts based on input similarity could enhance learning efficiency and generalization, impacting training optimization.",
      "Irrelevant Aspects": "The paper covers image classification and regression tasks beyond language modeling. The biologically-inspired aspects may be more theoretical than practical for implementation concerns. There's no explicit mention of GPU utilization, throughput, or latency metrics in the abstract. The paper doesn't provide specific hardware optimizations or implementation details relevant to LLM deployment.",
      "Summary": "COMET proposes a novel sparse neural architecture with a fixed routing mechanism that creates overlapping experts based on input similarity. While this has potential implications for LLM training and inference efficiency through reduced redundant computation, the paper's broader focus across multiple domains and lack of specific GPU utilization metrics makes it moderately relevant to my research interests in LLM optimization."
    }
  },
  {
    "id": "WOt1owGfuN",
    "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing",
    "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of latency reduction compared to the state-of-the-art method at a 40\\% pruning ratio.",
    "authors": [
      "~Qi_Le1",
      "~Enmao_Diao1",
      "~Ziyan_Wang9",
      "~Xinran_Wang3",
      "~Jie_Ding2",
      "~Li_Yang6",
      "~Ali_Anwar1"
    ],
    "pdf": "/pdf/35b88087714febf8164e8cf410e29d60aa725061.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces Probe Pruning (PP), a framework for dynamic, structured pruning of LLMs, which directly addresses inference optimization. It aims to enhance efficiency, reduce latency, and improve GPU utilization. The method is compatible with existing models without requiring fine-tuning and is evaluated on popular LLM models like LLaMA-2/3 and OPT.",
      "Irrelevant Aspects": "The paper focuses solely on pruning as the optimization technique without discussing other approaches like quantization, distillation, or more efficient attention mechanisms. It doesn't address training optimization, system-level optimizations, distributed strategies, or memory optimization techniques.",
      "Summary": "Probe Pruning (PP) is a novel framework for online, dynamic, structured pruning of LLMs that operates in a batch-wise manner. It comprises three stages: probing, history-informed pruning, and full inference. PP selects crucial hidden states to run a few model layers ahead during probing, then integrates these states with historical states for strategic pruning. The method is compatible with existing models without requiring additional modules or fine-tuning. Evaluations on LLaMA-2/3 and OPT models show PP can enhance efficiency with minimal probing (1.5% of FLOPs), achieving a better ratio of performance degradation per unit of latency reduction compared to state-of-the-art methods."
    }
  },
  {
    "id": "pHOH8FVrTp",
    "title": "No Need to Talk: Asynchronous Mixture of Language Models",
    "abstract": "We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each\nmodel of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Unlike prior works on asynchronous LLM training, our routing method does not rely on full corpus clustering or access to metadata, making it more suitable for real-world applications.  Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks.",
    "authors": [
      "~Anastasiia_Filippova2",
      "~Angelos_Katharopoulos1",
      "~David_Grangier1",
      "~Ronan_Collobert1"
    ],
    "pdf": "/pdf/0d33d824bdcbb4c965da08be237acf32807802b4.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper introduces an asynchronous training approach for mixture of language models, addressing training optimization and improving GPU utilization by reducing synchronization overhead. The inference scheme with a lightweight router using only a fraction of parameters is highly relevant to inference optimization, potentially improving both throughput and latency. The method's asynchronous nature suggests improved scalability without requiring high-bandwidth communication between nodes.",
      "Irrelevant Aspects": "The specific comparisons to other asynchronous LLM training methods and the downstream evaluations on various tasks, while demonstrating effectiveness, are less directly relevant to the core system-level optimizations that are the primary focus of my expertise.",
      "Summary": "SMALLTALK LM presents a highly relevant approach to my research interests by introducing an asynchronous mixture of language models that optimizes both training and inference. The method addresses key challenges in GPU utilization, scalability, throughput, and latency through its asynchronous training and efficient inference routing, making it a valuable contribution to the field of large language model system optimization."
    }
  },
  {
    "id": "BI2int5SAC",
    "title": "Human-inspired Episodic Memory for Infinite Context LLMs",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient, human-inspired access to relevant information. Experiments on the LongBench and $\\infty$-Bench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 10 million tokens -- a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting parallels between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.",
    "authors": [
      "~Zafeirios_Fountas1",
      "~Martin_Benfeghoul1",
      "~Adnan_Oomerjee1",
      "~Fenia_Christopoulou1",
      "~Gerasimos_Lampouras2",
      "~Haitham_Bou_Ammar1",
      "~Jun_Wang2"
    ],
    "pdf": "/pdf/f599d6d89bc02040a5d13df95247a12909c463cf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational challenges with LLM context processing, proposes efficient retrieval mechanisms, enables processing of practically infinite context lengths efficiently, works online with resource efficiency comparable to RAG, and successfully handles retrieval across 10 million tokens - a scale computationally infeasible for standard models",
      "Irrelevant Aspects": "The focus on human-inspired mechanisms and parallels with biological memory, correlation with human-perceived events, lack of specific discussion on GPU utilization, and no focus on training optimization since the approach works with no fine-tuning",
      "Summary": "This paper introduces EM-LLM, which integrates human episodic memory concepts into LLMs to efficiently handle extensive contexts. It uses online event segmentation and efficient retrieval to maintain computational efficiency while processing practically infinite context lengths. The approach demonstrates superior performance to state-of-the-art retrieval models and even outperforms full-context models in most tasks, while requiring similar resources to RAG. The paper is highly relevant to computational efficiency and scalability aspects of LLM systems but focuses less on GPU utilization and training optimization specifically."
    }
  },
  {
    "id": "din0lGfZFd",
    "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
    "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.",
    "authors": [
      "~Nikunj_Saunshi1",
      "~Nishanth_Dikkala1",
      "~Zhiyuan_Li2",
      "~Sanjiv_Kumar1",
      "~Sashank_J._Reddi1"
    ],
    "pdf": "/pdf/3b6624b631e0aa8b60e9db8890eda637165a01fa.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper's approach of using looped transformers (smaller models applied multiple times) directly addresses inference optimization by potentially reducing memory requirements while maintaining performance. This has significant implications for GPU utilization and latency reduction. The connection between looped models and chain-of-thought reasoning provides insights into optimizing inference computation. The claim that k-layer models looped L times can match kL-layer models suggests potential improvements in scalability and throughput through architectural innovations rather than simply scaling up parameter count.",
      "Irrelevant Aspects": "The paper appears to focus more on the reasoning capabilities of looped models rather than detailed system-level performance optimizations. There's limited discussion of hardware-specific optimizations or detailed metrics on throughput and latency improvements. The theoretical proofs, while valuable, may not directly translate to practical system optimizations. The paper doesn't seem to address training efficiency optimizations, focusing primarily on the model architecture for inference.",
      "Summary": "This paper introduces looped transformers as an efficient alternative to deep models for reasoning tasks. It demonstrates that a smaller transformer looped multiple times can match the performance of a much deeper non-looped model, with potential benefits for GPU utilization, memory efficiency, and inference latency. The work connects looped models to chain-of-thought reasoning and suggests that many reasoning tasks can be solved effectively through iterative algorithms implemented as model loops. While the paper focuses more on reasoning capabilities than system-level optimizations, its architectural approach has significant implications for making large language models more efficient during inference."
    }
  },
  {
    "id": "9HK2rHNAhd",
    "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget",
    "abstract": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However, most of these methods treat all layers equally, allocating the same KV budget to each layer. This approach is suboptimal, as some layers may be less sensitive to input tokens yet still receive the same budget as others. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based on our observations regarding layer-wise importance in inference, we propose \\sys to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative sequence-wise algorithms to compress the KV-cache for each layer with its very own budget. Specifically, we first measure each layer's importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers. Based on this similarity, we then categorize the layers into two groups and adjust their KV budgets accordingly. By optimizing the KV-cache from both sequence's and layer's dimensions, \\sys achieves around 30\\% to 70\\% of the memory reductions and up to 2.2 $\\times$ of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.",
    "authors": [
      "~Zihao_Wang29",
      "~Bin_CUI2",
      "~Shaoduo_Gan1"
    ],
    "pdf": "/pdf/02bf1c8ee9c5ceb1992664429cc9a984bad33e8f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses KV-cache optimization during LLM inference, which is critical for improving GPU utilization and memory efficiency. It proposes a 2D optimization approach (sequence-wise and layer-wise) that achieves significant memory reductions (30-70%) and throughput improvements (up to 2.2×). The method works across various LLMs, demonstrating scalability. The layer-wise importance measurement and budget allocation is a novel approach for inference optimization.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization aspects. It doesn't explicitly discuss latency improvements, only throughput. There's no mention of distributed computing aspects of scalability. The evaluation doesn't seem to include detailed GPU utilization metrics.",
      "Summary": "SqueezeAttention introduces a novel 2D approach to KV-cache management in LLM inference that optimizes both sequence-wise and layer-wise aspects. By measuring layer importance through cosine similarity of input prompts before and after self-attention layers, the method dynamically allocates KV budgets to different layers. This technique achieves significant memory reductions and throughput improvements across various LLMs, making it highly relevant for inference optimization research focused on efficiency and resource utilization."
    }
  },
  {
    "id": "tyEyYT267x",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
    "authors": [
      "~Marianne_Arriola1",
      "~Aaron_Gokaslan1",
      "~Justin_T_Chiu1",
      "~Zhihan_Yang1",
      "~Zhixuan_Qi1",
      "~Jiaqi_Han2",
      "~Subham_Sekhar_Sahoo1",
      "~Volodymyr_Kuleshov1"
    ],
    "pdf": "/pdf/4e4363bc2a1f91eed00109d9aee07bf2c209afb1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference efficiency through KV caching and parallel token sampling, which directly relates to GPU utilization and throughput optimization. It proposes an efficient training algorithm and focuses on performance improvements through variance minimization. The block diffusion approach supports parallelized generation, which is valuable for GPU utilization. The method handles arbitrary-length sequences, suggesting scalability improvements.",
      "Irrelevant Aspects": "The paper appears to focus more on model architecture and algorithmic improvements rather than specific GPU utilization techniques or hardware-level optimizations. There's limited discussion of throughput metrics or latency quantification. The approach doesn't seem to address memory optimization techniques or specific hardware accelerators that would be central to inference optimization research.",
      "Summary": "Block Diffusion introduces a hybrid approach between autoregressive and diffusion language models that enables efficient parallel generation with KV caching. While relevant to inference optimization through its parallelization aspects and efficient training algorithms, it focuses more on model architecture than hardware-specific optimizations. The paper contributes to performance improvements but lacks deep technical details about GPU utilization techniques or throughput/latency measurements that would be most central to my research interests."
    }
  },
  {
    "id": "yUC8pU508S",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "abstract": "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (**APE**), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98\\% and 93\\% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6\\% and 7.9\\%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$ speedup by reducing 28$\\times$ prefilling time for a 128K-length context. The code is available at \nhttps://github.com/Infini-AI-Lab/APE.",
    "authors": [
      "~Xinyu_Yang4",
      "~Tianqi_Chen1",
      "~Beidi_Chen1"
    ],
    "pdf": "/pdf/94df95d5f53caa62fc995bccd990653808aff8bd.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "Focuses on inference optimization for context-augmented generation",
        "Proposes parallel encoding with caching of KV states to reduce computational overhead",
        "Achieves 4.5x end-to-end speedup and reduces prefilling time by 28x for 128K contexts",
        "Addresses GPU utilization concerns by efficiently handling long contexts",
        "Improves scalability by encoding hundreds of contexts in parallel",
        "Maintains model performance (93-98% of sequential encoding) while optimizing efficiency"
      ],
      "Irrelevant Aspects": [
        "Does not address training optimization techniques",
        "Limited focus on general LLM optimization beyond context-augmented scenarios",
        "Does not explicitly discuss hardware-specific optimizations"
      ],
      "Summary": "APE introduces Adaptive Parallel Encoding to optimize inference for context-augmented generation by pre-computing and caching KV states of individual contexts. The method addresses attention misalignments through shared prefix, attention temperature, and scaling factors. It achieves significant speedup (4.5x end-to-end, 28x reduction in prefilling time) while maintaining 93-98% of sequential encoding performance. The approach scales to hundreds of contexts and 128K-length inputs, making it highly relevant for inference optimization, throughput improvement, and scalability in large language models."
    }
  },
  {
    "id": "wg1PCg3CUP",
    "title": "Scaling Laws for Precision",
    "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
    "authors": [
      "~Tanishq_Kumar1",
      "~Zachary_Ankner1",
      "~Benjamin_Frederick_Spector1",
      "~Blake_Bordelon1",
      "~Niklas_Muennighoff1",
      "~Mansheej_Paul1",
      "~Cengiz_Pehlevan2",
      "~Christopher_Re1",
      "~Aditi_Raghunathan1"
    ],
    "pdf": "/pdf/08a4f2e3575ef19266dc1f22c3d3fefefd8ae539.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on precision optimization in both training and inference of language models, which is directly related to optimization goals. It discusses how low precision affects model quality and cost, aligning with goals of better GPU utilization and lower latency. The paper develops precision-aware scaling laws to predict loss from low precision training and quantization. It suggests training larger models in lower precision can be compute optimal, relevant to scalability. The work covers both training and inference optimization, directly matching research interests.",
      "Irrelevant Aspects": "The paper doesn't specifically focus on GPU utilization techniques or throughput optimization methods. The model sizes discussed (up to 1.7B parameters) are smaller than today's largest language models. The paper doesn't seem to address distributed training or inference optimization.",
      "Summary": "The paper 'Scaling Laws for Precision' presents precision-aware scaling laws for both training and inference of language models. It introduces the concept that low precision training reduces the model's 'effective parameter count' and can predict additional loss from training in low precision and post-training quantization. The work finds that degradation from post-training quantization increases as models are trained on more data. The authors suggest that training larger models in lower precision can be compute optimal. Their unified scaling laws predict degradation from both training and inference in varied precisions, validated on models up to 1.7B parameters trained on up to 26B tokens."
    }
  },
  {
    "id": "MCHuGOkExF",
    "title": "SFS: Smarter Code Space Search improves LLM Inference Scaling",
    "abstract": "We frame code generation as a black-box optimization problem within the code\nspace and demonstrate how optimization-inspired techniques can enhance inference\nscaling over text. Based on this perspective, we propose **SCATTERED FOREST\nSEARCH (SFS)**, a novel approach that improves solution diversity during evolutionary search,\nthereby avoiding local optima. Our theoretical analysis illustrates how these\nmethods improve exploration and enhance efficiency. Extensive experiments\non *HumanEval, MBPP, APPS, CodeContests,* and *Leetcode* reveal significant\nperformance gains. For instance, our method achieves a **pass@1 rate of 67.1% on\nHumanEval+** and **87.2% on HumanEval with GPT-3.5**, marking improvements of\n**8.6%** and **4.3%** over the state-of-the-art, while also halving the iterations needed\nto find the correct solution. Furthermore, our approach scales more efficiently\nthan existing search techniques, including **tree search, line search,** and **repeated\nsampling (Best of N)**.",
    "authors": [
      "~Jonathan_Light1",
      "~Yue_Wu12",
      "~Yiyou_Sun1",
      "~Wenchao_Yu1",
      "~Yanchi_Liu1",
      "~Xujiang_Zhao1",
      "~Ziniu_Hu1",
      "~Haifeng_Chen1",
      "~Wei_Cheng1"
    ],
    "pdf": "/pdf/8ddc54850447e434606c96f01c2e3dbfc84939af.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Direct focus on LLM inference optimization, novel approach for improving inference scaling, claims of improved efficiency over existing techniques, addresses the challenge of avoiding local optima in search, demonstrates performance improvements with reduced computational requirements",
      "Irrelevant Aspects": "Limited to code generation tasks, not general LLM inference, uses evolutionary search methods which may have limited applicability to other optimization domains, doesn't explicitly address GPU utilization, throughput, or latency metrics, may be more algorithmic than system-level optimization",
      "Summary": "This paper presents SCATTERED FOREST SEARCH (SFS), a novel approach for optimizing LLM inference specifically for code generation tasks. It frames code generation as a black-box optimization problem and uses optimization-inspired techniques to enhance inference scaling. The method improves solution diversity during evolutionary search to avoid local optima. The paper demonstrates significant performance improvements on various code benchmarks, with better scaling efficiency compared to existing techniques like tree search and repeated sampling. While highly relevant to LLM inference optimization and scaling, its focus is specifically on code generation rather than general LLM optimization, and it doesn't explicitly address GPU utilization, throughput, or latency metrics."
    }
  },
  {
    "id": "SFN6Wm7YBI",
    "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM pretraining",
    "abstract": "The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens requires sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes requires non-trivial engineering effort.\n\nThis paper introduces **TORCHTITAN**$^1$, a PyTorch-native distributed training system that unifies and advances state-of-the-art techniques, streamlining integration and reducing engineering overhead. TORCHTITAN enables seamless application of 4D parallelism in a modular and composable manner, while featuring elastic scaling to adapt to changing computational requirements. The system provides comprehensive logging, efficient checkpointing, and debugging tools, ensuring production-ready training. Moreover, TORCHTITAN incorporates innovative hardware-software co-designed solutions, leveraging cutting-edge features like Float8 training and SymmetricMemory to maximize hardware utilization.\n\nAs a flexible experimental test bed, TORCHTITAN facilitates the curation and comparison of custom recipes for diverse training contexts. By leveraging TORCHTITAN, we developed optimized training recipes for the Llama 3.1 family and provide actionable guidance on selecting and combining distributed training techniques to maximize training efficiency, based on our hands-on experiences.\n\nWe thoroughly assess TORCHTITAN on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations ranging from 65.08% on Llama 3.1 8B at 128 GPU scale (1D), 12.59% on Llama 3.1 70B at 256 GPU scale (2D), to 30% on Llama 3.1 405B at 512 GPU scale (3D) on NVIDIA H100 GPUs over optimized baselines. We also demonstrate the effectiveness of 4D parallelism in enabling long context training.\n\n$^1$ GitHub: [https://github.com/pytorch/torchtitan](https://github.com/pytorch/torchtitan)",
    "authors": [
      "~Wanchao_Liang1",
      "~Tianyu_Liu14",
      "~Less_Wright2",
      "~Will_Constable1",
      "~Andrew_Gu1",
      "~Chien-Chin_Huang1",
      "~Iris_Zhang2",
      "~Wei_Feng16",
      "~Howard_Huang2",
      "~Junjie_Wang17",
      "~Sanket_Purandare1",
      "~Gokul_Nadathur1",
      "~Stratos_Idreos1"
    ],
    "pdf": "/pdf/0fe074ee9c7c300a539cb6e104951c5b426f5325.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "PyTorch native solution for LLM pretraining optimization",
        "Sophisticated distributed systems for scaling across thousands of accelerators",
        "4D parallelism in a modular and composable manner",
        "Elastic scaling for changing computational requirements",
        "Comprehensive logging, efficient checkpointing, and debugging tools",
        "Hardware-software co-designed solutions including Float8 training and SymmetricMemory",
        "Optimized training recipes for Llama 3.1 family",
        "Actionable guidance on selecting and combining distributed training techniques",
        "Demonstrated accelerations across different model sizes and GPU scales",
        "Open-source implementation with GitHub repository"
      ],
      "Irrelevant Aspects": [
        "Limited focus on inference optimization",
        "No specific mention of latency improvements",
        "Limited discussion of numerical precision trade-offs beyond Float8"
      ],
      "Summary": "TorchTitan is a PyTorch-native distributed training system focused on LLM pretraining optimization that enables 4D parallelism, elastic scaling, and incorporates hardware-software co-designs like Float8 training. It provides production-ready tools and demonstrates significant performance improvements across various model scales, making it highly relevant to research interests in training optimization, GPU utilization, and scalability, though with limited focus on inference optimization."
    }
  },
  {
    "id": "nrvoWOWcyg",
    "title": "Chunk-Distilled Language Modeling",
    "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream applications. Code and data will be made publicly available.",
    "authors": [
      "~Yanhong_Li1",
      "~Karen_Livescu1",
      "~Jiawei_Zhou1"
    ],
    "pdf": "/pdf/7a80d7a2a272e395279ee6f037bd5484e0494728.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Multi-token generation in single decoding step, addressing inefficiency in token-level generation, potential for improved inference speed and GPU utilization, adaptation without expensive retraining, reduced latency in text generation",
      "Irrelevant Aspects": "More focused on retrieval and knowledge incorporation than hardware optimization, no explicit mention of GPU memory optimization or distributed training, limited focus on computational architecture",
      "Summary": "This paper introduces Chunk-Distilled Language Modeling (CD-LM) which addresses the sequential generation bottleneck in LLMs by producing multiple tokens at once. This approach directly targets inference efficiency, which is central to my research interests. While the paper doesn't explicitly focus on all aspects of GPU optimization, the chunk-based generation method has clear implications for improving utilization, throughput, and reducing latency in LLM inference systems. The ability to adapt without retraining also offers scalability advantages for different domains."
    }
  },
  {
    "id": "LWMS4pk2vK",
    "title": "Palu: KV-Cache Compression with Low-Rank Projection",
    "abstract": "Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tenors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) an optimized GPU kernel with matrix fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89× speedup on the RoPE-based attention module. When combined with quantization, Palu’s\ninherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91× speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu’s superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu.",
    "authors": [
      "~Chi-Chih_Chang1",
      "~Wei-Cheng_Lin2",
      "~Chien-Yu_Lin1",
      "~Chong-Yan_Chen2",
      "~Yu-Fang_Hu1",
      "~Pei-Shuo_Wang1",
      "~Ning-Chi_Huang1",
      "~Luis_Ceze1",
      "~Mohamed_S._Abdelfattah1",
      "~Kai-Chiang_Wu1"
    ],
    "pdf": "/pdf/bcb7e2da88d0937fb5299d480da43bbc390f57bc.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focus on KV-Cache compression for LLM inference optimization; utilizes low-rank projection to reduce memory usage; implements optimized GPU kernels with matrix fusion for better performance; delivers significant speedup (up to 2.91×) while maintaining accuracy; addresses critical bottlenecks in LLM inference; compatible with quantization methods for additional optimization",
      "Irrelevant Aspects": "Limited discussion on training optimization aspects; specific focus on RoPE-based attention which may not cover all attention mechanisms",
      "Summary": "Palu presents a hidden dimension compression approach using low-rank projection to address memory challenges in LLM inference. The method decomposes linear layers into low-rank matrices and incorporates several optimizations including medium-grained decomposition, efficient rank search, and quantization compatibility. With demonstrated 50% KV-Cache compression and up to 2.91× speedup, this work directly contributes to improving GPU utilization and reducing inference latency for large language models."
    }
  },
  {
    "id": "bIlnpVM4bc",
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling",
    "abstract": "Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this\nwork, we present Samba, a simple hybrid architecture that layer-wise combines\nMamba, a selective State Space Model (SSM), with Sliding Window Attention\n(SWA). Samba selectively compresses a given sequence into recurrent hidden\nstates while still maintaining the ability to precisely recall recent memories with the\nattention mechanism. We scale Samba up to 3.8B parameters with 3.2T training\ntokens and demonstrate that it significantly outperforms state-of-the-art models\nacross a variety of benchmarks. Pretrained on sequences of 4K length, Samba\nshows improved perplexity in context lengths of up to 1M in zero-shot. When\nfinetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits\nsuperior retrieval extrapolation on the challenging Phonebook task compared to\nfull-attention models. As a linear-time sequence model, Samba achieves a 3.73×\nhigher throughput compared to Transformers with grouped-query attention for user\nprompts of 128K length, and a 3.64× speedup when generating 64K tokens with\nunlimited streaming.",
    "authors": [
      "~Liliang_Ren1",
      "~Yang_Liu50",
      "~Yadong_Lu1",
      "~yelong_shen1",
      "~Chen_Liang3",
      "~Weizhu_Chen1"
    ],
    "pdf": "/pdf/afcc61b2a3af1b85ec0707ec1a8d10ef4f128970.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Linear-time sequence modeling, hybrid architecture combining selective SSM with attention, 3.73× throughput improvement for 128K contexts, 3.64× speedup for 64K token generation, efficient handling of unlimited context length, quadratic complexity solution, scaling to 3.8B parameters, perfect memory recall on long contexts, streaming generation capability",
      "Irrelevant Aspects": "Limited focus on specific GPU utilization techniques, minimal details on distributed training implementation, lacks specific memory optimization strategies, focuses more on model capabilities than infrastructure optimization, limited discussion of hardware-specific optimizations",
      "Summary": "Samba introduces a hybrid architecture combining Mamba (selective State Space Model) with Sliding Window Attention to achieve linear-time complexity sequence modeling. The approach significantly improves throughput (3.73× for 128K prompts and 3.64× for 64K token generation) compared to standard Transformers while enabling efficient modeling of extremely long contexts (up to 1M tokens). When scaled to 3.8B parameters with 3.2T training tokens, Samba outperforms existing models and demonstrates superior extrapolation capabilities to very long contexts with perfect memory recall."
    }
  },
  {
    "id": "ZTpWOwMrzQ",
    "title": "Radar: Fast Long-Context Decoding for Any Transformer",
    "abstract": "Transformer models have demonstrated exceptional performance across a wide range of applications. Though forming the foundation of Transformer models, the dot-product attention does not scale well to long-context data since its time requirement grows quadratically with context length. In this work, we propose Radar, a training-free approach that accelerates inference by dynamically searching for the most important context tokens. For any pre-trained Transformer, Radar can reduce the decoding time complexity without training or heuristically evicting tokens. Moreover, we provide theoretical justification for our approach, demonstrating that Radar can reliably identify the most important tokens with high probability. We conduct extensive comparisons with the previous methods on a wide range of tasks. The results demonstrate that Radar achieves the state-of-the-art performance across different architectures with reduced time complexity, offering a practical solution for efficient long-context processing of Transformers. The code is publicly available at https://github.com/BorealisAI/radar-decoding.",
    "authors": [
      "~Yongchang_Hao1",
      "~Mengyao_Zhai1",
      "~Hossein_Hajimirsadeghi1",
      "~Sepidehsadat_Hosseini2",
      "~Frederick_Tung1"
    ],
    "pdf": "/pdf/5e644ebd717959e7e9ba3c3d0e18cff1c0774823.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training-free approach for inference acceleration, addressing quadratic time complexity of attention, practical solution for existing pre-trained models without retraining, dynamic identification of important context tokens, claimed state-of-the-art performance across architectures, directly improves throughput and reduces latency",
      "Irrelevant Aspects": "Focus limited to long-context scenarios, no specific details about GPU memory utilization, lacks quantitative metrics on actual GPU utilization improvements, unclear hardware-specific optimizations, doesn't address training optimization which is part of my research focus",
      "Summary": "This paper presents Radar, a training-free method to accelerate transformer inference for long-context processing by dynamically identifying important context tokens. It addresses the quadratic complexity issue in attention mechanisms, potentially improving throughput and reducing latency without requiring model retraining. While highly relevant to inference optimization, it appears more focused on algorithmic improvements rather than hardware-specific optimizations or GPU utilization strategies, and is specifically tailored to long-context scenarios rather than general inference optimization."
    }
  },
  {
    "id": "4es2oO9tw1",
    "title": "Compute-Constrained Data Selection",
    "abstract": "Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and training are budgeted for. We first formalize the problem of data selection with a cost-aware utility function, \nand model the data selection problem as trading off initial-selection cost for training gain. We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling finetuning tokens, model sizes, and data selection compute. Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective. For compute-optimal training, we find that perplexity and gradient data selection require training-to-selection model size ratios of 5x and 10x, respectively.",
    "authors": [
      "~Junjie_Yin2",
      "~Alexander_M_Rush1"
    ],
    "pdf": "/pdf/a2a08dc8053684507d4f080bc8a089b8213db868.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for LLMs under compute constraints, focusing on making finetuning more efficient through strategic data selection. It considers compute budgets and resource constraints, analyzes trade-offs between selection cost and training gains, and provides empirical findings about compute-optimal training strategies. The research relates to GPU utilization and scalability by finding optimal resource allocation between data selection and model training phases.",
      "Irrelevant Aspects": "The paper primarily focuses on data selection rather than comprehensive training pipeline optimization. It doesn't address inference optimization directly. It may not cover hardware-specific optimizations, model parallelism strategies, or other low-level optimization techniques that maximize throughput and minimize latency.",
      "Summary": "This paper examines compute-constrained data selection for LLM finetuning, formalizing the problem with a cost-aware utility function. Through experiments across multiple tasks, compute budgets, and model sizes, it finds that many sophisticated data selection methods are not compute-optimal, with simpler alternatives often performing better. The paper identifies optimal training-to-selection model size ratios (5x for perplexity and 10x for gradient-based selection) for compute-optimal training."
    }
  },
  {
    "id": "FSjIrOm1vz",
    "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
    "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge.  However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs’ ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this,  we further develop the computation allocation model to estimate RAG performance across different inference configurations.  The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
    "authors": [
      "~Zhenrui_Yue1",
      "~Honglei_Zhuang1",
      "~Aijun_Bai1",
      "~Kai_Hui1",
      "~Rolf_Jagerman2",
      "~Hansi_Zeng1",
      "~Zhen_Qin5",
      "~Dong_Wang21",
      "~Xuanhui_Wang1",
      "~Michael_Bendersky1"
    ],
    "pdf": "/pdf/4e52072dbc5ed6660b8d4180f52c426c1c8a06ae.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs, focusing on scaling test-time computation which directly impacts GPU utilization, throughput, and latency. It develops scaling laws for inference and a computation allocation model to predict optimal inference parameters under various constraints. The work explores strategies like increasing retrieved documents or generation steps to enhance contextual information utilization, which are key techniques for optimizing inference performance. These approaches help maximize GPU utilization during inference and improve overall system throughput.",
      "Irrelevant Aspects": "The paper has a specific focus on Retrieval Augmented Generation (RAG) systems rather than general LLM inference optimization. While the principles likely transfer to other contexts, the narrow focus on RAG might limit direct applicability to all inference optimization scenarios. Additionally, the paper appears to focus more on computational allocation strategies rather than architectural changes to models or system-level optimizations like memory management or parallel processing techniques.",
      "Summary": "This paper investigates inference scaling laws for long-context LLMs in the context of Retrieval Augmented Generation, exploring how to optimally allocate test-time computation between different strategies like in-context learning and iterative prompting. The authors develop models to predict optimal inference parameters under various computation constraints, demonstrating up to 58.9% performance gains on benchmarks. While focused on RAG systems, the paper provides valuable insights into inference optimization, computational resource allocation, and developing scaling laws that are relevant to LLM inference efficiency."
    }
  },
  {
    "id": "4FWAwZtd2n",
    "title": "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on performance, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models (PRMs); and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to, as effectively as possible, allocate test-time compute per prompt in an adaptive manner. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling for math reasoning problems by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute  can be used to outperform a 14x larger model.",
    "authors": [
      "~Charlie_Victor_Snell1",
      "~Jaehoon_Lee2",
      "~Kelvin_Xu2",
      "~Aviral_Kumar2"
    ],
    "pdf": "/pdf/c6b1928c3af73839a4844529a49346b199cffc28.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates scaling test-time computation in LLMs, which is directly related to inference optimization. It explores compute-optimal scaling strategies to allocate test-time compute efficiently, comparing performance between using more inference time compute versus larger models. The paper reports a 4x improvement in efficiency for math reasoning problems and demonstrates that smaller models with more test-time compute can outperform models 14x larger on certain problems. These findings relate to my interest in optimizing inference systems.",
      "Irrelevant Aspects": "The paper primarily focuses on reasoning capabilities and output quality rather than system performance metrics like latency and throughput. It doesn't directly address GPU utilization or hardware-specific optimizations that are central to my research. The emphasis is more on the effectiveness of reasoning strategies rather than the efficiency of the computing systems themselves.",
      "Summary": "This paper explores how scaling test-time computation can improve LLM performance, particularly for reasoning tasks. It introduces compute-optimal scaling strategies that adaptively allocate inference resources based on prompt difficulty. While the paper provides valuable insights into the tradeoffs between model size and inference computation, it focuses more on reasoning quality than on system-level optimization metrics like GPU utilization, throughput, or latency that are central to my research interests."
    }
  },
  {
    "id": "HD6bWcj87Y",
    "title": "Data Shapley in One Training Run",
    "abstract": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",
    "authors": [
      "~Jiachen_T._Wang1",
      "~Prateek_Mittal1",
      "~Dawn_Song1",
      "~Ruoxi_Jia1"
    ],
    "pdf": "/pdf/2e054ad2d03dbe6a6e0db00880b7eeb35a3a8c79.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational efficiency in large-scale model training, focuses on foundation models, claims negligible runtime overhead compared to standard training, deals with scaling techniques for large models, and is concerned with GPU utilization and scalability by reducing computational overhead.",
      "Irrelevant Aspects": "The focus on data attribution and Shapley values is tangential to the core interest in system optimization, discusses copyright implications which is outside the technical focus, and emphasizes data curation rather than direct system optimization.",
      "Summary": "This paper introduces 'In-Run Data Shapley' to efficiently calculate data contributions during a single training run, eliminating the need for computationally expensive retraining. The method scales to foundation models with minimal overhead, making data attribution feasible for pretraining. While it has clear connections to training optimization and scalability, its primary focus on data attribution rather than core system optimization makes it moderately relevant to my research interests."
    }
  },
  {
    "id": "T2d0geb6y0",
    "title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    "abstract": "The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models  such as Mamba were designed to replace attention with an almost linear time alternative.\n\nIn this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time – whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason – cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.",
    "authors": [
      "~Josh_Alman1",
      "~Hantao_Yu1"
    ],
    "pdf": "/pdf/eaf31d87dab4b744f14cd3483db92fd0390435c5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational efficiency of Transformers and their alternatives, which is fundamental to training optimization and inference optimization. It examines subquadratic approaches to attention mechanisms, which directly impacts GPU utilization and scalability. The theoretical limits established could inform optimization strategies for higher throughput and lower latency in LLM systems.",
      "Irrelevant Aspects": "The paper focuses primarily on theoretical complexity rather than practical optimization techniques. It establishes limitations rather than proposing new optimization methods. It may not provide specific implementation guidance for improving GPU utilization or reducing memory usage. The emphasis on fine-grained complexity theory might be less directly applicable to practical optimization scenarios.",
      "Summary": "This paper investigates fundamental limitations on subquadratic alternatives to Transformer attention mechanisms. It proves that certain tasks involving document similarity cannot be performed in subquadratic time, suggesting that attention's quadratic complexity may be unavoidable for some important functions. While theoretically valuable for understanding computational bounds in LLM architectures, the paper may not directly provide practical optimization techniques for improving GPU utilization, throughput, or latency in real-world deployments."
    }
  },
  {
    "id": "9VMW4iXfKt",
    "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference",
    "abstract": "Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50\\% model-level sparsity, resulting in a significant 43\\% end-to-end efficient improvements with customized kernels.",
    "authors": [
      "~Zhenyu_Zhang4",
      "~Zechun_Liu1",
      "~Yuandong_Tian1",
      "~Harshit_Khaitan1",
      "~Zhangyang_Wang1",
      "~Steven_Li3"
    ],
    "pdf": "/pdf/275a3d7973c0201712b5c5a6b07ccc0133dfde54.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization for LLMs, addressing activation sparsity to reduce computation and memory movement for more efficient inference. It claims to achieve 43% end-to-end efficiency improvements with customized kernels, which relates to higher throughput and lower latency. The method is training-free, designed for advanced LLMs (Llama-2/3 and Mistral), and addresses limitations with non-ReLU activation functions.",
      "Irrelevant Aspects": "The paper doesn't directly address scalability in terms of distributed computing or multiple GPUs, focusing more on single-device optimization. There's no explicit mention of GPU utilization metrics or how the method improves GPU utilization specifically. The paper doesn't discuss training optimization, only inference optimization.",
      "Summary": "The paper presents R-Sparse, a training-free approach to achieve activation sparsity in LLMs, particularly those using non-ReLU activation functions. The method replaces linear layers in LLMs with a rank-aware sparse inference method, claiming to achieve comparable performance at 50% model-level sparsity and 43% end-to-end efficiency improvements. This directly addresses inference optimization for LLMs with a focus on higher throughput and lower latency, but primarily for single-device scenarios rather than distributed systems."
    }
  },
  {
    "id": "gyHoR6uFhU",
    "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches",
    "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PORTLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B,Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PORTLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2× in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs’ personalization.",
    "authors": [
      "~Rana_Shahroz1",
      "~Pingzhi_Li1",
      "~Sukwon_Yun1",
      "~Zhenyu_Wang14",
      "~Shahriar_Nirjon1",
      "~Chau-Wai_Wong1",
      "~Tianlong_Chen1"
    ],
    "pdf": "/pdf/8d7a8326aaf224bd2aa7f204db2ec4d47e9f7493.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training-free approach that eliminates training costs, lightweight model update patches that improve memory efficiency, up to 12.2× reduction in GPU memory usage compared to LoRA, comparable performance to LoRA without computational overhead, scalability across multiple models (Mistral-7B, Llama2, Llama3.1, and Gemma2), minimal cost for continual personalization, and focus on efficient resource utilization for users with limited computing resources.",
      "Irrelevant Aspects": "Primary focus on patch methodology rather than system-level optimizations, lack of explicit discussion on inference optimization, limited attention to throughput improvements, minimal consideration of latency reduction, theoretical justification aspects not directly related to practical optimizations, and emphasis on personalization over system performance.",
      "Summary": "PortLLM presents a training-free framework for creating portable model patches that enable efficient personalization of evolving LLMs with minimal resources. While the paper offers significant improvements in memory efficiency (up to 12.2× reduction compared to LoRA) and eliminates training requirements, it primarily focuses on the methodology of creating and transferring patches rather than directly addressing inference optimization, throughput, or latency improvements, which are central to the research interest in LLM system optimization."
    }
  },
  {
    "id": "bsFWJ0Kget",
    "title": "GeoLoRA: Geometric integration for parameter efficient fine-tuning",
    "abstract": "Low-Rank Adaptation (LoRA) has become a widely used method for parameter-efficient fine-tuning of large-scale, pre-trained neural networks. However, LoRA and its extensions face several challenges, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process. We introduce GeoLoRA, a novel approach that addresses these limitations by leveraging dynamical low-rank approximation theory. GeoLoRA requires only a single backpropagation pass over the small-rank adapters, significantly reducing computational cost as compared to similar dynamical low-rank training methods and making it faster than popular baselines such as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated parameter budget across the model, achieving smaller low-rank adapters compared to heuristic methods like AdaLoRA and LoRA, while maintaining critical convergence, descent, and error-bound theoretical guarantees. The resulting method is not only more efficient but also more robust to varying hyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several state-of-the-art benchmarks, showing that it outperforms existing methods in both\naccuracy and computational efficiency",
    "authors": [
      "~Steffen_Schotthöfer1",
      "~Emanuele_Zangrando1",
      "~Gianluca_Ceruti1",
      "~Francesco_Tudisco1",
      "~Jonas_Kusch1"
    ],
    "pdf": "/pdf/a0f23a0adfb85d02fcee11ff17c200c15968ae80.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Parameter-efficient fine-tuning of LLMs, computational efficiency improvements, reduced backpropagation passes, smaller low-rank adapters, faster than baselines like AdaLoRA, better parameter budget allocation, improved GPU utilization (implicit), higher throughput potential during fine-tuning",
      "Irrelevant Aspects": "Focus on fine-tuning rather than pre-training, limited discussion of inference optimization, doesn't directly address scalability beyond efficiency improvements, theoretical guarantees not directly tied to GPU performance metrics",
      "Summary": "GeoLoRA introduces a parameter-efficient fine-tuning method for LLMs using dynamical low-rank approximation theory. It requires only a single backpropagation pass over small-rank adapters, significantly reducing computational cost compared to methods like AdaLoRA. The approach efficiently adapts parameter budgets across the model, achieving smaller adapters while maintaining theoretical guarantees. This results in better computational efficiency and robustness, with demonstrated improvements in accuracy and efficiency. The method is particularly relevant for LLM training optimization by reducing computational requirements and improving parameter efficiency, leading to better GPU utilization and higher throughput during fine-tuning."
    }
  },
  {
    "id": "1durmugh3I",
    "title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians",
    "abstract": "The foundation model (FM) paradigm is transforming Machine Learning Force Fields (MLFFs), leveraging general-purpose representations and scalable training to perform a variety of computational chemistry tasks. Although MLFF FMs have begun to close the accuracy gap relative to first-principles methods, there is still a strong need for faster inference speed. Additionally, while research is increasingly focused on general-purpose models which transfer across chemical space, practitioners typically only study a small subset of systems at a given time. At test time, MLFFs must also obey physical constraints unique to the downstream use case, such as energy conservation for molecular dynamics simulations. This underscores the need for fast, specialized MLFFs relevant to specific downstream applications, which preserve test-time physical soundness while maintaining train-time scalability. In this work, we introduce a method for transferring general-purpose representations from MLFF foundation models to smaller, faster MLFFs specialized to specific regions of chemical space. We formulate our approach as an architecture-agnostic knowledge distillation procedure, where the smaller \"student\" MLFF is trained to match the Hessians of the energy predictions of the \"teacher\" foundation model. We demonstrate our approach across multiple recent foundation models, large-scale datasets, chemical subsets, and downstream tasks. Our specialized MLFFs can be up to 20 times faster than the original foundation model, while retaining, and in some cases exceeding, its performance and that of undistilled models. We also show that distilling from a teacher model with a direct force parameterization into a student model trained with conservative forces (i.e., computed as derivatives of the potential energy) successfully leverages the representations from the large-scale teacher for improved accuracy, while maintaining energy conservation during test-time molecular dynamics simulations. More broadly, our work suggests a new paradigm for MLFF development, in which foundation models are released along with smaller, specialized simulation ``engines\" for common chemical subsets. The implementation of our method is available at https://github.com/ASK-Berkeley/MLFF-distill.",
    "authors": [
      "~Ishan_Amin1",
      "~Sanjeev_Raja1",
      "~Aditi_S._Krishnapriyan1"
    ],
    "pdf": "/pdf/78b81ec4144a24c844ba46d8fa9ded6e591894c9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on knowledge distillation from large foundation models to smaller, specialized models, which directly relates to my expertise in model optimization for better efficiency. The reported 20x speedup in inference aligns perfectly with my research interest in lower latency and higher throughput. The architecture-agnostic approach suggests broad applicability of the optimization techniques, and the work addresses the critical challenge of maintaining performance while reducing computational requirements - a core aspect of my expertise.",
      "Irrelevant Aspects": "The paper's domain is computational chemistry and molecular dynamics rather than natural language processing where my expertise is focused. The specific optimization techniques involving energy Hessians are domain-specific and not directly transferable to language models. The paper doesn't explicitly discuss GPU utilization or distributed systems scalability, which are central to my research interests.",
      "Summary": "This paper presents a knowledge distillation method for specialized Machine Learning Force Fields, achieving significant speed improvements while maintaining accuracy. Though applied to a different domain than language models, the fundamental approach of distilling large foundation models into smaller, more efficient ones aligns well with my research on model optimization. The paper's focus on inference speed and efficiency is highly relevant, though it lacks specific discussion of GPU utilization and distributed systems that are key to my expertise."
    }
  },
  {
    "id": "L9eBxTCpQG",
    "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000× larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across a range of model scales. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance\nof mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is submitted.",
    "authors": [
      "~Tianjin_Huang1",
      "~Ziquan_Zhu2",
      "~Gaojie_Jin1",
      "~Lu_Liu19",
      "~Zhangyang_Wang1",
      "~Shiwei_Liu2"
    ],
    "pdf": "/pdf/20b9a32ba0ad74fbce1320d498494bb999b4ff6b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses LLM training optimization, which is a core area of interest. It proposes techniques to improve training stability and reduce memory consumption through a novel optimizer called SPAM. The paper discusses memory-efficient training approaches that could enhance GPU utilization by implementing sparse momentum. It also claims to outperform other memory-efficient optimizers, which could lead to better resource efficiency during training.",
      "Irrelevant Aspects": "The paper does not address inference optimization, focusing solely on training aspects. There is no explicit discussion of latency reduction or throughput improvements. While it deals with memory efficiency, it doesn't directly address broader scalability concerns beyond memory constraints. The focus is primarily on training stability rather than overall system performance optimization.",
      "Summary": "SPAM introduces a novel optimizer that addresses gradient spikes during LLM training through momentum reset and spike-aware gradient clipping. While it contributes to training stability and memory efficiency, it lacks direct coverage of inference optimization, latency reduction, or explicit throughput improvements. The paper is relevant but somewhat narrowly focused on training stability rather than the full spectrum of LLM optimization challenges."
    }
  },
  {
    "id": "vVxeFSR4fU",
    "title": "Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity",
    "abstract": "Analyzing the similarity of internal representations within and across different models has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on Centered Kernel Alignment (CKA), rely on statistical properties of the representations for a set of data points. In this paper, we focus on transformer models and study the similarity of representations between the hidden layers of individual transformers. In this context, we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA. Our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer. We provide a theoretical justification for this phenomenon under the geodesic curve assumption for the learned transformer, a property that may approximately hold for residual networks. We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation. This offers a justification for {\\it saturation events}, where the model's top prediction remains unchanged across subsequent layers, indicating that the shallow layer has already learned the necessary knowledge. We then propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations, with trained models that enjoy the following properties: (1) more early saturation events, (2) layer-wise accuracies monotonically increase and reveal the minimal depth needed for the given task, (3) when served as multi-exit models, they achieve on-par performance with standard multi-exit architectures which consist of additional classifiers designed for early exiting in shallow layers. To our knowledge, our work is the first to show that one common classifier is sufficient for multi-exit models. We conduct experiments on both vision and NLP tasks to demonstrate the performance of the proposed aligned training.",
    "authors": [
      "~Jiachen_Jiang1",
      "~Jinxin_Zhou2",
      "~Zhihui_Zhu1"
    ],
    "pdf": "/pdf/686202fed1e16f7076e45db1c7a53dca75ae4211.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper's focus on transformer models and representation similarity analysis is directly relevant to LLM optimization. The early saturation events and aligned training method that improves shallow layer effectiveness could lead to inference optimizations with reduced latency. Their finding that one common classifier is sufficient for multi-exit models could simplify inference systems and improve efficiency. The identification of minimal depth needed for tasks directly informs model pruning or early exit strategies for LLMs.",
      "Irrelevant Aspects": "The paper primarily focuses on theoretical understanding of representation similarity rather than practical implementation for GPU utilization. There's no explicit discussion of GPU memory management, parallelization strategies, or distributed systems. The experiments mentioned include vision tasks, not specifically addressing challenges unique to very large language models. Hardware optimization aspects that are central to my expertise are not addressed.",
      "Summary": "This paper analyzes layer-wise representation similarity in transformer models, finding that simple cosine similarity aligns with more complex metrics like CKA. They show that increased similarity between layers leads to saturation events where predictions stabilize early. They propose an aligned training method that enhances internal representation similarity, resulting in models with more early saturation events and predictable minimal depth requirements. While primarily theoretical, these insights have practical applications for inference optimization through early exit strategies and reduced computational depth, though the paper doesn't directly address GPU utilization or hardware optimization specifics."
    }
  },
  {
    "id": "Fty0wTcemV",
    "title": "DELIFT: Data Efficient Language model Instruction Fine-Tuning",
    "abstract": "Fine-tuning large language models (LLMs) is crucial for task specialization but often becomes resource-intensive due to redundant or uninformative data. Existing data selection methods typically rely either on computationally expensive gradient-based metrics or static embeddings that fail to adapt dynamically to the model’s evolving state, thus limiting their practical effectiveness. To address this,\nwe propose DELIFT (Data Efficient Language model Instruction Fine-Tuning), leveraging a novel, computationally efficient utility metric inspired by In-Context Learning (ICL). Our ICL-based metric measures the informational value of each data sample by quantifying its effectiveness as an in-context example in improving model predictions for other samples, reflecting its actual contribution relative to the model’s current state. Integrated with tailored submodular optimization methods, DELIFT systematically selects diverse, informative subsets optimized specifically for each fine-tuning stage: instruction tuning, task-specific adaptation, and continual fine-tuning. Experimental results across multiple datasets and model scales show DELIFT reduces fine-tuning data requirements by up to 70% without compromising performance, consistently outperforming existing methods by up to 26% in effectiveness and efficiency.",
    "authors": [
      "~Ishika_Agarwal1",
      "~Krishnateja_Killamsetty1",
      "~Lucian_Popa1",
      "~Marina_Danilevsky1"
    ],
    "pdf": "/pdf/a90372465680c53ab839a4a33b8e45e77de9f374.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on training optimization for LLMs by reducing data requirements by up to 70%, which directly improves computational efficiency and resource utilization. Introduces a computationally efficient utility metric that replaces expensive gradient-based methods, potentially improving GPU utilization. Addresses scalability across multiple model scales. Claims to maintain performance while reducing computational overhead, which aligns with goals of efficient resource usage.",
      "Irrelevant Aspects": "Primarily focuses on data selection rather than hardware-specific optimizations. Lacks discussion of inference optimization techniques, memory management strategies, or GPU-level optimizations. Does not address throughput and latency optimization directly. Centers on methodological improvements to data selection rather than infrastructure optimizations for actual training execution.",
      "Summary": "DELIFT addresses an important aspect of LLM training optimization by introducing an efficient data selection method that reduces fine-tuning data requirements significantly. The ICL-based utility metric and submodular optimization approach offer computational benefits that can translate to better resource utilization. However, the paper focuses on data-centric optimization rather than the hardware and infrastructure-level optimizations that are central to maximizing GPU utilization, throughput, and minimizing latency in both training and inference phases."
    }
  },
  {
    "id": "JDm7oIcx4Y",
    "title": "Accelerated training through iterative gradient propagation along the residual path",
    "abstract": "Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.\nSuch models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.\nHowever, the computational cost of backpropagation remains a major burden, accounting for most of the training time.\nTaking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.\nThrough an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation.",
    "authors": [
      "~Erwan_Fagnou1",
      "~Paul_Caillon1",
      "~Blaise_Delattre1",
      "~Alexandre_Allauzen1"
    ],
    "pdf": "/pdf/a6881bbbb082af261bdd067646a94111cda3efed.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training acceleration through parallelizable backpropagation, which directly relates to LLM training optimization. It targets scalability issues in deep models and claims to achieve speedups with minimal performance degradation. The method applies to Transformers, which are fundamental to modern large language models. By making gradient propagation more parallelizable, it could improve GPU utilization and training throughput.",
      "Irrelevant Aspects": "The paper focuses exclusively on training optimization without addressing inference optimization, which is another critical aspect of my expertise. It doesn't mention memory optimization techniques, which are essential for large model training. There's no explicit discussion of latency improvements, only general speedups. The paper's approach seems to be specific to models with residual connections rather than being a general optimization technique.",
      "Summary": "This paper introduces Highway backpropagation, a parallelizable alternative to traditional backpropagation that leverages residual connections to accelerate training. The method works by accumulating gradient estimates along residual paths and backpropagating them through layers in parallel, potentially improving GPU utilization and training throughput. While directly relevant to my interest in training optimization for large language models, it lacks coverage of inference optimization and doesn't explicitly address memory concerns."
    }
  },
  {
    "id": "G4wARwjF8M",
    "title": "SLMRec: Distilling Large Language Models into Small for Sequential Recommendation",
    "abstract": "Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. \nThe SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. \nRecent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance.\nMotivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13\\% of the parameters found in LLM-based recommendation models while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR.",
    "authors": [
      "~Wujiang_Xu1",
      "~Qitian_Wu1",
      "~Zujie_Liang1",
      "~Jiaojiao_Han1",
      "~Xuying_Ning1",
      "~Yunxiao_Shi1",
      "~Wenfang_Lin1",
      "~Yongfeng_Zhang1"
    ],
    "pdf": "/pdf/0744ad8ec274379fa6e5d41fc71d0f84dd405d71.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM efficiency optimization through knowledge distillation, reporting 13% parameter reduction with maintained performance. It shows significant training (6.6x) and inference (8.0x) speedups, directly relating to GPU utilization, scalability, throughput, and latency optimization - my core research interests. The approach is orthogonal to other efficiency techniques like quantization and pruning, making it potentially valuable as part of a broader optimization pipeline.",
      "Irrelevant Aspects": "The focus is specifically on sequential recommendation tasks rather than general LLM optimization, potentially limiting applicability to broader scenarios. The paper appears more application-driven than focusing on fundamental improvements to model architectures or training/inference frameworks. There may be limited technical implementation details relevant to general LLM systems optimization.",
      "Summary": "SLMRec presents a knowledge distillation approach to create smaller, more efficient language models for sequential recommendation tasks. The research demonstrates that most intermediate layers of LLMs are redundant for this task, enabling significant parameter reduction (to 13% of original size) while maintaining performance. The resulting model achieves substantial improvements in training and inference efficiency (6.6x and 8.0x speedups respectively), addressing key challenges in deploying LLMs at scale. While domain-specific, the distillation methodology and efficiency findings have valuable implications for general LLM optimization strategies focused on resource utilization and performance."
    }
  },
  {
    "id": "TuOTSAiHDn",
    "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
    "abstract": "The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general\npurpose reasoning tasks (GENERAL REASONING: +2.51%).",
    "authors": [
      "~Syeda_Nahida_Akter1",
      "~Shrimai_Prabhumoye1",
      "~John_Kamalu1",
      "~Sanjeev_Satheesh3",
      "~Eric_Nyberg1",
      "~Mostofa_Patwary1",
      "~Mohammad_Shoeybi1",
      "~Bryan_Catanzaro1"
    ],
    "pdf": "/pdf/5313c64ed4eed9b848e009dd9d70799be396e7f8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses LLM training optimization through improved data quality and synthetic data generation. It presents techniques for enhancing specific reasoning capabilities (mathematical) during pretraining, showing significant performance improvements in downstream tasks (GSM8K: +13.42%, MATH: +2.30%). The paper discusses effective methods for integrating synthetic and raw data during pretraining, which is relevant to training optimization research.",
      "Irrelevant Aspects": "The paper doesn't focus on GPU utilization optimization, scalability concerns, or inference optimization techniques. It's primarily focused on mathematical reasoning capabilities rather than general computational efficiency improvements. There's no discussion of memory optimization, compute efficiency, or distributed training strategies.",
      "Summary": "MIND introduces a synthetic dialogue generation method to improve mathematical reasoning in LLMs through enhanced pretraining data. While relevant to training optimization through data quality improvements, it lacks focus on hardware utilization and scalability aspects that are central to my research. The paper demonstrates significant gains in specific reasoning capabilities but doesn't address computational efficiency or resource optimization challenges."
    }
  },
  {
    "id": "GbgCRJedQ7",
    "title": "SMT: Fine-Tuning Large Language Models with Sparse Matrices",
    "abstract": "Various parameter-efficient fine-tuning (PEFT) methods, including LoRA and its variants, have gained popularity for reducing computational costs. However, there is often an accuracy gap between PEFT approaches and full fine-tuning (FT), and this discrepancy has not yet been systematically explored. In this work, we introduce a method for selecting sparse sub-matrices that aims to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational costs and memory costs. We explored both gradient-based and activation-based parameter selection methods to identify the most significant sub-matrices for downstream tasks, updating only these blocks during fine-tuning. In our experiments, we demonstrated that SMT consistently surpasses other PEFT\nbaselines (e.g., LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issues.",
    "authors": [
      "~Haoze_He1",
      "~Juncheng_B_Li1",
      "~Xuan_Jiang3",
      "~Heather_Miller1"
    ],
    "pdf": "/pdf/7e1dce59ed8ced951755c7680265e7ee96ccd6a0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Parameter-Efficient Fine-Tuning (PEFT) methods for reducing computational costs",
        "GPU memory reduction (67% compared to full fine-tuning)",
        "Computational cost reduction during fine-tuning",
        "Sparse matrix operations for efficiency",
        "Potential scalability improvements through reduced resource requirements",
        "Performance improvements over existing PEFT methods (LoRA and DoRA)"
      ],
      "Irrelevant Aspects": [
        "Focus on training rather than inference optimization",
        "Lack of explicit latency considerations",
        "No explicit mention of throughput improvements",
        "Limited detail on GPU utilization techniques"
      ],
      "Summary": "SMT introduces a sparse matrix approach for parameter-efficient fine-tuning of LLMs, focusing on reducing computational and memory costs while maintaining performance. The method selects significant sub-matrices and updates only these during fine-tuning, outperforming PEFT baselines and reducing GPU memory usage by 67% compared to full fine-tuning. While it addresses training optimization and GPU utilization, it doesn't focus on inference optimization, latency, or throughput improvements."
    }
  },
  {
    "id": "IDxZhXrpNf",
    "title": "SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling",
    "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor --- a memory-efficient approximation of Adam --- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: **S**hampo**O** with **A**dam in the **P**reconditioner's eigenbasis (SOAP).\nWith regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40\\% and wall clock time by over 35\\% compared to AdamW, with approximately 20\\% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.",
    "authors": [
      "~Nikhil_Vyas1",
      "~Depen_Morwani1",
      "~Rosie_Zhao1",
      "~Itai_Shapira1",
      "~David_Brandfonbrener1",
      "~Lucas_Janson2",
      "~Sham_M._Kakade1"
    ],
    "pdf": "/pdf/ba37fd3bf6298002e0b0164171483254f63e6bc1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses language model training optimization, proposing a new algorithm (SOAP) that improves computational efficiency with 35% reduction in wall clock time compared to AdamW. It specifically focuses on large batch regime training, which relates to GPU utilization and scalability. The work provides empirical results on 360m and 660m sized models and includes a publicly available implementation.",
      "Irrelevant Aspects": "The paper focuses on algorithmic optimizations rather than system-level implementations. It doesn't explicitly discuss GPU utilization metrics or distributed training considerations. The work is limited to training optimization and doesn't address inference optimization, which is part of my research interest.",
      "Summary": "SOAP is a new optimization algorithm for language model training that combines benefits of Shampoo and Adam. It improves computational efficiency by maintaining Adam's second-moment updates in Shampoo's coordinate basis, reducing iterations by over 40% and wall clock time by over 35% compared to AdamW. While it addresses training efficiency, it focuses more on algorithmic improvements than system-level optimizations and doesn't cover inference optimization."
    }
  },
  {
    "id": "Rz0kozh3LE",
    "title": "Mixture of Attentions For Speculative Decoding",
    "abstract": "The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy.\nSpeculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel.\nSmall models that utilise activations from the LLM currently achieve the fastest decoding speeds.\nHowever, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. \nTo address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server.\nIn a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%.\nIn a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process.",
    "authors": [
      "~Matthieu_Zimmer1",
      "~Milan_Gritta1",
      "~Gerasimos_Lampouras2",
      "~Haitham_Bou_Ammar1",
      "~Jun_Wang2"
    ],
    "pdf": "/pdf/8c5394d4897a186b71f2dbb3399d2b082db04fd5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative decoding for LLM inference optimization, which directly addresses latency and throughput improvements. It introduces a novel 'Mixture of Attentions' architecture that achieves state-of-the-art speedups (improving EAGLE-2 by 9.5%) and higher acceptance lengths. The work covers deployment scenarios including single-device and client-server settings, addressing scalability and resource utilization challenges. The approach reduces computational requirements and improves efficiency in LLM deployment, which is central to achieving better GPU utilization.",
      "Irrelevant Aspects": "The paper appears to focus primarily on inference optimization rather than training optimization for LLMs. While it mentions a limitation related to 'lack of on-policyness during training,' the main contributions are in the inference domain rather than training optimization techniques. The abstract doesn't provide extensive details about specific GPU utilization strategies or low-level optimization techniques.",
      "Summary": "This paper presents a novel approach to optimize LLM inference through speculative decoding using a Mixture of Attentions architecture. It demonstrates significant speed improvements over existing methods and addresses deployment scenarios from single devices to client-server architectures. While it makes valuable contributions to inference optimization and reducing computational requirements, it focuses less on training optimization aspects of LLM systems."
    }
  },
  {
    "id": "HMrcv7Q4Ub",
    "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
    "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.",
    "authors": [
      "~Dezhan_Tu1",
      "~Danylo_Vashchilenko1",
      "~Yuzhe_Lu1",
      "~Panpan_Xu3"
    ],
    "pdf": "/pdf/1df041a20a9b32089aa311da66ea1e6f16e0b920.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focus on KV Cache Compression, Vision-Language Models specialization, GPU memory optimization, performance improvements (2.33x end-to-end, 7.08x decoding), sparsity patterns investigation, modality-aware optimization",
      "Irrelevant Aspects": "Training optimization focus, multi-GPU scalability discussion, some highly vision-specific details",
      "Summary": "Presents VL-Cache, a method for optimizing KV cache storage specifically for Vision-Language Models during inference, achieving 90% memory reduction while maintaining accuracy with only 10% of KV cache, leading to substantial inference speedups."
    }
  },
  {
    "id": "TJo6aQb7mK",
    "title": "Surprising Effectiveness of pretraining Ternary  Language Model at Scale",
    "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs.",
    "authors": [
      "~Ayush_Kaushal1",
      "~Tejas_Vaidhya1",
      "~Arnab_Kumar_Mondal1",
      "~Tejas_Pandey2",
      "~Aaryan_Bhagat1",
      "~Irina_Rish1"
    ],
    "pdf": "/pdf/b7f47612d7d5627f433233234f8b7f7779ca9daf.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses memory capacity and bandwidth bottlenecks in LLM inference, investigates pretraining low-bitwidth models specifically for better GPU utilization, presents comprehensive analysis of scaling behavior across different bit-widths, demonstrates unexpected performance benefits of ternary models at scale, and focuses on making LLMs more efficient for production use.",
      "Irrelevant Aspects": "Limited focus on latency measurements and more narrow scope on just ternary quantization rather than broader optimization techniques.",
      "Summary": "This paper is highly relevant as it directly tackles memory bottlenecks in LLM inference through pretraining ternary language models at scale. The surprising finding that large TriLMs can match FloatLM performance while using significantly fewer bits provides crucial insights for improving GPU utilization and efficiency in both training and inference scenarios."
    }
  },
  {
    "id": "QhxjQOMdDF",
    "title": "Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models",
    "abstract": "Fine-tuning language models (LMs) with the standard Adam optimizer often demands excessive memory, limiting accessibility. The ``in-place'' version of Stochastic Gradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO) have been proposed as solutions to improve memory efficiency. However, IP-SGD still requires a decent amount of memory, and MeZO suffers from slow convergence and degraded final performance due to its zeroth-order nature. This paper introduces Addax, a novel method that improves both memory efficiency and algorithm performance of IP-SGD by integrating it with MeZO. Specifically, Addax computes the zeroth-order or first-order gradient of the data points in the minibatch based on their memory consumption and combines zeroth- and first-order gradient estimates to obtain the updated direction in each step.\nBy computing the zeroth-order order gradient of data points that require more memory and the first-order gradient of the ones that require less memory, Addax overcomes the slow convergence of MeZO and excessive memory requirement of IP-SGD. Additionally, the zeroth-order gradient acts as a regularizer for the first-order gradient, further enhancing the model's final performance.\nTheoretically, we establish the convergence of Addax under mild assumptions, demonstrating faster convergence and less restrictive hyper-parameter choices than MeZO. Our extensive experiments with diverse LMs and tasks show that Addax consistently outperforms MeZO in terms of accuracy and convergence speed, while having a comparable memory footprint. \nIn particular, our experiments using one A100 GPU on OPT-13B model reveal that, on average, Addax outperforms MeZO in terms of accuracy/F1 score by 14%, and runs $15\\times$ faster, while having a comparable memory footprint to MeZO. In our experiments on the larger OPT-30B model, on average, Addax outperforms MeZO in terms of accuracy/F1 score by >16% and runs $30\\times$ faster on a single H100 GPU. Moreover, Addax surpasses the performance of standard fine-tuning approaches, such as IP-SGD and Adam, in most tasks in terms of Accuracy/F1 score with significantly less memory requirement.",
    "authors": [
      "~Zeman_Li1",
      "~Xinwei_Zhang1",
      "~Peilin_Zhong1",
      "~Yuan_Deng1",
      "~Meisam_Razaviyayn1",
      "~Vahab_Mirrokni2"
    ],
    "pdf": "/pdf/61d61288bed018a572c240226df3ac7ed3ec5f20.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses memory efficiency during fine-tuning of large language models, which is critical for GPU utilization and scalability. It introduces a novel optimization method that combines zeroth-order and first-order gradients, improving convergence speed and performance. The experiments show significant improvements in accuracy (14% on OPT-13B, >16% on OPT-30B) and substantial speed gains (15x and 30x faster). The method enables training large models on limited hardware resources, enhancing accessibility and throughput.",
      "Irrelevant Aspects": "The paper focuses exclusively on fine-tuning rather than pre-training or inference optimization. It doesn't address latency improvements specifically, which is one of my research interests. The zeroth-order gradient approach, while novel for memory efficiency, may not generalize to all training scenarios.",
      "Summary": "Addax presents a hybrid optimization method combining zeroth-order and first-order gradients to improve memory efficiency during fine-tuning of large language models. By selectively applying each gradient type based on memory requirements, it outperforms existing methods like MeZO and IP-SGD in both accuracy and speed while maintaining a small memory footprint. The approach enables fine-tuning of very large models (13B and 30B parameters) on single GPUs with significant performance gains, making large language model fine-tuning more accessible and efficient."
    }
  },
  {
    "id": "xQVxo9dSID",
    "title": "Consistency Models Made Easy",
    "abstract": "Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an effective scheme for training CMs that largely improves the efficiency of building such models. Specifically, by expressing CM trajectories via a particular differential equation, we argue that diffusion models can be viewed as a special case of CMs. We can thus fine-tune a consistency model starting from a pretrained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly reduced training times while improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained for hundreds of GPU hours. Owing to this computational efficiency, we investigate the scaling laws of CMs under ECT, showing that they obey the classic power law scaling, hinting at their ability to improve efficiency and performance at larger scales. Our [code](https://github.com/locuslab/ect) is available.",
    "authors": [
      "~Zhengyang_Geng1",
      "~Ashwini_Pokle1",
      "~Weijian_Luo1",
      "~Justin_Lin2",
      "~J_Zico_Kolter1"
    ],
    "pdf": "/pdf/f88e97400e7d0e7f78e29cf623840514af8c4a5d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper demonstrates significant improvements in training efficiency (reducing training from a week on 8 GPUs to 1 hour on a single A100), directly addressing GPU utilization optimization. It investigates scaling laws for consistency models under their proposed method, which is relevant to scalability concerns in ML systems. The approach of progressively fine-tuning from a pretrained model has potential applications for efficient training of large models.",
      "Irrelevant Aspects": "The paper focuses on Consistency Models for image generation rather than large language models, which is a different domain than my primary research interest. The specific techniques are tailored to diffusion-based models rather than transformer-based LLMs. The evaluation metrics (FID for image quality) are domain-specific and not directly applicable to language models.",
      "Summary": "This paper introduces Easy Consistency Tuning (ECT), a method that significantly improves training efficiency for Consistency Models (a type of diffusion model for image generation). By leveraging the mathematical relationship between diffusion models and CMs, they can fine-tune from pretrained diffusion models, achieving similar quality with vastly reduced computational requirements (1 hour on a single GPU vs. hundreds of GPU hours). They also investigate scaling laws for CMs, finding they follow classic power law scaling. While the domain is image generation rather than language models, the efficiency optimization techniques and scaling law insights have potential relevance to my research on LLM training and inference optimization."
    }
  },
  {
    "id": "HN8V0flwJF",
    "title": "World Model on Million-Length Video And Language With Blockwise RingAttention",
    "abstract": "Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we aim to address these challenges by providing a comprehensive exploration of the full development process for producing 1M context language models and video-language models, setting new benchmarks in language retrieval and new capabilities in long video understanding. We detail our long context data curation process, progressive context extension from 4K to 1M tokens, and present an efficient open-source implementation for scalable training on long sequences. Additionally, we open-source a family of 7B parameter models capable of processing long text documents and videos exceeding 1M tokens.",
    "authors": [
      "~Hao_Liu1",
      "~Wilson_Yan1",
      "~Matei_Zaharia1",
      "~Pieter_Abbeel2"
    ],
    "pdf": "/pdf/097eee532bf14539451d65b4a72a5fe9e6d2ab6d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Scaling sequence models to 1M token contexts, Blockwise RingAttention for efficient attention computation, Efficient open-source implementation for scalable training, Progressive context extension from 4K to 1M tokens, Training 7B parameter models on long sequences, GPU utilization challenges in processing video-language data",
      "Irrelevant Aspects": "Focus on application capabilities (video understanding) rather than optimization techniques, Potential lack of detailed technical optimizations for GPU utilization, No explicit mention of inference optimization, No direct discussion of latency or throughput optimization",
      "Summary": "The paper presents work on extending context length for language and video-language models to 1 million tokens using Blockwise RingAttention. It covers the full development process including data curation, progressive context extension, and efficient training implementation. While it addresses scalability challenges in training large models on long sequences, the abstract doesn't explicitly focus on GPU utilization optimization, throughput, or latency aspects that are central to my research interests."
    }
  },
  {
    "id": "mnna9LUg7P",
    "title": "Quamba: A Post-Training Quantization Recipe for Selective State Space Models",
    "abstract": "State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of SSMs in long sequence modeling positions them favorably over Transformers in many scenarios. However, improving the efficiency of SSMs on request-intensive cloud-serving and resource-limited edge applications is still a formidable task. SSM quantization is a possible solution to this problem, making SSMs more suitable for wide deployment, while still maintaining their accuracy. Quantization is a common technique to reduce the model size and to utilize the low bit-width acceleration features on modern computing units, yet existing quantization techniques are poorly suited for SSMs. Most notably, SSMs have highly sensitive feature maps within the selective scan mechanism (i.e., linear recurrence) and massive outliers in the output activations which are not present in the output of token-mixing in the self-attention modules. To address this issue, we propose a static 8-bit per-tensor SSM quantization method which suppresses the maximum values of the input activations to the selective SSM for finer quantization precision and quantizes the output activations in an outlier-free space with Hadamard transform. Our 8-bit weight-activation quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a 1.72 $\\times$ lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9\\% drop in average accuracy on zero-shot tasks. When quantizing Jamba, a 52B parameter SSM-style language model, we observe only a $1\\%$  drop in accuracy, demonstrating that our SSM quantization method is both effective and scalable for large language models, which require appropriate compression techniques for deployment. The experiments demonstrate the effectiveness and practical applicability of our approach for deploying SSM-based models of all sizes on both cloud and edge platforms.",
    "authors": [
      "~Hung-Yueh_Chiang1",
      "~Chi-Chih_Chang1",
      "~Natalia_Frumkin1",
      "~Kai-Chiang_Wu1",
      "~Diana_Marculescu4"
    ],
    "pdf": "/pdf/0c4ea9cb96dd9e16a33f3931fb1a1347d00b5817.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on quantization techniques for State Space Models (SSMs), which directly addresses my interest in inference optimization for large language models. It specifically tackles improving GPU utilization, reducing latency, and enabling model deployment on both cloud and edge platforms. The quantization approach discussed leads to 1.72x lower generation latency with minimal accuracy loss, which is exactly the kind of efficiency improvement that aligns with my research focus. The scalability demonstration on a 52B parameter model is also highly relevant to my interest in large-scale systems.",
      "Irrelevant Aspects": "The paper is primarily focused on post-training quantization rather than training optimization techniques. It doesn't explore training-time efficiency improvements or distributed training strategies that would also be relevant to my research interests. Additionally, the techniques are specifically tailored for SSMs rather than general transformers, which limits their immediate applicability to the broader LLM ecosystem.",
      "Summary": "Quamba presents an 8-bit quantization method specifically designed for State Space Models, addressing the unique challenges they present compared to transformers. The approach suppresses activation outliers and transforms outputs to an outlier-free space, enabling efficient quantization with minimal accuracy loss. The paper demonstrates significant latency improvements (1.72x on Nvidia Orin) with only 0.9% accuracy drop on a 2.8B model and similar results on a 52B model, showing both effectiveness and scalability for deploying large language models on various hardware platforms."
    }
  },
  {
    "id": "MxbEiFRf39",
    "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals",
    "abstract": "We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.\n\nCode, documentation, and tutorials are available at https://nnsight.net/.",
    "authors": [
      "~Jaden_Fried_Fiotto-Kaufman1",
      "~Alexander_Russell_Loftus1",
      "~Eric_Todd1",
      "~Jannik_Brinkmann1",
      "~Koyena_Pal1",
      "~Dmitrii_Troitskii1",
      "~Michael_Ripa1",
      "~Adam_Belfki1",
      "~Can_Rager1",
      "~Caden_Juang1",
      "~Aaron_Mueller1",
      "~Samuel_Marks1",
      "~Arnab_Sen_Sharma1",
      "~Francesca_Lucchetti1",
      "~Nikhil_Prakash1",
      "~Carla_Brodley1",
      "~Arjun_Guha3",
      "~Jonathan_Bell1",
      "~Byron_C_Wallace1",
      "~David_Bau1"
    ],
    "pdf": "/pdf/32c8015aac1c476874e9d4fbc781c4c4c8349b5e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on large language models, NDIF as a scalable inference service, GPU resource sharing and management, performance benchmarking, handling very large neural networks, deferred remote execution architecture, transparent access to model internals without hosting overhead",
      "Irrelevant Aspects": "Primary focus on research democratization rather than pure performance optimization, emphasis on studying model internals rather than training efficiency, limited discussion of training optimization techniques",
      "Summary": "This paper introduces NNsight and NDIF, technologies that enable efficient access to large neural network internals through deferred remote execution and shared GPU resources. While primarily focused on democratizing research access to foundation models, the technologies provide valuable insights into inference optimization, GPU utilization, and resource sharing for large language models. The intervention graph architecture and scalable inference service are directly relevant to optimizing throughput and latency in large-scale inference systems, though the paper's emphasis is on enabling research rather than pure performance optimization."
    }
  },
  {
    "id": "eU39PDsZtT",
    "title": "GraphRouter: A Graph-based Router for LLM Selections",
    "abstract": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to leverage\ncontextual interactions among tasks, queries, and LLMs, as well as their depen-\ndence on a transductive learning framework. To address these shortcomings, we\nintroduce a novel inductive graph framework, named as GraphRouter, which\nfully utilizes the contextual information among tasks, queries, and LLMs to en-\nhance the LLM selection process. GraphRouter constructs a heterogeneous\ngraph comprising task, query, and LLM nodes, with interactions represented as\nedges, which efficiently captures the contextual information between the query’s\nrequirements and the LLM’s capabilities. Through an innovative edge prediction\nmechanism, GraphRouter is able to predict attributes (the effect and cost of\nLLM response) of potential edges, allowing for optimized recommendations that\nadapt to both existing and newly introduced LLMs without requiring retraining.\nComprehensive experiments across three distinct effect-cost weight scenarios have\nshown that GraphRouter substantially surpasses existing routers, delivering a\nminimum performance improvement of 12.3%. In addition, it achieves enhanced\ngeneralization across new LLMs settings and supports diverse tasks with at least a\n9.5% boost in effect and a significant reduction in computational demands. This\nwork endeavors to apply a graph-based approach for the contextual and adaptive\nselection of LLMs, offering insights for real-world applications.",
    "authors": [
      "~Tao_Feng5",
      "~Yanzhen_Shen1",
      "~Jiaxuan_You2"
    ],
    "pdf": "/pdf/2452b8ff98e5d19eedb55a812aabf2db832a2ec4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on inference optimization through intelligent LLM selection, addresses performance-cost trade-offs, provides scalability to accommodate new LLMs without retraining, reduces computational demands, potentially improves throughput and latency",
      "Irrelevant Aspects": "Limited focus on training optimization, doesn't directly address GPU utilization techniques, emphasis on model selection rather than individual model optimization, lacks deep technical details on inference systems or hardware optimization",
      "Summary": "GraphRouter presents a graph-based framework for selecting the most appropriate LLM for a given query, optimizing the trade-off between performance and computational cost. While not directly addressing training optimization or specific GPU techniques, it contributes to inference efficiency through intelligent query routing. The system shows strong scalability and promises reduced computational demands, which could indirectly benefit throughput and latency in real-world applications."
    }
  },
  {
    "id": "BkwCrIsTbR",
    "title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation",
    "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. \nThrough a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.",
    "authors": [
      "~Linda_He1",
      "~Jue_WANG1",
      "~Maurice_Weber1",
      "~Shang_Zhu1",
      "~Ben_Athiwaratkun1",
      "~Ce_Zhang1"
    ],
    "pdf": "/pdf/d27ba793e6e10160b5544a0bc16a6636bf85e79b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses LLM scalability challenges, specifically extending context windows to 1M tokens. It introduces a post-training synthetic data generation strategy and step-by-step RoPE scaling training, which are training optimization techniques. Long-context processing has significant implications for GPU memory utilization and computational efficiency, making this work relevant to hardware utilization concerns.",
      "Irrelevant Aspects": "The focus is primarily on synthetic data generation rather than direct hardware efficiency optimization. There's no explicit discussion of throughput measurements, latency optimization, or specific GPU utilization techniques. The paper doesn't address computational complexity reduction strategies beyond the quadratic scaling mention.",
      "Summary": "This paper presents a novel approach to scale LLMs to million-token contexts through synthetic data generation and hierarchical training strategies. While it addresses important scaling aspects relevant to my research, it focuses more on data strategies than direct computational or hardware optimization, though extending to such extreme context lengths necessarily involves considerations of memory management and processing efficiency."
    }
  },
  {
    "id": "zfeso8ceqr",
    "title": "Deconstructing What Makes a Good Optimizer for Autoregressive Language Models",
    "abstract": "Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the most effective approach. We aim to compare several optimization algorithms, including SGD, Adafactor, Adam, Lion, and Sophia in the context of autoregressive language modeling across a range of model sizes, hyperparameters, and architecture variants. Our findings indicate that, except for SGD, these algorithms all perform comparably both in their optimal performance and also in terms of how they fare across a wide range of hyperparameter choices. Our results suggest to practitioners that the choice of optimizer can be guided by practical considerations like memory constraints and ease of implementation, as no single algorithm emerged as a clear winner in terms of performance or stability to hyperparameter misspecification. Given our findings, we further dissect these approaches, examining two simplified versions of Adam: a) signed momentum (Signum)  which we see recovers both the performance and hyperparameter stability of Adam and b) Adalayer, a layerwise variant of Adam which we introduce to study the impact on Adam's preconditioning for different layers of the network. Examining Adalayer leads us to the conclusion that, perhaps surprisingly, adaptivity on *both* the last layer and LayerNorm parameters in particular are necessary for retaining performance and stability to learning rate.",
    "authors": [
      "~Rosie_Zhao1",
      "~Depen_Morwani1",
      "~David_Brandfonbrener1",
      "~Nikhil_Vyas1",
      "~Sham_M._Kakade1"
    ],
    "pdf": "/pdf/e83be9c4c6919229fdc278ba964e49d69def63b2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses optimization algorithms for autoregressive language models, which is central to my interest in LLM training optimization. It compares multiple optimizers (Adam, SGD, Adafactor, Lion, Sophia) across different model sizes, which relates to scalability concerns. The examination of memory constraints and implementation considerations connects to GPU utilization efficiency. The study of simplified versions of Adam (Signum and Adalayer) could provide insights into more efficient training approaches that might improve throughput.",
      "Irrelevant Aspects": "The paper focuses primarily on theoretical comparisons of optimizers rather than empirical measurements of GPU utilization, throughput, or latency. It appears to address training optimization without consideration of inference optimization. There's no explicit mention of distributed training techniques or parallelization strategies, which are crucial for scalability. The analysis of layerwise optimization, while interesting, may not directly translate to performance improvements at scale.",
      "Summary": "This paper provides a comprehensive comparison of optimization algorithms for language model training, finding that modern optimizers perform comparably except for SGD. The research suggests practical considerations should guide optimizer choice rather than performance alone. The deconstruction of Adam into simplified components reveals the importance of adaptivity in specific layers. While this offers valuable insights for training optimization, the paper lacks focus on hardware utilization metrics and scalability considerations that are central to my research interests in GPU utilization and performance optimization."
    }
  },
  {
    "id": "CtM5xjRSfm",
    "title": "Accelerating neural network training: An analysis of the AlgoPerf competition",
    "abstract": "The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in the self-tuning ruleset they must be completely hyperparameter-free. In both rulesets, submissions are compared on time-to-result across multiple deep learning workloads, training on fixed hardware. This paper presents the inaugural AlgoPerf competition's results, which drew 18 diverse submissions from 10 teams. Our investigation reveals several key findings: (1) The winning submission in the external tuning ruleset, using Distributed Shampoo, demonstrates the effectiveness of non-diagonal preconditioning over popular methods like Adam, even when compared on wall-clock runtime. (2) The winning submission in the self-tuning ruleset, based on the Schedule Free AdamW algorithm, demonstrates a new level of effectiveness for completely hyperparameter-free training algorithms. (3) The top-scoring submissions were surprisingly robust to workload changes. We also discuss the engineering challenges encountered in ensuring a fair comparison between different training algorithms. These results highlight both the significant progress so far, and the considerable room for further improvements.",
    "authors": [
      "~Priya_Kasimbeg1",
      "~Frank_Schneider1",
      "~Runa_Eschenhagen1",
      "~Juhan_Bae2",
      "~Chandramouli_Shama_Sastry1",
      "~Mark_Saroufim1",
      "~BOYUAN_FENG1",
      "~Less_Wright2",
      "~Edward_Z._Yang1",
      "~Zachary_Nado1",
      "~Sourabh_Medapati1",
      "~Philipp_Hennig1",
      "~Michael_Rabbat1",
      "~George_E._Dahl1"
    ],
    "pdf": "/pdf/81b6cf1313d59e0fd88ca42067785f5deb3a0402.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training algorithm optimization, wall-clock runtime improvements, GPU utilization implications, scalability considerations (distributed methods), throughput improvements, algorithm comparison on fixed hardware",
      "Irrelevant Aspects": "No specific focus on large language models, no discussion of inference optimization, no explicit discussion of latency, limited coverage of system-level optimizations",
      "Summary": "This paper presents results from the AlgoPerf competition focused on accelerating neural network training through algorithmic improvements. It highlights Distributed Shampoo as outperforming Adam and introduces Schedule Free AdamW as an effective hyperparameter-free approach. While directly relevant to training optimization, GPU utilization, and throughput, it doesn't specifically address large language models or inference optimization."
    }
  },
  {
    "id": "TDy5Ih78b4",
    "title": "Provence: efficient and robust context pruning for retrieval-augmented generation",
    "abstract": "Retrieval-Augmented Generation improves various aspects of large language models (LLMs) generation,  but suffers from computational overhead caused by long contexts, and the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are limited, and do not present a universal model that would be both _efficient_ and _robust_ in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of  Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work.",
    "authors": [
      "~Nadezhda_Chirkova1",
      "~Thibault_Formal1",
      "~Vassilina_Nikoulina1",
      "~Stéphane_CLINCHANT2"
    ],
    "pdf": "/pdf/cfe32291a501fa2a10511133e5d32ed8017c1611.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for retrieval-augmented generation by proposing an efficient context pruning method. This relates to my expertise in LLM inference optimization, GPU utilization improvement, and reducing latency. The approach claims to enable context pruning with negligible performance impact at almost no computational cost, which aligns perfectly with optimizing LLM systems for better efficiency.",
      "Irrelevant Aspects": "The paper doesn't focus on training optimization of the LLMs themselves, but rather on training a separate context pruner component. It also doesn't address broader ML system architecture or distributed training aspects that might be part of my expertise.",
      "Summary": "Provence introduces an efficient and robust context pruning method for retrieval-augmented generation that addresses computational overhead from long contexts and propagation of irrelevant information. By formulating pruning as sequence labeling and combining it with reranking, the approach claims to improve efficiency without sacrificing performance. This work is highly relevant to LLM inference optimization, GPU utilization, and system performance improvements, though less focused on training optimization aspects of the core LLM models."
    }
  },
  {
    "id": "OQqNieeivq",
    "title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models",
    "abstract": "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA.",
    "authors": [
      "~Fan_Wang20",
      "~Juyong_Jiang3",
      "~Chansung_Park1",
      "~Sunghun_Kim1",
      "~Jing_Tang5"
    ],
    "pdf": "/pdf/b2eb4cdfc0bad2bb2e790450def3b099049688b2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses parameter-efficient fine-tuning (PEFT) methods for LLMs, which directly impacts training optimization, memory usage, and GPU utilization. It introduces KaSA as an improvement over LoRA using singular value decomposition with knowledge-aware singular values. This approach reduces computational overhead and memory requirements during adaptation, which enhances scalability and potentially improves training throughput.",
      "Irrelevant Aspects": "The paper focuses primarily on fine-tuning efficiency rather than inference optimization techniques. It doesn't discuss quantization, distillation, or other runtime optimization strategies specifically aimed at reducing inference latency. The evaluation seems centered on model performance metrics rather than detailed analysis of GPU utilization, throughput, or latency during inference.",
      "Summary": "KaSA introduces a novel parameter-efficient fine-tuning method that leverages singular value decomposition with knowledge-aware singular values to dynamically activate task-relevant knowledge. It demonstrates improved performance over full fine-tuning and 14 popular PEFT baselines across multiple benchmarks. While highly relevant to training optimization and resource efficiency, it doesn't significantly address inference optimization concerns."
    }
  },
  {
    "id": "mUMvr33FTu",
    "title": "CipherPrune:  Efficient and Scalable Private Transformer Inference",
    "abstract": "Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference.  Building on these observations, we propose \\textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\\times$ for 128-token inputs and $10.6\\times$  for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.",
    "authors": [
      "~Yancheng_Zhang1",
      "~Jiaqi_Xue1",
      "~Mengxin_Zheng1",
      "~Mimi_Xie1",
      "~Mingzhe_Zhang2",
      "~Lei_Jiang1",
      "~Qian_Lou1"
    ],
    "pdf": "/pdf/3a5c044b17cc4be8106b61d52e2682629e57e346.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses Transformer inference optimization, which is central to my research interests. It tackles efficiency issues by reducing runtime overhead through token pruning and polynomial degree reduction. It addresses scalability challenges for long-token inputs, claiming significant speedup (6.1x for 128-token inputs and 10.6x for 512-token inputs). The techniques for reducing computational complexity from quadratic scaling are relevant to my work on inference optimization. The focus on improving throughput in specialized inference scenarios aligns with my research goals.",
      "Irrelevant Aspects": "The paper's primary context is cryptographic protocols for privacy-preserving ML, which is a specific application rather than general LLM optimization. The techniques are specialized for encrypted/private inference scenarios, which may not transfer directly to standard inference optimization. The paper doesn't specifically address GPU utilization optimization, which is a key focus of my research. The added complexity of privacy-preserving methods differs from standard LLM inference optimization techniques.",
      "Summary": "CipherPrune proposes an efficient private inference framework with secure encrypted token pruning and polynomial reduction protocols to address runtime overhead and scalability issues in private Transformer inference. While its specialized focus on cryptographic protocols for privacy-preserving ML differs from my general LLM optimization focus, the paper's contributions to reducing computational complexity, improving throughput, and handling scalability challenges for long inputs make it relevant to my research on inference optimization and scalability."
    }
  },
  {
    "id": "bMC1t7eLRc",
    "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
    "abstract": "Data selection is of great significance in  pretraining large language models, given the  variation in quality within the large-scale available training corpora. \nTo achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores.  However, this approach has several limitations. \n(1) Calculating the accurate influence of all available data is time-consuming.\n(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.\nIn this paper, we introduce $\\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.\nTo compute the influence ($i.e.,$ the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. \nFor the diversity, $\\texttt{Quad}$ clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.  Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that $\\texttt{Quad}$ significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.",
    "authors": [
      "~Chi_Zhang62",
      "~Huaping_Zhong2",
      "~Kuan_Zhang1",
      "~Chengliang_Chai1",
      "~Rui_Wang59",
      "~Xinlin_Zhuang2",
      "~Tianyi_Bai1",
      "~Qiu_Jiantao1",
      "~Lei_Cao1",
      "~Ju_Fan1",
      "~Ye_Yuan15",
      "~Guoren_Wang1",
      "~Conghui_He2"
    ],
    "pdf": "/pdf/21e27e7285f7eed44f3f338bab7c1c6e2174df93.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on optimizing pretraining of large language models through efficient data selection, directly addressing training optimization. It introduces computational improvements using Kronecker product acceleration for influence calculation, experiments with 7B models, and claims to achieve state-of-the-art results with low FLOPs consumption. This aligns with interests in computational efficiency and training optimization.",
      "Irrelevant Aspects": "The paper doesn't directly address GPU utilization strategies, parallelization techniques, memory optimization, or infrastructure-specific optimizations. It focuses primarily on data selection methodology rather than hardware-level efficiency improvements. There's no discussion of inference optimization, which is part of my research interest.",
      "Summary": "The paper presents 'Quad,' a data selection method that improves large language model pretraining by balancing quality and diversity. It optimizes influence calculation through Kronecker product acceleration and uses clustering for diversity. While relevant to training optimization through its focus on reducing computational costs, it doesn't directly address GPU utilization, scalability strategies, or inference optimization that are central to my research interests."
    }
  },
  {
    "id": "icDoYdUhRa",
    "title": "Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences",
    "abstract": "Multi-task trade-offs in machine learning can be addressed via Pareto Front Learning (PFL) methods that parameterize the Pareto Front (PF) with a single model. PFL permits to select the desired operational point during inference, contrary to traditional Multi-Task Learning (MTL) that optimizes for a single trade-off decided prior to training. However, recent PFL methodologies suffer from limited scalability, slow convergence, and excessive memory requirements, while exhibiting inconsistent mappings from preference to objective space. We introduce PaLoRA, a novel parameter-efficient method that addresses these limitations in two ways. First, we augment any neural network architecture with task-specific low-rank adapters and continuously parameterize the Pareto Front in their convex hull. Our approach steers the original model and the adapters towards learning general and task-specific features, respectively. Second, we propose a deterministic sampling schedule of preference vectors that reinforces this division of labor, enabling faster convergence and strengthening the validity of the mapping from preference to objective space throughout training. Our experiments show that PaLoRA outperforms state-of-the-art MTL and PFL baselines across various datasets, scales to large networks, reducing the memory overhead $23.8-31.7$ times compared with competing PFL baselines in scene understanding benchmarks.",
    "authors": [
      "~Nikolaos_Dimitriadis1",
      "~Pascal_Frossard1",
      "~François_Fleuret2"
    ],
    "pdf": "/pdf/73f63d5b8bfbdda4c63385b12f9c903cf3273b78.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on efficient multi-task learning using low-rank adapters, which is directly relevant to parameter-efficient fine-tuning in LLMs. It demonstrates significant memory efficiency improvements (23.8-31.7 times) and scalability to large networks, which align with my interest in GPU utilization and scalability. The faster convergence property is also relevant to training optimization efficiency.",
      "Irrelevant Aspects": "The focus on Pareto Front Learning and preference-based multi-task optimization is not directly aligned with my core expertise in LLM optimization. The mathematical details of convex hull parameterization and deterministic sampling schedules, while interesting, are more about algorithmic optimization than system-level optimization. The application to scene understanding benchmarks is also not directly relevant to my LLM focus.",
      "Summary": "PaLoRA introduces a parameter-efficient method for multi-task learning using low-rank adapters and Pareto front learning. It offers significant improvements in memory efficiency and scalability to large networks, with faster convergence. While the focus on multi-task learning and Pareto optimization is somewhat outside my primary LLM domain, the techniques and efficiency improvements are highly relevant to my interest in model optimization, scalability, and resource utilization."
    }
  },
  {
    "id": "7nyJBVCTGQ",
    "title": "LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning",
    "abstract": "We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks.",
    "authors": [
      "~Minyoung_Kim2",
      "~Timothy_Hospedales1"
    ],
    "pdf": "/pdf/12f9094c468b3d890fa51b038ec3e1004484358e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Parameter-efficient fine-tuning (PEFT) for large models which impacts GPU utilization during fine-tuning",
        "Focus on LoRA modules, a popular technique for efficient adaptation of large language models",
        "Novel SGLD-Gibbs sampling algorithm with computational efficiency focus",
        "Online EM algorithm for maintaining posterior representation efficiently",
        "Applications to NLP benchmarks and multi-task learning scenarios",
        "Addresses scalability challenges when fine-tuning across many tasks"
      ],
      "Irrelevant Aspects": [
        "Primary focus on Bayesian modeling rather than system optimization",
        "Emphasis on generalization to unseen tasks rather than throughput/latency optimization",
        "Inclusion of vision applications outside my language model focus",
        "Theoretical aspects of meta-learning not directly tied to system performance",
        "Gaussian mixture representation focus more on statistical modeling than computational optimization"
      ],
      "Summary": "LiFT introduces a Bayesian approach to parameter-efficient fine-tuning using LoRA modules for multi-task learning scenarios. While its focus on PEFT methods like LoRA is relevant to GPU utilization optimization during fine-tuning, the paper primarily emphasizes improving generalization through meta-learning rather than direct system performance metrics like throughput and latency. The proposed efficient inference algorithms could have practical implications for optimization, but the core contribution appears to be methodological rather than system-focused."
    }
  },
  {
    "id": "gkUyYcY1W9",
    "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
    "abstract": "Long-context Large Language Models (LLMs) have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBENCH (SharedContextBENCH), a comprehensive benchmark for evaluating long-context methods from a KV cache centric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, and 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With SCBench, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs (Codestal-Mamba), Mamba-Attention hybrids (Jamba-1.5-Mini), and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on six Transformer-based long-context LLMs: Llama-3.1-8B/70B, Qwen2.5-72B/32B, Llama-3-8B-262K, and GLM-4-9B. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios.",
    "authors": [
      "~YUCHENG_LI2",
      "~Huiqiang_Jiang2",
      "~Qianhui_Wu1",
      "~Xufang_Luo1",
      "~Surin_Ahn1",
      "~Chengruidong_Zhang1",
      "~Amir_H._Abdi1",
      "~Dongsheng_Li2",
      "~Jianfeng_Gao1",
      "~Yuqing_Yang1",
      "~Lili_Qiu3"
    ],
    "pdf": "/pdf/264842d66ae06cc7f27d1881a403f6d8816aca4d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on KV cache optimization for long-context LLMs, which is directly relevant to inference optimization. It analyzes various efficient methods including sparse attention, KV cache dropping, quantization, and retrieval - all crucial for improving GPU utilization and scalability. The evaluation considers real-world multi-turn scenarios with KV cache reuse, and examines memory efficiency patterns in Transformer-based models. The findings about dynamic sparsity and layer-level optimization in hybrid architectures provide valuable insights for inference optimization.",
      "Irrelevant Aspects": "The paper is primarily benchmark-focused rather than optimization technique development. While it evaluates optimization methods, it doesn't propose novel optimization algorithms. Some emphasis is on specific evaluation tasks and benchmark construction rather than deep GPU utilization analysis. The attention distribution shift finding is interesting but less directly applicable to optimization strategies.",
      "Summary": "SCBench is a highly relevant paper that provides a comprehensive KV cache-centric analysis of long-context inference methods. It evaluates eight categories of optimization solutions across multiple Transformer-based models, revealing important insights about memory-computation tradeoffs, sparsity patterns, and multi-turn scenario performance. While primarily a benchmark paper, its extensive evaluation of KV cache optimization techniques and analysis of their effectiveness in real-world inference scenarios makes it extremely valuable for research on LLM inference optimization."
    }
  },
  {
    "id": "wUtXB43Chi",
    "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
    "abstract": "The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $\\mathcal{O}(N^2)$ memory dependency and reducing attention latency through IO-aware memory optimizations. However, its native support for certain attention mask types is limited, and it does not inherently accommodate more complex masking requirements. Previous approaches resort to using dense masks with $\\mathcal{O}(N^2)$ memory complexity, leading to inefficiencies. In this paper, we propose \\ours{}, an extension of FlashAttention that introduces a column-wise sparse representation of attention masks. This approach efficiently represents a wide range of mask types and facilitates the development of optimized kernel implementations. By adopting this novel representation, \\ours{} achieves linear memory complexity $\\mathcal{O}(N)$, making it suitable for modeling long-context sequences. Moreover, this representation enables kernel optimizations that eliminate unnecessary computations by leveraging sparsity in the attention mask, without sacrificing computational accuracy, resulting in higher computational efficiency. We evaluate \\ours{}'s performance in fine-tuning and alignment training of LLMs such as SFT, LoRA, DPO, and RM. \\ours{} achieves significant throughput improvements, with end-to-end speedups ranging from 1.65x to 3.22x compared to existing FlashAttention dense method. Additionally, our kernel-level comparisons demonstrate that \\ours{} surpasses the latest counterpart, FlexAttention, by 12.1\\% to 60.7\\% in terms of kernel TFLOPs/s, achieving 37.8\\% to 62.3\\% of the theoretical maximum FLOPs/s on the A100 GPU. The code is open-sourced on PaddlePaddle\\footnote{\\url{https://github.com/PaddlePaddle/Paddle}} and integrated into PaddleNLP\\footnote{\\url{https://github.com/PaddlePaddle/PaddleNLP}}, supporting models with over 100 billion parameters for contexts extending up to 128K tokens.",
    "authors": [
      "~Guoxia_Wang1",
      "~Jinle_Zeng1",
      "~Xiyuan_Xiao1",
      "~Siming_Wu1",
      "~Jiabin_Yang1",
      "~Lujing_Zheng1",
      "~Zeyu_Chen3",
      "~Jiang_Bian5",
      "~Dianhai_Yu3",
      "~Haifeng_Wang3"
    ],
    "pdf": "/pdf/a7b733270042cd3e14738309159a5716bac441f5.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Focus on computational and memory optimization for attention mechanisms; Addresses quadratic scaling issue in transformers; Builds upon FlashAttention which is a key optimization for LLMs; Reduces memory complexity from O(N²) to O(N) for masking; Implements kernel optimizations to improve GPU utilization; Reports significant throughput improvements (1.65x to 3.22x); Evaluates on LLM training scenarios (SFT, LoRA, DPO, RM); Provides specific performance metrics on A100 GPUs; Supports large models with long contexts up to 128K tokens",
      "Irrelevant Aspects": "Specific to the PaddlePaddle framework implementation; Limited to masking operations rather than complete training pipeline optimization",
      "Summary": "FlashMask extends FlashAttention with an efficient column-wise sparse representation of attention masks, reducing memory complexity from quadratic to linear and improving computational efficiency. The paper demonstrates significant throughput improvements in LLM fine-tuning scenarios and achieves 37.8%-62.3% of theoretical maximum FLOPs on A100 GPUs, making it highly relevant for LLM training optimization and GPU utilization."
    }
  },
  {
    "id": "dsP91M4hDL",
    "title": "TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice",
    "abstract": "The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters.\nThe effectiveness of MoE models depends primarily on their routing mechanisms, with the widely adopted Top-K routing scheme used for activating experts.\nHowever, the Top-K scheme has notable limitations,\nincluding unnecessary activations and underutilization of experts.\nIn this work, \nrather than modifying the routing mechanism as done in previous studies,\nwe propose the Ternary Choice MoE (TC-MoE),\na novel approach that expands the expert space by applying the ternary set {-1, 0, 1} to each expert.\nThis expansion allows more efficient and effective expert activations without incurring significant computational costs.\nAdditionally, \ngiven the unique characteristics of the expanded expert space,\nwe introduce a new load balance loss and reward loss to ensure workload balance and achieve a flexible trade-off between effectiveness and efficiency.\nExtensive experiments demonstrate that TC-MoE achieves an average improvement of over 1.1% compared with traditional approaches,\nwhile reducing the average number of activated experts by up to 9%.\nThese results confirm that TC-MoE effectively addresses the inefficiencies of conventional routing schemes,\noffering a more efficient and scalable solution for MoE-based large language models.\nCode and models are available at https://github.com/stiger1000/TC-MoE.",
    "authors": [
      "~Shen_Yan5",
      "~Xingyan_Bin1",
      "~Sijun_Zhang1",
      "~Yisen_Wang1",
      "~Zhouchen_Lin1"
    ],
    "pdf": "/pdf/c530b8fc32e10429d210060fa063638233b4a81e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses optimization of Mixture of Experts (MoE) architecture which is critical for scaling large language models. It focuses on reducing computational overhead and improving efficiency by introducing a new expert selection mechanism that activates fewer experts (up to 9% reduction). This directly impacts GPU utilization, throughput, and latency during both training and inference. The paper also introduces load balancing mechanisms which are essential for efficient distributed processing across GPUs. The approach of expanding expert space with ternary choices {-1, 0, 1} offers a novel way to achieve better resource utilization in MoE models.",
      "Irrelevant Aspects": "The paper does not focus on specific GPU implementation details or hardware-level optimizations. There's limited discussion on specific throughput or latency benchmarks that would directly quantify improvements in these metrics. The abstract doesn't mention distributed training strategies or specific hardware utilization techniques beyond the expert selection mechanism. The focus is primarily on the algorithmic improvement to MoE rather than system-level implementation optimizations.",
      "Summary": "TC-MoE presents a novel approach to optimize Mixture of Experts models by expanding the expert space with ternary choices, resulting in more efficient expert activations. The method improves model performance while reducing the number of activated experts, which translates to better computational efficiency and potential improvements in GPU utilization and inference latency. The paper's focus on load balancing and efficient expert selection makes it highly relevant to large language model optimization, though it lacks specific details on GPU-level implementations or throughput benchmarks."
    }
  },
  {
    "id": "xDrFWUmCne",
    "title": "Learning to Discretize Denoising Diffusion ODEs",
    "abstract": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at https://github.com/vinhsuhi/LD3.",
    "authors": [
      "~Vinh_Tong1",
      "~Dung_Trung_Hoang1",
      "~Anji_Liu1",
      "~Guy_Van_den_Broeck1",
      "~Mathias_Niepert1"
    ],
    "pdf": "/pdf/0dba9643763af4085d7629b1440fc45861444d86.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "The paper addresses computational efficiency in neural models, which is central to optimizing GPU utilization",
        "LD3 reduces the number of neural function evaluations (NFEs), directly impacting throughput and latency",
        "The approach focuses on inference optimization, a key area of interest for efficient model deployment",
        "The framework is lightweight and requires minimal additional training time (5-10 minutes), showing efficient optimization strategies",
        "The method improves sampling efficiency across multiple pre-trained models, demonstrating scalability",
        "The approach maintains generation quality while reducing computational cost, addressing a key trade-off in optimization"
      ],
      "Irrelevant Aspects": [
        "The paper focuses on diffusion models for image generation rather than large language models specifically",
        "The optimization techniques are targeted at vision tasks rather than natural language processing",
        "The evaluation metrics (FID) are specific to image generation and not applicable to language models",
        "The paper does not address training efficiency of the diffusion models themselves, only the sampling process",
        "The implementation details specific to pixel-space and latent-space DPMs may not transfer to LLM architectures"
      ],
      "Summary": "The paper 'Learning to Discretize Denoising Diffusion ODEs' presents LD3, a lightweight framework that optimizes the sampling process for diffusion models by learning optimal time discretization. While not directly addressing large language models, it provides valuable insights into inference optimization, reducing computational overhead, and improving efficiency in generative models. The approach reduces neural function evaluations while maintaining quality, which aligns with goals of higher throughput and lower latency. However, the focus on image generation rather than language models limits its direct relevance to LLM optimization, making it moderately relevant to my specific research interests."
    }
  },
  {
    "id": "0fJfVOSUra",
    "title": "ThunderKittens: Simple, Fast, and $\\textit{Adorable}$ Kernels",
    "abstract": "The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse capabilities of GPUs suggests we might we need a wide variety of techniques to achieve high performance. However, our work explores if a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like operations, (2) at the thread-block level, we provide templates for asynchronously overlapping operations, and (3) at the grid-level, TK helps hide block launch, tear-down, and memory costs. We show the value of TK by providing simple & diverse kernels that match or outperform prior art. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40$\\% on attention backwards, $8\\times$ on state space models, and $14\\times$ on linear attention.",
    "authors": [
      "~Benjamin_Frederick_Spector1",
      "~Simran_Arora1",
      "~Aaryan_Singhal1",
      "~Arjun_Parthasarathy1",
      "~Daniel_Y_Fu1",
      "~Christopher_Re1"
    ],
    "pdf": "/pdf/f4b2b2d3f597357551880dae1c1a4286791aadc5.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper presents ThunderKittens, a framework for optimizing AI kernels on GPUs which directly addresses GPU utilization and performance optimization. It claims to match or outperform established solutions like CuBLAS and FlashAttention-3 on GEMM and attention operations. The framework addresses all three levels of GPU hierarchy, which is critical for achieving optimal utilization and scalability. It shows significant performance improvements on attention mechanisms (10-40% on backwards), which are crucial components in LLMs. The work directly targets the hardware-software bottleneck in AI systems.",
      "Irrelevant Aspects": "While the paper focuses on kernel optimization, it doesn't specifically address end-to-end LLM training pipelines or distributed training strategies. The work appears more focused on individual kernel performance rather than system-level optimizations. There's limited discussion of how these optimizations translate to real-world training time reductions or cost savings in large-scale deployments.",
      "Summary": "ThunderKittens presents a framework for writing performant AI kernels that simplifies the process of mapping AI operations to GPU hardware. It provides abstractions at the warp-level (16x16 matrix tiles), thread-block level (asynchronous operations), and grid-level (launch/tear-down optimization). The framework demonstrates performance improvements across several important operations including attention mechanisms, matching or outperforming established solutions like CuBLAS and FlashAttention-3."
    }
  },
  {
    "id": "EQgEMAD4kv",
    "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
    "abstract": "Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper, we introduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach that frames KV cache eviction as a ``cake-slicing problem.''\nCAKE assesses layer-specific preferences by considering attention dynamics in both spatial and temporal dimensions, allocates rational cache size for layers accordingly, and manages memory constraints in a cascading manner. This approach enables a global view of cache allocation, adaptively distributing resources across diverse attention mechanisms while maintaining memory budgets.\nCAKE also employs a new eviction indicator that considers the shifting importance of tokens over time, addressing limitations in existing methods that overlook temporal dynamics.\nComprehensive experiments on LongBench and NeedleBench show that CAKE maintains model performance with only 3.2\\% of the KV cache and consistently outperforms current baselines across various models and memory constraints, particularly in low-memory settings. Additionally, CAKE achieves over 10$\\times$ speedup in decoding latency compared to full cache when processing contexts of 128K tokens with FlashAttention-2. Our code is available at https://github.com/antgroup/cakekv.",
    "authors": [
      "~Ziran_Qin1",
      "~Yuchen_Cao5",
      "~Mingbao_Lin1",
      "~Wen_Hu1",
      "~Shixuan_Fan1",
      "~Ke_Cheng3",
      "~Weiyao_Lin1",
      "~Jianguo_Li2"
    ],
    "pdf": "/pdf/b1d9910c4ac08cd3ef1dfd4459a1f3196028444c.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "KV cache eviction for LLM inference optimization, addressing memory efficiency with only 3.2% of KV cache while maintaining performance, achieving 10× speedup in decoding latency for long sequences (128K tokens), adaptive resource allocation across layers with different attention patterns, addressing scalability challenges for long context processing",
      "Irrelevant Aspects": "Limited focus on training optimization, minimal discussion of throughput improvements, no direct analysis of GPU utilization metrics",
      "Summary": "CAKE presents a novel KV cache eviction approach that frames the problem as a 'cake-slicing problem,' allocating resources rationally across layers based on their attention patterns. It considers both spatial and temporal dimensions in attention dynamics, manages memory constraints cascadingly, and introduces a new eviction indicator that accounts for shifting token importance over time. The method shows significant performance gains with minimal cache usage and substantial latency improvements, making it highly relevant for LLM inference optimization, particularly for long sequences."
    }
  },
  {
    "id": "VxvnV6slP0",
    "title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model",
    "abstract": "The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost. Existing MoE methods in LVLM encourage different experts to specialize in different tokens, and they usually employ a router to predict the routing of each token. However, the router is not optimized concerning distinct parameter optimization directions generated from tokens within an expert. This may lead to severe interference between tokens within an expert. To address this problem, we propose to use the token-level gradient analysis to Solving Token Gradient Conflict (STGC) in this paper. Specifically, we first use token-level gradients to identify conflicting tokens in experts. After that, we add a regularization loss tailored to encourage conflicting tokens routing from their current experts to other experts, for reducing interference between tokens within an expert. Our method can serve as a plug-in for diverse LVLM methods, and extensive experimental results demonstrate its effectiveness. demonstrate its effectiveness. \nThe code will be publicly available at https://github.com/longrongyang/STGC.",
    "authors": [
      "~Longrong_Yang2",
      "~Dong_Shen1",
      "~Chaoxiang_Cai1",
      "~Fan_Yang30",
      "~Tingting_Gao1",
      "~Di_ZHANG3",
      "~Xi_Li2"
    ],
    "pdf": "/pdf/9c71280305feb6c5a47aa01f36ffbe856fd36bba.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on inference optimization through Mixture-of-Experts (MoE) approach which activates fewer parameters during inference, reducing computational costs. Addresses training optimization by solving token gradient conflicts that cause interference between tokens within experts. Directly impacts GPU utilization, scalability, throughput and latency concerns. The method is designed as a plug-in for diverse LVLM methods, suggesting practical applicability in real ML systems.",
      "Irrelevant Aspects": "The paper focuses specifically on Large Vision-Language Models rather than pure Large Language Models. Limited discussion on explicit GPU utilization metrics or detailed throughput/latency measurements. The vision-language aspect may introduce nuances not directly applicable to text-only models.",
      "Summary": "This paper presents a method to solve token gradient conflict in MoE for LVLMs by using token-level gradient analysis to identify and reroute conflicting tokens. The approach improves both training efficiency and inference performance by reducing parameter activation and interference, directly addressing several optimization aspects relevant to my research in LLM training and inference optimization, GPU utilization, and system scalability."
    }
  },
  {
    "id": "eW4yh6HKz4",
    "title": "CBQ: Cross-Block Quantization for Large Language Models",
    "abstract": "Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency.",
    "authors": [
      "~Xin_Ding4",
      "~Xiaoyu_Liu6",
      "~Zhijun_Tu1",
      "~Yun_Zhang5",
      "~Wei_Li60",
      "~Jie_Hu8",
      "~Hanting_Chen1",
      "~Yehui_Tang1",
      "~Zhiwei_Xiong1",
      "~Baoqun_Yin1",
      "~Yunhe_Wang1"
    ],
    "pdf": "/pdf/125e705802d355656a73ec2253d6094be3f61719.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Post-training quantization for LLMs, Ultra-low bit precision quantization (W4A4, W4A8, W2A16), Addressing performance degradation at low precision, Cross-block reconstruction technique, Efficient quantization of large models (4.3 hours for LLAMA1-65B), Improving quantization accuracy, Adaptive LoRA-Rounding technique, Potential impact on GPU utilization and inference throughput",
      "Irrelevant Aspects": "Focus primarily on quantization rather than other optimization techniques, Limited discussion of specific GPU architecture optimizations, No explicit mention of latency or throughput measurements in abstract, Primarily addresses post-training quantization rather than training optimization",
      "Summary": "The paper proposes CBQ, a cross-block reconstruction-based post-training quantization method for LLMs that addresses performance degradation at ultra-low precision. It leverages cross-block dependencies and integrates an adaptive LoRA-Rounding technique to improve quantization accuracy. The method demonstrates superior performance in low-bit quantization (W4A4, W4A8, W2A16) compared to existing methods across various LLMs and datasets. The approach is efficient, requiring only 4.3 hours to quantize a 4-bit LLAMA1-65B model. This work is highly relevant to inference optimization for LLMs, which directly impacts GPU utilization, scalability, and potentially throughput and latency."
    }
  },
  {
    "id": "cZWCjan02B",
    "title": "Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond",
    "abstract": "While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length.\nSeveral subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution sequence models (LCSMs), such as Hyena, address this issue at training time but remain quadratic during inference. We propose a method for speeding up LCSMs' exact inference to quasilinear time, identify the key properties that make this possible, and propose a general framework that exploits these. Our approach, inspired by previous work on relaxed polynomial interpolation, is based on a tiling which helps decrease memory movement and share computation. It has the added benefit of allowing for almost complete parallelization across layers of the position-mixing part of the architecture. Empirically, we provide a proof of concept implementation for Hyena, which gets up to $7.8\\times$ end-to-end improvement over standard inference by improving $110\\times$ within the position-mixing part.",
    "authors": [
      "~Costin-Andrei_Oncescu1",
      "~Sanket_Purandare1",
      "~Stratos_Idreos1",
      "~Sham_M._Kakade1"
    ],
    "pdf": "/pdf/fafa4ac13ee1c370fae90f0cf158ae71ed4dca2e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on inference optimization, addresses computational efficiency and time complexity, improves GPU utilization through parallelization, reduces memory movement, provides empirical performance improvements (7.8× improvement), addresses scalability for long sequences",
      "Irrelevant Aspects": "Focuses primarily on Long Convolution Sequence Models (LCSMs) rather than transformers, implementation is specifically for Hyena though claims to be a general framework",
      "Summary": "This paper directly addresses inference optimization for sequence models, presenting a method to reduce computational complexity from quadratic to near-linear time. The focus on reducing memory movement, improving parallelization, and achieving substantial empirical performance improvements aligns well with goals of better GPU utilization and scalability. While focused on LCSMs rather than transformers, the principles could potentially be applied more broadly."
    }
  },
  {
    "id": "BChpQU64RG",
    "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance.  While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradient norms across layers. This allows all parts of the network—both shallow and deep layers—to contribute effectively to training. Extensive experiments with various model sizes demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning, highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN.",
    "authors": [
      "~Pengxiang_Li2",
      "~Lu_Yin1",
      "~Shiwei_Liu2"
    ],
    "pdf": "/pdf/6d1df929c93e610ca2f2d42d8f89d81939f022a0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses a training optimization technique for Large Language Models (LLMs), improves training efficiency by ensuring more effective utilization of all layers, promotes balanced gradient norms throughout the network, enhances the overall quality of LLM pre-training, and potentially enhances model capacity without increasing model size, which could lead to better GPU utilization during training.",
      "Irrelevant Aspects": "The paper doesn't directly address inference optimization, throughput or latency improvements, GPU utilization or scalability during inference, and focuses on model architecture rather than system-level optimizations for deployment.",
      "Summary": "The paper introduces Mix-LN, a novel normalization technique that combines Pre-Layer Normalization and Post-Layer Normalization to address the issue of ineffective deeper layers in LLMs. While this addresses training optimization, which is part of my research interest, it doesn't directly focus on the system-level aspects like GPU utilization, scalability, throughput, or latency improvements that are also important components of my research."
    }
  },
  {
    "id": "LgzRo1RpLS",
    "title": "MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba",
    "abstract": "The inherent quadratic complexity of the attention mechanism in transformer models has driven the research community to explore alternative architectures with sub-quadratic complexity, such as state-space models. Mamba has established itself as a leading model within this emerging paradigm, achieving state-of-the-art results in various language modeling benchmarks. However, despite its impressive performance, Mamba's effectiveness is limited by its pre-training context length, resulting in a pronounced degradation when the model is tasked with handling longer contexts. Our investigation reveals that Mamba's inability to generalize effectively to long contexts is primarily due to the out-of-distribution (OOD) discretization steps. To address this critical limitation, we introduce _**MambaExtend**_, a novel framework designed to significantly enhance the context extension capabilities of Mamba. Specifically, MambaExtend leverages a _**training-free**_ approach to calibrate _only_ the scaling factors of discretization modules for different layers. We demonstrate both gradient-based and gradient-free zeroth-order optimization to learn the optimal scaling factors for each Mamba layer, requiring orders of magnitude fewer updates as opposed to the parameter fine-tuning-based alternatives. \nUsing this approach, we achieve a training-free context extension of up to 32x, expanding the context from 2k to 64k tokens with minimal increases in perplexity. In contrast to existing fine-tuning methods, MambaExtend selectively calibrates the scaling factors, requiring up to $\\mathbf{5.42 * 10^6} \\times$ fewer parameter updates and incurring up to $\\mathbf{3.87} \\times$ lower peak memory usage, while delivering comparable or superior long-context performance across multiple tasks. Codes and checkpoints are available here$^1$.",
    "authors": [
      "~Seyedarmin_Azizi1",
      "~Souvik_Kundu2",
      "~Mohammad_Erfan_Sadeghi1",
      "~Massoud_Pedram1"
    ],
    "pdf": "/pdf/f1f70d718d90155c62d9c0fae5c8af6c4eb7400a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents a training-free approach to extend context window in Mamba models, achieving significant reduction in parameter updates (5.42*10^6× fewer) and lower memory usage (3.87× lower) while maintaining performance. It addresses efficiency in adapting models without full retraining, which aligns with optimization of computational resources and GPU utilization. The approach directly contributes to more efficient inference for long contexts by avoiding the quadratic complexity of attention mechanisms.",
      "Irrelevant Aspects": "The method is specifically tailored to Mamba architecture rather than providing general optimization techniques applicable across different model types. It lacks detailed discussion on GPU-level optimization strategies and doesn't explicitly measure inference latency. The paper doesn't provide extensive analysis of scalability to distributed computing environments or extremely large models.",
      "Summary": "MambaExtend introduces a training-free framework to extend the context window of Mamba models from 2k to 64k tokens by calibrating discretization scaling factors layer by layer. The approach uses both gradient-based and gradient-free zeroth-order optimization to learn optimal factors with orders of magnitude fewer computational requirements than fine-tuning. It achieves comparable long-context performance while drastically reducing parameter updates and memory usage, contributing to more efficient model deployment without additional training."
    }
  },
  {
    "id": "LQzN6TRFg9",
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos that align seamlessly with text prompts, with a frame rate of 16 fps and resolution of 768 x 1360 pixels. \nPrevious video generation models often struggled with limited motion and short durations.\nIt is especially difficult to generate videos with coherent narratives based on text. \nWe propose several designs to address these issues. \nFirst, we introduce a 3D Variational Autoencoder (VAE) to compress videos across spatial and temporal dimensions, enhancing both the compression rate and video fidelity. \nSecond, to improve text-video alignment, we propose an expert transformer with expert adaptive LayerNorm to facilitate the deep fusion between the two modalities.\nThird, by employing progressive training and multi-resolution frame packing, CogVideoX excels at generating coherent, long-duration videos with diverse shapes and dynamic movements. \nIn addition, we develop an effective pipeline that includes various pre-processing strategies for text and video data.\nOur innovative video captioning model significantly improves generation quality and semantic alignment. \nResults show that  CogVideoX achieves state-of-the-art performance in both automated benchmarks and human evaluation.\nWe publish the code and model checkpoints of CogVideoX along with our VAE model and video captioning model at https://github.com/THUDM/CogVideo.",
    "authors": [
      "~Zhuoyi_Yang1",
      "~Jiayan_Teng1",
      "~Wendi_Zheng1",
      "~Ming_Ding1",
      "~Shiyu_Huang2",
      "~Jiazheng_Xu1",
      "~Yuanming_Yang1",
      "~Wenyi_Hong1",
      "~Xiaohan_Zhang6",
      "~Guanyu_Feng1",
      "~Da_Yin1",
      "~Yuxuan.Zhang16",
      "~Weihan_Wang2",
      "~Yean_Cheng1",
      "~Bin_Xu1",
      "~Xiaotao_Gu1",
      "~Yuxiao_Dong1",
      "~Jie_Tang1"
    ],
    "pdf": "/pdf/65974c11461d3a61ea8e24ab562e77f86183402f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Large-scale model architecture requiring substantial computational resources, transformer-based approach similar to large language models, progressive training strategies for efficient training, expert transformer architecture with potential computational benefits, multi-resolution frame packing as an optimization technique",
      "Irrelevant Aspects": "Primary focus on video generation quality rather than computational efficiency metrics, limited discussion of GPU utilization optimization, insufficient coverage of throughput and latency measurements, 3D VAE component primarily for video compression not computational optimization",
      "Summary": "CogVideoX presents a large-scale text-to-video diffusion transformer model with several architectural innovations that likely required computational optimizations. The expert transformer design, progressive training, and multi-resolution processing techniques have some relevance to computational efficiency, but the paper primarily focuses on video generation quality rather than the specific optimization concerns of GPU utilization, scalability, throughput, and latency that are central to my research interest."
    }
  },
  {
    "id": "8EfxjTCg2k",
    "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
    "abstract": "Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources.\nWhile recent compression methods based on low-rank matrices show potential\nsolutions, they often suffer from significant loss of accuracy or introduce substantial\noverhead in parameters and inference time. In this paper, we introduce Modular De-\ncomposition (MoDeGPT), a new, efficient, and structured compression framework\nthat overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu-\ntive subcomponents within Transformer blocks, reduces hidden dimensions through\noutput reconstruction on a larger structural scale than conventional low-rank meth-\nods, and repurposes three classical matrix decomposition algorithms—Nyström\napproximation, CR decomposition, and SVD—to ensure bounded errors in our\nnovel decomposition approach. Our experiments show that MoDeGPT, without\nrelying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a\n98% reduction in compute costs when compressing a 13B-parameter model. On\nLLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performance\nwith compression rates of 25-30%. The compression process can be completed on\na single GPU in a few hours, boosting inference throughput by up to 46%.",
    "authors": [
      "~Chi-Heng_Lin1",
      "~Shangqian_Gao1",
      "~James_Seale_Smith1",
      "~Abhishek_Patel2",
      "~Shikhar_Tuli1",
      "~Yilin_Shen1",
      "~Hongxia_Jin1",
      "~Yen-Chang_Hsu1"
    ],
    "pdf": "/pdf/d33cd5b9c19cf8b57d3c83994ff98ee5b59605f1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on LLM compression for better inference, claims 46% boost in inference throughput, demonstrates scalability with 13B-parameter models, achieves 98% reduction in compute costs, efficient compression process on single GPU, maintains 90-95% performance at 25-30% compression rates",
      "Irrelevant Aspects": "Limited focus on training optimization aspects, no mention of distributed systems, doesn't specifically address GPU hardware utilization, lacks information on memory management strategies, no discussion of batching or request scheduling techniques",
      "Summary": "MoDeGPT presents a structured compression framework for large language models that jointly decomposes consecutive Transformer components. The method leverages classical matrix decomposition algorithms (Nyström approximation, CR decomposition, and SVD) to achieve bounded errors while reducing model size by 25-30% and maintaining 90-95% of zero-shot performance. The compression can be completed on a single GPU in hours and leads to up to 46% increase in inference throughput."
    }
  },
  {
    "id": "ud8FtE1N4N",
    "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
    "abstract": "Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). \nWhile many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. \nIn this work, we present the first systematic exploration of optimal sparse pre-training configurations for LLMs through an examination of 80 unique pruning schedules across different sparsity levels and training durations.\nWe find that initiating pruning at 25\\% of total training compute and concluding at 75\\% achieves near-optimal final evaluation loss.\nThese findings provide valuable insights for efficient and effective sparse pre-training of LLMs.\nFurthermore, we propose a new scaling law that modifies the Chinchilla scaling law to use the average parameter count over pre-training.\nThrough empirical and theoretical validation, we demonstrate that this modified scaling law accurately models evaluation loss for both sparsely and densely pre-trained LLMs, unifying scaling laws across pre-training paradigms.\nOur findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.",
    "authors": [
      "~Tian_Jin1",
      "~Ahmed_Imtiaz_Humayun1",
      "~Utku_Evci1",
      "~Suvinay_Subramanian1",
      "~Amir_Yazdanbakhsh1",
      "~Dan_Alistarh7",
      "~Gintare_Karolina_Dziugaite1"
    ],
    "pdf": "/pdf/9321413980d3350f66b0647695c7b78d1d31d3ad.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM training optimization through sparse pre-training, which can improve GPU utilization and reduce computational demands. It proposes a new scaling law that models evaluation loss for both sparse and dense pre-training, contributing to scalability understanding. Most importantly, it highlights inference benefits through reduced model size, which directly relates to improving throughput and reducing latency - key aspects of my research focus.",
      "Irrelevant Aspects": "The paper appears more focused on theoretical aspects of scaling laws rather than implementation details of GPU-specific optimizations. There's limited discussion of distributed training strategies or specific techniques for maximizing GPU utilization. The work doesn't address memory management strategies or hardware-level optimizations that would be directly relevant to my research.",
      "Summary": "This paper investigates sparse pre-training for LLMs as an optimization approach to reduce computational demands. Through extensive experiments with 80 unique pruning schedules, it identifies an optimal pruning window (25-75% of training compute). The key contribution is a modified scaling law based on average parameter count during pre-training, which unifies sparse and dense models' scaling behavior. The paper demonstrates that sparse pre-training achieves model quality comparable to dense training while enabling computational savings during inference due to reduced model size, making it highly relevant to my research on training and inference optimization for LLMs."
    }
  },
  {
    "id": "EjJGND0m1x",
    "title": "MIND over Body: Adaptive Thinking using Dynamic Computation",
    "abstract": "While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance.\n  Yet, these parameters are used inefficiently as the networks employ the same amount of computation for inputs of the same size, regardless of the input's complexity.\n  We address this inefficiency by introducing self-introspection capabilities to the network, enabling it to adjust the number of used parameters based on the internal representation of the task and adapt the computation time based on the task complexity.\n  This enables the network to adaptively reuse parameters across tasks, dynamically adjusting the computational effort to match the complexity of the input.\n  We demonstrate the effectiveness of this method on language modeling and computer vision tasks.\n  Notably, our model achieves 96.62\\% accuracy on ImageNet with just a three-layer network, surpassing much larger ResNet-50 and EfficientNet. When applied to a transformer architecture, the approach achieves 95.8\\%/88.7\\% F1 scores on the SQuAD v1.1/v2.0 datasets at negligible parameter cost.\n  These results showcase the potential for dynamic and reflective computation, contributing to the creation of intelligent systems that efficiently manage resources based on input data complexity.",
    "authors": [
      "~Mrinal_Mathur1",
      "~Barak_A._Pearlmutter1",
      "~Sergey_Plis1"
    ],
    "pdf": "/pdf/1fe53a8f1d346f03b1ec2eb57745e80166c8f821.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Dynamic computation for inference optimization, adaptive parameter usage for improved GPU utilization, application to transformer architectures for LLMs, and potential latency/throughput improvements through efficient resource allocation.",
      "Irrelevant Aspects": "Focus on computer vision tasks outside language model domain, lack of multi-GPU scalability discussion, and no clear integration path with existing LLM infrastructure.",
      "Summary": "This paper presents a dynamic computation approach that enables neural networks to adaptively adjust parameter usage and computation time based on input complexity. Its application to transformer architectures is relevant to LLM optimization and could improve GPU utilization, reduce latency, and increase throughput. However, it lacks discussion of system-level scalability and integration challenges critical for practical deployment in large-scale LLM systems."
    }
  },
  {
    "id": "dSneEp59yX",
    "title": "Training Free Exponential Context Extension via Cascading KV Cache",
    "abstract": "The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.",
    "authors": [
      "~Jeffrey_Willette1",
      "~Heejun_Lee1",
      "~Youngwan_Lee1",
      "~Myeongjae_Jeon3",
      "~Sung_Ju_Hwang1"
    ],
    "pdf": "/pdf/65cbdb90f87a561732cdcf35c6964317e03a44d4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization through efficient KV cache management, linear complexity attention mechanism, significant latency reduction (6.8x faster prefill stage), improved scalability for long sequences, resource utilization efficiency, real-time application enablement",
      "Irrelevant Aspects": "Explicitly 'training free' approach doesn't address training optimization, lacks specific GPU utilization metrics and analysis",
      "Summary": "This paper presents a novel cascading KV cache mechanism that extends transformer context windows without additional training. It achieves linear inference complexity while selectively retaining relevant tokens, significantly reducing latency and improving scalability for long sequences. The approach focuses on inference optimization with practical applications in resource-constrained environments, though it doesn't address training optimization or provide specific GPU utilization details."
    }
  },
  {
    "id": "jj7b3p5kLY",
    "title": "The AdEMAMix Optimizer: Better, Faster, Older",
    "abstract": "Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations which lose their relevance as the iterate moves along the loss landscape. This work questions the use of a single EMA to accumulate past gradients and empirically demonstrates how this choice can be sub-optimal: a single EMA cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients. Building on this observation, we propose AdEMAMix, a simple modification of the Adam optimizer with a mixture of two EMAs to better take advantage of past gradients. Our experiments on language modeling and image classification show---quite surprisingly---that gradients can stay relevant for tens of thousands of steps. They help to converge faster, and often to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B tokens performs comparably to an AdamW model trained on $197$B tokens ($+95\\%$). Moreover, our method significantly slows-down model forgetting during training. Our work motivates further exploration of different types of functions to leverage past gradients, beyond EMAs.",
    "authors": [
      "~Matteo_Pagliardini1",
      "~Pierre_Ablin2",
      "~David_Grangier1"
    ],
    "pdf": "/pdf/5404db858bad54eb0d399082e6ef0227b45b3db6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces AdEMAMix, a modification of the Adam optimizer that improves training efficiency for large language models. It demonstrates significant performance gains, showing a 1.3B parameter LLM trained with AdEMAMix on 101B tokens performs comparably to an AdamW model trained on 197B tokens (+95%). The method helps models converge faster and reach lower minima, which directly impacts training efficiency and resource utilization. The research addresses how historical gradients can be better leveraged to optimize the training process, which is central to training optimization.",
      "Irrelevant Aspects": "The paper focuses exclusively on training optimization without addressing inference optimization, latency, or throughput improvements. There's no discussion of GPU-specific optimizations, hardware utilization strategies, or distributed training approaches. The work doesn't cover model architecture optimizations or inference-time performance improvements that would affect deployment efficiency.",
      "Summary": "This paper is highly relevant to the training optimization aspect of my research, introducing an improved optimizer that significantly enhances training efficiency for large language models. While it doesn't address inference optimization or hardware-specific optimizations, its contribution to making training more efficient and reducing the computational resources needed represents a valuable advancement in the field of LLM training optimization."
    }
  },
  {
    "id": "tePFpDgyqg",
    "title": "Scaling Long Context Training Data by Long-Distance Referrals",
    "abstract": "Training large language models for long context understanding faces the challenge of data shortage.\nPrevious data engineering approaches mechanically concatenate short documents, which may create many pseudo long documents but raise concerns about data quality.\nIn this paper, we study the core attribute of high quality data for long context training, and provide a data pipeline, LongPack, to scale\nsuch data.\nWe found that long distance referrals, which occur in natural long documents, are crucial for long-context training.\nHowever, simply concatenating short documents does not reliably generate these relations.\nWe further show that the density of long-distance referrals, which is higher in longer documents, has a key role in training efficiency, making previous upsampling methods suboptimal.\nTo enrich long documents, we propose LongPack, a data pipeline that constructs long documents by packing shorter ones based on referral relationships.\nSpecifically, for web pages, which are the primary source for language model training, we found hyper-link a native signal for such a relation.\nBy packing web pages through their hyper-link connection, we can create longer, high-quality documents.\nOur experiments demonstrate that LongPackis highly scalable, generating a corpus of long documents equivalent in size to an entire pretraining dataset using just 0.5% root documents.\nFurthermore, the constructed documents have a ‘near-natural’ quality as innate long documents for long context training, reaching a 32.7% higher score than previous state-of-the-art methods.",
    "authors": [
      "~Yonghao_Zhuang1",
      "~Lanxiang_Hu1",
      "~Longfei_Yun1",
      "~Souvik_Kundu2",
      "~Zhengzhong_Liu1",
      "~Eric_Xing1",
      "~Hao_Zhang2"
    ],
    "pdf": "/pdf/5b80158cb1f4e40b480e9f3a75f2f324ca04bb1e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper is highly relevant to LLM training optimization, specifically on the data side. It addresses the critical challenge of scaling long-context training by improving data quality and quantity, which is a fundamental aspect of building large-scale training systems. The focus on increasing 'training efficiency' directly relates to my goals of better GPU utilization and higher throughput. The proposed 'LongPack' pipeline is a scalable system for data engineering, a key component of efficient LLM development.",
      "Irrelevant Aspects": "The paper does not focus on low-level compute optimizations such as kernel fusion, memory management strategies, or model parallelism algorithms (like ZeRO, Tensor Parallelism). It also does not cover inference optimization techniques like quantization, KV cache management, or batching strategies aimed at reducing latency. The research is centered on data creation and its impact on training effectiveness, not on the computational efficiency of the training or inference process itself.",
      "Summary": "This paper presents a novel data pipeline, LongPack, to address the scarcity of high-quality long-context training data for LLMs. It identifies 'long-distance referrals' as a key attribute of good data and uses hyperlinks to construct more natural and effective long documents from shorter web pages. This work is highly relevant to training optimization and scalability as it directly improves training efficiency and enables the creation of massive, high-quality training corpora, leading to better utilization of computational resources. However, it does not delve into low-level compute or inference performance optimizations."
    }
  },
  {
    "id": "TvGPP8i18S",
    "title": "MELODI: Exploring Memory Compression for Long Contexts",
    "abstract": "We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both transformer layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline - the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8.",
    "authors": [
      "~Yinpeng_Chen1",
      "~DeLesley_Hutchins1",
      "~Aren_Jansen2",
      "~Andrey_Zhmoginov1",
      "~David_Racz1",
      "~Jesper_Sparre_Andersen1"
    ],
    "pdf": "/pdf/f0d794615cc082cad1ed5b1e2a0b709f556d3a6f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory architecture optimization for long context processing, hierarchical compression scheme across transformer layers, significant memory footprint reduction (8x), recurrent compression across multiple layers, comparison with Memorizing Transformer baseline showing superior performance, efficient GPU memory utilization techniques",
      "Irrelevant Aspects": "Limited discussion of throughput and latency metrics, minimal focus on distributed training across multiple GPUs, architectural focus rather than training technique optimization, no explicit hyperparameter optimization analysis",
      "Summary": "MELODI presents a memory architecture with hierarchical compression to efficiently handle long contexts, reducing memory footprint by 8x compared to the Memorizing Transformer baseline. This approach optimizes both short-term and long-term memory representation, which directly contributes to better GPU utilization. While the paper focuses primarily on memory architecture rather than explicit throughput or latency measurements, the substantial memory reduction has significant implications for LLM training and inference efficiency, making it highly relevant to optimization research."
    }
  },
  {
    "id": "WwpYSOkkCt",
    "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
    "abstract": "Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve  performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines---and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show  that this has the potential to lead to significant (2-3$\\times$) gains in inference throughput.",
    "authors": [
      "~Sangmin_Bae1",
      "~Adam_Fisch2",
      "~Hrayr_Harutyunyan1",
      "~Ziwei_Ji1",
      "~Seungyeon_Kim1",
      "~Tal_Schuster1"
    ],
    "pdf": "/pdf/ffc95371f376b304e08817bb00f82383e3458d4e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Parameter sharing to reduce model size (impacts GPU memory usage)",
        "Converting existing pretrained models to smaller recursive variants",
        "Layer-wise LoRA modules for parameter efficiency",
        "Continuous Depth-wise Batching inference paradigm",
        "Early exiting for improved throughput",
        "Theoretical 2-3x inference throughput gains",
        "Model compression without significant performance loss",
        "Recursive architecture enabling efficient inference patterns"
      ],
      "Irrelevant Aspects": [
        "Limited focus on training optimization techniques",
        "Less emphasis on multi-GPU scalability",
        "Minimal discussion of latency improvements (focuses more on throughput)",
        "Less attention to quantization or other compression techniques"
      ],
      "Summary": "This paper introduces Recursive Transformers that share parameters across layers to reduce model size for more efficient deployment. The method converts existing pretrained models into smaller versions by repeating a single block of unique layers. The authors enhance this approach with depth-wise LoRA modules to maintain performance while preserving model compactness. The paper also proposes Continuous Depth-wise Batching, a new inference paradigm that leverages the recursive structure with early exiting to potentially achieve 2-3x gains in inference throughput. The research focuses on improving GPU utilization and throughput for LLM deployment through parameter sharing and efficient inference patterns."
    }
  },
  {
    "id": "EMMnAd3apQ",
    "title": "ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts",
    "abstract": "Vision-language (VL) learning requires extensive visual perception capabilities, such as fine-grained object recognition and spatial perception. Recent works typically rely on training huge models on massive datasets to develop these capabilities. As a more efficient alternative, this paper proposes a new framework that Transfers the knowledge from a hub of Vision Experts (ToVE) for efficient VL learning, leveraging pre-trained vision expert models to promote visual perception capability. Specifically, building on a frozen CLIP image encoder that provides vision tokens for image-conditioned language generation, ToVE introduces a hub of multiple vision experts and a token-aware gating network that dynamically routes expert knowledge to vision tokens. In the transfer phase, we propose a \"residual knowledge transfer\" strategy, which not only preserves the generalizability of the vision tokens but also allows selective detachment of low-contributing experts to improve inference efficiency. Further, we explore to merge these expert knowledge to a single CLIP encoder, creating a knowledge-merged CLIP that produces more informative vision tokens without expert inference during deployment. Experiment results across various VL tasks demonstrate that the proposed ToVE achieves competitive performance with two orders of magnitude fewer training data.",
    "authors": [
      "~Yuanchen_Wu1",
      "~Junlong_Du1",
      "~Ke_Yan2",
      "~Shouhong_Ding3",
      "~Xiaoqiang_Li2"
    ],
    "pdf": "/pdf/7803d556ac892b51ae491176d9bd60a7825eee6e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Efficient training with fewer data, selective expert detachment for inference optimization, knowledge merging to avoid expert inference during deployment, dynamic routing mechanism for resource efficiency, addressing scalability by leveraging existing expert models",
      "Irrelevant Aspects": "Primary focus on vision-language models rather than pure language models, less emphasis on GPU-specific utilization details, lacks explicit throughput and latency measurements, more focused on model architecture than system-level optimization",
      "Summary": "ToVE proposes an efficient framework for vision-language learning that transfers knowledge from vision experts through a token-aware gating network. It introduces residual knowledge transfer and expert merging techniques to improve inference efficiency while maintaining performance with significantly less training data. The approach addresses model efficiency but doesn't deeply explore GPU utilization optimization."
    }
  },
  {
    "id": "cPD2hU35x3",
    "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
    "abstract": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and\nleading proprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context un-\nderstanding and retrieval-augmented generation (RAG) capabilities. These two\ncapabilities are complementary to each other and essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt. We present\na detailed continued training recipe to extend the context window of Llama3-\n70B-base from 8K to 128K tokens, along with a three-stage instruction tun-\ning process to enhance the model’s instruction-following, RAG performance,\nand long-context understanding capabilities. Our results demonstrate that the\nLlama3-ChatQA-2-70B model outperforms most existing state-of-the-art models,\nincluding GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-\nInstruct, on ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark\nusing only a 4K context window, showing the strong long context capability across\nvarying sequence lengths. We further provide extensive comparisons between\ndirect long-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution using\nthe same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B and\nQwen2-72B-Instruct) on both 32K and 128K benchmarks. We open-source the\nmodel weights, training data, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/",
    "authors": [
      "~Peng_Xu7",
      "~Wei_Ping1",
      "~Xianchao_Wu1",
      "~Chejian_Xu1",
      "~Zihan_Liu2",
      "~Mohammad_Shoeybi1",
      "~Bryan_Catanzaro1"
    ],
    "pdf": "/pdf/3f73db38a0fa1c60fbec350ee7e0f2accc0d61d7.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper provides detailed insights into training optimization for extending context windows from 8K to 128K tokens, which directly impacts GPU utilization, memory management, and scalability. The three-stage instruction tuning process is relevant to inference optimization. The focus on long context understanding and RAG capabilities addresses throughput and latency concerns in processing large volumes of information.",
      "Irrelevant Aspects": "The paper places more emphasis on comparative performance analysis rather than detailed optimization techniques. Some aspects of the evaluation methodology and benchmark comparisons are less directly relevant to training/inference optimization. The open source contribution, while valuable for the community, doesn't directly address optimization concerns.",
      "Summary": "ChatQA 2 presents an optimized Llama 3.0-based model with a 128K context window, offering valuable insights into training optimization for long contexts and instruction tuning. While it focuses on bridging performance gaps with proprietary models, it provides limited details on specific GPU utilization techniques or scalability strategies. The paper is highly relevant for researchers working on LLM optimization, particularly for long context applications, though some aspects are more evaluation-focused than optimization-focused."
    }
  },
  {
    "id": "VpWki1v2P8",
    "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
    "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
    "authors": [
      "~Jui-Nan_Yen1",
      "~Si_Si1",
      "~Zhao_Meng1",
      "~Felix_Yu1",
      "~Sai_Surya_Duvvuri1",
      "~Inderjit_S_Dhillon1",
      "~Cho-Jui_Hsieh1",
      "~Sanjiv_Kumar1"
    ],
    "pdf": "/pdf/aac7a789463539d3cde8fa973a483f89909bca2a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focuses on LoRA optimization for LLM training, addressing memory requirements and training efficiency. Introduces an adaptive matrix preconditioning method that remains computationally efficient. Demonstrates improvements on multiple LLM models (Gemma 2B, 7B, mT5-XXL) and benchmarks. Enhances existing optimizers like Adam, directly addressing training optimization challenges.",
      "Irrelevant Aspects": "Does not explicitly address inference optimization, latency reduction, or throughput improvements. Lacks direct discussion of GPU utilization metrics. Focuses primarily on optimization quality rather than system-level performance. No specific emphasis on scalability aspects beyond testing different model sizes.",
      "Summary": "This paper introduces LoRA-RITE, a novel optimization method for LoRA fine-tuning that addresses transformation invariance issues in current LoRA optimizers. While highly relevant to LLM training optimization and showing consistent accuracy improvements across multiple models and benchmarks, the paper does not directly address inference optimization, GPU utilization, or system-level performance metrics that would make it completely aligned with all research interests."
    }
  },
  {
    "id": "ACSNlt77hq",
    "title": "Efficient Inference for Large Language Model-based Generative Recommendation",
    "abstract": "Large Language Model (LLM)-based generative recommendation has achieved notable success, yet its practical deployment is costly particularly due to excessive inference latency caused by autoregressive decoding. For lossless LLM decoding acceleration, Speculative Decoding (SD) has emerged as a promising solution. However, applying SD to generative recommendation presents unique challenges due to the requirement of generating top-K items (i.e., K distinct token sequences) as a recommendation list by beam search. This leads to more stringent verification in SD, where all the top-K sequences from the target LLM must be successfully drafted by the draft model at each decoding step. To alleviate this, we consider 1) boosting top-K sequence alignment between the draft model and the target LLM, and 2) relaxing the verification strategy to reduce trivial LLM calls. To this end, we propose an alignment framework named AtSpeed, which presents the AtSpeed-S optimization objective for top-K alignment under the strict top-K verification. Moreover, we introduce a relaxed sampling verification strategy that allows high-probability non-top-K drafted sequences to be accepted, significantly reducing LLM calls. Correspondingly, we propose AtSpeed-R for top-K alignment under this relaxed sampling verification. Empirical results on two real-world datasets demonstrate that AtSpeed significantly accelerates LLM-based generative recommendation, e.g., near 2x speedup under strict top-K verification and up to 2.5x speedup under relaxed sampling verification. The codes and datasets are available at~\\url{https://github.com/Linxyhaha/AtSpeed}.",
    "authors": [
      "~Xinyu_Lin3",
      "~Chaoqun_Yang2",
      "~Wenjie_Wang1",
      "~Yongqi_Li1",
      "~Cunxiao_Du3",
      "~Fuli_Feng1",
      "~See-Kiong_Ng1",
      "~Tat-Seng_Chua2"
    ],
    "pdf": "/pdf/8d0f8085f0ed6580f6a4d9bf03954095d4873f7a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on LLM inference optimization using Speculative Decoding techniques to reduce latency and improve throughput. It introduces AtSpeed-S and AtSpeed-R optimization objectives to align draft and target models for better inference speed. The approach addresses practical deployment challenges and claims 2-2.5x speedups, directly relevant to GPU utilization and inference performance optimization.",
      "Irrelevant Aspects": "The paper is specifically focused on generative recommendation systems rather than general LLM applications. It doesn't address multi-GPU scaling, distributed training optimization, or broader LLM deployment strategies beyond the recommendation use case.",
      "Summary": "This paper presents specialized inference optimization techniques for LLM-based generative recommendation systems. While it addresses important aspects of inference latency reduction through Speculative Decoding and model alignment, its focus is narrow to the recommendation domain, limiting its broader applicability to general LLM optimization research."
    }
  },
  {
    "id": "BQwsRy1h3U",
    "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection",
    "abstract": "KV cache has become a *de facto* technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. \nAs the size of the model and data grows, the KV cache can, yet, quickly become a bottleneck within the system in both storage and memory transfer.\nTo address this, prior studies usually focus on the first three axes of the cache tensors for compression.  \nThis paper supplements them, focusing on the feature dimension axis, \nby utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. \nWe begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). \nWe identify the drawback of PCA projection that model performance degrades rapidly under relatively low compression rates (less than 60%).\nThis phenomenon is elucidated by insights derived from the principles of attention mechanisms.\nTo bridge the gap, we propose to directly tune the orthogonal projection matrix on the continual pre-training or supervised fine-tuning datasets with an elaborate Matryoshka learning strategy.\nThanks to such a strategy, we can adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. \nCompared to Multi-head Latent Attention (MLA), our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. \nWe witness the high data efficiency of our training procedure and find that our method can sustain over 90\\% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2 and Mistral.",
    "authors": [
      "~Bokai_Lin1",
      "~Zihao_Zeng1",
      "~Zipeng_Xiao1",
      "~Siqi_Kou1",
      "~TianQi_Hou1",
      "~Xiaofeng_Gao2",
      "~Hao_Zhang2",
      "~Zhijie_Deng1"
    ],
    "pdf": "/pdf/67dc714dbb4ab7f3fbace2be58ef87450064721a.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses KV cache optimization for LLM inference, a critical bottleneck affecting GPU utilization, memory transfer, and scalability. It proposes a novel trainable orthogonal projection method that compresses the feature dimension axis of KV cache tensors. The Matryoshka learning strategy enables adaptive compression rates across different layers and heads. Results show maintaining over 90% performance with 60-75% KV cache compression on popular models like LLaMA2 and Mistral, without requiring architectural changes. This directly impacts inference efficiency, memory usage, and potentially throughput.",
      "Irrelevant Aspects": "The paper focuses solely on KV cache compression and doesn't address other inference optimization techniques like quantization, attention mechanism redesign, or structural pruning. There's no discussion of distributed inference across multiple GPUs or scaling considerations. Training optimization aspects aren't covered. The paper doesn't provide explicit measurements of throughput improvements or latency reductions, though these would be implicit benefits of reduced KV cache size.",
      "Summary": "MatryoshkaKV presents a novel approach to compressing KV caches in LLM inference by applying trainable orthogonal projection matrices to reduce the feature dimension. Building on PCA concepts but overcoming their limitations, the method uses a Matryoshka learning strategy to find optimal compression rates for different layers and heads. The approach achieves 60-75% compression while maintaining over 90% of model performance on popular LLMs like LLaMA2 and Mistral. This technique provides a practical solution for reducing memory bandwidth and storage requirements during LLM inference without requiring architectural changes, directly addressing a key bottleneck in LLM deployment."
    }
  },
  {
    "id": "4GT9uTsAJE",
    "title": "AdaGrad under Anisotropic Smoothness",
    "abstract": "Adaptive gradient methods have been widely adopted in training large-scale deep neural networks, especially large foundation models. Despite the huge success in practice, their theoretical advantages over classical gradient methods with uniform step sizes across all coordinates (e.g. SGD) have not been fully understood, especially in the large batch-size setting commonly used in practice. This is because the only theoretical result that can demonstrate this benefit was obtained in the original paper of Adagrad for convex nonsmooth objective functions, which is insufficient for large batch algorithms. In this work, we attempt to resolve this gap between theory and practice by proposing a novel anisotropic generalized smoothness assumption and providing corresponding analysis of Adagrad. It is shown that under anisotropic smoothness and noise conditions, AdaGrad can achieve faster convergence guarantees in terms of better dimensional dependence than algorithms with uniform step sizes across all coordinates. Experiments in logistic regression and instruction following fine-tuning tasks provide strong evidence to support our novel assumption and theoretical analysis.",
    "authors": [
      "~Yuxing_Liu1",
      "~Rui_Pan4",
      "~Tong_Zhang2"
    ],
    "pdf": "/pdf/3140eb0c6a03fbc3d1cabf43de7ec0f077326c63.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Analysis of adaptive gradient methods for large-scale neural networks and foundation models, Focus on large batch-size training settings which directly impact GPU utilization and throughput, Theoretical insights into convergence guarantees that could inform training optimization strategies, Experiments on instruction-following fine-tuning tasks relevant to large language models",
      "Irrelevant Aspects": "Primary focus on theoretical analysis rather than practical implementation details, Limited discussion of inference optimization and latency considerations, Some experiments (logistic regression) less relevant to large language models specifically",
      "Summary": "This paper provides theoretical analysis of AdaGrad, an adaptive gradient method, under an anisotropic smoothness assumption. It demonstrates advantages over classical methods like SGD in large batch settings, which are common for training large language models. While the theoretical insights could inform training optimization strategies that impact GPU utilization and scalability, the paper doesn't directly address implementation details, inference optimization, or latency concerns that are central to my research focus."
    }
  },
  {
    "id": "j9VVzueEbG",
    "title": "ZETA: Leveraging $Z$-order Curves for Efficient Top-$k$ Attention",
    "abstract": "Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing existing top-$k$ attention methods from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging Z-Order Curves for Efficient Top-k Attention, to enable parallel querying of past tokens for entire sequences. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leveraging Z-order curves to map low-dimensional keys and queries into one-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA~matches the performance of standard attention on synthetic tasks Associative Recall and outperforms attention and its variants on Long-Range Arena and WikiText-103 language modeling.",
    "authors": [
      "~QIUHAO_Zeng1",
      "~Jerry_Huang1",
      "~Peng_Lu6",
      "~Gezheng_Xu2",
      "~Boxing_Chen1",
      "~Charles_Ling1",
      "~Boyu_Wang3"
    ],
    "pdf": "/pdf/d9ca9819d5ebfa1bec70a7800c567e5289b1364c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing the attention mechanism in Transformer models, which is a major computational bottleneck in LLMs. It introduces top-k attention to reduce quadratic complexity and enables parallel querying through Z-order curves. This directly addresses computational efficiency, memory usage, and training acceleration - all critical for LLM training and inference optimization. The approach of dimensionality reduction and parallel sorting aligns well with improving GPU utilization and throughput.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU-specific optimization strategies or hardware-specific implementations. It doesn't provide detailed measurements of throughput or latency improvements, nor does it focus on multi-GPU or distributed training scenarios, which are important aspects of scalability research.",
      "Summary": "ZETA presents a novel approach to optimize attention mechanisms in Transformers through top-k selection enabled by Z-order curves. The paper addresses a fundamental bottleneck in LLMs - the quadratic complexity of attention - by reducing dimensionality and enabling parallel processing. While not explicitly GPU-focused, the efficiency gains and parallel processing capabilities would likely translate to improved GPU utilization and better throughput in both training and inference scenarios."
    }
  },
  {
    "id": "bEqI61iBue",
    "title": "Second-Order Fine-Tuning without Pain for LLMs: A Hessian Informed Zeroth-Order Optimizer",
    "abstract": "Fine-tuning large language models (LLMs) is necessary for specific downstream tasks, but classic first-order optimizer entails prohibitive GPU memory because of the back propagation. Recent works such as MeZO have turned to zeroth-order optimizers for fine-tuning, which reduce substantial memory by using two forward passes. However, heterogeneous curvatures across different parameter dimensions in LLMs often cause model convergence instability or even failure. In this work, we propose HiZOO, a diagonal Hessian informed Zeroth-Order Optimizer , which is the first work to leverage the diagonal Hessian to enhance ZOO for fine-tuning LLMs. We provide theoretical proof for HiZOO and visualize the optimization trajectories on test functions to illustrate how it improves convergence in handling heterogeneous curvatures. Extensive experiments on various models (RoBERTa, OPT, Phi-2 and LLama3, with 350M$\\sim$66B parameters) indicate that HiZOO significantly reduces training steps and enhances model accuracy, while keeping the memory advantage of ZOO. For example, on SST2 task HiZOO achieves $8\\times$ speedup and better accuracy over MeZO across different models. We also propose HiZOO-L, which reduces the Hessian memory cost to 10\\% of the MeZO, while maintaining almost same performance. Compared with ZO-Adam, HiZOO-L achieves a 4.3\\% improvement, just using 50\\% of the GPU memory. Code is available at https://anonymous.4open.science/r/HiZOO-27F8.",
    "authors": [
      "~Yanjun_Zhao2",
      "~Sizhe_Dang1",
      "~Haishan_Ye2",
      "~Guang_Dai1",
      "~Yi_Qian4",
      "~Ivor_Tsang1"
    ],
    "pdf": "/pdf/4679b332210ba3d0e5b15d0dbcdd0f21c32f9022.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM fine-tuning with memory efficiency, addressing GPU utilization and scalability. HiZOO significantly reduces training steps (8x speedup) while maintaining memory advantages. It works across models of varying sizes (350M to 66B parameters), showing good scalability. HiZOO-L further reduces memory usage to 10% of MeZO while maintaining performance. The approach achieves better results with 50% of the GPU memory compared to ZO-Adam.",
      "Irrelevant Aspects": "The paper focuses specifically on fine-tuning rather than pre-training or inference optimization. It doesn't directly address latency optimization, which is a key aspect of my research interests. The paper's focus is on training optimization rather than inference optimization.",
      "Summary": "This paper introduces HiZOO, a diagonal Hessian informed Zeroth-Order Optimizer for fine-tuning LLMs that addresses GPU memory constraints and convergence instability. It demonstrates significant speedup (8x on SST2) and better accuracy over prior methods like MeZO while maintaining memory advantages. The approach is scalable across various models and parameter sizes, with HiZOO-L further reducing memory requirements to just 10% of MeZO. The paper's focus on fine-tuning optimization, GPU memory efficiency, and training speedup aligns well with my research interests in LLM training optimization and GPU utilization."
    }
  },
  {
    "id": "6qUUgw9bAZ",
    "title": "Learning How Hard to Think: Input-Adaptive Allocation of LM Computation",
    "abstract": "Computationally intensive decoding procedures---including search, reranking, and self-critique---can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog.\nExisting work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-$k$ procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to quality.",
    "authors": [
      "~Mehul_Damani1",
      "~Idan_Shenfeld1",
      "~Andi_Peng1",
      "~Andreea_Bobu1",
      "~Jacob_Andreas1"
    ],
    "pdf": "/pdf/64afc03d54a355f1cafd8cf9cfae4806eaa6f7bf.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference optimization of language models through adaptive computation allocation, which directly addresses better GPU utilization and throughput. It presents techniques for dynamically allocating resources based on input complexity, claiming up to 50% reduction in computation without quality loss. This approach aligns with goals of optimizing inference efficiency, reducing latency, and improving resource utilization.",
      "Irrelevant Aspects": "The paper primarily focuses on inference optimization rather than training optimization, which is only part of the stated research interests. It doesn't explicitly address GPU-specific optimizations, low-level hardware considerations, or distributed system scalability. The specific focus on best-of-k and routing procedures might be narrower than general optimization techniques.",
      "Summary": "This paper presents an approach for input-adaptive allocation of language model computation during the decoding phase. By predicting which inputs will benefit from additional computation, the method can dynamically adjust resources applied to each input. The paper demonstrates this technique in two decoding procedures: adaptive best-of-k sampling and routing between different decoding approaches. Across various tasks, the method reduces computation by up to 50% while maintaining output quality."
    }
  },
  {
    "id": "IDJUscOjM3",
    "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts",
    "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data, each equipping a shared base LLM with distinct domain-specific capabilities, activated via self-optimized routing. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade-offs in performances on non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial improvements (6.5%p on average) over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity, the applicability of Self-MoE to multiple base LLMs, and the potential of self-improvement in achieving efficient, scalable, and adaptable systems.",
    "authors": [
      "~Junmo_Kang1",
      "~Leonid_Karlinsky3",
      "~Hongyin_Luo1",
      "~Zhen_Wang6",
      "~Jacob_A_Hansen1",
      "~James_R._Glass1",
      "~David_Daniel_Cox1",
      "~Rameswar_Panda1",
      "~Rogerio_Feris1",
      "~Alan_Ritter1"
    ],
    "pdf": "/pdf/e99a2ed2cb4ba438185b0e6ba703170c2c90e88e.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Mixture of Experts (MoE) architecture for improving LLM efficiency, transformation of monolithic LLMs into compositional modular systems, potential for better GPU utilization through selective expert activation, focus on scalability without adding parameters, performance improvements across multiple benchmarks, self-optimization routing mechanisms",
      "Irrelevant Aspects": "Limited discussion of specific GPU utilization metrics, minimal focus on inference optimization techniques like quantization or pruning, more emphasis on capability enhancement than runtime optimization, lack of detailed throughput and latency measurements",
      "Summary": "Self-MoE presents an approach to transform monolithic LLMs into compositional systems of self-specialized experts, which directly addresses my research interests in LLM optimization and efficiency. The paper's focus on MoE architecture offers potential for improved GPU utilization by activating only relevant parts of the model for each input, potentially increasing throughput and reducing latency. While it doesn't cover all optimization aspects I'm interested in, its core approach to creating more efficient and scalable modular systems is highly relevant to my work on LLM training and inference optimization."
    }
  },
  {
    "id": "ZSdubdbOoi",
    "title": "Self-Improving Robust Preference Optimization",
    "abstract": "Online and offline $\\mathtt{RLHF}$ methods, such as $\\mathtt{PPO}$ and $\\mathtt{DPO}$, have been highly successful in aligning AI with human preferences. Despite their success, however, these methods suffer from fundamental limitations: $\\mathbf{(a)}$ Models trained with $\\mathtt{RLHF}$ can learn from mistakes or negative examples through RL mechanism or contrastive loss during training. However, at inference time, they lack an innate self-improvement mechanism for error corrections. $\\mathbf{(b)}$ The optimal solution of existing methods is highly task-dependent, making it difficult for them to generalize to new tasks. To address these challenges, we propose Self-Improving Robust Preference Optimization ($\\mathtt{SRPO}$), a practical and mathematically principled offline $\\mathtt{RLHF}$ framework. The key idea behind $\\mathtt{SRPO}$ is to cast the problem of learning from human preferences as a self-improvement process, mathematically formulated as a min-max objective that jointly optimizes a self-improvement policy and a generative policy in an adversarial fashion. Crucially, the solution for this optimization problem is independent of the training task, which makes it robust to its changes. We then show that this objective can be reformulated as a non-adversarial offline loss, which can be efficiently optimized using standard supervised learning techniques at scale. To demonstrate $\\mathtt{SRPO}$’s effectiveness, we evaluate it using AI Win-Rate (WR) against human (GOLD) completions. When tested on the XSum dataset, $\\mathtt{SRPO}$ outperforms $\\mathtt{DPO}$ by a margin of $\\mathbf{15}$% after $5$ self-revisions, achieving an impressive $\\mathbf{90}$% WR. Moreover, on the challenging Arena-Hard prompts, $\\mathtt{SRPO}$ outperforms both $\\mathtt{DPO}$ and $\\mathtt{IPO}$ (by $\\mathbf{4}$% without revision and $\\mathbf{6}$% after a single revision), reaching a $\\mathbf{56}$% WR against against $\\mathtt{Llama-3.1-8B-Instruct}$.",
    "authors": [
      "~Eugene_Choi1",
      "~Arash_Ahmadian1",
      "~Matthieu_Geist1",
      "~Olivier_Pietquin1",
      "~Mohammad_Gheshlaghi_Azar2"
    ],
    "pdf": "/pdf/0be85f01c86eb47cd301a99c56260ebd03901fde.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper proposes SRPO, an offline RLHF framework for LLM training that could improve training efficiency. It addresses the self-improvement capability at inference time, which relates to inference optimization. The method claims to be efficiently optimized using standard supervised learning techniques at scale, suggesting potential improvements in GPU utilization. It demonstrates significant performance improvements over existing methods like DPO and IPO, which could lead to better resource utilization during training.",
      "Irrelevant Aspects": "The paper focuses more on the alignment aspect of LLMs rather than specifically on GPU utilization, scalability metrics, or inference throughput/latency optimization. There's no explicit discussion about hardware utilization or model parallelization techniques. The performance metrics used (AI Win-Rate) don't directly measure computational efficiency.",
      "Summary": "SRPO introduces a self-improving framework for preference optimization in LLMs with potential training efficiency benefits. While not directly addressing GPU utilization or latency optimization, its efficient training approach and inference-time self-improvement capability could indirectly benefit these aspects of ML systems."
    }
  },
  {
    "id": "PxlfzEePC0",
    "title": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing",
    "abstract": "Diffusion transformers have been widely adopted for text-to-image synthesis. While scaling these models up to billions of parameters shows promise, the effectiveness of scaling beyond current sizes remains underexplored and challenging. By explicitly exploiting the computational heterogeneity of image generations, we develop a new family of Mixture-of-Experts (MoE) models (EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns to adaptively optimize the compute allocated to understand the input texts and generate the respective image patches, enabling heterogeneous computation aligned with varying text-image complexities. This heterogeneity provides an efficient way of scaling EC-DIT up to 97 billion parameters and achieving significant improvements in training convergence, text-to-image alignment, and overall generation quality over dense models and conventional MoE models. Through extensive ablations, we show that EC-DIT demonstrates superior scalability and adaptive compute allocation by recognizing varying textual importance through end-to-end training. Notably, in text-to-image alignment evaluation, our largest models achieve a state-of-the-art GenEval score of 71.68% and still maintain competitive inference speed with intuitive interpretability.",
    "authors": [
      "~Haotian_Sun1",
      "~Tao_Lei1",
      "~Bowen_Zhang2",
      "~Yanghao_Li1",
      "~Haoshuo_Huang1",
      "~Ruoming_Pang2",
      "~Bo_Dai1",
      "~Nan_Du1"
    ],
    "pdf": "/pdf/e01f7df6f94cbd5ab80543ba1c299cf684693a82.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Mixture-of-Experts (MoE) architecture for scaling large models; Adaptive expert-choice routing for efficient computation; Scaling to 97 billion parameters; Training convergence improvements; Maintaining competitive inference speed; Optimization of compute allocation based on input complexity",
      "Irrelevant Aspects": "Focus on diffusion models for text-to-image synthesis rather than language models; Domain-specific evaluation metrics (GenEval score) for image generation; Text-to-image alignment assessment",
      "Summary": "EC-DIT introduces a Mixture-of-Experts approach with expert-choice routing for scaling diffusion transformers to 97 billion parameters. It adaptively allocates computational resources based on input complexities, improving training convergence and maintaining inference speed. While focused on image generation, the MoE techniques and optimization approaches are highly relevant to scaling large language models efficiently."
    }
  },
  {
    "id": "frsg32u0rO",
    "title": "Block Verification Accelerates Speculative Decoding",
    "abstract": "Speculative decoding is an  effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample from the target model. In prior works, draft verification is performed independently token-by-token. Surprisingly, we show that this approach is not optimal. We propose *Block Verification*, a simple draft verification algorithm that verifies the entire block jointly and provides additional wall-clock speedup. We prove that the proposed mechanism is optimal in the expected number of tokens produced each iteration and specifically is never worse than the standard token-level verification.\nEmpirically, block verification provides modest but consistent wall-clock speedups over the standard token verification algorithm of 5\\%-8\\% in a range of tasks and datasets.\nGiven that block verification does not increase code complexity, maintains the strong lossless guarantee of the standard speculative decoding verification algorithm, cannot deteriorate performance, and, in fact, consistently improves it, it can be used as a good default in speculative decoding implementations.",
    "authors": [
      "~Ziteng_Sun1",
      "~Uri_Mendlovic1",
      "~Yaniv_Leviathan1",
      "~Asaf_Aharoni1",
      "~Jae_Hun_Ro1",
      "~Ahmad_Beirami1",
      "~Ananda_Theertha_Suresh1"
    ],
    "pdf": "/pdf/025620026d4788112bdbf12a316d8a1221f554e6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on inference optimization for large language models, specifically improving speculative decoding which is a technique for lossless acceleration. The paper proposes Block Verification, an optimization that improves GPU utilization and provides consistent speedups of 5-8% while maintaining the lossless guarantee. This directly relates to my expertise in LLM inference optimization, better GPU utilization, higher throughput, and lower latency.",
      "Irrelevant Aspects": "The paper does not address training optimization of LLMs, which is part of my expertise. It has a relatively narrow focus on one specific aspect of speculative decoding rather than broader optimization challenges. The speedup improvements, while consistent, are modest (5-8%).",
      "Summary": "This paper presents Block Verification, an improved verification algorithm for speculative decoding in LLM inference. It verifies entire blocks of tokens jointly rather than token-by-token, proving optimal in expected token production per iteration. The method maintains lossless guarantees while improving GPU utilization and achieving consistent 5-8% speedups across tasks. The authors recommend it as a default for speculative decoding implementations due to its simplicity, performance guarantees, and consistent improvements."
    }
  },
  {
    "id": "2c7pfOqu9k",
    "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
    "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose **Flattened Tree KV Splitting**, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation, DeFT achieves up to 2.23/3.59$\\times$ speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
    "authors": [
      "~Jinwei_Yao1",
      "~Kaiqi_Chen7",
      "~Kexun_Zhang1",
      "~Jiaxuan_You2",
      "~Binhang_Yuan1",
      "~Zeke_Wang1",
      "~Tao_Lin1"
    ],
    "pdf": "/pdf/639bbf460121942ab57f854016bb693f884849b5.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM inference efficiency, specifically addressing GPU utilization and scalability through an attention algorithm designed for tree-structured inference. It introduces KV-Guided Grouping and Flattened Tree KV Splitting techniques to reduce redundant KV cache I/O operations and improve load balancing, directly targeting memory access optimization and GPU utilization. The demonstrated speedups (2.23/3.59×) in end-to-end/attention latency show significant performance improvements in inference optimization.",
      "Irrelevant Aspects": "The paper focuses solely on inference optimization without addressing training optimization. It is specifically tailored for tree-structured inference scenarios, which, while important, represents a specialized case rather than general LLM optimization techniques.",
      "Summary": "DeFT proposes a hardware-efficient attention algorithm for tree-structured LLM inference that addresses KV cache partitioning issues in existing systems. The approach reduces redundant memory operations through KV-Guided Grouping and improves load balancing via Flattened Tree KV Splitting, achieving significant reductions in KV cache IO and substantial speedups in practical workloads."
    }
  },
  {
    "id": "77gQUdQhE7",
    "title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models",
    "abstract": "Recent studies indicate that effectively utilizing inference-time compute is crucial for attaining good performance from large language models (LLMs). Specifically, the Best-of-N (BoN) inference strategy, where an LLM generates multiple responses and a verifier selects the best, has shown strong empirical performance. Motivated by this, we develop a novel inference-aware fine-tuning paradigm, which encompasses the BoN-aware inference framework as a special case. We devise the first imitation learning and reinforcement learning (RL) methods for fine-tuning LLMs using BoN, overcoming the challenging, non-differentiable argmax operator in BoN. We empirically demonstrate that our BoN-aware models implicitly learn a per-example \"meta-strategy\", which interleaves best responses with more diverse responses that might be better suited to a test-time input—a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the BoN performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and Pass@K from 60% to 67%.",
    "authors": [
      "~Yinlam_Chow1",
      "~Guy_Tennenholtz2",
      "~Izzeddin_Gur1",
      "~Vincent_Zhuang2",
      "~Bo_Dai1",
      "~Aviral_Kumar2",
      "~Rishabh_Agarwal2",
      "~Sridhar_Thiagarajan1",
      "~Craig_Boutilier2",
      "~Aleksandra_Faust1"
    ],
    "pdf": "/pdf/3274f331a8e9f86bc72c72c6562f2f607d1b5639.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference-time compute optimization in LLMs through the Best-of-N sampling strategy, which directly relates to my research interest in inference optimization. It proposes fine-tuning methods specifically designed to improve the performance of this inference strategy, potentially leading to better GPU utilization during inference. The paper addresses the exploration-exploitation trade-off in generating responses, which is relevant for optimizing throughput and quality.",
      "Irrelevant Aspects": "The paper primarily evaluates performance on task-specific metrics (Hendrycks MATH, Pass@K) rather than system-level metrics like latency and throughput. It doesn't appear to directly address GPU memory utilization during training or scalability aspects related to distributed computing. The focus is more on response quality optimization rather than raw inference speed.",
      "Summary": "This paper introduces inference-aware fine-tuning methods specifically designed to optimize LLMs for Best-of-N sampling at inference time. It develops imitation learning and RL techniques to fine-tune models that learn a 'meta-strategy' for generating diverse responses during BoN inference. The approach shows performance improvements on math benchmarks, demonstrating how fine-tuning can be optimized for specific inference strategies. While highly relevant to inference optimization, it focuses more on response quality than system performance metrics."
    }
  },
  {
    "id": "N1L5TgtkAw",
    "title": "Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits",
    "abstract": "We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models.  At each step, a  token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token.  For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability.  Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.",
    "authors": [
      "~Ashish_J_Khisti1",
      "~MohammadReza_Ebrahimi1",
      "~Hassan_Dbouk1",
      "~Arash_Behboodi1",
      "~Roland_Memisevic1",
      "~Christos_Louizos1"
    ],
    "pdf": "/pdf/dd975c1d37bdc7c2b8979d134b6a37b7b0979f2d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models through multi-draft speculative sampling, which aims to improve GPU utilization and reduce latency. It presents an optimal token selection scheme that can potentially increase throughput and scalability of language model inference. The theoretical analysis and proposed selection schemes could lead to more efficient inference pipelines.",
      "Irrelevant Aspects": "The paper focuses solely on inference optimization and does not address training optimization, which is part of my research interest. It primarily presents theoretical results without extensive implementation details or hardware-specific optimizations. The work doesn't cover quantization, memory optimization, or other compression techniques that are important for my research.",
      "Summary": "This paper introduces an approach to multi-draft speculative sampling for language model inference, presenting an optimal token selection scheme and theoretical analysis. The research decomposes the optimal selection process into a two-step solution and proposes weighted importance sampling techniques. While highly relevant to inference optimization, the paper has limited coverage of training optimization and implementation-specific optimizations."
    }
  },
  {
    "id": "lXRDQsiP2v",
    "title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "abstract": "The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ($\\texttt{TSSA}$). $\\texttt{TSSA}$ has $\\textit{linear computational and memory complexity}$ and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping $\\texttt{TSSA}$ for standard self-attention, which we refer to as the Token Statistics Transformer ($\\texttt{ToST}$), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable.  Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures.",
    "authors": [
      "~Ziyang_Wu1",
      "~Tianjiao_Ding1",
      "~Yifu_Lu1",
      "~Druv_Pai1",
      "~Jingyuan_Zhang2",
      "~Weida_Wang1",
      "~Yaodong_Yu4",
      "~Yi_Ma4",
      "~Benjamin_David_Haeffele1"
    ],
    "pdf": "/pdf/ffb27d677f2306e7b44371d607f237b6668bc403.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses computational efficiency of transformers, specifically reducing attention mechanism complexity from quadratic to linear scaling. This directly impacts GPU utilization, scalability, throughput, and latency for large language models. The proposed Token Statistics Self-Attention (TSSA) maintains competitive performance while being significantly more computationally efficient for both training and inference of language models.",
      "Irrelevant Aspects": "The paper includes experiments on vision tasks which are not directly relevant to language model optimization. The theoretical mathematical derivations and variational formulations may not translate directly to practical implementation optimizations. The focus is on architectural changes rather than system-level implementation details.",
      "Summary": "This paper presents a novel attention mechanism called Token Statistics Self-Attention (TSSA) with linear computational complexity, addressing one of the major bottlenecks in transformers. The resulting Token Statistics Transformer (ToST) achieves competitive performance on language tasks while significantly improving computational efficiency, making it highly relevant for optimizing large language model training and inference."
    }
  },
  {
    "id": "8VtGeyJyx9",
    "title": "LoLCATs: On Low-Rank Linearizing of Large Language Models",
    "abstract": "Recent works show we can linearize large language models (LLMs)—swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention—avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still requires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer (LoLCATs), a simple two-step method that improves LLM linearizing quality with orders of magnitudes less memory and compute. We base these steps on two findings. First, we can replace an LLM's softmax attentions with closely-approximating linear attentions, simply by *training* the linear attentions to match their softmax counterparts with an output MSE loss (“attention transfer”). Then, this enables adjusting for approximation errors and recovering LLM quality simply with *low-rank* adaptation (LoRA). LoLCATs significantly improves linearizing quality, training efficiency, and scalability. We significantly reduce the linearizing quality gap and produce state-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with only 0.2% of past methods' model parameters and 0.04-0.2% of their training tokens. Finally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50$\\times$ that of prior work). When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8\\% and 78.1\\% on 5-shot MMLU.",
    "authors": [
      "~Michael_Zhang4",
      "~Simran_Arora1",
      "~Rahul_Chalamala1",
      "~Benjamin_Frederick_Spector1",
      "~Alan_Wu3",
      "~Krithik_Ramesh1",
      "~Aaryan_Singhal1",
      "~Christopher_Re1"
    ],
    "pdf": "/pdf/6878f5a04bcbe6a65f04b4176e2b7aa63474099a.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper addresses key optimization challenges for LLMs by replacing quadratic attention with linear attention, significantly reducing computational complexity from O(n²) to O(n). This directly impacts inference speed, memory usage, and GPU utilization efficiency. The method enables scaling to very large models (70B and 405B) with minimal computational resources (using only 0.2% of past methods' parameters and 0.04-0.2% of training tokens). The technique maintains high model quality while improving throughput and reducing latency, which are critical for production deployments. The substantial improvements on MMLU benchmarks (20+ points) demonstrate the effectiveness of the approach without sacrificing model performance.",
      "Irrelevant Aspects": "The paper focuses more on architectural conversion than specific GPU hardware optimization techniques. It lacks detailed implementation specifics for GPU optimization or kernel-level improvements. Limited discussion on how this technique integrates with other optimization approaches like quantization or model parallelism. Minimal analysis of real-world deployment latency measurements across different hardware configurations.",
      "Summary": "LoLCATs presents a highly relevant method for optimizing large language models by converting them to linear attention architectures with minimal quality loss. The approach significantly reduces computational requirements while scaling to massive models (405B parameters), directly addressing core challenges in LLM deployment: computational efficiency, GPU utilization, and scalability. The method achieves these improvements with dramatically reduced training requirements, making it a promising technique for production environments where compute resources are constrained."
    }
  },
  {
    "id": "FJFVmeXusW",
    "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning",
    "abstract": "Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark.",
    "authors": [
      "~Yu_Fu2",
      "~Zefan_Cai1",
      "~Abedelkadir_Asi1",
      "~Wayne_Xiong1",
      "~Yue_Dong2",
      "~Wen_Xiao2"
    ],
    "pdf": "/pdf/8a8cb82656124e983fa17aa3ca0a724128cc16c7.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "KV Cache Optimization for inference efficiency, Memory overhead reduction, Head-level compression approach, Performance with reduced resources, Low-resource settings effectiveness, Inference efficiency focus",
      "Irrelevant Aspects": "Training optimization techniques, Novel model architectures, Retrieval and reasoning as primary focus (not just for compression impact), Benchmark testing methodology",
      "Summary": "Highly relevant paper focusing on KV cache compression at the head level to optimize LLM inference. Addresses memory overhead, GPU utilization, and scalability concerns by maintaining 97% performance with just 1.5% of the KV cache. The approach operates at individual attention head level and shows significant improvements in low-resource settings, directly contributing to better inference efficiency."
    }
  },
  {
    "id": "meKEKDhdnx",
    "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
    "abstract": "Prompts to large language models (LLMs) have evolved beyond simple user questions.\nFor LLMs to solve complex problems, today’s practices are to include domain-specific\ninstructions, illustration of tool usages, and/or long context such as textbook chapters in\nprompts. As such, many parts of prompts are repetitive across requests. Recent works\npropose to cache and reuse KV state of prompts. However, they are all confined to a single-\nGPU optimization, while production LLM serving systems are distributed by nature.\n\nThis paper proposes Preble, the first distributed LLM serving platform that targets and op-\ntimizes for prompt sharing. We designed a distributed scheduling system that co-optimizes\nKV state reuse and computation load-balancing with a new scheduling algorithm and a\nhierarchical scheduling mechanism. Our evaluation of Preble with real workloads and re-\nquest arrival patterns on two open-source LLMs shows that Preble outperforms the SOTA\nserving systems by 1.5× to 14.5× on average latency and 2× to 10× on p99 latency.",
    "authors": [
      "~Vikranth_Srivatsa1",
      "~Zijian_He5",
      "~Reyna_Abhyankar1",
      "~Dongming_Li1",
      "~Yiying_Zhang2"
    ],
    "pdf": "/pdf/0004a913d4f9164bac5d0a28d03059b6f5a09d37.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM serving optimization, which is directly related to inference optimization. It addresses prompt sharing and KV state reuse, which are important for reducing redundant computations and improving efficiency. It specifically targets distributed LLM serving systems, which is crucial for scalability in production environments. The paper claims significant improvements in latency (1.5× to 14.5× average latency and 2× to 10× p99 latency), which directly relates to performance optimization. It proposes a distributed scheduling system that co-optimizes KV state reuse and load balancing, which is essential for maximizing GPU utilization across distributed systems.",
      "Irrelevant Aspects": "The paper doesn't seem to address training optimization, which is one of my interests. While it mentions GPU utilization indirectly through load balancing, it doesn't explicitly focus on GPU utilization metrics. The paper doesn't appear to cover model architecture optimizations or quantization techniques, which are also areas of interest in my field. There's no mention of throughput improvements specifically, though latency improvements often correlate with throughput gains.",
      "Summary": "Preble introduces a distributed LLM serving platform that optimizes for prompt sharing through a distributed scheduling system. It focuses on reusing KV state of repetitive prompt parts while maintaining load balancing across distributed systems. The approach shows significant latency improvements (1.5× to 14.5× average latency and 2× to 10× p99 latency) compared to state-of-the-art serving systems when evaluated on real workloads with open-source LLMs."
    }
  },
  {
    "id": "lBntjGbyv0",
    "title": "BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments",
    "abstract": "Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from $\\textit{capability}$ to $\\textit{availability}$, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce $\\textbf{BitStack}$, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.",
    "authors": [
      "~Xinghao_Wang1",
      "~Pengyu_Wang2",
      "~Bo_Wang38",
      "~Dong_Zhang9",
      "~Yunhua_Zhou1",
      "~Xipeng_Qiu1"
    ],
    "pdf": "/pdf/c20261a5c7dacf5490639ae0b68cb6e3decce5a0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses LLM compression for inference optimization, which directly relates to better memory utilization and deployment in constrained environments. BitStack's training-free approach allows applying optimization to existing models without retraining. The method enables dynamic model size adjustment based on memory availability, which can improve scalability. The paper compares against quantization baselines, a key inference optimization technique, and claims to match or surpass them particularly at extreme compression ratios.",
      "Irrelevant Aspects": "The paper focuses primarily on local device deployment rather than data center or cloud environments where my expertise is concentrated. It doesn't address GPU-specific optimizations or parallel processing strategies. There's no mention of throughput metrics or latency measurements that would be relevant to my focus on performance optimization. The approach is about weight compression rather than optimizing the computational aspects of model execution.",
      "Summary": "BitStack introduces a training-free weight compression method that allows dynamic adjustment of model size in memory-constrained environments. While relevant to memory optimization for LLM deployment, it doesn't specifically address GPU utilization or computational efficiency aspects that are central to my research interests. The paper shows promise for inference optimization but focuses on a different aspect than the GPU utilization and throughput optimization that I specialize in."
    }
  },
  {
    "id": "6VhDQP7WGX",
    "title": "Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters",
    "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations into the token inputs of Large Language Models (LLMs). However, their real-world deployment is often constrained by high latency during inference due to the substantial compute required by the LLM to process the large number of input tokens, predominantly arising from the image. To reduce inference costs, one can either downsize the LLM or reduce the number of input tokens needed to represent the image, the latter of which has been the focus of many recent efforts around token compression. However, it is unclear what the optimal trade-off is given a fixed inference budget. \nWe first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs is achieved by using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take the first steps toward designing token compression algorithms tailored for high-compression settings, utilizing prompt-based compression of tokens. Our work underscores the performance and efficiency benefits of operating in low visual token regimes and the importance of developing tailored token reduction algorithms for such conditions.",
    "authors": [
      "~Kevin_Li3",
      "~Sachin_Goyal1",
      "~João_D._Semedo1",
      "~J_Zico_Kolter1"
    ],
    "pdf": "/pdf/eb5b700e81c0c2b3a79264d347002729f052e90f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for Vision Language Models, focusing on reducing latency through optimal trade-offs between visual token count and model parameters. It establishes scaling laws to understand performance variations, which is highly relevant for optimizing GPU utilization and throughput. The research on token compression aligns with inference optimization goals, and the findings suggest practical approaches for improving scalability under fixed inference budgets.",
      "Irrelevant Aspects": "The paper primarily focuses on VLMs rather than general LLM optimization, limiting its broader applicability. It appears to emphasize theoretical understanding over implementation details of specific compression techniques. The study doesn't address training optimization, which is another critical aspect of my research interests.",
      "Summary": "This paper reveals that for visual reasoning tasks, inference-optimal VLMs should use the largest possible LLM within the budget while minimizing visual tokens to as few as one. It challenges the current token reduction approaches that only modestly decrease token counts, arguing for much higher compression ratios. The paper introduces prompt-based compression methods for high-compression settings and highlights the benefits of operating in low visual token regimes."
    }
  },
  {
    "id": "EkfLaCJ7bk",
    "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
    "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to $2.1\\times$.",
    "authors": [
      "~Lijie_Yang1",
      "~Zhihao_Zhang2",
      "~Zhuofu_Chen1",
      "~Zikun_Li1",
      "~Zhihao_Jia2"
    ],
    "pdf": "/pdf/b8a75441bd6427a0373b36df4245c962cda980c2.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "LLM inference optimization, decoding phase optimization, KV cache memory reduction, latency reduction (up to 2.1x), sparse attention mechanisms, GPU utilization improvement, throughput improvement, system-level implementation of an algorithm",
      "Irrelevant Aspects": "No focus on training optimization (only inference), limited discussion on hardware-specific optimizations beyond general GPU utilization, no evaluation of energy efficiency",
      "Summary": "TidalDecode introduces a sparse attention mechanism for LLM decoding that reduces memory constraints and latency by maintaining persistent token positions across layers, using full attention only in selected layers. It achieves 2.1x speedup in decoding while maintaining generation quality."
    }
  },
  {
    "id": "wLnls9LS3x",
    "title": "Improved Algorithms for Kernel  Matrix-Vector Multiplication Under Sparsity Assumptions",
    "abstract": "Motivated by the problem of fast processing of attention matrices, we study fast algorithms for computing matrix-vector products for asymmetric Gaussian Kernel matrices $K\\in \\mathbb{R}^{n\\times n}$. \n$K$'s columns are indexed by a set of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$ queries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} = e^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given a vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to output a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in time subquadratic in $n$ and linear in $d$. Our algorithms rely on the following modelling assumption about the matrices $K$: the sum of the entries of $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We validate this assumption experimentally, for Gaussian kernel matrices encountered in various settings such as fast attention computation in LLMs. Under this assumption, we obtain the first subquadratic time algorithm for kernel matrix-vector multiplication for unrestricted vectors.",
    "authors": [
      "~Piotr_Indyk1",
      "~Michael_Kapralov1",
      "~Kshiteej_Sheth1",
      "~Tal_Wagner1"
    ],
    "pdf": "/pdf/bb9e7e2467acfacf392eb036df3ba3c75c775bc8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Fast processing of attention matrices, subquadratic algorithms for attention computation, focus on linear vs quadratic scaling, computational efficiency for LLMs, matrix-vector multiplication optimization",
      "Irrelevant Aspects": "Focus on Gaussian kernels rather than standard scaled dot-product attention, reliance on specific sparsity assumptions, theoretical focus rather than practical GPU optimization details",
      "Summary": "This paper presents subquadratic algorithms for kernel matrix-vector multiplication, specifically motivated by attention processing in LLMs. While its focus on Gaussian kernels differs from standard attention mechanisms, its insights on linear vs quadratic scaling and computational efficiency are relevant to LLM optimization. The paper's theoretical nature and specific kernel focus limit its immediate applicability to practical GPU optimization tasks."
    }
  },
  {
    "id": "FhTAG591Ve",
    "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models",
    "abstract": "The dominant paradigm for RLHF is *online* and *on-policy* RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training  on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but *off-policy* RLHF: learning on samples from previous iterations of our model which give a worse training signal. We tackle the fundamental challenge in this regime: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we test, online DPO is found to be most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. We verify the scalability of asynchronous RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an instruction-following task $\\sim$40\\% faster than a synchronous run while matching final performance. Finally, we extend our results to math and reasoning to demonstrate asynchronous RL can finetune Rho 1B on GSM8k $\\sim$70\\% faster while matching synchronous accuracy.",
    "authors": [
      "~Michael_Noukhovitch1",
      "~Shengyi_Huang1",
      "~Sophie_Xhonneux1",
      "~Arian_Hosseini1",
      "~Rishabh_Agarwal2",
      "~Aaron_Courville3"
    ],
    "pdf": "/pdf/d0759a26adbc702480d4dafff3b0bc31aa5c6240.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing RLHF training for language models, which is directly relevant to training optimization. It proposes asynchronous training to improve GPU utilization and training efficiency, achieving 40-70% faster training while maintaining performance. The work addresses compute optimizations for asynchronous RLHF, which aligns with the goal of better GPU utilization and scalability.",
      "Irrelevant Aspects": "The paper doesn't focus on inference optimization, latency reduction, model compression, or specific GPU architecture optimizations. It also doesn't address quantization techniques which are often important for inference optimization.",
      "Summary": "This paper introduces asynchronous RLHF, a method that separates generation and learning in RLHF to enable parallel processing. It achieves significant training speedups (40-70%) while maintaining performance by leveraging online but off-policy RL. The paper demonstrates that online DPO is most robust to off-policy data, with robustness increasing with model scale. While primarily focused on training optimization rather than inference, it offers valuable insights for improving GPU utilization and training efficiency in LLMs."
    }
  },
  {
    "id": "Y5LjYI4N6P",
    "title": "Efficient stagewise pretraining via progressive subnetworks",
    "abstract": "Recent developments in large language models have sparked interest in efficient\npretraining methods. Stagewise training approaches to improve efficiency, like\ngradual stacking and layer dropping (Reddi et al., 2023; Zhang & He, 2020), have\nrecently garnered attention. The prevailing view suggests that stagewise dropping\nstrategies, such as layer dropping, are ineffective, especially when compared to\nstacking-based approaches. This paper challenges this notion by demonstrating\nthat, with proper design, dropping strategies can be competitive, if not better, than\nstacking methods. Specifically, we develop a principled stagewise training framework, progressive subnetwork training, which only trains subnetworks within the\nmodel and progressively increases the size of subnetworks during training, until it\ntrains the full network. We propose an instantiation of this framework — Random\nPart Training (RAPTR) — that selects and trains only a random subnetwork (e.g.\ndepth-wise, width-wise) of the network at each step, progressively increasing the\nsize in stages. We show that this approach not only generalizes prior works like\nlayer dropping but also fixes their key issues. Furthermore, we establish a theoretical basis for such approaches and provide justification for (a) increasing complexity of subnetworks in stages, conceptually diverging from prior works on layer\ndropping, and (b) stability in loss across stage transitions in presence of key modern architecture components like residual connections and layer norms. Through\ncomprehensive experiments, we demonstrate that RAPTR can significantly speed\nup training of standard benchmarks like BERT and UL2, up to 33% compared to\nstandard training and, surprisingly, also shows better downstream performance on\nUL2, improving QA tasks and SuperGLUE by 1.5%; thereby, providing evidence\nof better inductive bias.",
    "authors": [
      "~Abhishek_Panigrahi1",
      "~Nikunj_Saunshi1",
      "~Kaifeng_Lyu2",
      "~Sobhan_Miryoosefi1",
      "~Sashank_J._Reddi1",
      "~Satyen_Kale2",
      "~Sanjiv_Kumar1"
    ],
    "pdf": "/pdf/a180d61fb84768afb2e041c507fe29dcf5f6f8d0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models, which is a core part of my research interest. It discusses methods to speed up training (up to 33% faster), which relates to improving GPU utilization and throughput. It presents a novel approach (RAPTR) that trains subnetworks progressively, which could lead to more efficient resource utilization during training. The paper mentions improving downstream performance, which suggests the optimization doesn't sacrifice model quality. It addresses scalability concerns in training large models. It provides a theoretical basis for the approaches, which could inform further optimization strategies.",
      "Irrelevant Aspects": "The paper focuses primarily on pretraining optimization, with less emphasis on inference optimization, which is another key part of my research interest. There's no explicit discussion of GPU utilization metrics or direct GPU efficiency measurements. The paper doesn't address latency concerns, which is important for inference optimization. There's limited discussion on how this approach would scale across multiple GPUs or distributed systems.",
      "Summary": "This paper introduces RAPTR, a progressive subnetwork training approach that can significantly speed up LLM pretraining (up to 33% faster) while maintaining or even improving model quality. It focuses on training optimization efficiency through stagewise training of subnetworks, providing a theoretical foundation for this approach. While highly relevant to training optimization aspects of my research, it has limited discussion on inference optimization, GPU utilization metrics, latency concerns, and distributed system scalability."
    }
  },
  {
    "id": "KrK6zXbjfO",
    "title": "SoundCTM: Unifying Score-based and Consistency Models for Full-band Text-to-Sound Generation",
    "abstract": "Sound content creation, essential for multimedia works such as video games and films, often involves extensive trial-and-error, enabling creators to semantically reflect their artistic ideas and inspirations, which evolve throughout the creation process, into the sound.\nRecent high-quality diffusion-based Text-to-Sound (T2S) generative models provide valuable tools for creators. However, these models often suffer from slow inference speeds, imposing an undesirable burden that hinders the trial-and-error process.\nWhile existing T2S distillation models address this limitation through $1$-step generation, the sample quality of $1$-step generation remains insufficient for production use.\nAdditionally, while multi-step sampling in those distillation models improves sample quality itself, the semantic content changes due to their lack of deterministic sampling capabilities.\nThus, developing a T2S generative model that allows creators to efficiently conduct trial-and-error while producing high-quality sound remains a key challenge.\nTo address these issues, we introduce Sound Consistency Trajectory Models (SoundCTM), which allow flexible transitions between high-quality $1$-step sound generation and superior sound quality through multi-step deterministic sampling. \nThis allows creators to efficiently conduct trial-and-error with $1$-step generation to semantically align samples with their intention, and subsequently refine sample quality with preserving semantic content through deterministic multi-step sampling.\nTo develop SoundCTM, we reframe the CTM training framework, originally proposed in computer vision, and introduce a novel feature distance using the teacher network for a distillation loss. \nAdditionally, while distilling classifier-free guided trajectories, we introduce a $\\nu$-sampling, a new algorithm that offers another source of quality improvement. For the $\\nu$-sampling, we simultaneously train both conditional and unconditional student models.\nFor production-level generation, we scale up our model to 1B trainable parameters, making SoundCTM-DiT-1B the first large-scale distillation model in the sound community to achieve both promising high-quality $1$-step and multi-step full-band (44.1kHz) generation.\nAudio samples are available at \\url{https://anonymus-soundctm.github.io/soundctm_iclr/}.",
    "authors": [
      "~Koichi_Saito1",
      "~Dongjun_Kim1",
      "~Takashi_Shibuya1",
      "~Chieh-Hsin_Lai2",
      "~Zhi_Zhong2",
      "~Yuhta_Takida1",
      "~Yuki_Mitsufuji1"
    ],
    "pdf": "/pdf/ba2174871a7fcd54d45ef2402a1613d1ef06f050.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses heavily on inference optimization through model distillation techniques, which is directly relevant to optimizing LLM inference. It addresses slow inference speeds in existing models and proposes a distillation approach to enable fast 1-step generation while maintaining quality. The scaling to 1B parameters and discussion of deterministic sampling capabilities provide insights applicable to large language model systems. The novel distillation framework and ν-sampling algorithm could inform training optimization strategies for LLMs.",
      "Irrelevant Aspects": "The application domain is audio/sound generation rather than text generation for LLMs. The specific techniques (CTM, ν-sampling) are tailored for audio generation. The paper doesn't address language-specific challenges in LLMs such as tokenization, attention mechanisms, or language-specific optimization strategies. Evaluation metrics and quality measures are specific to audio generation rather than text generation.",
      "Summary": "SoundCTM introduces a unified framework for score-based and consistency models in text-to-sound generation, with a strong emphasis on inference optimization through distillation techniques. While the domain is audio rather than language, the core optimization principles, scaling considerations, and distillation approaches offer valuable insights for LLM inference and training optimization. The paper's focus on reducing inference latency while maintaining quality through multi-step deterministic sampling is highly relevant to my research interests in optimizing large model systems."
    }
  },
  {
    "id": "YFxfcQMLWX",
    "title": "PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer",
    "abstract": "We present Polynomial Attention Drop-in Replacement (PADRe), a novel and unifying framework designed to replace the conventional self-attention mechanism in transformer models. Notably, several recent alternative attention mechanisms, including Hyena, Mamba, SimA, Conv2Former, and Castling-ViT, can be viewed as specific instances of our PADRe framework.  PADRe leverages polynomial functions and draws upon established results from approximation theory, enhancing computational efficiency without compromising accuracy.  PADRe's key components include multiplicative nonlinearities, which we implement using straightforward, hardware-friendly operations such as Hadamard products, incurring only linear computational and memory costs. PADRe further avoids the need for using complex functions such as Softmax, yet it maintains comparable or superior accuracy compared to traditional self-attention. We assess the effectiveness of PADRe as a drop-in replacement for self-attention across diverse computer vision tasks. These tasks include image classification, image-based 2D object detection, and 3D point cloud object detection. Empirical results demonstrate that PADRe runs significantly faster than the conventional self-attention (11x~43x faster on server GPU and mobile NPU) while maintaining similar accuracy when substituting self-attention in the transformer models.",
    "authors": [
      "~Pierre-David_Letourneau1",
      "~Manish_Kumar_Singh1",
      "~Hsin-Pai_Cheng1",
      "~Shizhong_Han2",
      "~Yunxiao_Shi2",
      "~Dalton_Jones1",
      "~Matthew_Harper_Langston1",
      "~Hong_Cai1",
      "~Fatih_Porikli2"
    ],
    "pdf": "/pdf/a7ea4cbd8d0039ca17c319f4aa11bca4b687837b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on attention mechanism optimization which is central to transformer efficiency. It claims significant speed improvements (11x-43x) on server GPUs, which directly relates to inference optimization. It reduces computational complexity to linear costs, impacting GPU utilization and scalability. The paper unifies several recent attention mechanisms (Hyena, Mamba, SimA, etc.), potentially providing insights for broader optimization strategies. It maintains accuracy while improving efficiency, which is a key goal in optimization work.",
      "Irrelevant Aspects": "The focus is specifically on vision transformers rather than language models. Evaluation is on vision tasks (image classification, object detection) rather than NLP tasks. It mentions mobile NPU optimization which is less central to large-scale systems focus. There's no mention of training optimization specifically, only inference improvements.",
      "Summary": "This paper is moderately relevant as it addresses transformer efficiency with potential transferability to LLMs. While its vision-specific focus reduces direct applicability, the attention optimization techniques and claimed 11x-43x speed improvements on server GPUs are highly relevant to inference optimization work."
    }
  },
  {
    "id": "JCiF03qnmi",
    "title": "How Does Critical Batch Size Scale in Pre-training?",
    "abstract": "Training large-scale models under given resources requires careful design of parallelism strategies. In particular, the efficiency notion of critical batch size (CBS), concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing returns. To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and careful control of factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS. Then we fit scaling laws with respect to model and data sizes to decouple their effects. Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression. Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations.",
    "authors": [
      "~Hanlin_Zhang1",
      "~Depen_Morwani1",
      "~Nikhil_Vyas1",
      "~Jingfeng_Wu1",
      "~Difan_Zou1",
      "~Udaya_Ghai1",
      "~Dean_Foster1",
      "~Sham_M._Kakade1"
    ],
    "pdf": "/pdf/c55a8642ec258b0ecb13caf67ba2c61bffd74929.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Critical batch size relates to GPU utilization and scalability, the paper studies optimization of training through hyper-parameter sweeps, analyzes large language models (85 million to 1.2 billion parameters), examines how CBS scales with model and data size which is crucial for throughput optimization, and findings could inform parallelism strategies for better resource utilization",
      "Irrelevant Aspects": "Primarily focuses on pre-training rather than inference optimization, theoretical analysis of infinite-width limits might not directly translate to practical optimization, doesn't appear to address latency directly",
      "Summary": "The paper is highly relevant as it addresses critical batch size, a key parameter affecting GPU utilization and throughput in large language model training. Understanding CBS scaling is crucial for optimizing parallelism strategies and resource utilization. While it doesn't directly address inference optimization or latency, the training efficiency insights could inform system design impacting the entire ML pipeline."
    }
  },
  {
    "id": "sgbI8Pxwie",
    "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix",
    "abstract": "Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our empirical results demonstrate the effectiveness of our non-linear pruning approach in maintaining model performance while significantly reducing computational costs, which is beyond the current state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.",
    "authors": [
      "~Yingyu_Liang1",
      "~Jiangxuan_Long1",
      "~Zhenmei_Shi1",
      "~Zhao_Song3",
      "~Yufa_Zhou1"
    ],
    "pdf": "/pdf/2c275b3f4e28f71f72d98caefdabd495a451492a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through a novel pruning approach that focuses on the attention matrix. It claims to significantly reduce computational costs while maintaining model performance, which is directly relevant to improving GPU utilization, throughput, and latency. The approach outperforms existing methods like SparseGPT and Wanda, and provides theoretical guarantees for convergence.",
      "Irrelevant Aspects": "The focus seems more on edge device deployment rather than server environments where scalability and GPU utilization optimization are primary concerns. The abstract doesn't provide specific details about how the pruning approach impacts memory bandwidth utilization or parallel execution, which are key factors in my research area.",
      "Summary": "This paper introduces a novel non-linear pruning approach for attention matrices in transformers that optimizes LLM inference efficiency. While its focus on edge device deployment makes it slightly less aligned with server environment optimization, the direct impact on reducing computational costs while maintaining performance makes it highly relevant to my research interests in LLM inference optimization."
    }
  },
  {
    "id": "ERv8ptegFi",
    "title": "GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS",
    "abstract": "Multi-agent learning algorithms have been successful at generating superhuman planning in various games but have had limited impact on the design of deployed multi-agent planners. A key bottleneck in applying these techniques to multi-agent planning is that they require billions of steps of experience. To enable the study of multi-agent planning at scale, we present GPUDrive, a GPU-accelerated, multi-agent simulator built on top of the Madrona Game Engine capable of generating over a million simulation steps per second. Observation, reward, and dynamics functions are written directly in C++, allowing users to define complex, heterogeneous agent behaviors that are lowered to high-performance CUDA. Despite these low-level optimizations, GPUDrive is fully accessible through Python, offering a seamless and efficient workflow for multi-agent, closed-loop simulation. Using GPUDrive, we train reinforcement learning agents on the Waymo Open Motion Dataset, achieving efficient goal-reaching in minutes and scaling to thousands of scenarios in hours. We open-source the code and pre-trained agents at \\url{www.github.com/Emerge-Lab/gpudrive}.",
    "authors": [
      "~Saman_Kazemkhani1",
      "~Aarav_Pandya1",
      "~Daphne_Cornelisse1",
      "~Brennan_Shacklett1",
      "~Eugene_Vinitsky1"
    ],
    "pdf": "/pdf/58416eb8dcfad96ca7cb5ada85252ab5f1d42c94.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses heavily on GPU acceleration and optimization, achieving 1 million FPS through CUDA implementations. This directly relates to my research interest in GPU utilization optimization. The system demonstrates impressive throughput performance and scalability to thousands of scenarios. The Python integration with C++/CUDA backend represents a practical approach to maintaining developer productivity while achieving performance gains, which is relevant to my focus on optimization strategies.",
      "Irrelevant Aspects": "The application domain (driving simulation) differs from my focus on large language models. The paper concentrates on multi-agent reinforcement learning rather than LLM training or inference specifically. The simulation environment, while optimized, serves a different purpose than the direct model optimization I focus on.",
      "Summary": "GPUDrive presents a GPU-accelerated simulator achieving exceptional performance (1M FPS) through CUDA optimizations. While the domain is driving simulation rather than LLMs, the technical approach to GPU utilization, performance optimization, and scalability offers valuable insights applicable to my research interests in machine learning systems optimization."
    }
  },
  {
    "id": "bAFVlpFQvT",
    "title": "Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures",
    "abstract": "Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding small mini-batch coresets that closely match the gradient of larger mini-batches. However, this approach becomes infeasible and ineffective for LLMs, due to the highly imbalanced mixture of sources in language data, use of the Adam optimizer, and the very large gradient dimensionality of LLMs. In this work, we address the above challenges by proposing *Coresets for Training LLMs* (CoLM). First, we show that mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance. Second, we normalize the gradients by their historical exponential to find mini-batch coresets for training with Adam. Finally, we leverage zeroth-order methods to find smooth gradient of the last *V*-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with existing memory-efficient training methods like LoRA, further reducing the memory requirements of training LLMs.",
    "authors": [
      "~Dang_Nguyen2",
      "~Wenhan_Yang5",
      "~Rathul_Anand1",
      "~Yu_Yang4",
      "~Baharan_Mirzasoleiman1"
    ],
    "pdf": "/pdf/e2c705a531f857c83aeb29df92db7cd33881fe42.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Memory-efficient training for LLMs",
        "Mini-batch optimization techniques",
        "Gradient dimensionality reduction",
        "GPU memory optimization",
        "Scalability improvements for large model training",
        "Integration with existing memory-efficient methods like LoRA",
        "Data mixture handling for imbalanced sources",
        "Adam optimizer compatibility"
      ],
      "Irrelevant Aspects": [
        "Focus primarily on fine-tuning rather than pre-training",
        "Limited evaluation on specific models (Phi, Zephyr, Llama families)",
        "Application to specific benchmarks (MathInstruct, SuperGLUE) rather than general training"
      ],
      "Summary": "The paper introduces CoLM, a method for memory-efficient language model training using mini-batch coresets that match gradients of larger mini-batches. It addresses key challenges in LLM training by reducing memory requirements by 2x while maintaining or improving performance. The approach handles imbalanced data mixtures, works with Adam optimizer, and integrates with methods like LoRA. While highly relevant to memory optimization and training efficiency, its focus on fine-tuning specific models slightly limits broader applicability to all training scenarios."
    }
  },
  {
    "id": "3cvwO5DBZn",
    "title": "On Speeding Up Language Model Evaluation",
    "abstract": "Developing prompt-based methods with Large Language Models (LLMs) requires making numerous decisions, which give rise to a combinatorial search problem over hyper-parameters. This exhaustive evaluation can be time-consuming and costly. In this paper, we propose an \\textit{adaptive} approach to explore this space. We are exploiting the fact that often only few samples are needed to identify clearly superior or inferior settings, and that many evaluation tests are highly correlated. We lean on multi-armed bandits to sequentially identify the next (method, validation sample)-pair to evaluate and utilize low-rank matrix factorization to fill in missing evaluations. We carefully assess the efficacy of our approach on several competitive benchmark problems and show that it can identify the top-performing method using only 5-15% of the typical resources---resulting in 85-95% LLM cost savings. Our code is available at https://github.com/kilian-group/banditeval.",
    "authors": [
      "~Jin_Peng_Zhou1",
      "~Christian_K_Belardi1",
      "~Ruihan_Wu1",
      "~Travis_Zhang1",
      "~Carla_P_Gomes1",
      "~Wen_Sun1",
      "~Kilian_Q_Weinberger1"
    ],
    "pdf": "/pdf/ad5ada1fdf5fb68608b555da96073b6f1f7fefe1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM evaluation efficiency, which is directly relevant to resource utilization in ML systems. It addresses reducing computational costs (85-95% savings) for LLM processes, which impacts GPU utilization and overall system efficiency. The use of multi-armed bandits and matrix factorization for adaptive evaluation represents an optimization approach that could scale to larger systems and potentially be applied to other ML optimization challenges.",
      "Irrelevant Aspects": "The focus is on evaluation phase optimization rather than training/inference optimization specifically. The paper doesn't directly address latency or throughput improvements during actual model deployment. The optimization technique is more about experimental design efficiency rather than runtime performance of the models themselves.",
      "Summary": "This paper presents an adaptive approach to accelerate LLM evaluation using multi-armed bandits and low-rank matrix factorization. It achieves 85-95% cost savings by intelligently selecting which evaluations to perform. While primarily focused on evaluation rather than training/inference optimization, its substantial resource savings and scalable methodology make it highly relevant to efficient LLM system development."
    }
  },
  {
    "id": "90DC0IvlSs",
    "title": "LevAttention: Time, Space and Streaming Efficient Algorithm for Heavy Attentions",
    "abstract": "A central problem related to transformers can be stated as follows: given two $n \\times d$ matrices $Q$ and $K$, and a non-negative function $f$, define the matrix $A$ as follows: (1) apply the function $f$ to each entry of the $n \\times n$ matrix $Q K^T$, and then (2) normalize each of the row sums of $A$ to be equal to $1$. The matrix $A$ can be computed in $O(n^2 d)$ time assuming $f$ can be applied to a number in constant time, but the quadratic dependence on $n$ is prohibitive in applications where it corresponds to long context lengths. For a large class of functions $f$, we show how to find all the \"large attention scores\", i.e., entries of $A$ which are at least a positive value $\\varepsilon$, in time with linear dependence on $n$ (i.e., $n \\cdot \\textrm{poly}(d/\\varepsilon)$) for a positive parameter $\\varepsilon > 0$. Our class of functions include all functions $f$ of the form $f(x) = |x|^p$, as explored recently in transformer models. Using recently developed tools from randomized numerical linear algebra, we prove that for any $K$, there is a \"universal set\" $U \\subset [n]$ of size independent of $n$, such that for any $Q$ and any row $i$, the large attention scores $A_{i,j}$ in row $i$ of $A$ all have $j \\in U$. We also find $U$ in $n \\cdot \\textrm{poly}(d/\\varepsilon)$ time. Notably, we \n(1) make no assumptions on the data, (2) our workspace does not grow with $n$, and (3) our algorithms can be computed in streaming and parallel settings. We empirically show the benefits of our scheme for vision transformers, showing how to train new models that use our universal set while training as well, showing that our model is able to consistently select \"important keys'\" during training. We also provide theoretical motivation by formulating a planted model in which our efficient algorithms provably identify relevant keys for \neach query.",
    "authors": [
      "~Ravindran_Kannan1",
      "~Chiranjib_Bhattacharyya1",
      "~Praneeth_Kacham1",
      "~David_Woodruff1"
    ],
    "pdf": "/pdf/023378fee6afaa67593d97cb2bb8eb8142dfe1e9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses the quadratic computational complexity of attention mechanisms in transformers, which is a major bottleneck for both training and inference efficiency. By reducing time complexity from O(n²d) to O(n·poly(d/ε)), it has significant implications for GPU utilization, scalability, throughput, and latency. The algorithm's compatibility with streaming and parallel settings is particularly valuable for distributed systems. The concept of a 'universal set' with size independent of sequence length offers promising memory efficiency benefits. These aspects align well with optimization goals for large language models.",
      "Irrelevant Aspects": "The paper appears primarily theoretical with limited practical implementation details specific to GPU architectures. The empirical validation focuses on vision transformers rather than language models, though the principles should transfer. There's insufficient discussion of integration with existing training/inference pipelines or specific GPU memory management strategies. The approach emphasizes finding 'large attention scores' rather than full attention computation efficiency, which may limit its applicability in some scenarios.",
      "Summary": "LevAttention presents a theoretically grounded approach to reduce attention computation complexity from quadratic to linear time, which is highly relevant to transformer efficiency optimization. The algorithm identifies a 'universal set' of important attention scores with constant space complexity, offering potential for significant improvements in GPU utilization and scalability. While the theoretical foundations are strong and the approach shows promise for parallel/streaming settings, the paper's focus on theory and vision transformers rather than practical implementation details for language models slightly limits its immediate applicability to my specific research interests in LLM optimization."
    }
  },
  {
    "id": "hrOlBgHsMI",
    "title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs",
    "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.",
    "authors": [
      "~Shane_Bergsma1",
      "~Nolan_Simran_Dey1",
      "~Gurpreet_Gosal2",
      "~Gavia_Gray1",
      "~Daria_Soboleva1",
      "~Joel_Hestness2"
    ],
    "pdf": "/pdf/5b7fd3e5e1f45010d000a49a21c8b5deb4fc79e1.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for LLMs through learning rate scheduling, which is a key component of my research interests. It demonstrates significant compute savings (60% in some cases) by using linear decay-to-zero (D2Z) schedules instead of standard cosine decay with 10x decay. This has important implications for GPU utilization and scalability in large-scale training. The paper provides empirical evidence across various model sizes including large models like Llama2-7B, showing the method's effectiveness at scale.",
      "Irrelevant Aspects": "The paper focuses primarily on training optimization and doesn't address inference optimization techniques, which is another key aspect of my research interests. There's limited discussion on direct throughput improvements or latency reduction strategies, though compute efficiency could indirectly affect these metrics. The paper's theoretical explanation based on AdamW as an exponential moving average is somewhat tangential to my core interests.",
      "Summary": "This paper presents a highly relevant study on optimizing LLM training through learning rate scheduling. It shows that linearly decaying the learning rate to zero (D2Z) consistently outperforms standard cosine decay with 10x decay when training at compute-optimal dataset sizes. The method demonstrates compute savings of up to 60% and works across various model sizes, making it valuable for improving GPU utilization and scalability in large-scale LLM training. While it doesn't address inference optimization, its focus on training efficiency makes it highly relevant to my research interests."
    }
  },
  {
    "id": "lJ66m0ibQL",
    "title": "Diversity-Rewarded CFG Distillation",
    "abstract": "Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives:  (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality.  According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://musicdiversity.github.io/.",
    "authors": [
      "~Geoffrey_Cideron1",
      "~Andrea_Agostinelli1",
      "~Johan_Ferret1",
      "~Sertan_Girgin1",
      "~Romuald_Elie3",
      "~Olivier_Bachem1",
      "~Sarah_Perrin1",
      "~Alexandre_Rame1"
    ],
    "pdf": "/pdf/d4f240c258493c5d13418b76e0955446e4bb17c8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization by eliminating the computational overhead of Classifier-Free Guidance (CFG) through distillation. It addresses reducing inference cost (doubles without optimization) and improving throughput by removing the need for CFG during deployment. The finetuning procedure optimizes model weights to directly generate outputs that would otherwise require CFG, which is directly relevant to inference optimization. The paper also discusses model merging strategies for deployment-time control over quality-diversity trade-offs.",
      "Irrelevant Aspects": "The paper specifically applies to music generation models (MusicLM) rather than general LLMs. It doesn't explicitly address GPU utilization or distributed computing scalability. The focus is primarily on quality-diversity trade-offs rather than raw computational performance metrics. Limited discussion of memory optimization or parallel computing strategies that would be relevant to large-scale LLM deployment.",
      "Summary": "This paper introduces a distillation method to eliminate the inference overhead of Classifier-Free Guidance in generative models, specifically applied to music generation. While the application domain differs from general LLMs, the core technique of distilling inference-time computation into model weights to improve efficiency is directly relevant to inference optimization goals. The paper successfully reduces inference cost without sacrificing quality or diversity, though it doesn't address GPU utilization or distributed scalability concerns."
    }
  },
  {
    "id": "HE6pJoNnFp",
    "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
    "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Speciﬁcally, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, ﬁltering out undesirable contexts enhances the model’s focus on relevant context, inherently improving its generation quality. Evaluation results on four datasets show that Sparse RAG can be used to strike an optimal balance between generation quality and computational efﬁciency, demonstrating its generalizability across tasks.",
    "authors": [
      "~Yun_Zhu5",
      "~Jia-Chen_Gu1",
      "~Caitlin_Sikora1",
      "~Ho_Ko1",
      "~Yinxiao_Liu2",
      "~Chu-Cheng_Lin1",
      "~Lei_Shu1",
      "~Liangchen_Luo1",
      "~Lei_Meng2",
      "~Bang_Liu1",
      "~Jindong_Chen2"
    ],
    "pdf": "/pdf/769d19628371c26a5cfb2ed069a75308ace66fd5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference latency optimization for LLMs, introduces sparsity to reduce computational costs, implements parallel encoding of retrieved documents for better throughput, and focuses on balancing generation quality with computational efficiency. These aspects align with the focus on LLM inference optimization, GPU utilization, and throughput improvement.",
      "Irrelevant Aspects": "The paper is narrowly focused on Retrieval-Augmented Generation systems rather than general LLM optimization. There's no explicit mention of GPU-specific optimizations or training optimization techniques. The abstract doesn't provide details about hardware-specific implementations or scalability across distributed systems.",
      "Summary": "This paper proposes Sparse RAG, a method to accelerate inference in retrieval-augmented generation systems through selective attention to relevant contexts. While it addresses important aspects of inference optimization (latency reduction, computational efficiency), it's specialized to RAG systems and lacks discussion of broader LLM optimization techniques or GPU-specific implementations."
    }
  },
  {
    "id": "ZadnlOHsHv",
    "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
    "abstract": "Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more energy-efficient than LLMs with similar parameters. Inspired by this, we redesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01\\% WikiText2 perplexity and improves 2.55\\% accuracy of common scene reasoning on a LLAMA-7B W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs. Our code is publicly available at https://github.com/Xingrun-Xing2/SpikeLLM.",
    "authors": [
      "~Xingrun_Xing1",
      "~Boyan_Gao1",
      "~Zheng_Liu4",
      "~David_A._Clifton1",
      "~Shitao_Xiao1",
      "~Wanpeng_Zhang1",
      "~Li_Du3",
      "~Zheng_Zhang12",
      "~Guoqi_Li1",
      "~Jiajun_Zhang1"
    ],
    "pdf": "/pdf/05e3c18312d73c7a31a3d0b034e607cae68271fc.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "The paper directly addresses energy efficiency in LLM inference, which is a critical aspect of my GPU optimization research",
        "It proposes methods to compress spike length from T to log2T bits, which directly impacts memory bandwidth and computational efficiency",
        "The work covers models from 7B to 70B parameters, which is directly relevant to my focus on scalability of large models",
        "The paper compares against quantized models and shows improvements in perplexity and accuracy, suggesting practical performance gains",
        "The approach could lead to reduced GPU memory usage during inference, which is key for throughput optimization"
      ],
      "Irrelevant Aspects": [
        "The paper focuses primarily on spiking neural networks rather than traditional optimization approaches like quantization, pruning, or distillation",
        "It doesn't explicitly discuss GPU utilization metrics, kernels optimization, or memory access patterns",
        "The biological plausibility angle is less relevant to practical GPU optimization",
        "Limited discussion of latency measurements or throughput comparisons with standard inference optimization techniques"
      ],
      "Summary": "SpikeLLM introduces a novel approach to LLM inference optimization by leveraging spiking neural networks to improve energy efficiency. The paper presents methods to compress spike representation and claims performance improvements over quantized models. While fundamentally different from typical GPU optimization techniques, this work could offer complementary approaches to reducing computational requirements for large language models during inference."
    }
  },
  {
    "id": "Zhdhg6n2OG",
    "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
    "abstract": "Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.",
    "authors": [
      "~Jason_Ramapuram1",
      "~Federico_Danieli1",
      "~Eeshan_Gunesh_Dhekane1",
      "~Floris_Weers1",
      "~Dan_Busbridge1",
      "~Pierre_Ablin2",
      "~Tatiana_Likhomanenko1",
      "~Jagrit_Digani1",
      "~Zijin_Gu1",
      "~Amitis_Shidani1",
      "~Russell_Webb1"
    ],
    "pdf": "/pdf/963c799c2cb84f0433a9e6b680cc54dcfb6b48e3.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "FLASHSIGMOID implementation achieving 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs, hardware-aware and memory-efficient implementation, performance improvements in transformer attention mechanisms, applicability across multiple domains and scales for better scalability",
      "Irrelevant Aspects": "Theoretical proofs about universal function approximation, improved regularity properties compared to softmax attention, detailed analysis of attention norm stabilization during early training, function approximation properties of sigmoid attention",
      "Summary": "The paper introduces FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention that achieves 17% faster inference than FLASHATTENTION2 on H100 GPUs. While it includes theoretical analysis of function approximation properties, its most valuable contribution is the practical speed improvement for inference on modern GPU hardware, directly addressing my research focus on inference optimization and better GPU utilization."
    }
  },
  {
    "id": "DJSZGGZYVi",
    "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
    "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
    "authors": [
      "~Sihyun_Yu2",
      "~Sangkyung_Kwak1",
      "~Huiwon_Jang1",
      "~Jongheon_Jeong1",
      "~Jonathan_Huang1",
      "~Jinwoo_Shin1",
      "~Saining_Xie2"
    ],
    "pdf": "/pdf/7154d22b1b281a6a63a840a36f74c3e3133dc22a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization with 17.5x speedup for diffusion transformers, improved GPU utilization through more efficient training, better scalability by reducing training steps from 7M to under 400K, higher throughput, transformer architecture optimization techniques",
      "Irrelevant Aspects": "Focus on vision/generation models rather than language models, no direct discussion of inference optimization, no explicit mention of latency improvements, representation alignment technique is specific to visual domain",
      "Summary": "The paper introduces REPresentation Alignment (REPA), a regularization method that significantly improves training efficiency for diffusion transformers. While focused on vision models rather than language models, the 17.5x training speedup demonstrates valuable optimization techniques for transformer architectures that could inform broader ML system optimization strategies. The lack of inference optimization discussion prevents a higher score."
    }
  },
  {
    "id": "lqHv6dxBkj",
    "title": "SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs",
    "abstract": "We propose SLoPe, a Double-Pruned **S**parse Plus **L**azy L**o**w-rank Adapter **P**r**e**training method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference. In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass. SLoPe accelerates the training and inference of models with billions of parameters up to 1.25× and 1.54× respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to 0.63× and 0.61× for training and inference respectively.",
    "authors": [
      "~Mohammad_Mozaffari1",
      "~Amir_Yazdanbakhsh1",
      "~Zhao_Zhang1",
      "~Maryam_Mehri_Dehnavi2"
    ],
    "pdf": "/pdf/43aa6bb4bbee6b860c8986ef13a382eb4c92568b.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Sparse training and inference optimization, memory footprint reduction, acceleration of training and inference (1.25× for training and 1.54× for inference), low-rank adapter techniques, N:M sparsity structures, backward pass optimization, large-scale model experiments (OPT-33B and OPT-66B)",
      "Irrelevant Aspects": "Focus on practical implementation rather than theoretical foundations, no novel hardware-specific optimizations beyond general GPU utilization, approach limited to final 1% iterations of pretraining",
      "Summary": "SLoPe proposes a method combining double-pruned sparse representations with low-rank adapters to improve LLM pretraining and inference efficiency. The technique delivers significant speedups (1.25× for training, 1.54× for inference) and memory reductions (0.63× for training, 0.61× for inference) on large models, directly addressing GPU utilization, scalability, throughput, and latency concerns in LLM systems."
    }
  },
  {
    "id": "uJqKf24HGN",
    "title": "UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models",
    "abstract": "We introduce UniCon, a novel architecture designed to enhance control and efficiency in training adapters for large-scale diffusion models. Unlike existing methods that rely on bidirectional interaction between the diffusion model and control adapter, UniCon implements a unidirectional flow from the diffusion network to the adapter, allowing the adapter alone to generate the final output. UniCon reduces computational demands by eliminating the need for the diffusion model to compute and store gradients during adapter training. Our results indicate that UniCon reduces GPU memory usage by one-third and increases training speed by 2.3 times, while maintaining the same adapter parameter size. Additionally, without requiring extra computational resources, UniCon enables the training of adapters with double the parameter volume of existing ControlNets. In a series of image conditional generation tasks, UniCon has demonstrated precise responsiveness to control inputs and exceptional generation capabilities.",
    "authors": [
      "~Fanghua_Yu1",
      "~Jinjin_Gu1",
      "~Jinfan_Hu1",
      "~Zheyuan_Li1",
      "~Chao_Dong4"
    ],
    "pdf": "/pdf/40cde075d8f999b4fa0bb185e5290bf90362c616.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses training optimization for large-scale models by reducing GPU memory usage by one-third and increasing training speed by 2.3 times. It introduces a novel architectural approach (unidirectional information flow) that improves system efficiency. The method enables training larger adapters within the same computational constraints, which relates to scalability concerns. These optimizations of GPU utilization and training speed are highly relevant to machine learning system optimization.",
      "Irrelevant Aspects": "The paper focuses on diffusion models for image generation rather than large language models, which is my primary research focus. The control mechanisms and adapter applications are specific to image conditional generation tasks rather than text processing or language understanding tasks.",
      "Summary": "UniCon presents a unidirectional information flow architecture for optimizing adapter training in diffusion models, achieving significant improvements in GPU memory usage (33% reduction) and training speed (2.3x increase). While applied to image generation rather than language models, the core optimization techniques for large-scale model training and the architectural innovations for efficiency are transferable concepts that could inform LLM training and inference optimization research."
    }
  },
  {
    "id": "IjduZQK8gM",
    "title": "From Attention to Activation: Unraveling the Enigmas of Large Language Models",
    "abstract": "We study two strange phenomena in auto-regressive Transformers: (1) the dominance of the ﬁrst token in attention heads; (2) the occurrence of large outlier activations in the hidden states. We ﬁnd that popular large language models, such as Llama attend maximally to the first token in 98% of attention heads, a behaviour we attribute to the softmax function. To mitigate this issue, we propose a reformulation of softmax to softmax-1. Furthermore, we identify adaptive optimisers, e.g. Adam, as the primary contributor to the large outlier activations and introduce OrthoAdam, a novel optimiser that utilises orthogonal matrices to transform gradients, to address this issue. Finally, not only do our methods prevent these phenomena from occurring, but additionally, they enable Transformers to sustain their performance when quantised using basic algorithms, something that standard methods are unable to do. In summary, our methods reduce the attention proportion on the first token from 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to 3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3. Code is available at https://github.com/prannaykaul/OrthoAdam",
    "authors": [
      "~Prannay_Kaul1",
      "~Chengcheng_Ma1",
      "~Ismail_Elezi1",
      "~Jiankang_Deng1"
    ],
    "pdf": "/pdf/cef243c0bdc5d2e5b2994c34f9998a455687ffba.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Studies phenomena in auto-regressive Transformers, proposes softmax-1 reformulation and OrthoAdam optimizer, addresses quantization for LLMs, improves attention distribution and activation properties which can affect training and inference efficiency",
      "Irrelevant Aspects": "Limited explicit focus on GPU utilization, throughput optimization, or latency metrics, more focused on explaining phenomena than system-level optimization",
      "Summary": "The paper investigates two phenomena in Transformers: first token dominance in attention and outlier activations. It proposes softmax-1 and OrthoAdam optimizer to address these issues, enabling better quantization performance which can impact inference efficiency and memory usage, though it doesn't explicitly focus on system-level performance metrics."
    }
  },
  {
    "id": "aLsMzkTej9",
    "title": "KBLaM: Knowledge Base augmented Language Model",
    "abstract": "In this paper, we propose Knowledge Base augmented Language Model (KBLAM), a new method for augmenting Large Language Models (LLMs) with external knowledge. KBLAM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and\nintegrating them into pre-trained LLMs via a specialized rectangular attention mechanism. Unlike Retrieval-Augmented Generation, KBLAM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB size rather than quadratically. Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. Experiments demonstrate KBLAM’s effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its use of the augmented knowledge. Code and datasets are available at https://github.com/microsoft/KBLaM/",
    "authors": [
      "~Xi_Wang4",
      "~Taketomo_Isazawa1",
      "~Liana_Mikaelyan1",
      "~James_Hensman1"
    ],
    "pdf": "/pdf/f3991f41e22bfc1aff4f7ca53c75b20636fb21ba.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "KBLaM focuses on efficient integration of external knowledge with LLMs, which addresses GPU utilization by enabling large KB integration on a single A100 GPU. The method reduces computational complexity with linear scaling versus quadratic scaling of in-context learning, improving scalability and throughput. It eliminates external retrieval modules during inference, potentially reducing latency. The specialized rectangular attention mechanism represents an architectural optimization for better resource usage. Dynamic knowledge updates without retraining improve system efficiency and flexibility.",
      "Irrelevant Aspects": "The paper primarily focuses on knowledge integration rather than training optimization techniques. It doesn't address distributed training or inference systems, concentrating on single GPU deployment. There's limited discussion of model compression techniques like quantization or pruning. The approach builds upon pre-trained models rather than introducing novel training architectures. Hardware-specific optimizations beyond the single GPU case are not explored.",
      "Summary": "KBLaM presents a knowledge base augmentation method for LLMs that improves inference efficiency by integrating knowledge directly into the model through vector representations and a specialized attention mechanism. It demonstrates efficient resource utilization by running large knowledge bases with 8B models on single GPUs. The approach scales linearly with KB size, improving throughput and potentially reducing latency compared to retrieval-augmented methods. While highly relevant to inference optimization and resource utilization, it focuses less on training optimization techniques and distributed systems considerations."
    }
  },
  {
    "id": "pQqeQpMkE7",
    "title": "On Scaling Up 3D Gaussian Splatting Training",
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS",
    "authors": [
      "~Hexu_Zhao2",
      "~Haoyang_Weng2",
      "~Daohan_Lu1",
      "~Ang_Li11",
      "~Jinyang_Li1",
      "~Aurojit_Panda1",
      "~Saining_Xie2"
    ],
    "pdf": "/pdf/ebc7c3bfc09b3459f9b5aa35f10b10869ea6fa15.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Distributed training system design with parameter partitioning across multiple GPUs, sparse all-to-all communication optimization, dynamic load balancing for better GPU utilization, batched training approaches, hyperparameter scaling strategies (sqrt(batch-size) rule), scaling up model parameters to improve performance, handling memory constraints in large-scale training",
      "Irrelevant Aspects": "Specific 3D reconstruction algorithms and techniques, visual quality metrics like PSNR which are not applicable to language models, 3D Gaussian Splatting specific data structures and rendering methods, camera view processing which is specific to visual reconstruction",
      "Summary": "This paper presents Grendel, a distributed system for scaling up 3D Gaussian Splatting training across multiple GPUs. While focused on 3D reconstruction rather than language models, it offers valuable insights into distributed training optimization, communication patterns, and batch processing strategies that could inform approaches to LLM training. The techniques for partitioning parameters, implementing sparse communication, and dynamic load balancing are particularly relevant to maximizing GPU utilization in distributed ML systems."
    }
  },
  {
    "id": "KDGP8yAz5b",
    "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
    "abstract": "The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but \\textit{the maximum supported context length in length extrapolation} remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the self-attention mechanism, which cannot effectively and efficiently capture the semantic relationships within infinitely long contexts via the limited pre-trained positional information and attention scope. In this work, we propose \\textbf{ReAttention}, a training-free approach enabling LLM based on the self-attention mechanism to support an infinite context with a finite attention scope under sufficient memory resources. ReAttention performs the position-agnostic top-$k$ attention before the ordinary position-aware self-attention, freeing LLMs from the length extrapolation issue. We validate the performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and demonstrate that it is on par with traditional methods. Furthermore, we also apply ReAttention on mainstream LLMs, including LLaMA3.1-8B and Mistral-v0.3-7B, enabling them to support context lengths of at least 1M and even expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M without any further training in Needle-In-A-Haystack tests. We also improve the efficiency of ReAttention with Triton and achieve an efficient extrapolation without additional overhead. The code is available at \\url{https://github.com/OpenMOSS/ReAttention}.",
    "authors": [
      "~Xiaoran_Liu1",
      "~Ruixiao_Li1",
      "~Zhigeng_Liu1",
      "~Qipeng_Guo1",
      "~Yuerong_Song1",
      "~Kai_Lv2",
      "~Hang_Yan2",
      "~Linlin_Li3",
      "~Qun_Liu1",
      "~Xipeng_Qiu1"
    ],
    "pdf": "/pdf/c4a79047e1d2262249d8b3871819ff0cdd4e3d66.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training-free approach for inference optimization, enables infinite context with finite attention scope, improves memory utilization, achieves efficient extrapolation with Triton, enhances scalability of existing LLMs, supports context lengths up to 4M without additional training, addresses higher throughput and lower latency goals",
      "Irrelevant Aspects": "Focuses only on inference optimization without addressing training optimization techniques, limited architectural modifications, primarily validates on length extrapolation benchmarks rather than system-level performance metrics",
      "Summary": "ReAttention introduces a training-free method to extend LLM context length by performing position-agnostic top-k attention before standard self-attention. It enables mainstream LLMs to handle contexts up to 4M tokens without additional training, with implementation optimizations using Triton that achieve efficient extrapolation without overhead. While valuable for inference optimization and memory efficiency, it doesn't address training optimization techniques."
    }
  },
  {
    "id": "LB5cKhgOTu",
    "title": "QERA: an Analytical Framework for Quantization Error Reconstruction",
    "abstract": "The growing number of parameters and computational demands of large language models (LLMs) present significant challenges for their efficient deployment.\nRecently, there is an increasing interest in quantizing weights to extremely low precision while offsetting the resulting error with low-rank, high-precision error reconstruction terms.\nThe combination of quantization and low-rank approximation is now popular in both adapter-based, parameter-efficient fine-tuning methods such as LoftQ and low-precision inference techniques including ZeroQuant-V2.\nUsually, the low-rank terms are calculated via the singular value decomposition (SVD) of the weight quantization error,\nminimizing the Frobenius and spectral norms of the weight approximation error.\nRecent methods like LQ-LoRA and LQER introduced hand-crafted heuristics to minimize errors in layer outputs (activations) rather than weights, resulting improved quantization results.\nHowever, these heuristic methods lack an analytical solution to guide the design of quantization error reconstruction terms.\nIn this paper, we revisit this problem and formulate an analytical framework, named Quantization Error Reconstruction Analysis (QERA),\nand offer a closed-form solution to the problem.\nWe show QERA benefits both existing low-precision fine-tuning and inference methods --\nQERA achieves a fine-tuned accuracy gain of $\\Delta_{\\text{acc}}$ = 6.05\\% of 2-bit RoBERTa-base on GLUE compared to LoftQ;\nand obtains $\\Delta_{\\text{acc}}$ = 2.97\\% higher post-training quantization accuracy of 4-bit Llama-3.1-70B on average than ZeroQuant-V2 and $\\Delta_{\\text{ppl}}$ = $-$ 0.28 lower perplexity on WikiText2 than LQER.",
    "authors": [
      "~Cheng_Zhang21",
      "~Jeffrey_T._H._Wong1",
      "~Can_Xiao1",
      "~George_Anthony_Constantinides1",
      "~Yiren_Zhao2"
    ],
    "pdf": "/pdf/1fa584648945d11a1aa509b91189ff43549d0638.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM quantization to extremely low precision (2-bit, 4-bit), which is crucial for memory optimization and better GPU utilization. It focuses on both inference and training optimization through quantization error reconstruction techniques. The QERA framework provides an analytical solution for improving quantization accuracy in large models like Llama-3.1-70B, demonstrating significant scalability benefits. The method shows substantial accuracy gains in low-precision scenarios, which directly impacts the feasibility of deploying optimized models. The work applies to both parameter-efficient fine-tuning (LoftQ) and inference (ZeroQuant-V2), covering the full optimization pipeline.",
      "Irrelevant Aspects": "The paper doesn't explicitly focus on throughput, latency measurements, or GPU-specific optimization details. There's limited discussion about hardware-specific implementations or the direct impact on computational efficiency beyond accuracy preservation. The emphasis is primarily on maintaining model accuracy after quantization rather than on performance metrics like FLOPS, memory bandwidth utilization, or actual inference speed improvements.",
      "Summary": "QERA presents an analytical framework for quantization error reconstruction in LLMs, offering closed-form solutions to optimize extremely low-precision quantization (2-bit, 4-bit). The method demonstrates significant accuracy improvements over existing approaches like LoftQ, ZeroQuant-V2, and LQER across both training and inference scenarios. By providing an analytical foundation rather than heuristics for error reconstruction, QERA enables more systematic optimization of quantized models, which is directly relevant to improving GPU utilization and scalability of LLM deployment. The approach shows particular effectiveness with very large models (Llama-3.1-70B), making it highly relevant to scalable optimization strategies."
    }
  },
  {
    "id": "nT2u0M0nf8",
    "title": "CAMEx: Curvature-aware Merging of Experts",
    "abstract": "Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (Curvature-Aware Merging of Experts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifold's geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method. The code is publicly available at: https://github.com/kpup1710/CAMEx.",
    "authors": [
      "~Viet_Dung_Nguyen2",
      "~Minh_Nguyen_Hoang1",
      "~Luc_Nguyen1",
      "~Rachel_Teo1",
      "~Tan_Minh_Nguyen1",
      "~Linh_Duy_Tran1"
    ],
    "pdf": "/pdf/74dbcfaab05a1f086cf72ae33311f70971f80093.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Expert merging optimization for large language models, resource utilization optimization, efficient scaling of LLMs, memory-efficient training methods",
      "Irrelevant Aspects": "Primarily theoretical focus rather than system optimization, limited discussion of inference optimization, lack of explicit throughput/latency metrics",
      "Summary": "CAMEx introduces a curvature-aware expert merging method that improves training optimization and scalability for large language models by accounting for the non-Euclidean geometry of parameter spaces. While it addresses important aspects of training efficiency and memory usage, it focuses more on the mathematical foundations than system-level optimization of throughput and latency."
    }
  },
  {
    "id": "VNckp7JEHn",
    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
    "abstract": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. \nWe study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
    "authors": [
      "~Yangzhen_Wu1",
      "~Zhiqing_Sun1",
      "~Shanda_Li1",
      "~Sean_Welleck1",
      "~Yiming_Yang1"
    ],
    "pdf": "/pdf/d569f9b13d71b1935119a03c2ecff6b3870ff5a4.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs, focusing on compute-optimal configurations and trade-offs between model size and inference computation. It analyzes different inference strategies like best-of-n and tree search algorithms, which directly impact GPU utilization and computational efficiency. The finding that smaller models with advanced inference techniques can outperform larger models is highly relevant to resource optimization. The empirical analysis of cost-performance trade-offs provides insights into achieving better throughput and more efficient resource utilization.",
      "Irrelevant Aspects": "The paper focuses more on task-specific performance (MATH benchmark) than on system-wide throughput and latency metrics. While it addresses compute efficiency, it doesn't specifically discuss GPU memory management, batching strategies, or distributed inference systems. The analysis appears limited to problem-solving tasks rather than covering broader LLM inference scenarios.",
      "Summary": "This paper presents a comprehensive empirical analysis of inference scaling laws for LLMs, examining how different inference strategies can optimize compute utilization. The research finds that advanced inference algorithms applied to smaller models can achieve Pareto-optimal trade-offs between cost and performance, often outperforming larger models with simpler inference methods. While the paper doesn't address all aspects of system optimization, its focus on compute-optimal inference directly contributes to understanding how to achieve better GPU utilization and cost-effective LLM deployment."
    }
  },
  {
    "id": "8HuLgtjqOD",
    "title": "SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process",
    "abstract": "Training Large Language Models (LLMs) presents a significant communication bottleneck, predominantly due to the growing scale of the gradient to communicate across multi-device clusters. However, how to mitigate communication overhead in practice remains a formidable challenge due to the weakness of the methodology of the existing compression methods, especially the neglect of the characteristics of the gradient. In this paper, we consider and demonstrate the low-rank properties of gradient and Hessian observed in LLMs training dynamic, and take advantage of such natural properties to design SEPARATE, a simple low-rank projection for gradient compression in modern large-scale model training processes. SEPARATE realizes dimensional reduction by common random Gaussian variables and an improved moving average error-feedback technique. We theoretically demonstrate that SEPARATE-based optimizers maintain the original convergence rate for SGD and Adam-Type optimizers for general non-convex objectives. Experimental results show that SEPARATE accelerates training speed by up to 2× for GPT-2-Medium pre-training, and improves performance on various benchmarks for LLAMA2-7B fine-tuning.",
    "authors": [
      "~Hanzhen_Zhao2",
      "~Xingyu_Xie1",
      "~Cong_Fang1",
      "~Zhouchen_Lin1"
    ],
    "pdf": "/pdf/f5f0245765c8ef57b67798b6450e1e465967d577.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses training optimization for large language models through gradient compression, which significantly improves GPU utilization and scalability in multi-device clusters. It demonstrates practical improvements in training speed (2× acceleration) for GPT-2 and LLAMA2 models, enhancing throughput in distributed training scenarios. The technical focus on low-rank projection for gradients aligns perfectly with training optimization techniques for LLMs.",
      "Irrelevant Aspects": "The paper does not address inference optimization techniques or latency considerations during model serving. It focuses exclusively on training phase optimization rather than the complete ML lifecycle including deployment.",
      "Summary": "SEPARATE presents a gradient compression technique exploiting low-rank properties in LLM training dynamics, reducing communication overhead across multi-device clusters. The method maintains convergence guarantees for SGD and Adam optimizers while delivering 2× speedup in GPT-2 pre-training and improved performance in LLAMA2-7B fine-tuning. This work is highly relevant to LLM training optimization, improving scalability and resource utilization in distributed training environments."
    }
  },
  {
    "id": "5BjQOUXq7i",
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on diverse data mixtures, uses regression to predict performance of unseen mixtures, and applies the best predicted mixture to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens to fit the regression model and predict the best data mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000× larger and 25× longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Furthermore, RegMix consistently outperforms human selection in experiments involving models up to 7B models trained on 100B tokens, while matching or exceeding DoReMi using just 10% of the computational resources. Our experiments also show that (1) Data mixtures significantly impact performance; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws. Our code is available at https://github.com/sail-sg/regmix.",
    "authors": [
      "~Qian_Liu2",
      "~Xiaosen_Zheng1",
      "~Niklas_Muennighoff1",
      "~Guangtao_Zeng1",
      "~Longxu_Dou1",
      "~Tianyu_Pang1",
      "~Jing_Jiang1",
      "~Min_Lin1"
    ],
    "pdf": "/pdf/d0d62a2b2d872b279bdb0902d8ca7331d0b16f59.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization through data mixture selection, resource efficiency (uses 10% of resources compared to alternatives), scalable approach validated on models up to 7B parameters, performance optimization techniques that transcend scaling laws, practical insights for improving training efficiency and GPU utilization",
      "Irrelevant Aspects": "No focus on inference optimization techniques, no GPU architecture-specific optimizations, no system-level implementation details, no latency optimization, limited throughput analysis",
      "Summary": "RegMix addresses a critical aspect of training optimization by formulating data mixture selection as a regression problem, enabling efficient training of better-performing language models using fewer computational resources. While highly relevant to training optimization, it doesn't address inference optimization which is equally important to my research focus."
    }
  },
  {
    "id": "78Nn4QJTEN",
    "title": "When Attention Sink Emerges in Language Models: An Empirical View",
    "abstract": "Auto-regressive language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as **attention sink**. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others.  Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in auto-regressive LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how *optimization*, *data distribution*, *loss function*, and *model architecture* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, *storing extra attention scores*, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at https://github.com/sail-sg/Attention-Sink.",
    "authors": [
      "~Xiangming_Gu1",
      "~Tianyu_Pang1",
      "~Chao_Du1",
      "~Qian_Liu2",
      "~Fengzhuo_Zhang1",
      "~Cunxiao_Du3",
      "~Ye_Wang3",
      "~Min_Lin1"
    ],
    "pdf": "/pdf/9aaa67673456a03600b010729b9555f2b0fe7fd4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper investigates attention sink phenomenon which has been widely adopted in KV cache optimization, inference acceleration, and model quantization - all critical aspects of LLM inference optimization. It explores how pre-training optimization influences attention sink emergence and studies alternative attention mechanisms (sigmoid vs softmax) that could lead to more efficient models. These insights are directly valuable for improving GPU utilization, throughput, and reducing latency in production systems.",
      "Irrelevant Aspects": "The paper doesn't provide explicit GPU utilization measurements, throughput benchmarks, or latency comparisons. It focuses more on empirical analysis of the attention sink phenomenon rather than deployment-specific optimizations. There's limited discussion on hardware-specific implementation details that would be directly applicable to maximizing GPU performance.",
      "Summary": "This paper provides valuable insights into the attention sink phenomenon in autoregressive LMs, which has direct implications for inference optimization strategies. By understanding how attention sinks emerge during pre-training and how they affect model behavior, we can design more efficient attention mechanisms and KV cache strategies. The exploration of alternative attention mechanisms (sigmoid attention) that prevent attention sink emergence could lead to more computationally efficient models. While not directly focused on GPU utilization or latency metrics, these findings are highly relevant to optimizing LLM deployment for better scalability and performance."
    }
  },
  {
    "id": "eoln5WgrPx",
    "title": "Why Does the Effective Context Length of LLMs Fall Short?",
    "abstract": "Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. In this work, we attribute this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information.\nTo address this challenge, we introduce Shifted Rotray Position Embedding (STRING).  STRING  shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. \nExperimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with STRING  even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat.",
    "authors": [
      "~Chenxin_An1",
      "~Jun_Zhang27",
      "~Ming_Zhong2",
      "~Lei_Li14",
      "~Shansan_Gong1",
      "~Yao_Luo3",
      "~Jingjing_Xu1",
      "~Lingpeng_Kong1"
    ],
    "pdf": "/pdf/27e57436cfb9357e370c7531d1fa1358cb834e5d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on inference optimization for large language models without additional training, addresses performance improvements for large-scale models like Llama3.1 70B and Qwen2 72B, and involves position embedding optimization during inference.",
      "Irrelevant Aspects": "Limited focus on hardware utilization, no explicit discussion of throughput or latency improvements, minimal discussion of scalability aspects, and no mention of training infrastructure optimizations.",
      "Summary": "The paper addresses effective context length limitations in LLMs with STRING, an inference-time position embedding technique that improves performance without additional training. While it shows significant model improvements, it doesn't directly address GPU utilization, throughput, or latency optimizations central to my research interest."
    }
  },
  {
    "id": "WG7GzGx3G9",
    "title": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference",
    "abstract": "Large language models have demonstrated promising capabilities upon scaling up parameters. However, serving large language models incurs substantial computation and memory movement costs due to their large scale. Quantization methods have been employed to reduce service costs and latency. Nevertheless, outliers in activations hinder the development of INT4 weight-activation quantization. Existing approaches separate outliers and normal values into two matrices or migrate outliers from activations to weights, suffering from high latency or accuracy degradation. Based on observing activations from large language models, outliers can be classified into channel-wise and spike outliers.\nIn this work, we propose Rotated Runtime Smooth (**RRS**), a plug-and-play activation smoother for quantization, consisting of Runtime Smooth and the Rotation operation. Runtime Smooth (**RS**) is introduced to eliminate **channel-wise outliers** by smoothing activations with channel-wise maximums during runtime. The Rotation operation can narrow the gap between **spike outliers** and normal values, alleviating the effect of victims caused by channel-wise smoothing.\nThe proposed method outperforms the state-of-the-art method in the LLaMA and Qwen families and improves WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference.",
    "authors": [
      "~Ke_Yi2",
      "~Zengke_Liu1",
      "~jianwei_zhang13",
      "~Chengyuan_Li1",
      "~Tong_Zhang14",
      "~Junyang_Lin1",
      "~Jingren_Zhou1"
    ],
    "pdf": "/pdf/278b1958f7ad2da84758ae59770dd4a0003f88e6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses INT4 quantization for LLM inference optimization, focusing on reducing service costs and latency - directly relevant to GPU utilization and performance optimization. The training-free approach means no expensive retraining is needed. The proposed RRS method improves inference performance while maintaining model accuracy, which is crucial for practical deployment. The technique is applied to major model families (LLaMA and Qwen), demonstrating broad applicability.",
      "Irrelevant Aspects": "The paper has a narrow focus on quantization techniques without addressing broader GPU utilization strategies. It doesn't cover distributed inference or multi-GPU scaling aspects. No discussion of training optimization is included. The evaluation is limited to perplexity metrics without comprehensive throughput or latency measurements across different hardware configurations.",
      "Summary": "RRS presents a training-free activation smoothing technique to enable accurate INT4 quantization for LLMs, addressing outlier challenges in activations. The method combines Runtime Smooth to eliminate channel-wise outliers with a Rotation operation to handle spike outliers. Results show significant perplexity improvements for LLaMA and Qwen models at INT4 precision, making it relevant for inference optimization research focused on efficient deployment."
    }
  },
  {
    "id": "uZ5K4HeNwd",
    "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
    "abstract": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable latency. Recent advances have indicated that search and repeated sampling can enhance performance in various applications, such as theorem proving, code generation, and alignment, by utilizing greater computational resources during inference. In this study, we demonstrate that diffusion language models are capable of generating at least 32 tokens simultaneously, while exceeding the performance of AR models in text quality and on the LAMBADA natural language understanding benchmark. This outcome is achieved through a novel distillation method for discrete diffusion models, which reduces the number of inference steps by a factor of 32-64. Practically, at the 1.3B parameters scale, diffusion models, even without caching, can generate tokens at a rate that is up to 8 times faster than AR models employing KV-caching, and we anticipate further improvements with the inclusion of caching. Moreover, we demonstrate the efficacy of our approach for diffusion language models with up to 860M parameters.",
    "authors": [
      "~Justin_Deschenaux1",
      "~Caglar_Gulcehre1"
    ],
    "pdf": "/pdf/e37cb727a352616456a59c5b0c55ca633ac592ba.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs by proposing a method to generate multiple tokens simultaneously rather than sequentially, which is highly relevant to my research on reducing latency and improving throughput. The reported 8x speedup compared to AR models is significant. The novel distillation method that reduces inference steps by a factor of 32-64 has substantial implications for computational efficiency. The paper examines models at 1.3B and 860M parameter scales, which falls within the range of interest for scalability considerations. The comparison with KV-caching strategies provides insights into optimization techniques for inference.",
      "Irrelevant Aspects": "The paper doesn't focus extensively on training optimization techniques, which is one of my primary interests. GPU utilization optimization is not specifically discussed, though faster generation would indirectly impact this. The models examined (up to 1.3B parameters) are relatively small compared to the largest current LLMs (hundreds of billions of parameters). The abstract doesn't mention distributed systems considerations for training or inference, which is important for scalability at the largest scales.",
      "Summary": "This paper presents a novel approach to LLM inference optimization using diffusion language models that can generate 32 tokens simultaneously rather than one at a time as in autoregressive models. Through a new distillation method for discrete diffusion models, they reduce inference steps by 32-64x and achieve up to 8x faster generation compared to AR models with KV-caching at the 1.3B parameter scale. The work demonstrates significant advances in inference speed and efficiency, directly addressing latency and throughput concerns, though it focuses on smaller models than the largest state-of-the-art LLMs and doesn't emphasize GPU utilization or distributed computing aspects."
    }
  },
  {
    "id": "EKJhH5D5wA",
    "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration",
    "abstract": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate LLM inference without compromising quality. It works by first employing a compact model to draft multiple tokens efficiently and then using the target LLM to verify them in parallel. While this technique has achieved notable speedups, most existing approaches necessitate either additional parameters or extensive training to construct effective draft models, thereby restricting their applicability across different LLMs and tasks. To address this limitation, we explore a novel plug-and-play SD solution with layer-skipping, which skips intermediate layers of the target LLM as the compact draft model. Our analysis reveals that LLMs exhibit great potential for self-acceleration through layer sparsity and the task-specific nature of this sparsity. Building on these insights, we introduce SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively selects intermediate layers of LLMs to skip during inference. SWIFT does not require auxiliary models or additional training, making it a plug-and-play solution for accelerating LLM inference across diverse input data streams. Our extensive experiments across a wide range of models and downstream tasks demonstrate that SWIFT can achieve over a $1.3\\times$$\\sim$$1.6\\times$ speedup while preserving the original distribution of the generated text. We release our code in https://github.com/hemingkx/SWIFT.",
    "authors": [
      "~Heming_Xia1",
      "~Yongqi_Li1",
      "~Jun_Zhang26",
      "~Cunxiao_Du3",
      "~Wenjie_Li1"
    ],
    "pdf": "/pdf/b933229447e7ef536d79ced264d09cd108d05c54.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on LLM inference acceleration through speculative decoding, which is directly relevant to my research in inference optimization. It introduces a novel approach called SWIFT that uses layer-skipping as a draft model, eliminating the need for auxiliary models or additional training. The method claims 1.3-1.6x speedup while preserving output quality, addressing GPU utilization and throughput concerns.",
      "Irrelevant Aspects": "The paper doesn't explicitly discuss GPU utilization metrics or detailed profiling. It appears to focus on single-GPU inference rather than distributed scenarios. The claimed speedups (1.3-1.6x) are relatively modest compared to some other optimization techniques.",
      "Summary": "SWIFT introduces a self-speculative decoding method that adaptively selects intermediate layers of LLMs to skip during inference. This plug-and-play approach leverages layer sparsity in LLMs without requiring additional training or models, achieving modest speedups across various models and tasks."
    }
  },
  {
    "id": "LvNROciCne",
    "title": "AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning",
    "abstract": "Training and fine-tuning large language models (LLMs) come with challenges related to memory and computational requirements due to the increasing size of the model weights and the optimizer states. To tackle these challenges, various techniques have been developed, such as low-rank adaptation (LoRA), which involves introducing a parallel trainable low-rank matrix to the fixed pre-trained weights at each layer. However, these methods often fall short compared to the full-rank weight training approach, as they restrict the parameter search to a low-rank subspace. This limitation can disrupt training dynamics and may require a full-rank warm start to mitigate the impact. \nIn this paper, we introduce a new method inspired by a phenomenon we formally prove: as training progresses, the rank of the estimated layer gradients gradually decreases and asymptotically approaches rank one. Leveraging this, our approach involves adaptively reducing the rank of the gradients during Adam optimization steps, using an efficient online-updating low-rank projections rule. We further present a randomized-svd scheme for efficiently finding the projection matrix. \nOur technique enables full-parameter fine-tuning with adaptive low-rank gradient updates, significantly reducing overall memory requirements during training compared to state-of-the-art methods while improving model performance in both pretraining and fine-tuning. Finally, we provide a convergence analysis of our method and demonstrate its merits for training and fine-tuning language and biological foundation models.",
    "authors": [
      "~Yehonathan_Refael1",
      "~Jonathan_Svirsky1",
      "~Boris_Shustin1",
      "~Wasim_Huleihel1",
      "~Ofir_Lindenbaum1"
    ],
    "pdf": "/pdf/12d53fe43c665cbf8057ade48d36273a780c4487.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses memory optimization during LLM training and fine-tuning, introduces a novel training optimization method through adaptive low-rank gradient updates, contributes to better GPU utilization by reducing memory requirements, addresses scalability challenges for training larger models, provides convergence analysis for the optimization method, and demonstrates practical applications for both language and biological foundation models.",
      "Irrelevant Aspects": "The paper primarily focuses on training and fine-tuning with less emphasis on inference optimization, lacks direct latency analysis for inference, uses specialized mathematical techniques that may not be directly transferable to all optimization scenarios, and includes applications to biological foundation models which are somewhat outside the primary focus on LLMs.",
      "Summary": "AdaRankGrad introduces a novel approach for memory-efficient LLM training by adaptively reducing gradient rank during optimization, leveraging the phenomenon that layer gradients asymptotically approach rank one during training. The method enables full-parameter fine-tuning with reduced memory requirements compared to existing techniques, potentially improving GPU utilization and scalability. While primarily focused on training optimization rather than inference efficiency, the paper offers valuable insights for resource-constrained LLM training scenarios and provides theoretical convergence analysis to support its approach."
    }
  },
  {
    "id": "zjeHLSiNv1",
    "title": "Ultra-Sparse Memory Network",
    "abstract": "It is widely acknowledged that the performance of Transformer models is logarithmically related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms MoE. In experiments, the largest UltraMem we train has \\textbf{20 million} memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget, paving the way for billions of slots or experts.",
    "authors": [
      "~Zihao_Huang6",
      "~Qiyang_Min1",
      "~Hongzhi_Huang1",
      "~Yutao_Zeng1",
      "~Defa_Zhu2",
      "~Ran_Guo1",
      "~zhou_Xun2"
    ],
    "pdf": "/pdf/d0c9ab4ac120a3bde534d00a425c3976d212eebf.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference latency reduction, which is central to LLM optimization. It introduces ultra-sparse memory layers to improve memory access costs, a critical factor for GPU utilization. The work investigates scaling laws, showing favorable scaling properties compared to Mixture of Experts models, which is highly relevant to scalability research. The claim of achieving state-of-the-art inference speed within computational budget constraints aligns perfectly with optimization goals for higher throughput and lower latency.",
      "Irrelevant Aspects": "The abstract lacks specific details about GPU utilization techniques and doesn't explicitly discuss throughput measurements. There's no mention of specific optimization techniques like quantization or hardware-specific optimizations. The paper doesn't appear to analyze the trade-offs between model performance and resource utilization in depth. Memory slot count is mentioned, but without context on how this translates to actual memory bandwidth utilization.",
      "Summary": "This paper introduces UltraMem, an ultra-sparse memory network architecture that addresses memory access challenges in large Transformer models. It demonstrates significant reductions in inference latency while maintaining model performance, with favorable scaling properties that outperform Mixture of Experts approaches. The work represents a valuable contribution to inference optimization for large language models, particularly regarding memory efficiency and latency reduction, though it lacks some details on specific GPU utilization techniques and throughput measurements."
    }
  },
  {
    "id": "9BiVepgmWW",
    "title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures",
    "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead due to the need to store activation values for back-propagation during gradient computation, particularly in long-context fine-tuning tasks. Zeroth-order (ZO) algorithms offer a promising alternative by approximating gradients using finite differences of function values, thus eliminating the need for activation storage. Nevertheless, existing ZO methods struggle to capture the low-rank gradient structure common in LLM fine-tuning, leading to suboptimal performance. This paper proposes a low-rank ZO gradient estimator and introduces a novel **lo**w-rank **ZO** algorithm (LOZO) that effectively captures this structure in LLMs. We provide convergence guarantees for LOZO by framing it as a subspace optimization method. Additionally, its low-rank nature enables LOZO to integrate with momentum techniques while incurring negligible extra memory costs. Extensive experiments across various model sizes and downstream tasks demonstrate that LOZO and its momentum-based variant outperform existing ZO methods and closely approach the performance of FO algorithms.",
    "authors": [
      "~Yiming_Chen12",
      "~Yuan_Zhang51",
      "~Liyuan_Cao1",
      "~Kun_Yuan4",
      "~Zaiwen_Wen1"
    ],
    "pdf": "/pdf/bbfa9db4c7beea6898adc09043e4720543d91d1a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory optimization for LLM fine-tuning, parameter-efficient fine-tuning approaches, novel zeroth-order optimization methods, scalability for different model sizes, integration with momentum techniques, convergence guarantees for optimization algorithms",
      "Irrelevant Aspects": "Primarily focuses on fine-tuning rather than pre-training, limited discussion of inference optimization, no direct mention of throughput or latency metrics",
      "Summary": "This paper proposes a memory-efficient fine-tuning approach for LLMs using a novel low-rank zeroth-order optimization method (LOZO) that eliminates the need to store activation values during backpropagation. The method captures low-rank gradient structures in LLMs, provides convergence guarantees, and can integrate with momentum techniques while maintaining performance comparable to first-order methods."
    }
  },
  {
    "id": "cWGCkd7mCp",
    "title": "Efficient Learning with Sine-Activated Low-Rank Matrices",
    "abstract": "Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.",
    "authors": [
      "~Yiping_Ji1",
      "~Hemanth_Saratchandran1",
      "~Cameron_Gordon2",
      "~Zeyu_Zhang11",
      "~Simon_Lucey2"
    ],
    "pdf": "/pdf/e62a2fa594c39d98025583ca75552c66c3c076a5.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses Large Language Models (LLMs), which is a core focus of my research. It deals with parameter efficiency, which is crucial for both training optimization and inference optimization in LLMs. Parameter efficiency techniques can lead to better GPU utilization, higher throughput, and lower latency, which are all central to my research interests. The approach is described as a 'plug-in enhancement,' suggesting it could be integrated into existing optimization pipelines without major architectural changes. By potentially reducing the number of parameters without sacrificing performance, this method could enable more efficient use of GPU resources.",
      "Irrelevant Aspects": "The paper also covers applications in Vision Transformers (ViT), Neural Radiance Fields (NeRF), and 3D shape modeling, which are outside my specific focus on LLMs. The paper appears to be more focused on model architecture and mathematical formulation rather than systems-level optimizations. The abstract doesn't explicitly mention throughput, latency, or GPU utilization metrics, which are central to my research interests. There's no clear indication that the method addresses training speed optimization specifically.",
      "Summary": "This paper presents a novel approach to low-rank decomposition in neural networks by integrating a sinusoidal function within the decomposition process. The method aims to maintain parameter efficiency while increasing the decomposition's rank to enhance model performance. As a plug-in enhancement for existing low-rank models, it has applications in Vision Transformers, Large Language Models, Neural Radiance Fields, and 3D shape modeling. The work is particularly relevant to my research interests in LLM optimization as it addresses parameter efficiency, which can impact GPU utilization and performance, though it doesn't explicitly focus on systems-level optimizations or detailed performance metrics central to my work."
    }
  },
  {
    "id": "gp32jvUquq",
    "title": "Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression",
    "abstract": "Large Language Models (LLMs) have achieved remarkable breakthroughs. However, the huge number of parameters in LLMs require significant amount of memory storage in inference, which prevents their practical deployment in many applications. To reduce memory storage of LLMs, singular value decomposition (SVD) provides a promising solution to approximate weight matrices for compressing LLMs. In this paper, we take a step further to explore parameter sharing across different layers with SVD to achieve more effective compression for LLMs. Specifically, weight matrices in different layers are decomposed and represented with a linear combination of a set of shared basis vectors and unique coefficients. The types of weight matrices and the layer selection for basis sharing are examined when compressing LLMs to maintain the performance. Comprehensive experiments demonstrate that Basis-Sharing outperforms state-of-the-art SVD-based compression approaches, especially at large compression ratios.",
    "authors": [
      "~Jingcun_Wang1",
      "~Yu-Guang_Chen1",
      "~Ing-Chao_Lin1",
      "~Bing_Li19",
      "~Grace_Li_Zhang1"
    ],
    "pdf": "/pdf/6928107ba668e9e53b1fe2ad9a1b98ed398d6118.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "LLM compression for inference optimization, SVD-based techniques, cross-layer parameter sharing, practical deployment considerations, performance maintenance during compression",
      "Irrelevant Aspects": "Limited focus on training optimization, lack of explicit GPU utilization details, no clear mention of throughput/latency measurements, insufficient information on multi-GPU scalability",
      "Summary": "This paper presents Basis Sharing, a novel compression technique for LLMs that extends SVD by enabling parameter sharing across different layers. The method decomposes weight matrices in different layers and represents them with linear combinations of shared basis vectors and unique coefficients. The research examines which types of weight matrices and which layers should participate in basis sharing to maintain model performance. While the paper addresses inference optimization through memory reduction, it appears to focus more on the compression methodology rather than the implementation details that directly impact GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "2rnOgyFQgb",
    "title": "SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning",
    "abstract": "How can we accurately quantize a pre-trained model without any data?\nQuantization algorithms are widely used for deploying neural networks on resource-constrained edge devices.\nZero-shot Quantization (ZSQ) addresses the crucial and practical scenario where training data are inaccessible for privacy or security reasons.\nHowever, three significant challenges hinder the performance of existing ZSQ methods: 1) noise in the synthetic dataset, 2) predictions based on off-target patterns, and the 3) misguidance by erroneous hard labels.\nIn this paper, we propose SynQ (Synthesis-aware Fine-tuning for Zero-shot Quantization),\na carefully designed ZSQ framework to overcome the limitations of existing methods.\nSynQ minimizes the noise from the generated samples by exploiting a low-pass filter.\nThen, SynQ trains the quantized model to improve accuracy by aligning its class activation map with the pre-trained model.\nFurthermore, SynQ mitigates misguidance from the pre-trained model's error by leveraging only soft labels for difficult samples.\nExtensive experiments show that SynQ provides the state-of-the-art accuracy, over existing ZSQ methods.",
    "authors": [
      "~Minjun_Kim5",
      "~Jongjin_Kim1",
      "~U_Kang1"
    ],
    "pdf": "/pdf/90914dabfc83f4f4a635c1781bd4eea1e0a128c8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Quantization for inference optimization, improved GPU utilization through reduced precision operations, model efficiency leading to better scalability, techniques for reducing latency and increasing throughput",
      "Irrelevant Aspects": "Limited focus on large language models specifically, emphasis on zero-shot data synthesis rather than training optimization, class activation map techniques more applicable to vision models, lack of direct discussion on scalability to multi-GPU or distributed systems",
      "Summary": "SynQ advances zero-shot quantization through synthetic data optimization and model alignment techniques. The work contributes to inference optimization by enabling accurate quantization without training data, potentially improving GPU utilization and deployment efficiency. While not LLM-specific, the quantization approaches are valuable for enhancing model deployment across various architectures."
    }
  },
  {
    "id": "sPuLtU32av",
    "title": "MAST: model-agnostic sparsified training",
    "abstract": "We introduce a novel optimization problem formulation that departs from the conventional way of minimizing machine learning model loss as a black-box function. Unlike traditional formulations, the proposed approach explicitly incorporates an initially pre-trained model and random sketch operators, allowing for sparsification of both the model and gradient during training. We establish insightful properties of the proposed objective function and highlight its connections to the standard formulation. Furthermore, we present several variants of the Stochastic Gradient Descent (SGD) method adapted to the new problem formulation, including SGD with general sampling, a distributed version, and SGD with variance reduction techniques. We achieve tighter convergence rates and relax assumptions, bridging the gap between theoretical principles and practical applications, covering several important techniques such as Dropout and Sparse training. This work presents promising opportunities to enhance the theoretical understanding of model training through a sparsification-aware optimization approach.",
    "authors": [
      "~Yury_Demidovich1",
      "~Grigory_Malinovsky1",
      "~Egor_Shulgin1",
      "~Peter_Richtárik1"
    ],
    "pdf": "/pdf/f23e93ba692d012bb80ab6a4b3782f5f6615c80a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on sparsified training, which is highly relevant for optimizing GPU utilization and reducing memory bandwidth requirements during model training. The approach that incorporates pre-trained models and sparsifies both model and gradient during training aligns with efficient fine-tuning strategies for large models. The distributed version of SGD and variance reduction techniques are relevant for scalable training of large language models. The theoretical connections to dropout and sparse training techniques provide insights for practical implementation.",
      "Irrelevant Aspects": "The paper appears to focus more on theoretical aspects rather than practical implementation details specific to transformer architectures. There is no explicit mention of inference optimization, which is a key part of my research focus. The abstract doesn't specifically address GPU utilization, throughput, or latency metrics that are central to my interests. The model-agnostic approach may not address the specific optimization challenges unique to transformer-based language models.",
      "Summary": "This paper introduces a sparsification-aware optimization approach that could enhance training efficiency of large models through model and gradient sparsification. While it offers theoretical insights and distributed training variants that align with scalable LLM training, it lacks explicit focus on transformer-specific optimizations and inference efficiency. The techniques described could potentially improve GPU utilization and training throughput, but the theoretical rather than implementation focus limits its immediate practical applicability to LLM optimization."
    }
  },
  {
    "id": "F5PlYMC5ik",
    "title": "LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently",
    "abstract": "Large-scale pre-trained language models (PLMs) require significant computational resources to train from scratch on large volumes of data. But in the real world, emerging data from diverse sources may not be initially available for pre-training. Recent studies on lifelong learning have tried to solve this problem by exploring the use of model growth techniques to effectively incorporate new knowledge without the need for complete re-training. However, model growth approaches utilized have issues with growth operators that do not ensure strict function preservation or growth schedules that only include a few growth dimensions, reducing lifelong learning's effect. Furthermore, existing approaches often assume that emerging data has the same distribution as pre-training data, causing catastrophic forgetting of previously acquired knowledge. To address the aforementioned issues, we introduce LOIRE, a framework for lifelong learning that enables PLMs to effectively grow their capacity using incremental data. LOIRE employs growth operators for all feasible dimensions and a growth schedule to generate the optimal expansion sequence in the field of lifelong learning. Specifically, we present a novel plug-in layer growth operator with residual connections that skip the newly added layer during initial training while ensuring function preservation. We additionally propose an iterative distillation strategy for LOIRE that allows an intermediate model in the growth stages to switch between being a student and a teacher, reducing catastrophic forgetting during growth. Experiments show that LOIRE can reduce computational expenses by an average of 29.22\\% while retaining equivalent or better downstream performance.",
    "authors": [
      "~Xue_Han3",
      "~Yitong_Wang2",
      "~Junlan_Feng3",
      "~wenchun.gao2",
      "~Qian_Hu5",
      "~Chao_Deng4"
    ],
    "pdf": "/pdf/b87c8fd1af16706a48a7227e5f6a4de6a18727e1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization by reducing computational expenses (29.22%) and avoiding complete re-training. It focuses on model growth techniques for pre-trained language models, which aligns with my interest in efficient training. The approach improves scalability by enabling models to incorporate new knowledge incrementally. It also introduces a novel plug-in layer growth operator with residual connections that maintains function preservation during expansion.",
      "Irrelevant Aspects": "The paper primarily focuses on training aspects rather than inference optimization. It doesn't specifically address inference latency improvements or throughput enhancements, which are key areas of my research interest. The paper doesn't discuss hardware-specific optimizations that might improve GPU utilization directly.",
      "Summary": "LOIRE is a framework for lifelong learning that enables pre-trained language models to efficiently grow their capacity using incremental data. It introduces growth operators for all feasible dimensions, a growth schedule to optimize expansion sequence, a novel plug-in layer growth operator with residual connections, and an iterative distillation strategy to reduce catastrophic forgetting. While the approach demonstrates significant computational savings during training (29.22% reduction) and addresses scalability challenges, it doesn't specifically focus on inference optimization, latency improvements, or throughput enhancements, which are central to my research interests."
    }
  },
  {
    "id": "P7KRIiLM8T",
    "title": "u-$\\mu$P: The Unit-Scaled Maximal Update Parametrization",
    "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a lower loss than comparable $\\mu$P models and working out-of-the-box in FP8.",
    "authors": [
      "~Charlie_Blake1",
      "~Constantin_Eichenberg1",
      "~Josef_Dean1",
      "~Lukas_Balles1",
      "~Luke_Yuri_Prince1",
      "~Björn_Deiseroth1",
      "~Andres_Felipe_Cruz-Salinas1",
      "~Carlo_Luschi1",
      "~Samuel_Weinbach1",
      "~Douglas_Orr1"
    ],
    "pdf": "/pdf/f99dd690308d80a71b40ee38b40d7498d1bb862b.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents u-μP, which combines Maximal Update Parametrization (μP) with Unit Scaling to enable hyperparameters that are independent of model size. This allows for cheaper proxy models for hyperparameter sweeps, improving training efficiency. The approach supports training in low-precision formats (FP8), directly impacting GPU memory efficiency and potentially increasing throughput. The method ensures models are 'out-of-the-box' efficient, reducing optimization time and improving GPU utilization.",
      "Irrelevant Aspects": "The paper primarily focuses on training optimization rather than inference optimization specifically. There's limited discussion of latency improvements or inference-specific optimizations. While FP8 training is mentioned, there's no detailed analysis of inference quantization benefits.",
      "Summary": "u-μP combines μP and Unit Scaling to create a parameterization scheme where optimal hyperparameters are size-independent, enabling efficient hyperparameter sweeps via proxy models. The approach supports FP8 training, improving GPU memory efficiency and potentially throughput. While directly addressing training optimization and GPU utilization, it has less focus on inference-specific optimizations like latency reduction."
    }
  },
  {
    "id": "ALzTQUgW8a",
    "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
    "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.",
    "authors": [
      "~Zhuoming_Chen1",
      "~Ranajoy_Sadhukhan1",
      "~Zihao_Ye1",
      "~Yang_Zhou27",
      "~Jianyu_Zhang2",
      "~Niklas_Nolte1",
      "~Yuandong_Tian1",
      "~Matthijs_Douze1",
      "~Leon_Bottou1",
      "~Zhihao_Jia2",
      "~Beidi_Chen1"
    ],
    "pdf": "/pdf/43b14f6239ec38e98f5b7495abf3bcee094899af.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Inference optimization for LLMs, throughput improvement (5×), latency reduction (54ms on RTX 4090), scalability for long contexts (96k tokens), GPU utilization through heterogeneous CPU-GPU system, attention mechanism optimization using LSH sampling, batch size capacity improvements",
      "Irrelevant Aspects": "No focus on training optimization, no model architecture changes, doesn't address quantization or model compression techniques",
      "Summary": "MagicPIG presents a highly relevant inference optimization technique for LLMs using LSH-based sampling to approximate attention computation. It achieves significant throughput improvements (5×) and reduced latency (54ms on RTX 4090) while supporting long contexts through a heterogeneous CPU-GPU approach. The work directly addresses key challenges in LLM systems optimization including GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "IC5RJvRoMp",
    "title": "Streamlining Redundant Layers to Compress Large Language Models",
    "abstract": "This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. \nLLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \\href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}.",
    "authors": [
      "~Xiaodong_Chen7",
      "~Yuxuan_Hu2",
      "~Jing_Zhang24",
      "~Yanling_Wang1",
      "~Cuiping_Li1",
      "~Hong_Chen5"
    ],
    "pdf": "/pdf/8c8ead81acbf572bcc235c95ae5095e05c9ec040.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Layer pruning for model compression, training efficiency improvements, inference optimization through model reduction, potential for better GPU utilization, scalability benefits, and throughput improvements",
      "Irrelevant Aspects": "Limited focus on low-level GPU utilization techniques, no mention of quantization or other compression methods, lack of hardware-specific optimizations, absence of system-level optimization strategies",
      "Summary": "LLM-Streamline introduces a layer pruning approach for compressing large language models, removing consecutive less important layers and replacing them with lightweight networks. This directly addresses model efficiency and can contribute to better GPU utilization, reduced latency, and higher throughput in both training and inference scenarios. While relevant to model compression, it doesn't cover the full spectrum of optimization techniques most relevant to my research interests."
    }
  },
  {
    "id": "iBExhaU3Lc",
    "title": "Adam-mini: Use Fewer Learning Rates To Gain More",
    "abstract": "We propose Adam-mini, an optimizer that achieves on-par or better performance than AdamW with $50$% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). By delving into the Hessian structure of neural nets, we find Adam’s $v$ might not function at its full potential as effectively as we expected. We find that $\\geq 99.9$% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our proposed principle on Hessian structure; (2) assign a single but good learning rate to each parameter block. We then provide one simple way to find good learning rates and propose Adam-mini. Empirically, we verify that Adam-mini performs on par or better than AdamW on various language models sized from 39M to 13B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs, thereby increasing throughput. For instance, Adam-mini achieves $49.6$% higher throughput than AdamW when pre-training Llama 2-7B on $2\\times$ A800-80GB GPUs, which saves 33% wall-clock time for pre-training.",
    "authors": [
      "~Yushun_Zhang1",
      "~Congliang_Chen1",
      "~Ziniu_Li1",
      "~Tian_Ding1",
      "~Chenwei_Wu5",
      "~Diederik_P_Kingma1",
      "~Yinyu_Ye1",
      "~Zhi-Quan_Luo1",
      "~Ruoyu_Sun1"
    ],
    "pdf": "/pdf/a7172df96f64394f473bb633cb7ecbfec4c6083d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "50% memory reduction in optimizer, 49.6% higher throughput than AdamW for Llama 2-7B pre-training, reduced communication overhead among GPUs improving scalability, tested on language models from 39M to 13B parameters covering pre-training, supervised fine-tuning, and RLHF",
      "Irrelevant Aspects": "No discussion of inference optimization, no mention of latency improvements, focuses on algorithm-level optimization rather than comprehensive system optimization",
      "Summary": "Adam-mini is an optimizer that reduces memory footprint by 50% while maintaining or improving performance compared to AdamW. It achieves this by carefully partitioning parameters into blocks and assigning a single learning rate to each block. The optimizer shows significant throughput improvements (49.6% higher for Llama 2-7B) and reduces wall-clock time by 33% for pre-training, directly addressing GPU utilization and scalability in LLM training."
    }
  },
  {
    "id": "m4eXBo0VNc",
    "title": "An Engorgio Prompt Makes Large Language Model Babble on",
    "abstract": "Auto-regressive large language models (LLMs) have yielded impressive performance in many real-world tasks. \nHowever, the new paradigm of these LLMs also exposes novel threats. \nIn this paper, we explore their vulnerability to inference cost attacks, where a malicious user crafts Engorgio prompts to intentionally increase the computation cost and latency of the inference process. We design Engorgio, a novel methodology, to efficiently generate adversarial Engorgio prompts to affect the target LLM's service availability. Engorgio has the following two technical contributions. \n(1) We employ a parameterized distribution to track LLMs' prediction trajectory. (2) Targeting the auto-regressive nature of LLMs' inference process, we propose novel loss functions to stably suppress the appearance of the <EOS> token, whose occurrence will interrupt the LLM's generation process. \nWe conduct extensive experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B. \nThe results show that Engorgio prompts can successfully induce LLMs to generate abnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90\\%+ of the output length limit)\nin a white-box scenario and our real-world experiment demonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is released at https://github.com/jianshuod/Engorgio-prompt.",
    "authors": [
      "~Jianshuo_Dong1",
      "~Ziyuan_Zhang2",
      "~Qingjie_Zhang2",
      "~Tianwei_Zhang1",
      "~Hao_Wang41",
      "~Hewu_Li1",
      "~Qi_Li12",
      "~Chao_Zhang18",
      "~Ke_Xu9",
      "~Han_Qiu3"
    ],
    "pdf": "/pdf/3965e2e1bc4e3063c0278f8dda6363c3f6e59192.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly examines LLM inference costs and latency, which are central to my research focus. It investigates how certain prompts can dramatically increase computation time during inference, providing insights into factors that affect GPU utilization and throughput. The research spans multiple model sizes (125M to 30B parameters), which is relevant to scalability considerations. Understanding these vulnerabilities is valuable for developing more robust inference optimization techniques.",
      "Irrelevant Aspects": "The paper approaches LLM efficiency from an adversarial perspective rather than an optimization one. It focuses on creating attacks that harm service availability rather than improving performance. There is no discussion of training optimization techniques. The goal is to exploit weaknesses rather than develop solutions for better GPU utilization.",
      "Summary": "This paper explores 'Engorgio prompts' - specially crafted inputs that intentionally increase LLM inference costs and latency by forcing models to generate unusually long outputs. While framed as a security vulnerability study, it provides valuable insights into factors affecting LLM inference efficiency. The paper demonstrates how adversarial prompts can increase output length by 2-13×, significantly impacting computational resources. Understanding these exploitation vectors is relevant to my research on inference optimization, though the paper focuses on attacks rather than optimizations."
    }
  },
  {
    "id": "f4gF6AIHRy",
    "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
    "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Source code\nis available at: https://github.com/MediaBrain-SJTU/DiSF.git.",
    "authors": [
      "~Ziqing_Fan1",
      "~Siyuan_Du1",
      "~Shengchao_Hu1",
      "~Pingjie_Wang1",
      "~Li_Shen1",
      "~Ya_Zhang1",
      "~Dacheng_Tao1",
      "~Yanfeng_Wang1"
    ],
    "pdf": "/pdf/56011985c21c00fb5fb49f5a6a51ab4fb3fcaaac.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on optimizing LLM pre-training through intelligent data selection, which directly addresses training efficiency and computational budget constraints. The authors demonstrate significant improvements in training efficiency (1.5x) and data efficiency (5x) while reducing the required dataset size by 98.5%. This approach could lead to better GPU utilization during training by reducing I/O requirements and computational overhead. The method's scalability is demonstrated across models from 120M to 1.1B parameters.",
      "Irrelevant Aspects": "The paper does not address inference optimization techniques, which is another key aspect of my research interests. The focus is solely on the data selection aspect of pre-training rather than model architecture optimizations or hardware-specific acceleration techniques. There's limited discussion about how this selection method might affect downstream inference latency or throughput.",
      "Summary": "This paper presents a data selection method (DiSF) that addresses dimensional collapse in LLM pre-training data, resulting in significant computational savings during the training phase. The approach demonstrates 1.5x training efficiency and 5x data efficiency while using only 1.5% of the original data. While highly relevant to training optimization, it doesn't cover inference optimization aspects."
    }
  },
  {
    "id": "JFPaD7lpBD",
    "title": "Jamba: Hybrid Transformer-Mamba Language Models",
    "abstract": "We present Jamba, a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. We implement two configurations: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-mini, with 12B active parameters. Built at large scale, Jamba models provide high throughput and small memory footprint compared to vanilla Transformers, especially at long-context tasks, with an effective context length of 256K tokens, the largest amongst open-weight models. At the same time, they are also competitive on standard language modeling and chatbot benchmarks. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. We also describe several interesting properties of this architecture that the training and evaluation of Jamba have revealed. The model weights are publicly available.",
    "authors": [
      "~Barak_Lenz1",
      "~Opher_Lieber1",
      "~Alan_Arazi1",
      "~Amir_Bergman1",
      "~Avshalom_Manevich1",
      "~Barak_Peleg1",
      "~Ben_Aviram1",
      "~Chen_Almagor1",
      "~Clara_Fridman1",
      "~Dan_Padnos1",
      "~Daniel_Gissin2",
      "~Daniel_Jannai1",
      "~Dor_Muhlgay1",
      "~Dor_Zimberg1",
      "~Edden_M._Gerber1",
      "~Elad_Dolev1",
      "~Eran_Krakovsky1",
      "~Erez_Safahi1",
      "~Erez_Schwartz1",
      "~Gal_Cohen1",
      "~Gal_Shachaf1",
      "~Haim_Rozenblum1",
      "~Hofit_Bata1",
      "~Ido_Blass1",
      "~Inbal_Magar1",
      "~Itay_Dalmedigos1",
      "~Jhonathan_Osin1",
      "~Julie_Fadlon1",
      "~Maria_Rozman1",
      "~Matan_Danos1",
      "~Michael_Gokhman1",
      "~Mor_Zusman2",
      "~Naama_Gidron1",
      "~Nir_Ratner2",
      "~Noam_Gat1",
      "~Noam_Rozen1",
      "~Oded_Fried1",
      "~Ohad_Leshno1",
      "~Omer_Antverg1",
      "~Omri_Abend1",
      "~Or_Dagan1",
      "~Orit_Cohavi1",
      "~Raz_Alon1",
      "~Ro'i_Belson1",
      "~Roi_Cohen2",
      "~Rom_Gilad1",
      "~Roman_Glozman1",
      "~Shahar_Lev1",
      "~Shai_Shalev-Shwartz1",
      "~Shaked_Haim_Meirom1",
      "~Tal_Delbari1",
      "~Tal_Ness1",
      "~Tomer_Asida1",
      "~Tom_Ben_Gal1",
      "~Tom_Braude1",
      "~Uriya_Pumerantz1",
      "~Josh_Cohen1",
      "~Yonatan_Belinkov1",
      "~Yuval_Globerson1",
      "~Yuval_Peleg_Levy1",
      "~Yoav_Shoham1"
    ],
    "pdf": "/pdf/7ee550df8426becb0228e11d74682c519c69f706.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Hybrid Transformer-Mamba architecture for optimization, MoE implementation for efficient parameter usage, high throughput and small memory footprint compared to Transformers, long-context processing (256K tokens), ExpertsInt8 quantization technique for memory efficiency, fitting large models on limited GPU resources, architectural decisions for large-scale modeling, cost-effective inference optimization",
      "Irrelevant Aspects": "Evaluation on language modeling benchmarks (more about quality than optimization), public availability of model weights",
      "Summary": "Jamba presents a highly relevant hybrid architecture combining Transformer and Mamba layers with MoE, specifically designed for improved GPU utilization, memory efficiency, and throughput. The paper directly addresses key optimization challenges including long-context processing, quantization techniques, and fitting large models on limited GPU resources, making it highly relevant to LLM training and inference optimization research."
    }
  },
  {
    "id": "IIVYiJ1ggK",
    "title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions",
    "abstract": "Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are open-sourced at https://github.com/codefuse-ai/rodimus.",
    "authors": [
      "~Zhihao_He1",
      "~Hang_Yu1",
      "~Zi_Gong1",
      "~Shizhan_Liu1",
      "~Jianguo_Li2",
      "~Weiyao_Lin1"
    ],
    "pdf": "/pdf/bd813e40789a74a74f5f29fe05db2ed1dc39d6da.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Addresses attention mechanism efficiency, reduces computational complexity from O(T) for per-token generation, implements memory optimization for recurrent models, demonstrates semantic compression with fixed-size hidden states, combines multiple compression techniques in a hybrid approach, shows superior performance against models trained on more tokens, has been implemented at 1.6B parameter scale trained on 1 trillion tokens, and provides open-source code and checkpoints for practical application.",
      "Irrelevant Aspects": "Doesn't explicitly address GPU utilization or hardware-specific optimizations, lacks discussion of scalability across multiple GPUs or distributed scenarios, doesn't directly measure throughput or latency metrics despite complexity improvements, and provides limited information on integration with existing optimization frameworks.",
      "Summary": "Rodimus* presents an efficient attention mechanism for LLMs that reduces computational complexity and memory usage while maintaining performance. The Rodimus+ hybrid approach combines multiple compression techniques and demonstrates superior performance against larger models. While highly relevant to efficiency optimization in LLMs, the paper doesn't explicitly focus on GPU utilization, scalability, throughput, and latency metrics that are central to my research expertise."
    }
  },
  {
    "id": "fswihJIYbd",
    "title": "ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
    "abstract": "Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language Models (PLMs) to downstream tasks by optimizing a small amount of soft virtual tokens, which are prepended to the input token embeddings. Recently, Decomposed Prompt Tuning (DePT) has demonstrated superior adaptation capabilities by decomposing the soft prompt into a shorter soft prompt and a pair of low-rank matrices. The product of the pair of low-rank matrices is added to the input token embeddings to offset them. Additionally, DePT achieves faster inference compared to PT due to the shorter soft prompt. However, in this paper, we find that the position-based token embedding offsets of DePT restricts its ability to generalize across diverse model inputs, and that the shared embedding offsets across many token embeddings result in sub-optimization. To tackle these issues, we introduce \\textbf{A}daptive \\textbf{De}composed \\textbf{P}rompt \\textbf{T}uning (ADePT), which is composed of a short soft prompt and a shallow token-shared feed-forward neural network. ADePT utilizes the token-shared feed-forward neural network to learn the embedding offsets for each token, enabling adaptive embedding offsets that vary according to the model input and better optimization of token embedding offsets. This enables ADePT to achieve superior adaptation performance without requiring more inference time or additional trainable parameters compared to vanilla PT and its variants. In comprehensive experiments across 23 natural language processing tasks and 4 typical PLMs of different scales, ADePT consistently surpasses the leading parameter-efficient fine-tuning methods, and even outperforms the full fine-tuning in certain scenarios. We also provide a theoretical analysis towards ADePT. Code is available at https://github.com/HungerPWAY/ADePT.",
    "authors": [
      "~Pengwei_Tang1",
      "~Xiaolin_Hu6",
      "~Yong_Liu7"
    ],
    "pdf": "/pdf/a6ded26c02235d13974fdd0b1f9ff1ffbefad1c9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper discusses Prompt Tuning and Decomposed Prompt Tuning for adapting PLMs, with a focus on parameter efficiency and inference speed. ADePT achieves superior adaptation performance without increasing inference time or trainable parameters. The method involves optimizing soft virtual tokens and using a shallow feed-forward network, relating to computational efficiency.",
      "Irrelevant Aspects": "The paper focuses more on adaptation capabilities and performance across NLP tasks rather than computational or infrastructure optimizations. The abstract doesn't explicitly discuss GPU utilization, scalability, or throughput metrics. It's more methodological rather than system-level optimization.",
      "Summary": "ADePT introduces an improved parameter-efficient fine-tuning method that maintains computational efficiency while enhancing adaptation performance. It uses a short soft prompt and a shallow token-shared feed-forward network to learn adaptive embedding offsets, achieving better performance without increased inference time or parameters."
    }
  },
  {
    "id": "kws76i5XB8",
    "title": "Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives",
    "abstract": "Large language models (LLMs) have sparked a new wave of AI applications; however, their substantial computational costs and memory demands pose significant challenges to democratizing access to LLMs for a broader audience. Singular Value Decomposition (SVD), a technique studied for decades, offers a hardware-independent and flexibly tunable solution for LLM compression. In this paper, we present new directions using SVD: we first theoretically analyze the optimality of truncating weights and truncating activations, then we further identify three key issues on SVD-based LLM compression, including (1) How can we determine the optimal truncation position for each weight matrix in LLMs? (2) How can we efficiently update the weight matrices based on truncation position? (3) How can we address the inherent \"injection\" nature that results in the information loss of the SVD? We propose an effective approach, **Dobi-SVD**, to tackle the three issues. \nFirst, we propose a **differentiable** truncation-value learning mechanism, along with gradient-robust backpropagation, enabling the model to adaptively find the optimal truncation positions. Next, we utilize the Eckart-Young-Mirsky theorem to derive a theoretically **optimal** weight update formula through rigorous mathematical analysis. Lastly, by observing and leveraging the quantization-friendly nature of matrices after SVD decomposition, we reconstruct a mapping between truncation positions and memory requirements, establishing a **bijection** from truncation positions to memory. \nExperimental results show that with a 40\\% parameter-compression rate, our method achieves a perplexity of 9.07 on the Wikitext2 dataset with the compressed LLama-7B model, a 78.7\\% improvement over the state-of-the-art SVD for LLM compression method. \nWe emphasize that Dobi-SVD is the first to achieve such a high-ratio LLM compression with minimal performance drop.  We also extend our Dobi-SVD to VLM compression, achieving a 20\\% increase in throughput with minimal performance degradation. We hope that the inference speedup—up to 12.4x on 12GB NVIDIA Titan Xp GPUs and 3x on 80GB A100 GPUs for LLMs, and 1.2x on 80GB A100 GPUs for VLMs—will bring significant benefits to the broader community such as robotics.",
    "authors": [
      "~Wang_Qinsi1",
      "~Jinghan_Ke2",
      "~Masayoshi_Tomizuka2",
      "~Kurt_Keutzer1",
      "~Chenfeng_Xu1"
    ],
    "pdf": "/pdf/0040a2ba2284bb19baf84133ac2054adbf1896d9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Focuses on LLM compression which directly impacts inference optimization",
        "Achieves significant inference speedup (up to 12.4x on NVIDIA Titan Xp GPUs and 3x on A100 GPUs)",
        "Improves throughput by 20% for VLMs with minimal performance degradation",
        "Addresses memory requirements for better GPU utilization",
        "Introduces differentiable truncation-value learning mechanism for optimal compression",
        "Derives theoretically optimal weight update formula through mathematical analysis",
        "Establishes bijection from truncation positions to memory requirements",
        "Enables efficient deployment of LLMs on resource-constrained environments"
      ],
      "Irrelevant Aspects": [
        "Does not address training optimization techniques",
        "Limited discussion on distributed training approaches",
        "Minimal coverage of quantization techniques for inference",
        "Lacks detailed analysis of scaling across different model sizes",
        "Does not directly address latency optimization",
        "Limited exploration of hardware-specific optimizations beyond GPUs"
      ],
      "Summary": "Dobi-SVD presents an innovative approach to LLM compression using differentiable SVD, offering significant inference speedups and throughput improvements. The paper addresses three key challenges in SVD-based compression through differentiable truncation-value learning, optimal weight update formulas, and memory requirement mapping. With results showing up to 12.4x speedup on consumer GPUs and minimal performance degradation at 40% compression rates, this work makes valuable contributions to inference optimization and GPU utilization for LLMs, though it does not cover training optimization aspects."
    }
  },
  {
    "id": "s7DkcgpRxL",
    "title": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models",
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81× (16.95×), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B). Code is available at https://github.com/junzhang-zj/LoRAM.",
    "authors": [
      "~Jun_Zhang26",
      "~Jue_WANG1",
      "~Huan_Li5",
      "~Lidan_Shou1",
      "~Ke_Chen11",
      "~Yang_You1",
      "~Guiming_Xie1",
      "~Xuejian_Gong1",
      "~Kunlong_Zhou1"
    ],
    "pdf": "/pdf/3dbd845c12a977f4681d0a5ccb84dcc3d2a1d0cc.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses memory efficiency in LLM training, which is a core concern in my research focus. It proposes a novel approach combining pruning and LoRA techniques that enables training a 70B parameter model on just 20GB of GPU memory - a dramatic improvement in GPU utilization. The technique reduces parameter storage by nearly 17x while maintaining performance, which represents a significant advancement in training optimization. The method also addresses both training and inference phases, with clear implications for better scalability and resource efficiency.",
      "Irrelevant Aspects": "The paper doesn't deeply explore system-level optimizations for inference latency and throughput. While it mentions inference efficiency, the primary focus is on training memory optimization rather than runtime performance characteristics. There's limited discussion of distributed training implications and how the approach scales across multiple nodes.",
      "Summary": "This paper introduces LoRAM, a memory-efficient approach for LoRA training that enables training large language models on significantly smaller GPU hardware by training adapter matrices on pruned models and then recovering them for the original model. The method achieves a 15-17x reduction in memory usage while maintaining performance, representing a substantial advancement in resource-efficient LLM training."
    }
  },
  {
    "id": "Hn5eoTunHN",
    "title": "RandLoRA: Full rank parameter-efficient fine-tuning of large models",
    "abstract": "Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. The low-rank nature of the weight update inherently limits the representation power of fine-tuned models, however, thus potentially compromising performance on complex tasks. This raises a critical question: when a performance gap between LoRA and standard fine-tuning is observed, is it due to the reduced number of train-\nable parameters or the rank deficiency? This paper aims to answer this question by introducing RandLoRA, a parameter-efficient method that performs full-rank updates using a learned linear combinations of low-rank, non-trainable random matrices. Our method limits the number of trainable parameters by restricting optimization to diagonal scaling matrices applied to the fixed random matrices. This allows us to effectively overcome the low-rank limitations while maintaining parameter and memory efficiency during training. Through extensive experimen-\ntation across vision, language, and vision-language benchmarks, we systematically evaluate the limitations of LoRA and existing random basis methods. Our findings reveal that full-rank updates are beneficial across vision and language tasks individually, and even more so for vision-language tasks, where RandLoRA significantly reduces—and sometimes eliminates—the performance gap between standard fine-tuning and LoRA, demonstrating its efficacy.",
    "authors": [
      "~Paul_Albert2",
      "~Frederic_Z._Zhang1",
      "~Hemanth_Saratchandran1",
      "~Cristian_Rodriguez-Opazo1",
      "~Anton_van_den_Hengel1",
      "~Ehsan_Abbasnejad3"
    ],
    "pdf": "/pdf/44215e86c16ae5835417e396f45daa8b9816a24b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-efficient fine-tuning for large models, memory optimization during training, maintaining performance with fewer trainable parameters, efficient use of computational resources for training, cross-modal applications (vision and language)",
      "Irrelevant Aspects": "Lacks focus on inference optimization, no discussion of latency measurements or improvements, doesn't address throughput optimization, limited discussion of GPU utilization metrics, not focused on distributed training or multi-GPU scalability",
      "Summary": "The paper introduces RandLoRA, a parameter-efficient fine-tuning method that addresses LoRA's rank deficiency by using full-rank updates with trainable diagonal scaling matrices. While it contributes to training optimization by maintaining parameter efficiency and improving performance across vision, language, and vision-language tasks, it doesn't cover several key aspects of my research interests including inference optimization, latency, throughput, and specific GPU utilization metrics."
    }
  },
  {
    "id": "jFcNXJGPGh",
    "title": "ComLoRA: A Competitive Learning Approach for Enhancing LoRA",
    "abstract": "We propose a Competitive Low-Rank Adaptation (ComLoRA) framework to address the limitations of the LoRA method, which either lacks capacity with a single rank-$r$ LoRA or risks inefficiency and overfitting with a larger rank-$Kr$ LoRA, where $K$ is an integer larger than 1. The proposed ComLoRA method initializes $K$ distinct LoRA components, each with rank $r$, and allows them to compete during training. This competition drives each LoRA component to outperform the others, improving overall model performance. The best-performing LoRA is selected based on validation metrics, ensuring that the final model outperforms a single rank-$r$ LoRA and matches the effectiveness of a larger rank-$Kr$ LoRA, all while avoiding extra computational overhead during inference. To the best of our knowledge, this is the first work to introduce and explore competitive learning in the context of LoRA optimization. The ComLoRA's code is available at https://github.com/hqsiswiliam/comlora.",
    "authors": [
      "~Qiushi_Huang1",
      "~Tom_Ko2",
      "~Lilian_Tang1",
      "~Yu_Zhang3"
    ],
    "pdf": "/pdf/88b42fc367b791825f66b53c2d0d9817eb947a05.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LoRA optimization for LLMs, which is central to training optimization. It proposes a method that improves performance without adding computational overhead during inference, aligning with inference optimization goals. The competitive learning approach for parameter-efficient fine-tuning could enhance GPU utilization and system scalability.",
      "Irrelevant Aspects": "The paper focuses specifically on LoRA optimization rather than broader training/inference techniques. It doesn't address comprehensive GPU utilization strategies beyond LoRA. The competitive learning mechanism, while novel, may have limited applicability outside the LoRA context.",
      "Summary": "ComLoRA introduces a competitive learning approach to optimize LoRA adaptations by training multiple rank-r LoRA components in competition and selecting the best performer. This method aims to match the effectiveness of larger rank LoRA adaptations while maintaining computational efficiency during inference, making it relevant to LLM training optimization and inference efficiency goals."
    }
  },
  {
    "id": "TwJrTz9cRS",
    "title": "HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models",
    "abstract": "We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code is available at https://github.com/hqsiswiliam/hira.",
    "authors": [
      "~Qiushi_Huang1",
      "~Tom_Ko2",
      "~Zhan_Zhuang1",
      "~Lilian_Tang1",
      "~Yu_Zhang3"
    ],
    "pdf": "/pdf/a4ac6d267c73f7309bca84d993f7c2dfd59f368f.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": [
        "Parameter-efficient fine-tuning (PEFT) for LLMs",
        "Improving upon existing methods (LoRA)",
        "Reducing resource demands",
        "Enhancing model capacity while maintaining efficiency",
        "Potential for better GPU utilization during training",
        "Scalability of fine-tuning large models"
      ],
      "Irrelevant Aspects": [
        "Limited apparent focus on inference optimization based on the abstract",
        "No apparent explicit mention of throughput measurements in the abstract",
        "No apparent explicit mention of latency analysis in the abstract"
      ],
      "Summary": "The paper introduces HiRA, a parameter-efficient fine-tuning method for LLMs that addresses limitations of LoRA by using a Hadamard product to retain high-rank update parameters. This approach aims to improve model capacity while maintaining efficiency, potentially leading to better GPU utilization and scalability during fine-tuning. The method is highly relevant to training optimization and scalability aspects of my research interest, though it doesn't appear to explicitly address inference optimization, throughput measurements, or latency analysis based on the abstract."
    }
  },
  {
    "id": "9CqkpQExe2",
    "title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs",
    "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing, which activates a fixed and equal number of experts for each token regardless of their significance within the context. In this paper, we propose a novel Ada-K routing strategy that dynamically adjusts the number of activated experts for each token, thereby improving the balance between computational efficiency and model performance. Specifically, our strategy incorporates learnable and lightweight allocator modules that decide customized expert resource allocation tailored to the contextual needs for each token. These allocators are designed to be fully pluggable, making it broadly applicable across all mainstream MoE-based LLMs. We leverage the Proximal Policy Optimization (PPO) algorithm to facilitate an end-to-end learning process for this non-differentiable decision-making framework. Extensive evaluations on four popular baseline models demonstrate that our Ada-K routing method significantly outperforms conventional Top-K routing. Compared to Top-K, our method achieves over 25% reduction in FLOPs and more than 20% inference speedup while still improving performance across various benchmarks. Moreover, the training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based LLM with more than 140B parameters, the training time is limited to 8 hours. Detailed analysis shows that harder tasks, middle layers, and content words tend to activate more experts, providing valuable insights for future adaptive MoE system designs. Both the training code and model checkpoints will be publicly available.",
    "authors": [
      "~Tongtian_Yue1",
      "~Longteng_Guo1",
      "~Jie_Cheng4",
      "~Xuange_Gao1",
      "~Hua_Huang1",
      "~Jing_Liu1"
    ],
    "pdf": "/pdf/5bc7159d3266170a34afd3d0b14ef2ea8ea20a85.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper presents a novel method for Mixture-of-Experts (MoE) models that dynamically adjusts the number of activated experts per token, resulting in over 25% FLOP reduction and 20% inference speedup. This directly addresses GPU utilization and inference optimization for large language models. The method scales to massive models (140B+ parameters) and maintains efficient training times (8 hours for Mixtral-8x22B), demonstrating both training and inference optimization benefits. The pluggable nature of the approach makes it broadly applicable across MoE architectures.",
      "Irrelevant Aspects": "The use of Proximal Policy Optimization (PPO) for training the allocator modules is a specific algorithm choice that may not be central to broader optimization interests. The linguistic analysis about which tokens activate more experts, while insightful, is more of a secondary finding related to the system optimization aspects.",
      "Summary": "This paper introduces Ada-K Routing, a dynamic expert allocation strategy for MoE-based LLMs that optimizes both training and inference efficiency. The method achieves significant computational savings while maintaining or improving performance, making it highly relevant for research in GPU utilization, scalability, throughput, and latency optimization for large language models."
    }
  },
  {
    "id": "Ze4aPP0tIn",
    "title": "Step-by-Step Reasoning for Math Problems  via Twisted Sequential Monte Carlo",
    "abstract": "Augmenting the multi-step reasoning abilities of Large Language Models (LLMs) has been a persistent challenge. Recently, verification has shown promise in improving solution consistency by evaluating generated outputs. However, current verification approaches suffer from sampling inefficiencies, requiring a large number of samples to achieve satisfactory performance. Additionally, training an effective verifier often depends on extensive process supervision, which is costly to acquire. In this paper, we address these limitations by introducing a novel verification method based on Twisted Sequential Monte Carlo (TSMC). TSMC sequentially refines its sampling effort to focus exploration on promising candidates, resulting in more efficient generation of high-quality solutions. We apply TSMC to LLMs by estimating the expected future rewards at partial solutions. This approach results in a more straightforward training target that eliminates the need for step-wise human annotations. We empirically demonstrate the advantages of our method across multiple math benchmarks, and also validate our theoretical analysis of both our approach and existing verification methods.",
    "authors": [
      "~Shengyu_Feng1",
      "~Xiang_Kong1",
      "~Shuang_Ma3",
      "~Aonan_Zhang1",
      "~Dong_Yin1",
      "~Chong_Wang8",
      "~Ruoming_Pang2",
      "~Yiming_Yang1"
    ],
    "pdf": "/pdf/93937396aaea40270bfaf023ef50f69bc2cc14dd.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents Twisted Sequential Monte Carlo (TSMC) for more efficient verification of LLM-generated mathematical reasoning, reducing sampling requirements during inference and eliminating need for step-wise human annotations during training. This approach optimizes both training data requirements and inference computational costs, potentially improving throughput, reducing latency, and enhancing GPU utilization.",
      "Irrelevant Aspects": "The paper primarily focuses on mathematical reasoning capabilities and verification techniques rather than directly addressing GPU utilization, scalability, or system-level optimizations. The theoretical analysis is centered on algorithmic performance rather than computational resource efficiency.",
      "Summary": "This paper introduces TSMC to optimize the verification process for mathematical reasoning in LLMs, reducing both computational requirements during inference and training data needs. While not explicitly targeting system optimization, its approach to improving efficiency in both training and inference has significant implications for resource utilization and operational performance of LLMs."
    }
  },
  {
    "id": "HzBfoUdjHt",
    "title": "$\\text{D}_{2}\\text{O}$: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models",
    "abstract": "Efficient generative inference in Large Language Models (LLMs) is impeded by the growing memory demands of Key-Value (KV) cache, especially for longer sequences. Traditional KV Cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduce **D**ynamic **D**iscriminative **O**perations ($\\mathbf{D_2 O}$), a novel method that optimizes KV cache size dynamically and discriminatively at two levels without fine-tuning, while preserving essential context. At **layer-level**, by observing the varying densities of attention weights between shallow and deep layers, we dynamically determine which layers should avoid excessive eviction via our proposed ***dynamic allocation strategy*** to minimize information loss. At **token-level**, for the eviction strategy in each layer, $\\mathbf{D_2 O}$ innovatively incorporates a ***compensation mechanism*** that maintains a similarity threshold to re-discriminate the importance of currently discarded tokens, determining whether they should be recalled and merged with similar tokens. Extensive experiments on various benchmarks and LLM architectures have shown that $\\mathbf{D_2 O}$ not only achieves significant memory savings and enhances inference throughput by more than 3$\\times$ but also maintains high-quality long-text generation.",
    "authors": [
      "~Zhongwei_Wan1",
      "~Xinjian_Wu1",
      "~Yu_Zhang60",
      "~Yi_Xin1",
      "~Chaofan_Tao1",
      "~Zhihong_Zhu1",
      "~Xin_Wang71",
      "~Siqi_Luo2",
      "~Jing_Xiong4",
      "~Longyue_Wang3",
      "~Mi_Zhang1"
    ],
    "pdf": "/pdf/40a8531ff80ead30be111a15d0f6cad31778b92c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through KV cache management, which is a critical bottleneck for GPU utilization. It claims over 3x throughput improvement without fine-tuning requirements, focusing on memory savings for long-context processing. The dynamic eviction strategy at both layer and token levels provides a novel approach to maintaining generation quality while reducing computational overhead.",
      "Irrelevant Aspects": "The paper focuses solely on inference optimization without addressing training optimization. It lacks discussion of distributed computing or multi-GPU strategies. No explicit latency measurements or GPU utilization metrics are mentioned. The approach seems to be a single-machine solution without consideration for cluster-level optimizations.",
      "Summary": "D2O introduces a two-level dynamic approach to optimize KV cache during LLM inference. At the layer-level, it dynamically determines eviction strategies based on attention patterns across different layers. At the token-level, it implements a compensation mechanism to potentially recall and merge discarded tokens. The method achieves significant memory savings and 3x throughput improvement while maintaining generation quality, requiring no model fine-tuning."
    }
  },
  {
    "id": "vtT09dYPGI",
    "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
    "abstract": "Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multimodal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the dynamic experts in existing MLLMs and showing that a standard MLLM can also be a mixture of experts. However, achieving this target is still notoriously challenging. The well-trained MLLMs are more accustomed to the fixed pathway and a drastic change in its inference manner also greatly impedes its performance. To address these issues, we propose a novel dynamic expert routing method for existing MLLMs, termed Routing Experts (RoE), which can achieve example-dependent optimal path routing without obvious structure tweaks. Meanwhile, a new structure sparsity regularization is also introduced to force the well-trained MLLMs to learn more short-cut pathways. In addition, we also address the alignment of the training and inference of MLLMs in terms of network routing. To validate RoE, we apply it to a set of existing MLLMs, including LLaVA-1.5, LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL benchmarks. The experiment results not only show the effectiveness of our RoE in improving MLLMs' efficiency, but also yield obvious advantages over MoE-LLaVA in both performance and speed, e.g.,  an average performance gain of 3.3% on 5 benchmarks while being 1.61 times faster. Our code is anonymously released at https://github.com/DoubtedSteam/RoE",
    "authors": [
      "~Qiong_Wu2",
      "~Zhaoxi_Ke1",
      "~Yiyi_Zhou1",
      "~Xiaoshuai_Sun3",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/c503a9774ba9f5fcde6316862f7147b4e882edb4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on Mixture of Experts (MoE) for LLM optimization, introduces a dynamic expert routing method (RoE) that improves inference efficiency, demonstrates concrete speed improvements (1.61x faster) while maintaining better performance, and addresses structure sparsity for computational efficiency.",
      "Irrelevant Aspects": "The work specifically targets multimodal LLMs rather than text-only models, lacks explicit discussion of GPU utilization or hardware-specific optimizations, and doesn't address scalability across distributed systems in detail.",
      "Summary": "This paper presents Routing Experts (RoE), a method for dynamic expert routing in existing multimodal LLMs that achieves better performance-to-efficiency trade-offs. The approach enables example-dependent optimal path routing without major structural changes, introduces structure sparsity regularization, and outperforms MoE-LLaVA by 3.3% on average across benchmarks while being 1.61 times faster. While the focus on multimodal models narrows its direct applicability, the MoE optimization techniques are highly relevant to LLM inference optimization."
    }
  },
  {
    "id": "dGSOn7sdWg",
    "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
    "abstract": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup. Our code and checkpoints are available at https://www.github.com/alanbaade/SyllableLM",
    "authors": [
      "~Alan_Baade1",
      "~Puyuan_Peng1",
      "~David_Harwath1"
    ],
    "pdf": "/pdf/68796dfc1385cc49d94a111e32fc873b998243b8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper demonstrates significant efficiency improvements with 30x reduction in training compute and 4x wall-clock inference speedup, directly addressing my focus on training and inference optimization. The coarser tokenization approach reduces tokens per word, which impacts memory requirements and computational cost. The novel distillation technique for improving model representations could have broader applications beyond speech models. These efficiency gains are achieved while maintaining or improving SotA results, showing an effective optimization approach.",
      "Irrelevant Aspects": "The research specifically focuses on speech data processing rather than text-based models, which are the primary domain for most large language models. Much of the technical contribution relates to syllabic segmentation and clustering, which is domain-specific to speech processing. The core innovation around semantic unit extraction for continuous audio data addresses fundamentally different challenges than optimizing discrete text tokenization for text-based LLMs.",
      "Summary": "SyllableLM introduces a self-supervised technique to create coarser syllable-like units for speech language models, achieving 30x reduction in training compute and 4x inference speedup. While the efficiency optimizations are highly relevant to my research interests, the speech-specific focus limits direct applicability to text-based LLMs. The distillation technique and tokenization efficiency approaches may offer transferable insights for general LLM optimization."
    }
  },
  {
    "id": "HMVDiaWMwM",
    "title": "Guided Score identity Distillation for Data-Free One-Step Text-to-Image Generation",
    "abstract": "Diffusion-based text-to-image generation models trained on extensive text-image pairs have demonstrated the ability to produce photorealistic images aligned with textual descriptions. However, a significant limitation of these models is their slow sample generation process, which requires iterative refinement through the same network. To overcome this, we introduce a data-free guided distillation method that enables the efficient distillation of pretrained Stable Diffusion models without access to the real training data, often restricted due to legal, privacy, or cost concerns. This method enhances Score identity Distillation (SiD) with Long and Short Classifier-Free Guidance (LSG), an innovative strategy that applies Classifier-Free Guidance (CFG) not only to the evaluation of the pretrained diffusion model but also to the training and evaluation of the fake score network. We optimize a model-based explicit score matching loss using a score-identity-based approximation alongside our proposed guidance strategies for practical computation. By exclusively training with synthetic images generated by its one-step generator, our data-free distillation method rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Notably, the one-step distillation of Stable Diffusion 1.5 achieves an FID of **8.15** on the COCO-2014 validation set, a record low value under the data-free setting.  Our code and checkpoints are available at https://github.com/mingyuanzhou/SiD-LSG.",
    "authors": [
      "~Mingyuan_Zhou1",
      "~Zhendong_Wang1",
      "~Huangjie_Zheng1",
      "~Hai_Huang5"
    ],
    "pdf": "/pdf/455edc90594a360f48dc075128308a031f2c32a8.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization by converting multi-step diffusion models into efficient one-step generators, which directly relates to my research on reducing latency and improving throughput. The distillation technique described optimizes computational efficiency and GPU utilization, which aligns with my interests in model optimization for better performance. The approach achieves significant speedups while maintaining quality, demonstrating practical value for inference systems.",
      "Irrelevant Aspects": "The paper focuses specifically on text-to-image diffusion models rather than large language models, though some optimization principles overlap. It doesn't address distributed training strategies or multi-GPU scaling techniques that are central to my research. The data-free aspect is motivated by privacy/legal concerns rather than training efficiency optimization.",
      "Summary": "This paper presents a guided score identity distillation method that efficiently compresses multi-step diffusion models into single-step generators. While the domain is text-to-image generation rather than language models, the core optimization techniques for improving inference speed, reducing latency, and increasing throughput are highly relevant to my research interests. The method achieves state-of-the-art FID scores while significantly reducing computational requirements, demonstrating effective model compression and inference acceleration techniques."
    }
  },
  {
    "id": "mtSSFiqW6y",
    "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
    "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.",
    "authors": [
      "~Gregor_Bachmann1",
      "~Sotiris_Anagnostidis1",
      "~Albert_Pumarola2",
      "~Markos_Georgopoulos1",
      "~Artsiom_Sanakoyeu1",
      "~Yuming_Du1",
      "~Edgar_Schönfeld1",
      "~Ali_Thabet1",
      "~Jonas_K_Kohler1"
    ],
    "pdf": "/pdf/7cea76e3219842445f3d5996e6585ff679c68346.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models, focusing on speculative decoding to improve throughput and reduce latency. It presents a novel approach that achieves 9x speedup over Llama-405B while maintaining quality. The method demonstrates better GPU utilization by reducing token rejections in speculative sampling. It provides concrete performance metrics like tokens per second (141 tokens/s for 8B/70B and 129 tokens/s for 8B/405B) and shows effectiveness in optimized inference frameworks. The approach enables efficient use of smaller draft models with larger target models, addressing scalability concerns.",
      "Irrelevant Aspects": "The paper doesn't focus on training optimization, which is part of the research interest. It doesn't explicitly discuss quantization or other model compression techniques. Limited information about memory optimization strategies during inference is provided. The specific dataset creation methodology (TokenCourt) is less relevant to the core system optimization focus.",
      "Summary": "This paper presents Judge Decoding, an enhancement to speculative sampling that significantly improves LLM inference speed. It addresses a key limitation in current speculative decoding where high-quality draft tokens are rejected due to misalignment between draft and target models. By introducing a compact 'judge' module trained on top of embeddings to evaluate continuations, the method achieves substantial throughput improvements (9x speedup over Llama-405B) while maintaining output quality. The approach demonstrates strong performance metrics across different model sizes and scales effectively with multiple H100 GPUs."
    }
  },
  {
    "id": "cFu7ze7xUm",
    "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
    "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges.\nCaching all Key and Value (KV) states across all attention heads consumes substantial memory.\nExisting KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements.\nIn this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens.\nIn contrast, all other heads, which primarily focus on recent tokens and attention sinks—referred to as Streaming Heads—do not require full attention.\nBased on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately.\nOur method significantly reduces long-context inference memory by up to 2.55$\\times$ for MHA and 1.67$\\times$ for GQA models while speeding up decoding by up to 2.18$\\times$ and 1.50$\\times$ and accelerating pre-filling by up to 1.73$\\times$ and 1.63$\\times$ for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention.\nNotably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.33 million context length measured on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.",
    "authors": [
      "~Guangxuan_Xiao1",
      "~Jiaming_Tang1",
      "~Jingwei_Zuo2",
      "~junxian_guo1",
      "~Shang_Yang1",
      "~Haotian_Tang1",
      "~Yao_Fu3",
      "~Song_Han5"
    ],
    "pdf": "/pdf/5723b4f3ab2bb241158f3f35ad3ac5b22b62192e.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper addresses key challenges in LLM inference optimization, particularly for long contexts. It presents a novel approach to KV cache management that reduces memory usage (up to 2.55× for MHA and 1.67× for GQA models), speeds up decoding (up to 2.18× for MHA and 1.50× for GQA models), and accelerates pre-filling (up to 1.73× for MHA and 1.63× for GQA models). This directly impacts GPU utilization, scalability, throughput, and latency - all central to my research interests. The ability to handle extremely long contexts (3.33 million tokens on a single A100) is particularly significant for scalability.",
      "Irrelevant Aspects": "The paper focuses primarily on inference optimization rather than training optimization. There is limited discussion of distributed inference scenarios or integration with other optimization techniques beyond quantization. The method requires identifying 'retrieval heads' using a lightweight optimization algorithm with synthetic data, which adds an initial setup step not directly related to runtime efficiency.",
      "Summary": "DuoAttention presents a novel framework for efficient long-context LLM inference by selectively applying full KV cache only to 'retrieval heads' while using a lightweight constant-length KV cache for 'streaming heads.' This approach significantly reduces memory requirements and improves both decoding and pre-filling speeds without compromising long-context capabilities. The method enables processing extremely long contexts (over 3 million tokens) on a single GPU, substantially improving scalability. While focused on inference rather than training, the paper's contributions to memory optimization, latency reduction, and throughput improvement make it highly relevant to research on LLM system optimization and GPU utilization."
    }
  },
  {
    "id": "Uhj5OxAz7I",
    "title": "Matryoshka Multimodal Models",
    "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose : Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2)  provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around 9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",
    "authors": [
      "~Mu_Cai1",
      "~Jianwei_Yang1",
      "~Jianfeng_Gao1",
      "~Yong_Jae_Lee2"
    ],
    "pdf": "/pdf/f7eb80c06c453cd205dbc2cbb08c4a344fb81594.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents a method to optimize inference for Large Multimodal Models by representing visual content as nested sets of tokens across multiple granularities. This directly addresses inference efficiency, allowing dynamic adjustment of token usage based on content complexity. This has significant implications for GPU utilization, latency reduction, and throughput improvement during inference.",
      "Irrelevant Aspects": "The paper doesn't focus on training optimization techniques for large models. It also doesn't address specific hardware-level optimizations or distributed computing strategies.",
      "Summary": "Matryoshka Multimodal Models introduces a novel approach for inference optimization in large multimodal models by enabling variable-length visual token representations. This allows for adaptive processing based on content complexity, potentially improving efficiency in terms of GPU utilization, throughput, and latency, which aligns well with research interests in inference optimization."
    }
  },
  {
    "id": "OPSpdc25IZ",
    "title": "DS-LLM: Leveraging Dynamical Systems to Enhance Both Training and Inference of Large Language Models",
    "abstract": "The training of large language models (LLMs) faces significant computational cost challenges, limiting their scalability toward artificial general intelligence (AGI) and broader adoption. With model sizes doubling approximately every 3.4 months and training costs escalating from 64 million USD for GPT-4 in 2020 to 191 million USD for Gemini Ultra in 2023, the economic burden has become unsustainable. While techniques such as quantization offer incremental improvements, they fail to address the fundamental computational bottleneck. In this work, we introduce DS-LLM, a novel framework that leverages dynamical system (DS)-based machines, which exploit Natural Annealing to rapidly converge to minimal energy states, yielding substantial efficiency gains. Unlike traditional methods, DS-LLM maps LLM components to optimization problems solvable via Hamiltonian configurations and utilizes continuous electric current flow in DS-machines for hardware-native gradient descent during training. We mathematically demonstrate the equivalence between conventional LLMs and DS-LLMs and present a method for transforming a trained LLM into a DS-LLM. Experimental evaluations across multiple model sizes demonstrate orders-of-magnitude improvements in speed and energy efficiency for both training and inference while maintaining consistent accuracy. Additionally, we provide an in-depth analysis of the challenges and potential solutions associated with this emerging computing paradigm, aiming to lay a solid foundation for future research.",
    "authors": [
      "~Ruibing_Song1",
      "~Chuan_Liu3",
      "~Chunshu_Wu1",
      "~Ang_Li11",
      "~Dongfang_Liu1",
      "~Ying_Nian_Wu1",
      "~Tong_Geng1"
    ],
    "pdf": "/pdf/2aa2ca79602b3d5556c4cf69d0b5c81a4d5fc250.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses computational efficiency improvements for both training and inference of large language models, claiming orders-of-magnitude improvements in speed and energy efficiency. It presents a novel framework (DS-LLM) that could potentially revolutionize how we optimize LLM operations. The focus on maintaining accuracy while improving performance aligns perfectly with research on LLM optimization. The approach claims to address fundamental computational bottlenecks rather than incremental improvements.",
      "Irrelevant Aspects": "The paper may focus heavily on theoretical aspects of dynamical systems without providing practical implementation details. If it requires specialized hardware (DS-machines) that aren't widely available, the practical applicability might be limited. The abstract doesn't specifically mention GPU utilization, memory optimization, or specific techniques like quantization, pruning, or distillation that are common in current optimization research.",
      "Summary": "DS-LLM introduces a novel approach to enhance both training and inference efficiency of large language models using dynamical systems and Natural Annealing. The paper claims significant improvements in computational efficiency while maintaining accuracy. If the claims are substantiated with practical implementations, this could represent a major advancement in LLM optimization. The approach appears to be fundamentally different from current optimization techniques, making it highly relevant to research interest in LLM training and inference optimization."
    }
  },
  {
    "id": "Gj5JTAwdoy",
    "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
    "abstract": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge.",
    "authors": [
      "~Zachary_Novack1",
      "~Ge_Zhu1",
      "~Jonah_Casebeer1",
      "~Julian_McAuley1",
      "~Taylor_Berg-Kirkpatrick1",
      "~Nicholas_J._Bryan1"
    ],
    "pdf": "/pdf/e0b307e9b7220cc8ec5717503ac193aff6bb2737.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on inference acceleration through distillation techniques, which directly aligns with optimization interests. It addresses reducing both sampling steps and computational cost per step, leading to significant latency improvements (10-18x faster). The distillation methods for model optimization are highly relevant to improving GPU utilization and throughput. The specific latency metrics and acceleration techniques could be applicable to large language models despite being applied to music generation models.",
      "Irrelevant Aspects": "The paper specifically focuses on diffusion-based text-to-music models rather than large language models, which is the primary focus of the research interest. It doesn't explicitly address scalability in terms of distributed systems or multi-GPU setups. The training optimization aspects are less emphasized compared to inference optimization.",
      "Summary": "This paper presents Presto!, an approach for accelerating diffusion-based text-to-music models through step and layer distillation. It achieves 10-18x speedup by reducing both sampling steps and computational cost per step. While the application domain (music generation) differs from large language models, the inference acceleration techniques and distillation methods are highly relevant to the research interests in optimization, GPU utilization, and latency reduction."
    }
  },
  {
    "id": "ZHhBawo3k5",
    "title": "Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference",
    "abstract": "Large language models (LLMs) have achieved remarkable success across diverse tasks, yet their inference processes are hindered by substantial time and energy demands due to single-token generation at each decoding step. While previous methods such as speculative decoding mitigate these inefficiencies by producing multiple tokens per step, each token is still generated by its single-token distribution,\nthereby enhancing speed without improving effectiveness. In contrast, our work simultaneously enhances inference speed and improves the output effectiveness. We consider multi-token joint decoding (MTJD), which generates multiple tokens from their joint distribution at each iteration, theoretically reducing perplexity and enhancing task performance. However, MTJD suffers from the high cost of sampling from the joint distribution of multiple tokens. Inspired by speculative decoding, we introduce multi-token assisted decoding (MTAD), a novel framework designed to accelerate MTJD. MTAD leverages a smaller auxiliary model to approximate the joint distribution of a larger model, incorporating a verification mechanism that not only ensures the accuracy of this approximation, but also improves the\ndecoding efficiency over conventional speculative decoding. Theoretically, we demonstrate that MTAD closely approximates exact MTJD with bounded error. Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B parameters across various tasks reveal that MTAD reduces perplexity by 21.2% and improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42× speed-up and consumes 1.54× less energy than conventional speculative decoding methods. These results highlight MTAD’s ability to make multi-token joint decoding both effective and efficient, promoting more sustainable and high-performance deployment of LLMs.",
    "authors": [
      "~Zongyue_Qin1",
      "~Ziniu_Hu1",
      "~Zifan_He1",
      "~Neha_Prakriya1",
      "~Jason_Cong1",
      "~Yizhou_Sun1"
    ],
    "pdf": "/pdf/9204db48409e1c0b38ddc68430605b0898d371b8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization, focusing on improving throughput through multi-token generation. It introduces MTAD (Multi-Token Assisted Decoding) which uses a smaller auxiliary model to accelerate the decoding process, achieving 1.42× speed-up and 1.54× less energy consumption compared to conventional speculative decoding. The empirical evaluations on large models (13B to 70B parameters) provide relevant insights into scalability and GPU utilization optimization.",
      "Irrelevant Aspects": "The paper places significant emphasis on reducing perplexity and improving task performance, which is less central to my focus on computational efficiency. More theoretical aspects about bounded error approximation are also less relevant than the practical implementation details for optimization.",
      "Summary": "This paper is highly relevant to my research interests in LLM inference optimization. It presents a novel approach to accelerate inference through multi-token generation using an auxiliary model, directly addressing throughput, latency, and energy efficiency concerns. The empirical results on large models provide valuable insights for scalability and GPU utilization optimization."
    }
  },
  {
    "id": "vo9t20wsmd",
    "title": "Faster Cascades via Speculative Decoding",
    "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while  speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines.",
    "authors": [
      "~Harikrishna_Narasimhan1",
      "~Wittawat_Jitkrittum1",
      "~Ankit_Singh_Rawat1",
      "~Seungyeon_Kim1",
      "~Neha_Gupta1",
      "~Aditya_Krishna_Menon1",
      "~Sanjiv_Kumar1"
    ],
    "pdf": "/pdf/43a6987941b5bf274281662662d929ea2d79aee9.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Language model inference optimization, speculative decoding techniques, GPU utilization improvement, latency reduction, cost-quality trade-offs, parallel execution in LM inference, model efficiency through selective processing",
      "Irrelevant Aspects": "Training optimization methods, multi-GPU scalability, hardware-specific optimizations, memory management techniques",
      "Summary": "This paper presents a hybrid approach combining cascades and speculative decoding to improve language model inference efficiency. It implements deferral rules through speculative execution, offering better cost-quality trade-offs than either approach alone. The work directly addresses my interests in inference optimization for better GPU utilization and lower latency, though it focuses primarily on inference rather than training optimization or distributed systems."
    }
  },
  {
    "id": "7igPXQFupX",
    "title": "CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference",
    "abstract": "Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same time---regardless of the model size---task-specific techniques continue to play a pivotal role in achieving optimal downstream performance. One of these techniques, called Chain-of-Thought (CoT), is particularly interesting since, as we point out in this work, it resembles employing a deeper transformer through re-applying the model multiple times. However, a key subtlety in computing the attention of past tokens differentiates CoT from simply applying the model several times. Based on this insight, we propose CoTFormer, a novel architecture which closely mimics CoT at the token level, allowing us to obtain significantly improved accuracies close to much larger models. While applying CoT introduces additional computation costs, we compensate for it by leveraging CoTFormer's special compatibility with token-wise variable depth. Through a compute adaptive model---which automatically allocates the compute to tokens that need it most---we show that it is possible to reduce the computation cost significantly without any reduction in accuracy, and with further compute cost reductions possible while maintaining a competitive accuracy.",
    "authors": [
      "~Amirkeivan_Mohtashami1",
      "~Matteo_Pagliardini1",
      "~Martin_Jaggi1"
    ],
    "pdf": "/pdf/2d5a0841718378feeff4b2b886c3222abb08a1ef.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Budget-adaptive computation cost at inference",
        "Token-wise variable depth architecture",
        "Compute allocation to tokens that need it most",
        "Reducing computation cost without accuracy reduction",
        "Performance close to much larger models",
        "Inference optimization techniques"
      ],
      "Irrelevant Aspects": [
        "Limited focus on training optimization",
        "Minimal discussion of GPU utilization specifically",
        "Not primarily focused on throughput and latency metrics",
        "Limited discussion of scalability to larger systems"
      ],
      "Summary": "CoTFormer introduces a novel architecture that mimics Chain-of-Thought reasoning at the token level, with budget-adaptive computation at inference. The key innovation is variable token-wise depth that automatically allocates compute to tokens that need it most, reducing computation costs without accuracy reduction. The paper is particularly relevant to inference optimization in compute-constrained environments and has implications for GPU utilization and processing efficiency, though it focuses more on accuracy preservation than explicit throughput or latency optimization."
    }
  },
  {
    "id": "WA84oMWHaH",
    "title": "Adaptive Pruning of Pretrained Transformer via Differential Inclusions",
    "abstract": "Large transformers have demonstrated remarkable success, making it necessary to compress these models to reduce inference costs while preserving their performance. Current compression algorithms prune transformers at fixed compression ratios, requiring a unique pruning process for each ratio, which results in high computational costs. In contrast, we propose pruning of pretrained transformers at any desired ratio within a single pruning stage, based on a differential inclusion for a mask parameter. This dynamic can generate the whole regularization solution path of the mask parameter, whose support set identifies the network structure. Therefore, the solution path identifies a Transformer weight family with various sparsity levels, offering greater flexibility and customization.In this paper, weintroduce such an effective pruning method, termed SPP (Solution Path Pruning). To achieve effective pruning, we segment the transformers into paired modules, including query-key pairs, value-projection pairs, and sequential linear layers, and apply low-rank compression to these pairs, maintaining the output structure while enabling structural compression within the inner states. Extensive experiments conducted on various well-known transformer backbones have demonstrated the efficacy of SPP.",
    "authors": [
      "~Yizhuo_Ding1",
      "~Ke_Fan1",
      "~Yikai_Wang1",
      "~Xinwei_Sun1",
      "~Yanwei_Fu2"
    ],
    "pdf": "/pdf/2650874e3d902fe2aef3f9e08b311e43b29c1677.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on model compression for transformers, which directly relates to inference optimization. It addresses computational costs by enabling flexible pruning ratios within a single process, potentially improving GPU utilization and scalability. The method maintains output structure while compressing internal states, which could help preserve model performance while reducing computational requirements.",
      "Irrelevant Aspects": "The paper focuses primarily on the theoretical aspects of pruning using differential inclusions rather than systems implementation. It doesn't explicitly address distributed training/inference, latency measurements, or throughput metrics. The approach is limited to pruning techniques without considering other optimization methods like quantization or knowledge distillation.",
      "Summary": "The paper presents SPP (Solution Path Pruning), a method for adaptive pruning of pretrained transformers that can generate multiple compression ratios within a single pruning process. By segmenting transformers into paired modules and applying low-rank compression, it aims to reduce inference costs while maintaining performance. While directly relevant to model compression for inference efficiency, it focuses more on the algorithmic approach than systems-level implementation details."
    }
  },
  {
    "id": "G7u4ue6ncT",
    "title": "Implicit In-context Learning",
    "abstract": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly adapt to unseen tasks at inference-time by prefixing a few demonstration examples before queries. Despite its versatility, ICL incurs substantial computational and memory overheads compared to zero-shot learning and is sensitive to the selection and order of demonstration examples. In this work, we introduce \\textbf{Implicit In-context Learning} (I2CL), an innovative paradigm that reduces the inference cost of ICL to that of zero-shot learning with minimal information loss. I2CL operates by first generating a condensed vector representation, namely a context vector, extracted from the demonstration examples. It then conducts an inference-time intervention through injecting a linear combination of the context vector and query activations back into the model’s residual streams. Empirical evaluation on nine real-world tasks across three model architectures demonstrates that I2CL achieves few-shot level performance at zero-shot inference cost, and it exhibits robustness against variations in demonstration examples. Furthermore, I2CL facilitates a novel representation of ``task-ids'', enhancing task similarity detection and fostering effective transfer learning. We also perform a comprehensive analysis and ablation study on I2CL, offering deeper insights into its internal mechanisms. Code is available at https://github.com/LzVv123456/I2CL.",
    "authors": [
      "~Zhuowei_Li1",
      "~Zihao_Xu2",
      "~Ligong_Han1",
      "~Yunhe_Gao2",
      "~Song_Wen1",
      "~Di_Liu3",
      "~Hao_Wang3",
      "~Dimitris_N._Metaxas1"
    ],
    "pdf": "/pdf/e4a47439a6f1ee9a5c9b22be5e9edad4f1bfc646.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Directly addresses inference optimization by reducing computational costs of ICL to zero-shot levels, aims to improve GPU utilization, potentially increases throughput and reduces latency, reduces memory overhead during inference, introduces a novel technique for integrating into existing inference pipelines, claims to maintain few-shot performance while significantly improving efficiency",
      "Irrelevant Aspects": "Does not address training optimization, limited focus on hardware-specific optimizations, doesn't provide detailed analysis of actual throughput and latency improvements on specific hardware, limited discussion of scalability in distributed inference settings",
      "Summary": "The paper introduces Implicit In-context Learning (I2CL), a novel approach that significantly reduces the inference cost of In-context Learning while maintaining performance. It condenses demonstration examples into context vectors and injects them back into the model's residual streams, addressing computational and memory efficiency during inference. This is highly relevant to LLM inference optimization research as it targets GPU utilization, throughput, and latency improvements, though it doesn't address training optimization or hardware-specific implementations."
    }
  },
  {
    "id": "T26f9z2rEe",
    "title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models",
    "abstract": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results. However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-$k$), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at \\url{https://github.com/LINs-lab/DynMoE}.",
    "authors": [
      "~Yongxin_Guo1",
      "~Zhenglin_Cheng1",
      "~Xiaoying_Tang2",
      "~Zhaopeng_Tu1",
      "~Tao_Lin1"
    ],
    "pdf": "/pdf/4655c716e3e570384c1f2d9a1aa4207e59394314.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper addresses Sparse Mixture of Experts (SMoE) for transformer efficiency, which is directly relevant to training and inference optimization of large language models. It focuses on reducing parameter activation while maintaining performance, which impacts GPU utilization and computational efficiency. The automatic expert activation and adaptive adjustment of expert numbers during training could significantly improve throughput and reduce latency.",
      "Irrelevant Aspects": "The paper includes Vision and Vision-Language applications beyond my focus on language models. It emphasizes architectural innovation rather than specific hardware-aware optimizations. There's limited discussion of specific GPU utilization strategies or implementation details.",
      "Summary": "DynMoE introduces a dynamic approach to Mixture of Experts where tokens automatically determine expert activation and the system adapts the number of experts during training. This addresses the hyperparameter sensitivity of traditional SMoE, potentially improving computational efficiency for large transformer models while reducing parameter activation."
    }
  },
  {
    "id": "hzVpZDrW73",
    "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption under decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines. Code is available at https://github.com/Osilly/dynamic_llava.",
    "authors": [
      "~Wenxuan_Huang2",
      "~Zijie_Zhai1",
      "~Yunhang_Shen1",
      "~Shaosheng_Cao2",
      "~Fei_Zhao5",
      "~Xiangfeng_Xu1",
      "~Zheyu_Ye1",
      "~Shaohui_Lin1"
    ],
    "pdf": "/pdf/a07b1b7ec31c8e57a857c631b1c5a7181d1e509d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper is highly relevant as it focuses on inference optimization for large language models, specifically addressing computation and memory efficiency during both prefill and decoding stages. It tackles GPU utilization, memory overhead, and computational efficiency, which directly impact throughput and latency - all key aspects of my research interest. The dynamic sparsification approach and its application to different inference modes (with and without KV cache) provides insights into scalable optimization strategies.",
      "Irrelevant Aspects": "The paper is specifically focused on multimodal large language models (MLLMs) rather than general large language models, which narrows its direct applicability. Additionally, it doesn't address training optimization, which is another key component of my research interest.",
      "Summary": "Dynamic-LLaVA proposes a vision-language context sparsification framework for efficient MLLM inference, achieving ~75% computation reduction in prefill and ~50% savings during decoding, with negligible performance degradation."
    }
  },
  {
    "id": "0tAXMiSufG",
    "title": "BOND: Aligning LLMs with Best-of-N Distillation",
    "abstract": "Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models.\nYet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates.\nIn this paper, we propose Best-of-N Distillation (BOND), a novel RLHF algorithm that seeks to emulate Best-of-N but without its significant computational overhead at inference time. Specifically, BOND is a distribution matching algorithm that forces the distribution of generations from the policy to get closer to the Best-of-N distribution. We use the Jeffreys divergence (a linear combination of forward and backward KL) to balance between mode-covering and mode-seeking behavior, and derive an iterative formulation that utilizes a moving anchor for efficiency. We demonstrate the effectiveness of our approach and several design choices through experiments on abstractive summarization and Gemma models.",
    "authors": [
      "~Pier_Giuseppe_Sessa1",
      "~Robert_Dadashi-Tazehozi1",
      "~Leonard_Hussenot1",
      "~Johan_Ferret1",
      "~Nino_Vieillard1",
      "~Alexandre_Rame1",
      "~Bobak_Shahriari1",
      "~Sarah_Perrin1",
      "~Abram_L._Friesen1",
      "~Geoffrey_Cideron1",
      "~Sertan_Girgin1",
      "~Piotr_Stanczyk1",
      "~Andrea_Michi1",
      "~Danila_Sinopalnikov1",
      "~Sabela_Ramos_Garea1",
      "~Amélie_Héliou1",
      "~Aliaksei_Severyn1",
      "~Matthew_Hoffman1",
      "~Nikola_Momchev1",
      "~Olivier_Bachem1"
    ],
    "pdf": "/pdf/a95974c6ddbf80c3786547203e88d56fba3e34a8.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization by reducing computational overhead, introduces a distillation method for model optimization, focuses on improving efficiency of Best-of-N sampling strategy, claims to eliminate significant computational overhead at inference time, which could lead to lower latency and potentially higher throughput",
      "Irrelevant Aspects": "The paper seems to focus more on the alignment aspect (RLHF) rather than pure optimization, might be more about the distillation algorithm itself rather than specific optimization techniques, doesn't explicitly mention GPU utilization or scalability metrics",
      "Summary": "BOND proposes a method to emulate Best-of-N sampling without its computational overhead, using a distribution matching algorithm with Jeffreys divergence. The approach aims to reduce inference time while maintaining the quality benefits of Best-of-N sampling, making it highly relevant to LLM inference optimization research focused on efficiency, throughput, and latency."
    }
  },
  {
    "id": "YTEwJaBdh0",
    "title": "On the Crucial Role of Initialization for Matrix Factorization",
    "abstract": "This work revisits the classical low-rank matrix factorization problem and unveils the critical role of initialization in shaping convergence rates for such nonconvex and nonsmooth optimization. We introduce Nystrom initialization, which significantly improves the global convergence of Scaled Gradient Descent (ScaledGD) in both symmetric and asymmetric matrix factorization tasks. Specifically, we prove that ScaledGD with Nystrom initialization achieves quadratic convergence in cases where only linear rates were previously known. Furthermore, we extend this initialization to low-rank adapters (LoRA) commonly used for finetuning foundation models. Our approach, NoRA, i.e., LoRA with Nystrom initialization, demonstrates superior performance across various downstream tasks and model scales, from 1B to 7B parameters, in large language and diffusion models.",
    "authors": [
      "~Bingcong_Li1",
      "~Liang_Zhang6",
      "~Aryan_Mokhtari3",
      "~Niao_He3"
    ],
    "pdf": "/pdf/4bb1cffa25d0487805b0bd2eeb75bb016caf8d36.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses LoRA optimization for large language models, demonstrates improvements across model scales from 1B to 7B parameters, and introduces faster convergence rates that could enhance training efficiency. These aspects are relevant to LLM training optimization and scalability concerns.",
      "Irrelevant Aspects": "The paper lacks focus on inference optimization techniques, GPU utilization strategies, throughput analysis, or latency improvements. It focuses specifically on initialization rather than comprehensive optimization frameworks.",
      "Summary": "This paper introduces Nystrom initialization to improve matrix factorization convergence, extending this to LoRA (NoRA) for fine-tuning large language models. The work demonstrates superior performance across model scales and claims quadratic convergence rates, making it relevant to training optimization aspects of my research interest, though less applicable to inference optimization, GPU utilization, and latency concerns."
    }
  },
  {
    "id": "vkakKdznFS",
    "title": "Text4Seg: Reimagining Image Segmentation as Text Generation",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks; however, effectively integrating image segmentation into these models remains a significant challenge. In this paper, we introduce Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. This unified representation allows seamless integration into the auto-regressive training pipeline of MLLMs for easier optimization. We demonstrate that representing an image with $16\\times16$ semantic descriptors yields competitive segmentation performance. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74\\% and accelerating inference by $3\\times$, without compromising performance. Extensive experiments across various vision tasks, such as referring expression segmentation and comprehension, show that Text4Seg achieves state-of-the-art performance on multiple datasets by fine-tuning different MLLM backbones. Our approach provides an efficient, scalable solution for vision-centric tasks within the MLLM framework.",
    "authors": [
      "~Mengcheng_Lan1",
      "~Chaofeng_Chen1",
      "~Yue_Zhou4",
      "~Jiaxing_Xu2",
      "~Yiping_Ke1",
      "~Xinjiang_Wang1",
      "~Litong_Feng1",
      "~Wayne_Zhang2"
    ],
    "pdf": "/pdf/ceeba9d23959899b63109abeb947f0097869a4a6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper introduces Text4Seg, which reformulates image segmentation as a text generation problem within Multimodal Large Language Models (MLLMs), eliminating additional decoders and simplifying the process. It focuses on integration into MLLMs' auto-regressive training pipeline for easier optimization. The introduction of Row-wise Run-Length Encoding (R-RLE) compresses redundant text sequences by 74% and accelerates inference by 3×, directly addressing inference optimization. The approach is described as an efficient, scalable solution for vision-centric tasks within the MLLM framework, aligning with my interest in scalability.",
      "Irrelevant Aspects": "The paper primarily focuses on vision-language tasks, specifically image segmentation, which differs from pure language model optimization. Performance evaluation is on segmentation tasks rather than traditional language model benchmarks.",
      "Summary": "Text4Seg presents a novel approach to image segmentation by reimagining it as a text generation problem within MLLMs. The key innovation is semantic descriptors - textual representations of segmentation masks. The paper introduces Row-wise Run-Length Encoding (R-RLE) to compress text sequences, resulting in 74% reduction in descriptor length and 3× faster inference without compromising performance. The approach achieves state-of-the-art results on multiple datasets by fine-tuning different MLLM backbones, offering an efficient, scalable solution for vision tasks within the MLLM framework."
    }
  },
  {
    "id": "Zkp1GuHerF",
    "title": "LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics",
    "abstract": "We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training. This strategy keeps the optimizer's memory footprint to a fraction of the model size. LDAdam relies on a new projection-aware update rule for the optimizer states that allows for transitioning between subspaces, i.e., estimation of the statistics of the projected gradients. To mitigate the errors due to low-rank projection, LDAdam integrates a new generalized error feedback mechanism, which explicitly accounts for both gradient and optimizer state compression. We prove the convergence of LDAdam under standard assumptions, and provide empirical evidence that LDAdam allows for efficient fine-tuning and pre-training of language models.",
    "authors": [
      "~Thomas_Robert2",
      "~Mher_Safaryan1",
      "~Ionut-Vlad_Modoranu1",
      "~Dan_Alistarh7"
    ],
    "pdf": "/pdf/9cfa255a3cc79e89a0d409b04129bf87c784fd1d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Memory-efficient optimizer for training large models, adaptive optimization in lower dimensional subspaces, reduced memory footprint to fraction of model size, projection-aware update rule, convergence proof, empirical evidence for language model training",
      "Irrelevant Aspects": "Does not specifically address inference optimization, lacks explicit discussion of GPU utilization patterns, minimal focus on throughput and latency metrics",
      "Summary": "LDAdam introduces a memory-efficient optimization approach for large language models that performs adaptive optimization in lower dimensional subspaces, significantly reducing the optimizer's memory footprint while maintaining convergence properties. This is highly relevant for optimizing large language model training, though it doesn't address inference optimization or specific GPU utilization strategies."
    }
  },
  {
    "id": "eWNEqdH0vk",
    "title": "Layerwise Recurrent Router for  Mixture-of-Experts",
    "abstract": "The scaling of large language models (LLMs) has revolutionized their capabilities in various tasks, yet this growth must be matched with efficient computational strategies. \nThe Mixture-of-Experts (MoE) architecture stands out for its ability to scale model size without significantly increasing training costs. \nDespite their advantages, current MoE models often display parameter inefficiency. \nFor instance, a pre-trained MoE-based LLM with 52 billion parameters might perform comparably to a standard model with 6.7 billion. \nBeing a crucial part of MoE, \ncurrent routers in different layers independently assign tokens without leveraging historical routing information, potentially leading to suboptimal token-expert combinations and the parameter inefficiency problem.\nTo alleviate this issue, we introduce the Layerwise Recurrent Router for Mixture-of-Experts (RMoE). \nRMoE leverages a Gated Recurrent Unit (GRU) to establish dependencies between routing decisions across consecutive layers.\nSuch layerwise recurrence can be efficiently parallelly computed for input tokens and introduces negotiable costs.\nOur extensive empirical evaluations demonstrate that RMoE-based language models consistently outperform a spectrum of baseline models. \nFurthermore, RMoE integrates a novel computation stage orthogonal to existing methods, allowing seamless compatibility with other MoE architectures. \nOur analyses attribute RMoE's gains to its effective cross-layer information sharing, which also improves expert selection and diversity.",
    "authors": [
      "~Zihan_Qiu1",
      "~Zeyu_Huang1",
      "~Shuang_Cheng1",
      "~Yizhi_Zhou3",
      "~Zili_Wang1",
      "~Ivan_Titov1",
      "~Jie_Fu2"
    ],
    "pdf": "/pdf/6dd2e78f7bd94cb60eb3d1eae7ecc7a4cea4579d.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on Mixture-of-Experts (MoE) architecture which is directly related to efficient LLM scaling; Addresses parameter efficiency in MoE models; Claims efficient parallelization of the recurrent router for input tokens; Introduces a technique that can scale model size without significantly increasing training costs; Claims compatibility with existing MoE architectures; Aims to improve expert selection and diversity which can impact computational efficiency",
      "Irrelevant Aspects": "Limited discussion of specific GPU utilization metrics; Focuses more on architectural improvements than implementation details for hardware optimization; Doesn't explicitly address inference optimization in terms of latency and throughput measurements; The abstract lacks quantitative results about computational efficiency gains",
      "Summary": "This paper presents the Layerwise Recurrent Router for Mixture-of-Experts (RMoE), which uses a GRU to establish dependencies between routing decisions across consecutive layers. The approach aims to address parameter inefficiency in MoE models by improving expert selection through cross-layer information sharing. While the paper claims efficient parallelization and compatibility with existing architectures, the abstract lacks specific metrics on GPU utilization, throughput, or latency improvements. The work is highly relevant to LLM scaling and MoE optimization, which are central to efficient training and inference systems."
    }
  },
  {
    "id": "dh4t9qmcvK",
    "title": "Transformer-Squared: Self-adaptive LLMs",
    "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific 'expert' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
    "authors": [
      "~Qi_Sun10",
      "~Edoardo_Cetin1",
      "~Yujin_Tang1"
    ],
    "pdf": "/pdf/3c0ee80b358cf1691123c95857f4b2cce4702101.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces a self-adaptation framework that adapts LLMs for unseen tasks in real-time, claiming to be more efficient than approaches like LoRA with fewer parameters. It describes a two-pass mechanism during inference with a dispatch system and dynamic mixing of expert vectors. The approach is presented as scalable and works across different LLM architectures and modalities, which could impact GPU utilization and efficiency.",
      "Irrelevant Aspects": "The focus seems to be more on the adaptation mechanism rather than detailed GPU utilization analysis. There's no explicit mention of throughput or latency measurements. The paper doesn't appear to focus on detailed training optimization for the base models, nor does it seem to address distributed training or inference systems at scale.",
      "Summary": "Transformer-Squared introduces a self-adaptive LLM framework that adjusts singular components of weight matrices in real-time during inference. While it presents efficiency improvements over LoRA and claims scalability, the paper's focus is on adaptive capabilities rather than the specific training and inference optimizations I'm most interested in. The two-pass mechanism and dynamic mixing of expert vectors could potentially improve GPU utilization, but this appears to be secondary to the adaptation functionality."
    }
  },
  {
    "id": "cmYScmfu4Q",
    "title": "Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference",
    "abstract": "Reward inference (learning a reward model from human preferences) is a critical intermediate step in the Reinforcement Learning from Human Feedback (RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF faces fundamental challenges such as distribution shift, reward model overfitting, and problem misspecification. An alternative approach is direct policy optimization without reward inference, such as Direct Preference Optimization (DPO), which provides a much simpler pipeline and has shown empirical success in LLM applications. However, DPO utilizes the closed-form expression between the optimal policy and the reward function, which is only suitable under the bandit setting or deterministic MDPs. This paper develops two RLHF algorithms without reward inference for general RL problems beyond bandits and deterministic MDPs, and general preference models beyond the Bradley-Terry model. The key idea is to estimate the local value function difference from human preferences and then approximate the policy gradient with a zeroth-order gradient approximator. For both algorithms, we establish polynomial convergence rates in terms of the number of policy gradient iterations, the number of trajectory samples, and human preference queries per iteration. Numerical experiments in stochastic environments validate the performance of our proposed algorithms, outperforming popular RLHF baselines such as DPO and PPO. Our paper shows there exist provably efficient methods to solve general RLHF problems without reward inference.",
    "authors": [
      "~Qining_Zhang2",
      "~Lei_Ying1"
    ],
    "pdf": "/pdf/420c80e7c7bef257b8b85ab8dc6421a909d695d6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": [
        "Focus on RLHF for LLM fine-tuning",
        "Proposes optimizations to RLHF pipeline",
        "Comparison against PPO widely used in LLM training",
        "Simplification of RLHF pipeline potentially improving efficiency",
        "Direct policy optimization without reward inference could reduce computational overhead",
        "Claims better performance than existing methods",
        "Potential for reduced computational steps in LLM training pipeline"
      ],
      "Irrelevant Aspects": [
        "No specific focus on GPU utilization or inference optimization",
        "Limited discussion of scalability, throughput, or latency",
        "More focused on algorithmic advances than system implementation",
        "Limited discussion of computational efficiency metrics"
      ],
      "Summary": "The paper presents algorithms for RLHF without reward inference, which could potentially improve training efficiency, but doesn't specifically address the system optimization aspects central to my research focus on GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "JElN0LJMKB",
    "title": "From Decoupling to Adaptive Transformation: a Wider Optimization Space for PTQ",
    "abstract": "Post-Training low-bit Quantization (PTQ) is useful to accelerate DNNs due to its high efficiency, the current SOTAs of which mostly adopt feature reconstruction with self-distillation finetuning. However, when bitwidth goes to be extremely low, we find the current reconstruction optimization space is not optimal. Considering all possible parameters and the ignored fact that integer weight can be obtained early before actual inference, we thoroughly explore different optimization space by quant-step decoupling, where a wider PTQ optimization space, which consistently makes a better optimum, is found out. Based on these, we propose an Adaptive Quantization Transformation (AdaQTransform) for PTQ reconstruction,  which makes the quantized output feature better fit the FP32 counterpart with adaptive per-channel transformation, thus achieves lower feature reconstruction error.  In addition,  it incurs negligible extra finetuning cost and no extra inference cost. Based on AdaQTransform, for the first time, we build a general quantization setting paradigm subsuming current PTQs, QATs and other potential forms. Experiments demonstrate AdaQTransform expands the optimization space for PTQ and helps current PTQs find a better optimum over CNNs, ViTs, LLMs and image super-resolution networks,  e.g., it improves NWQ by 5.7% on ImageNet for W2A2-MobileNet-v2.",
    "authors": [
      "~Zhaojing_Wen2",
      "~Qiulin_Zhang1",
      "~Yuan_Zhang11",
      "~Rudan_Chen1",
      "~Xichao_Yang1",
      "~Di_Xie1",
      "~Jiang_Zhu3"
    ],
    "pdf": "/pdf/38383819f0063baaf9568a04ef0d4bcc844a4b9a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on Post-Training Quantization (PTQ) for deep neural networks, which is directly relevant to inference optimization for LLMs. It specifically mentions LLMs as one of the model types that benefits from their Adaptive Quantization Transformation (AdaQTransform) method. The technique aims to reduce feature reconstruction error with negligible extra inference cost, which could improve GPU utilization and throughput during LLM inference. The paper addresses low-bit quantization challenges, which are important for efficient LLM deployment.",
      "Irrelevant Aspects": "The paper also focuses on CNNs, Vision Transformers (ViTs), and image super-resolution networks, which are outside the primary scope of LLM optimization. While it mentions applications to LLMs, the paper appears to be more focused on the quantization method itself rather than the broader system-level implications for GPU utilization, scalability, throughput, and latency. The evaluation metrics like NWQ on ImageNet for MobileNet-v2 aren't directly relevant to LLM performance.",
      "Summary": "The paper proposes Adaptive Quantization Transformation (AdaQTransform) for Post-Training Quantization, expanding the optimization space for PTQ. It claims to help find a better optimum across various model architectures including CNNs, ViTs, LLMs, and image super-resolution networks. The method aims to reduce feature reconstruction error with adaptive per-channel transformation while incurring negligible extra finetuning cost and no extra inference cost."
    }
  },
  {
    "id": "iYkhxre0In",
    "title": "PaCA: Partial Connection Adaptation for Efficient Fine-Tuning",
    "abstract": "Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants avoid this latency overhead by merging the low-rank adapter matrices with the pretrained weights during inference. However, those layers cannot be merged during training since the pretrained weights must remain frozen while the low-rank adapter matrices are updated continuously over the course of training. Furthermore, LoRA and its variants do not reduce activation memory, as the first low-rank adapter matrix still requires the input activations to the pretrained weights to compute weight gradients. To mitigate this issue, we propose **Pa**rtial **C**onnection **A**daptation (**PaCA**), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training with 23% longer sequence and improves throughput by 16\\% on both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is available at [https://github.com/WooSunghyeon/paca](https://github.com/WooSunghyeon/paca).",
    "authors": [
      "~Sunghyeon_Woo1",
      "~Sol_Namkung1",
      "~Sunwoo_Lee2",
      "~Inho_Jeong1",
      "~Beomseok_Kim1",
      "~Dongsuk_Jeon1"
    ],
    "pdf": "/pdf/24155a505d9266c139b08324b109e450b87f023c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on parameter-efficient fine-tuning (PEFT) algorithms for large language models, which directly addresses training optimization. It proposes a method that reduces training time by 22% and memory usage by 16% compared to LoRA while maintaining accuracy. PaCA eliminates sequential processing overhead between adapter and pretrained layers, reduces activation memory, and improves throughput by 16%. It's tested on large models like LLaMA3.1-70B and is compatible with quantization, showing practical scalability. The method enables training with 23% longer sequences, which is valuable for optimizing GPU utilization.",
      "Irrelevant Aspects": "The paper doesn't address inference optimization in detail, which is part of my research focus. It doesn't appear to discuss distributed training or scaling across multiple GPUs, which are important aspects of scalability. There's no mention of how PaCA affects model serving infrastructure or deployment considerations.",
      "Summary": "PaCA introduces a novel approach to parameter-efficient fine-tuning that directly fine-tunes selected connections within pretrained weights rather than adding adapter layers. This eliminates sequential processing overhead and reduces activation memory, achieving significant improvements in training efficiency while maintaining comparable accuracy across various benchmarks."
    }
  },
  {
    "id": "Pu3c0209cx",
    "title": "Tight Clusters Make Specialized Experts",
    "abstract": "Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying clustering structure of the input distribution in order to send input tokens to appropriate experts. However, latent clusters may be unidentifiable in high dimension, which causes slow convergence, susceptibility to data contamination, and overall degraded representations as the router is unable to perform appropriate token-expert matching. We examine the router through the lens of clustering optimization and derive optimal feature weights that maximally identify the latent clusters. We use these weights to compute the token-expert routing assignments in an adaptively transformed space that promotes well-separated clusters, which helps identify the best-matched expert for each token. In particular, for each expert cluster, we compute a set of weights that scales features according to whether that expert clusters tightly along that feature. We term this novel router the Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain three connected benefits: 1) faster convergence, 2) better robustness to data corruption, and 3) overall performance improvement, as experts are specialized in semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods when applied on a variety of MoE backbones for language modeling and image recognition tasks in both clean and corrupted settings.",
    "authors": [
      "~Stefan_Nielsen1",
      "~Rachel_Teo1",
      "~Laziz_Abdullaev1",
      "~Tan_Minh_Nguyen1"
    ],
    "pdf": "/pdf/9a1451feb82e2bf65941a799b11859152bc92db6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses Sparse Mixture-of-Experts (MoE) architectures which are directly relevant to scaling large language models efficiently. The optimization of the router component could potentially improve computational efficiency and model specialization. The approach enables faster convergence, which relates to training optimization. Better token-expert matching could lead to more efficient resource utilization in MoE systems.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization, throughput, or latency metrics. There's no specific discussion of inference optimization or system implementation details. The focus is on the mathematical/algorithmic aspects of clustering rather than hardware-specific optimizations. The paper includes image recognition tasks, which is less central to the focus on language models.",
      "Summary": "This paper introduces an Adaptive Clustering (AC) router for Sparse Mixture-of-Experts architectures. The approach computes feature weights that scale features based on how tightly expert clusters align with those features, improving token-expert assignments in a transformed space. This enables faster convergence, better robustness to data corruption, and overall performance improvement as experts specialize in semantically distinct regions. The method is evaluated on various MoE backbones for both language modeling and image recognition tasks in clean and corrupted settings."
    }
  },
  {
    "id": "hDBrQ4DApF",
    "title": "Real-Time Video Generation with Pyramid Attention Broadcast",
    "abstract": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. We further introduce broadcast sequence parallel for more efficient distributed inference. PAB demonstrates up to 10.5x speedup across three models compared to baselines, achieving real-time generation for up to 720p videos. We anticipate that our simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation.",
    "authors": [
      "~Xuanlei_Zhao1",
      "~Xiaolong_Jin2",
      "~Kai_Wang8",
      "~Yang_You1"
    ],
    "pdf": "/pdf/0e3e0adc0fd4d73344f909e993ed31cad907d950.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization for transformer-based models through attention optimization techniques, with insights about attention redundancy that could be transferable to language models. It explicitly addresses scalability through distributed inference and achieves higher throughput with a 10.5x speedup. The real-time generation capability indicates lower latency. The variance-based broadcast strategies could inspire similar adaptive approaches for language models.",
      "Irrelevant Aspects": "The paper is specifically about video generation using diffusion models, not language models. It doesn't address training optimization, which is one of my key interests. The specific pyramid broadcast technique is tailored to the multi-step diffusion process rather than language model inference.",
      "Summary": "This paper presents inference optimization techniques for video generation with DiT models. While focused on video generation, the attention optimization approaches and distributed inference strategy are based on transformer architectures and could have relevance to language model optimization. The paper addresses most of my research interests except training optimization, and the insights about attention redundancy and variance-based strategies could inspire similar approaches for optimizing language models."
    }
  },
  {
    "id": "SI2hI0frk6",
    "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
    "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.\nTransfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.\nWe pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.\nOur experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.\nBy introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.\nWe further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.",
    "authors": [
      "~Chunting_Zhou1",
      "~LILI_YU1",
      "~Arun_Babu1",
      "~Kushal_Tirumala1",
      "~Michihiro_Yasunaga1",
      "~Leonid_Shamis1",
      "~Jacob_Kahn1",
      "~Xuezhe_Ma1",
      "~Luke_Zettlemoyer1",
      "~Omer_Levy1"
    ],
    "pdf": "/pdf/52457a71c0fa8cc26ed032383021d8faaa193ceb.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper presents a unified architecture for multi-modal models that combines language modeling and diffusion, establishing scaling laws for these models up to 7B parameters. The approach includes compression techniques for images, showing better scalability than quantization approaches. The training at this scale (2T multi-modal tokens) inherently involves significant optimization challenges. The ability to generate both images and text with competitive performance using a single model could lead to more efficient resource utilization.",
      "Irrelevant Aspects": "The paper primarily focuses on performance metrics for specific tasks rather than system optimization. It appears to emphasize cross-modal capabilities and architectural innovations for unified understanding rather than direct optimization of GPU utilization, throughput, or latency. The qualitative analysis of cross-modal generation is less relevant to system optimization interests.",
      "Summary": "Transfusion introduces a unified multi-modal model architecture combining language modeling and diffusion for handling text and images. The paper establishes scaling laws for these models up to 7B parameters and demonstrates better scalability than alternative approaches. While primarily focused on the multi-modal capabilities and performance benchmarks, the work involves training at massive scale (2T tokens) and includes compression techniques that have implications for system efficiency. The paper provides valuable insights for large-scale model optimization despite not directly addressing GPU utilization or latency concerns."
    }
  },
  {
    "id": "t7P5BUKcYv",
    "title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts",
    "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) **Low Computing Overhead**: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) **High Performance**: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) **Deployment Friendly**: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1$\\sim$2.1$\\times$ expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.",
    "authors": [
      "~Peng_Jin4",
      "~Bo_Zhu5",
      "~Li_Yuan2",
      "~Shuicheng_YAN3"
    ],
    "pdf": "/pdf/31639500d8df83798156d775c1932e5bf79ce011.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses GPU utilization by enabling dynamic allocation of experts based on token complexity, resulting in 1.1-2.1× throughput improvement. It tackles scalability challenges by eliminating communication overhead and load balancing issues through zero-computation experts. The approach optimizes both training and inference efficiency in MoE models, which are increasingly important for large-scale LLM deployment.",
      "Irrelevant Aspects": "The focus is specifically on Mixture-of-Experts architectures rather than general LLM optimization techniques. Limited discussion on broader optimization strategies beyond expert selection and utilization. No detailed analysis of specific GPU hardware architectures or kernel-level optimizations.",
      "Summary": "MoE++ introduces zero-computation experts to improve both efficiency and performance of Mixture-of-Experts models. By allowing tokens to skip computation, use fewer experts, or be replaced with constant vectors, the framework reduces computational overhead while maintaining or improving model quality. The approach addresses key deployment challenges including communication overhead and load imbalance, directly contributing to better GPU utilization and throughput in MoE systems."
    }
  },
  {
    "id": "cqsw28DuMW",
    "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models",
    "abstract": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.\nA significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.\nTo address these issues, we introduce $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.\nOur comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for vision-language tasks.\nThese results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.",
    "authors": [
      "~Makoto_Shing1",
      "~Kou_Misaki2",
      "~Han_Bao2",
      "~Sho_Yokoi1",
      "~Takuya_Akiba2"
    ],
    "pdf": "/pdf/dba4e4bfe4b8b9eb3f36b71cc43a50da2840a243.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on knowledge distillation for compressing large language models, which directly relates to creating more efficient models for deployment. The approach addresses capacity gaps between teacher and student models, and claims to create compact foundation models that maintain performance. This aligns with my interest in inference optimization and making LLMs more accessible for resource-constrained environments.",
      "Irrelevant Aspects": "The paper doesn't appear to directly address GPU utilization optimization, scalability of training, or specific throughput/latency measurements. The focus is more on the distillation methodology rather than system-level optimizations. There's no explicit mention of implementation details that would impact actual GPU efficiency.",
      "Summary": "TAID presents a novel knowledge distillation approach that dynamically interpolates between student and teacher distributions to address common distillation challenges like mode collapse and capacity gaps. While relevant to creating more efficient models, the paper appears to focus on the distillation technique rather than system-level optimizations like GPU utilization or scalability."
    }
  },
  {
    "id": "oF6e2WwxX0",
    "title": "TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights",
    "abstract": "Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. \nHowever, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results.\nIn this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. \nHowever, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. \nAccordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward.\nInspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses.\nExperiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions.",
    "authors": [
      "~Aiwei_Liu1",
      "~Haoping_Bai1",
      "~Zhiyun_Lu1",
      "~Yanchao_Sun1",
      "~Xiang_Kong1",
      "~Xiaoming_Simon_Wang1",
      "~Jiulong_Shan2",
      "~Albin_Madappally_Jose1",
      "~Xiaojiang_Liu2",
      "~Lijie_Wen1",
      "~Philip_S._Yu1",
      "~Meng_Cao2"
    ],
    "pdf": "/pdf/c725dfeaf988fd720ae55b71922506fb30b0216f.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on training optimization for LLMs, specifically in the context of preference alignment, which is directly relevant to my research interest in LLM training optimization. The token-level importance sampling approach (TIS-DPO) could potentially improve the efficiency of the training process, which could indirectly contribute to better GPU utilization and higher throughput.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization, scalability, throughput, or latency from a systems perspective. It focuses more on the algorithmic aspects of preference optimization rather than the systems-level optimizations that are central to my research interests.",
      "Summary": "This paper proposes TIS-DPO, a token-level importance sampling approach for Direct Preference Optimization in LLMs. It contributes to training optimization for LLMs, which is directly relevant to my research focus, although it doesn't explicitly address the systems-level aspects of GPU utilization, scalability, throughput, and latency."
    }
  },
  {
    "id": "7JhGdZvW4T",
    "title": "DON’T STOP ME NOW: EMBEDDING BASED SCHEDULING FOR LLMS",
    "abstract": "Efficient scheduling is crucial for interactive Large Language Model (LLM) applications, where low request completion time directly impacts user engagement. Size-based scheduling algorithms like Shortest Remaining Process Time (SRPT) aim to reduce average request completion time by leveraging known or estimated request sizes and allowing preemption by incoming jobs with shorter service times. However, two main challenges arise when applying size-based scheduling to LLM systems. First, accurately predicting output lengths from prompts is challenging and often resource-intensive, making it impractical for many systems. As a result, the state-of-the-art LLM systems default to first-come, first-served scheduling, which can lead to head-of-line blocking and reduced system efficiency. Second, preemption introduces extra memory overhead to LLM systems as they must maintain intermediate states for unfinished (preempted) requests.\nIn this paper, we propose TRAIL, a method to obtain output predictions from the target LLM itself. After generating each output token, we recycle the embedding of its internal structure as input for a lightweight classifier that predicts the remaining length for each running request. Using these predictions, we propose a prediction-based SRPT variant with limited preemption designed to account for memory overhead in LLM systems. This variant allows preemption early in request execution when memory consumption is low but restricts preemption as requests approach completion to optimize resource utilization. On the theoretical side, we derive a closed-form formula for this SRPT variant in an M/G/1 queue model, which demonstrates its potential value. In our system, we implement this preemption policy alongside our embedding-based prediction method. Our refined predictions from layer embeddings achieve 2.66x lower mean absolute error compared to BERT predictions from sequence prompts. TRAIL achieves 1.66x to 2.01x lower mean latency on the Alpaca dataset and 1.76x to 24.07x lower mean time to the first token compared to the state-of-the-art serving system.",
    "authors": [
      "~Rana_Shahout1",
      "~eran_malach1",
      "~Chunwei_Liu1",
      "~Weifan_Jiang1",
      "~Minlan_Yu1",
      "~Michael_Mitzenmacher1"
    ],
    "pdf": "/pdf/fd689e53d25c326908c6d30555675d0656da9622.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Scheduling algorithms for LLM inference, reducing latency in LLM serving, memory management during inference, using embeddings for prediction, handling preemption in LLM systems, improving throughput through better scheduling, efficient GPU utilization",
      "Irrelevant Aspects": "Training optimization (paper focuses only on inference), specific implementation details that may not generalize across all systems",
      "Summary": "This paper presents TRAIL, an embedding-based scheduling approach for LLMs that uses the model's own embeddings to predict output length and implements a modified SRPT scheduling algorithm with limited preemption. The approach reportedly achieves significant reductions in mean latency (1.66x-2.01x lower) and time to first token (1.76x-24.07x lower) compared to existing systems. It directly addresses key challenges in LLM inference optimization by improving scheduling efficiency while managing memory overhead from preemption."
    }
  },
  {
    "id": "3OyaXFQuDl",
    "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
    "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.",
    "authors": [
      "~Hritik_Bansal2",
      "~Arian_Hosseini1",
      "~Rishabh_Agarwal2",
      "~Vinh_Q._Tran1",
      "~Mehran_Kazemi1"
    ],
    "pdf": "/pdf/cb8c559106c3bf5682e2e13302e65e6d1819ebf0.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses compute efficiency in LLM training, examines trade-offs between computational cost and model performance, focuses on synthetic data generation crucial for training optimization, challenges current practices for LLM training optimization, considers fixed inference budgets (FLOPs) relating to both training and inference optimization, and presents weak-to-strong improvement setup with implications for resource-efficient training strategies.",
      "Irrelevant Aspects": "The paper focuses more on data quality metrics than detailed technical implementation, doesn't directly address specific GPU utilization techniques, lacks clear focus on inference optimization techniques like quantization or pruning, and doesn't address distributed training or LLM scalability.",
      "Summary": "This paper investigates the compute-optimal approach for generating synthetic data to improve LLM reasoning performance. It compares data from stronger-but-expensive models versus weaker-but-cheaper models, finding that the latter leads to better performance despite being from weaker models. The research challenges current practices in LLM training optimization and suggests new approaches for achieving better performance with limited computational resources."
    }
  },
  {
    "id": "cJd1BgZ9CS",
    "title": "Distributed Speculative Inference (DSI): Speculation Parallelism for Provably Faster Lossless Language Model Inference",
    "abstract": "This paper introduces *distributed speculative inference (DSI)*, a novel inference algorithm that is provably faster than speculative inference (SI) [leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard autoregressive inference (non-SI). Like other SI algorithms, DSI operates on frozen language models (LMs), requiring no training or architectural modifications, and it preserves the target distribution. Prior studies on SI have demonstrated empirical speedups over non-SI—but rely on sufficiently fast and accurate drafters, which are often unavailable in practice. We identify a gap where SI can be slower than non-SI if drafters are too slow or inaccurate. We close this gap by proving that DSI is faster than both SI and non-SI—given any drafters. DSI is therefore not only faster than SI, but also unlocks the acceleration of LMs for which SI fails. DSI leverages *speculation parallelism (SP)*, a novel type of task parallelism, to orchestrate target and drafter instances that overlap in time, establishing a new foundational tradeoff between computational resources and latency. Our simulations show that DSI is 1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs and tasks. We open-source all our code.",
    "authors": [
      "~Nadav_Timor1",
      "~Jonathan_Mamou3",
      "~Daniel_Korat1",
      "~Moshe_Berchansky1",
      "~Oren_Pereg2",
      "~Moshe_Wasserblat1",
      "~Tomer_Galanti1",
      "~Michal_Gordon-Kiwkowitz1",
      "~David_Harel1"
    ],
    "pdf": "/pdf/083ee8a6446912d5d56434c2ccce02ac0a5ad587.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models, focusing on improving GPU utilization through a novel parallelism technique called speculation parallelism. It provides provable performance improvements (1.29-1.92x speedup) over existing methods, which aligns with my goals of higher throughput and lower latency. The approach is lossless, works with frozen models, and addresses real-world limitations of speculative inference methods, making it highly relevant to my research on optimizing LLM inference systems.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization aspects of my research interests. It doesn't discuss quantization, model compression techniques, or memory optimization strategies beyond the parallelism aspect. The method is also currently demonstrated primarily in single-node setups, though it has distributed potential.",
      "Summary": "Distributed Speculative Inference (DSI) introduces a novel inference algorithm that leverages speculation parallelism to achieve provably faster inference compared to both standard speculative inference and autoregressive approaches. The method addresses limitations of existing SI techniques by maintaining performance advantages regardless of drafter quality, while preserving the target distribution and requiring no model modifications. With demonstrated 1.29-1.92x speedups across various LMs, this work offers significant contributions to inference optimization through resource-latency tradeoffs and parallel execution of target and drafter models."
    }
  },
  {
    "id": "Ccwp4tFEtE",
    "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
    "abstract": "Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in large performance gains with Best-of-N, namely 5% → 45.3% on algorithmic tasks, 73% → 93.4% on GSM8K, and 28% →44.6% on easy-to-hard generalization on MATH. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that generative verifiers scale favorably with model size and inference-time compute.",
    "authors": [
      "~Lunjun_Zhang1",
      "~Arian_Hosseini1",
      "~Hritik_Bansal2",
      "~Mehran_Kazemi1",
      "~Aviral_Kumar2",
      "~Rishabh_Agarwal2"
    ],
    "pdf": "/pdf/b1d84910d45ea689f1c089639edb67b49797c70c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization through novel next-token prediction approach for verifiers, inference optimization via better test-time compute utilization, scalability with model size and compute, architectural integration leveraging LLM text generation capabilities, potential for improved GPU utilization through unified training and generation, and compute efficiency gains demonstrated through performance improvements.",
      "Irrelevant Aspects": "Focus primarily on task performance metrics rather than system optimization, lack of hardware-specific optimization details, limited discussion of throughput and latency measurements, emphasis on mathematical reasoning accuracy over computational efficiency, and absence of GPU-specific implementation considerations.",
      "Summary": "This paper introduces Generative Verifiers (GenRM), a novel approach to training LLM verifiers using next-token prediction rather than discriminative classification. The method shows significant performance improvements across various tasks and claims better scalability with model size and inference-time compute. While relevant to my research interests in training optimization and inference strategies, the paper focuses more on accuracy improvements than on system-level optimizations like GPU utilization and throughput. The approach's unified training methodology and test-time compute utilization strategies offer interesting insights for potential computational efficiency gains, making it moderately relevant to my work on LLM optimization."
    }
  },
  {
    "id": "XfKSDgqIRj",
    "title": "COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 Training",
    "abstract": "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (**C**ompressing **O**ptimizer States and **A**ctivations for FP8 **T**raining), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) **Dynamic Range Expansion**, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) **Mixed-Granularity Activation Quantization**, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by **1.54×** compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a **1.43×** end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. Code will be released upon publication.",
    "authors": [
      "~Haocheng_Xi1",
      "~Han_Cai1",
      "~Ligeng_Zhu1",
      "~Yao_Lu13",
      "~Kurt_Keutzer1",
      "~Jianfei_Chen1",
      "~Song_Han5"
    ],
    "pdf": "/pdf/04e621f1db47e60ffa276733ec1e3ecb464e7f0a.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "FP8 training optimization, memory efficiency improvements for large models, reduction of memory footprint by 1.54×, 1.43× training speedup compared to BF16, compression techniques for optimizer states and activations, enabling training of larger models on fewer GPUs, scalability improvements for distributed training, focus on Large Language Model pretraining and fine-tuning",
      "Irrelevant Aspects": "Limited discussion of inference optimizations, mentions Vision Language Model training which is outside the primary language model focus",
      "Summary": "COAT introduces a novel FP8 training framework that addresses memory efficiency challenges in large language model training through Dynamic Range Expansion for optimizer states and Mixed-Granularity Activation Quantization. It achieves significant memory reduction (1.54×) and speedup (1.43×) compared to BF16 while maintaining model performance, enabling more efficient GPU utilization and scaling of large model training."
    }
  },
  {
    "id": "wHLMsM1SrP",
    "title": "Needle Threading: Can LLMs Follow Threads Through Near-Million-Scale Haystacks?",
    "abstract": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often disparate\ndocuments containing mostly irrelevant information. Long-context LLMs appear\nwell-suited to this form of complex information retrieval and reasoning, which has\ntraditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address\nthis, we conduct a set of retrieval experiments designed to evaluate the capabilities\nof 17 leading LLMs, such as their ability to follow threads of information through\nthe context window. Strikingly, we find that many models are remarkably thread-\nsafe: capable of simultaneously following multiple threads without significant loss\nin performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as\nthe context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared—they often\ncorrespond to substantially different numbers of written characters. We release\nour code and long context experimental data.",
    "authors": [
      "~Jonathan_Roberts1",
      "~Kai_Han1",
      "~Samuel_Albanie2"
    ],
    "pdf": "/pdf/54f26fc4c4a998a39232ff4531b4cf69aa717b0d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper evaluates long-context LLMs' performance at near-million token scale, which directly relates to GPU memory management and efficient inference optimization. Understanding the 'effective context limit' versus supported context length is crucial for resource utilization optimization. The benchmarking of 17 leading models provides comparative performance data that can inform optimization strategies. The study reveals performance degradation patterns that are essential for developing inference optimizations.",
      "Irrelevant Aspects": "The paper focuses primarily on information retrieval evaluation rather than specific training or inference optimization techniques. It doesn't directly address GPU utilization strategies, throughput optimization, or latency reduction methods. The tokenizer comparison, while technically relevant, doesn't directly contribute to performance optimization research.",
      "Summary": "This paper investigates how effectively LLMs can follow information threads through extremely long contexts (near-million tokens). While not directly focused on optimization techniques, the findings about effective context limits and performance degradation patterns provide valuable insights for researchers working on inference efficiency, memory management, and resource utilization in large language models."
    }
  },
  {
    "id": "e7AUJpP8bV",
    "title": "PAD: Personalized Alignment of LLMs at Decoding-time",
    "abstract": "Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model’s predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment.",
    "authors": [
      "~Ruizhe_Chen1",
      "~Xiaotian_Zhang1",
      "~Meng_Luo2",
      "~Wenhao_Chai1",
      "~Zuozhu_Liu1"
    ],
    "pdf": "/pdf/f6043cd4d64f051f24b65e62221f0e9c475a1f6b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on inference-phase optimization, eliminating need for additional training, real-time application suitability, claims of scalability across different models, dynamic modification of model outputs during decoding",
      "Irrelevant Aspects": "Limited discussion of GPU utilization specifics, minimal focus on throughput metrics, doesn't address low-level hardware optimization, primary focus is on alignment rather than performance metrics",
      "Summary": "This paper is moderately relevant to my expertise as it focuses on decoding-time optimization in LLMs, which aligns with my interest in inference optimization. However, it primarily addresses personalized alignment rather than the technical aspects of GPU utilization, throughput, and latency optimization that form the core of my research. The paper's approach of modifying outputs during inference without additional training is valuable, but it lacks detailed discussion of the performance implications."
    }
  },
  {
    "id": "3TnLGGHhNx",
    "title": "From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities",
    "abstract": "Multimodal Large Language Models have made significant strides in integrating visual and textual information, yet they often struggle with effectively aligning these modalities. We introduce a novel image tokenizer that bridges this gap by applying the principle of Byte-Pair Encoding (BPE) to visual data. Unlike conventional approaches that rely on separate visual encoders, our method directly incorporates structural prior information into image tokens, mirroring the successful tokenization strategies used in text-only Large Language Models. This innovative approach enables Transformer models to more effectively learn and reason across modalities. Through theoretical analysis and extensive experiments, we demonstrate that our BPE Image Tokenizer significantly enhances MLLMs' multimodal understanding capabilities, even with limited training data. Leveraging this method, we develop Being-VL-0, a model that demonstrates superior performance across various benchmarks and shows promising scalability, potentially paving the way for more efficient and capable multimodal foundation models. For further details, visit our website https://github.com/BeingBeyond/Being-VL-0.",
    "authors": [
      "~Wanpeng_Zhang1",
      "~Zilong_Xie1",
      "~Yicheng_Feng1",
      "~Yijiang_Li1",
      "~Xingrun_Xing1",
      "~Sipeng_Zheng1",
      "~Zongqing_Lu2"
    ],
    "pdf": "/pdf/0832e56b5a32b64a6f10e9b070128b50ce457eb9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces a novel image tokenizer for multimodal Large Language Models that applies Byte-Pair Encoding (BPE) principles to visual data. This approach could potentially optimize how visual information is processed in MLLMs, affecting both training and inference efficiency. The development of Being-VL-0 model with 'superior performance' and 'promising scalability' aligns with interests in model efficiency and performance optimization.",
      "Irrelevant Aspects": "The paper focuses primarily on the tokenization method rather than specific GPU utilization techniques or latency optimization strategies. It doesn't explicitly discuss training/inference optimization techniques like quantization, pruning, or distributed approaches that would directly address throughput and scalability concerns.",
      "Summary": "This paper presents a novel BPE-based image tokenizer for multimodal LLMs that could improve model efficiency by better aligning visual and textual modalities. While relevant to the expert's interest in large language model optimization, it focuses more on a novel representation approach rather than specific computational optimization techniques for GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "E1EHO0imOb",
    "title": "Scaling FP8 training to trillion-token LLMs",
    "abstract": "We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a  SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a $\\sim$ 34 % throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed",
    "authors": [
      "~Maxim_Fishman1",
      "~Brian_Chmiel1",
      "~Ron_Banner1",
      "~Daniel_Soudry1"
    ],
    "pdf": "/pdf/2dc0ed4143628373b68f3b8239c906a01488b168.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "FP8 training for LLMs, scaling to trillion-token models, addressing training instabilities, quantization of Adam optimizer moments, 34% throughput improvement, multi-accelerator scaling (256 Intel Gaudi2), training optimization techniques, open-source implementation",
      "Irrelevant Aspects": "Specific focus on Intel Gaudi2 hardware rather than general GPU principles, detailed mathematical analysis of SwiGLU instabilities beyond their practical implications",
      "Summary": "This paper presents a breakthrough in training optimization for large language models using FP8 precision, successfully scaling to 2 trillion tokens - a 20x increase over previous limits. The authors identify and solve critical instabilities in prolonged FP8 training through their novel Smooth-SwiGLU modification and demonstrate the first FP8 quantization of Adam optimizer moments. These innovations enable training of a 7B parameter model with 34% throughput improvement while maintaining quality comparable to BF16 baseline, directly addressing the key challenges of training optimization, GPU utilization, and scalability for large language models."
    }
  },
  {
    "id": "tJHDw8XfeC",
    "title": "MiniPLM: Knowledge Distillation for Pre-training Language Models",
    "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. \nWhile effective in fine-tuning, KD during pre-training faces efficiency, flexibility, and effectiveness issues. \nExisting methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data.\nIn this work, we propose **MiniPLM**, a KD framework for pre-training LMs by refining the training data distribution with the teacher LM's knowledge.\nFor efficiency, MiniPLM performs offline teacher inference, allowing KD for multiple student LMs without adding training costs.\nFor flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families.\nFor effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the training data difficulty and diversity, helping student LMs acquire versatile and sophisticated knowledge.\nExtensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 common downstream tasks, improves language modeling capabilities, and reduces pre-training computation. \nThe benefit of MiniPLM extends to larger training scales, evidenced by the scaling curve extrapolation.\nFurther analysis reveals that MiniPLM supports KD across model families and enhances the pre-training data utilization. Our code, data, and models can be found at https://github.com/thu-coai/MiniPLM.",
    "authors": [
      "~Yuxian_Gu1",
      "~Hao_Zhou8",
      "~Fandong_Meng3",
      "~Jie_Zhou8",
      "~Minlie_Huang1"
    ],
    "pdf": "/pdf/1fc47e703f6d4659d655ddd93ae7268fcef59616.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Training optimization through knowledge distillation for pre-training, offline teacher inference for efficiency, reduction in pre-training computation, scalability to larger training scales, cross-model compatibility, performance enhancement for student models",
      "Irrelevant Aspects": "Less focus on direct GPU utilization optimization techniques, minimal emphasis on inference optimization, no clear focus on latency reduction, lack of throughput optimization techniques",
      "Summary": "The paper presents MiniPLM, a knowledge distillation framework for pre-training language models that optimizes training efficiency through offline teacher inference and data distribution refinement. While it addresses important training optimization concerns and scalability, it has less focus on inference optimization and specific GPU utilization techniques, making it highly relevant but not perfectly aligned with all aspects of the research interest."
    }
  },
  {
    "id": "W9FZEQj3vv",
    "title": "Variational Best-of-N Alignment",
    "abstract": "Best-of-N (BoN) is a popular and effective algorithm for aligning language models to human preferences. The algorithm works as follows: at inference time, N samples are drawn from the language model, and the sample with the highest reward, as judged by a reward model, is returned as the output. Despite its effectiveness, BoN is computationally expensive; it reduces sampling throughput by a factor of N. \nTo make BoN more efficient at inference time, one strategy is to fine-tune the language model to mimic what BoN does during inference. \nTo achieve this, we derive the distribution induced by the BoN algorithm. We then propose to fine-tune the language model to minimize backward KL divergence to the BoN distribution. Our approach is analogous to mean-field variational inference and, thus, we term it variational BoN (vBoN). To the extent this fine-tuning is successful and we end up with a good approximation, we have reduced the inference cost by a factor of N. Our experiments on controlled generation and summarization tasks show that BoN is the most effective alignment method, and our variational approximation to BoN achieves the closest performance to BoN and surpasses models fine-tuned using the standard KL-constrained RL objective. In the controlled generation task, vBoN appears more frequently on the Pareto frontier of reward and KL divergence compared to other alignment methods. In the summarization task, vBoN achieves high reward values across various sampling temperatures.",
    "authors": [
      "~Afra_Amini1",
      "~Tim_Vieira1",
      "~Elliott_Ash1",
      "~Ryan_Cotterell1"
    ],
    "pdf": "/pdf/aaa50fe97982b6bb2dd3a1217300f70a4a647653.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses inference optimization for large language models by proposing a method to reduce the computational cost of Best-of-N alignment. It focuses on improving efficiency at inference time, reducing the need for multiple samples (which reduces throughput) and replacing it with a fine-tuned model. This approach could lead to better GPU utilization since it reduces the number of samples needed at inference time. The paper evaluates their method on typical language model tasks and shows that their approach maintains performance while reducing computational costs.",
      "Irrelevant Aspects": "The paper doesn't focus specifically on GPU-level optimizations or hardware-specific techniques for efficiency. There's limited discussion of distributed inference strategies or scalability across multiple GPUs/nodes. The paper doesn't appear to provide detailed analysis of throughput and latency metrics or memory usage optimization, which would be directly relevant to the research interest.",
      "Summary": "Variational Best-of-N Alignment proposes a method to optimize inference for language models by approximating the Best-of-N sampling strategy through fine-tuning. Instead of drawing N samples at inference time (which reduces throughput), they fine-tune the model to mimic the BoN distribution, reducing inference cost by a factor of N. While this approach addresses inference efficiency and could improve overall system performance, it doesn't delve deeply into GPU-specific optimizations or detailed throughput and latency analysis that would be central to the specified research focus."
    }
  },
  {
    "id": "lTkHiXeuDl",
    "title": "HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts",
    "abstract": "Recent studies have combined Mixture of Experts (MoE) and Parameter-Efficient Fine-tuning (PEFT) to fine-tune large language models (LLMs), holding excellent performance in multi-task scenarios while remaining resource-efficient.  However, existing MoE approaches still exhibit the following limitations: (1) Current methods fail to consider that different LLM layers capture features at varying levels of granularity, leading to suboptimal performance. (2) Task-level routing methods lack generalizability to unseen tasks. (3) The uncertainty introduced by load imbalance loss undermines the effective specialization of the experts. To address these challenges, we propose HMoRA, a Hierarchical fine-tuning method that combines MoE and LoRA, employing hybrid routing that integrates token-level and task-level routing in a hierarchical manner. This hierarchical hybrid routing allows the model to more efficiently capture both fine-grained token information and broader task contexts. To improve the certainty of expert selection, a novel routing auxiliary loss is introduced. This auxiliary function also enhances the task router's ability to differentiate tasks and its generalization to unseen tasks. Additionally, several optional lightweight designs have been proposed to significantly reduce both the number of trainable parameters and computational costs. Experimental results demonstrate that HMoRA outperforms full fine-tuning across multiple NLP benchmarks, while fine-tuning only 3.9\\% of the parameters. The code is  available on: https://github.com/LiaoMengqi/HMoRA.",
    "authors": [
      "~Mengqi_Liao1",
      "~Wei_Chen38",
      "~Junfeng_Shen1",
      "~Shengnan_Guo1",
      "~Huaiyu_Wan1"
    ],
    "pdf": "/pdf/93092c263a428c4156d2d5a0fec3c16f75d1d93c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter-Efficient Fine-tuning (PEFT) reduces trainable parameters, Mixture of Experts (MoE) architecture for computational efficiency, addresses load imbalance in MoE systems which affects GPU utilization, reduces computational costs while maintaining performance, achieves strong results with only 3.9% of parameters fine-tuned",
      "Irrelevant Aspects": "Limited focus on GPU utilization metrics, insufficient discussion of scalability measurements, lacks detailed throughput and latency analysis, routing mechanisms focus on linguistic rather than hardware optimization levels",
      "Summary": "HMoRA combines Mixture of Experts with Parameter-Efficient Fine-tuning to optimize LLM training efficiency. While the paper effectively addresses parameter reduction and computational costs through hierarchical routing of LoRA experts, it doesn't directly focus on GPU utilization, scalability, throughput, or latency metrics that are central to my research interests. The approach is promising for training optimization but lacks the hardware-specific optimization focus that would make it highly relevant to my work on LLM systems performance."
    }
  },
  {
    "id": "e1wDDFmlVu",
    "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
    "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available at https://github.com/Time-MoE/Time-MoE",
    "authors": [
      "~Xiaoming_Shi2",
      "~Shiyu_Wang3",
      "~Yuqi_Nie1",
      "~Dianqi_Li1",
      "~Zhou_Ye3",
      "~Qingsong_Wen2",
      "~Ming_Jin3"
    ],
    "pdf": "/pdf/a01143050833825a6bf2b23c034e500857336d6f.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on training and inference optimization for large models using a sparse mixture-of-experts (MoE) design that activates only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. It addresses scaling laws, model efficiency, and computational budgets - all directly relevant to my research on training optimization, inference optimization, GPU utilization, scalability, throughput, and latency. The paper demonstrates scaling to 2.4 billion parameters while reducing inference costs, which is highly relevant to my interests in optimizing large models.",
      "Irrelevant Aspects": "The focus is on time series forecasting rather than general language models, though many techniques are transferable. The paper doesn't provide extensive details about specific GPU-level optimizations or hardware-specific considerations. There's limited discussion of distributed training systems or infrastructure-level optimizations that would be relevant to deployment at scale.",
      "Summary": "Time-MoE introduces a scalable architecture using mixture-of-experts to train larger time series foundation models while reducing inference costs. By activating only subsets of the network for each prediction, it achieves better computational efficiency without sacrificing model capacity. The paper demonstrates scaling to 2.4 billion parameters on their Time-300B dataset, validating scaling laws and showing significant improvements over dense models. The techniques for efficient scaling, sparse activation, and cost reduction during inference are directly applicable to my research interests in optimizing large language models for better GPU utilization and performance."
    }
  },
  {
    "id": "1uLW9eYNJB",
    "title": "MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards",
    "abstract": "The rapid scaling of large language models necessitates more lightweight finetuning methods to reduce the explosive GPU memory overhead when numerous customized models are served simultaneously.\nTargeting more parameter-efficient low-rank adaptation (LoRA), parameter sharing presents a promising solution. Empirically, our research into high-level sharing principles highlights the indispensable role of differentiation in reversing the detrimental effects of pure sharing.\nGuided by this finding, we propose Mixture of Shards (MoS), incorporating both inter-layer and intra-layer sharing schemes, and integrating four nearly cost-free differentiation strategies, namely subset selection, pair dissociation, vector sharding, and shard privatization. Briefly, it selects a designated number of shards from global pools with a Mixture-of-Experts (MoE)-like routing mechanism before sequentially concatenating them to low-rank matrices.\nHence, it retains all the advantages of LoRA while offering enhanced parameter efficiency, and effectively circumvents the drawbacks of peer parameter-sharing methods.\nOur empirical experiments demonstrate approximately $8\\times$ parameter savings in a standard LoRA setting. The ablation study confirms the significance of each component.\nOur insights into parameter sharing and MoS method may illuminate future developments of more parameter-efficient finetuning methods.\nThe code is officially available at https://github.com/Forence1999/MoS.",
    "authors": [
      "~Sheng_Wang12",
      "~Liheng_Chen2",
      "~Pengan_CHEN1",
      "~Jingwei_Dong1",
      "~Boyang_XUE1",
      "~Jiyue_Jiang1",
      "~Lingpeng_Kong1",
      "~Chuan_Wu1"
    ],
    "pdf": "/pdf/45e2fd345a7223e11d4b96dc9990ded1462e9ee4.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Parameter efficiency in large language models, GPU memory optimization, lightweight fine-tuning methods, serving multiple customized models simultaneously, parameter sharing techniques, LoRA improvements, memory footprint reduction strategies",
      "Irrelevant Aspects": "Limited focus on GPU utilization patterns, minimal discussion of inference latency/throughput metrics, primarily fine-tuning focus rather than full training pipeline optimization, lack of hardware-specific optimization strategies",
      "Summary": "MoS presents a parameter-efficient fine-tuning method that extends LoRA with inter-layer and intra-layer parameter sharing schemes. The method uses a Mixture-of-Experts-like routing to select shards from global pools, achieving approximately 8× parameter savings compared to standard LoRA. While addressing important memory optimization challenges for serving multiple language models, the paper doesn't extensively cover the detailed GPU utilization, throughput, or latency aspects that would maximize its relevance to my research interests."
    }
  },
  {
    "id": "2JihLwirxO",
    "title": "ParaSolver: A Hierarchical Parallel Integral Solver for Diffusion Models",
    "abstract": "This paper explores the challenge of accelerating the sequential inference process of Diffusion Probabilistic Models (DPMs). We tackle this critical issue from a dynamic systems perspective, in which the inherent sequential nature is transformed into a parallel sampling process. Specifically, we propose a unified framework that generalizes the sequential sampling process of DPMs as solving a system of banded nonlinear equations. Under this generic framework, we reveal that the Jacobian of the banded nonlinear equations system possesses a unit-diagonal structure, enabling further approximation for acceleration. Moreover, we theoretically propose an effective initialization approach for parallel sampling methods. Finally, we construct \\textit{ParaSolver}, a hierarchical parallel sampling technique that enhances sampling speed without compromising quality. Extensive experiments show that ParaSolver achieves up to \\textbf{12.1× speedup} in terms of wall-clock time. The source code is publicly available at https://github.com/Jianrong-Lu/ParaSolver.git.",
    "authors": [
      "~Jianrong_Lu2",
      "~Zhiyu_Zhu1",
      "~Junhui_Hou2"
    ],
    "pdf": "/pdf/2e1809b3b42fb8dcb2891d22f75de4c6fe327a24.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on inference optimization and achieving significant speedup (12.1x) without compromising quality. The hierarchical parallel sampling technique directly addresses GPU utilization and scalability concerns, which are key aspects of my research interest. The approach transforms sequential processes into parallel ones, which is fundamental to improving throughput and reducing latency in ML systems.",
      "Irrelevant Aspects": "The paper focuses specifically on Diffusion Probabilistic Models rather than Large Language Models, which is my primary area of interest. There is limited discussion of specific GPU utilization metrics or detailed scalability analysis across different hardware configurations. The mathematical formulation through dynamic systems and banded nonlinear equations is quite specialized and may not directly transfer to LLM optimization.",
      "Summary": "ParaSolver presents a hierarchical parallel integral solver for accelerating Diffusion Models by transforming sequential inference into a parallel sampling process. While it achieves impressive speedup (12.1x) and addresses important concepts like parallelization and efficiency, its specific focus on Diffusion Models rather than LLMs makes it moderately rather than highly relevant to my research interests. The parallel sampling techniques could potentially inform approaches for LLM optimization, but would require adaptation to the transformer-based architecture and inference patterns specific to language models."
    }
  },
  {
    "id": "ulCAPXYXfa",
    "title": "OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs",
    "abstract": "During the inference phase of Large Language Models (LLMs) with long context, a substantial portion of GPU memory is allocated to the KV cache, with memory usage increasing as the sequence length grows. To mitigate the GPU memory footprint associate with KV cache, some previous studies have discarded less important tokens based on the sparsity identified in attention scores in long context scenarios. However, we argue that attention scores cannot indicate the future importance of tokens in subsequent generation iterations, because attention scores are calculated based on current hidden states. Therefore, we propose OmniKV, a token-dropping-free and training-free inference method, which achieves a 1.68x speedup without any loss in performance. It is well-suited for offloading, significantly reducing KV cache memory usage by up to 75% with it. The core innovative insight of OmniKV is: Within a single generation iteration, there is a high degree of similarity in the important tokens identified across consecutive layers. Extensive experiments demonstrate that OmniKV achieves state-of-the-art performance across multiple benchmarks, with particularly advantages in chain-of-thoughts scenarios. OmniKV extends the maximum context length supported by a single A100 for Llama-3-8B from 128K to 450K. Our code is available at https://github.com/antgroup/OmniKV.git.",
    "authors": [
      "~Jitai_Hao1",
      "~Yuke_Zhu4",
      "~Tian_Wang13",
      "~Jun_Yu1",
      "~Xin_Xin2",
      "~Bo_Zheng8",
      "~Zhaochun_Ren1",
      "~Sheng_Guo4"
    ],
    "pdf": "/pdf/1167991c79de6f93e9cf9d64ae0ef95818fe7421.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs with a focus on GPU memory utilization and scalability. It presents a method to reduce KV cache memory usage by up to 75%, achieves 1.68x speedup without performance loss, and extends maximum context length from 128K to 450K on a single A100. These improvements directly impact throughput, latency, and resource efficiency in LLM inference systems.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization rather than training optimization. It doesn't address distributed training strategies or model compression techniques that might be relevant to my broader research interests. The work is specifically about long-context scenarios, which might limit its applicability to all inference use cases.",
      "Summary": "OmniKV is an inference optimization method that addresses GPU memory constraints in long-context LLMs by leveraging token importance similarity across consecutive layers. It achieves significant memory reduction and speedup without requiring training modifications or token dropping, making it highly relevant to LLM inference optimization research."
    }
  },
  {
    "id": "8jOqCcLzeO",
    "title": "Longhorn: State Space Models are Amortized Online Learners",
    "abstract": "The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as “sequence modeling.”  Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs.\nIn this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives.\nBased on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.",
    "authors": [
      "~Bo_Liu13",
      "~Rui_Wang1",
      "~Lemeng_Wu1",
      "~Yihao_Feng1",
      "~Peter_Stone1",
      "~qiang_liu4"
    ],
    "pdf": "/pdf/a1c72bd30a3e3c68e85b2b1320b201e73cad46dc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper presents State Space Models (SSMs) as an alternative to Transformers with linear decoding efficiency, which directly addresses computational optimization for large language models. It claims parallelizability during training and outperforms existing SSMs like Mamba on language modeling tasks. This work could significantly improve GPU utilization, throughput, and latency for LLM inference.",
      "Irrelevant Aspects": "The paper focuses more on the theoretical framework of online learning rather than practical implementation details for GPU optimization. It lacks explicit discussion of scaling characteristics and hardware-specific optimizations that would be most directly relevant to my research.",
      "Summary": "Longhorn presents a novel SSM architecture designed through an online learning lens, offering linear computational complexity for sequence modeling compared to Transformers' quadratic cost. By optimizing online regression objectives, it outperforms existing SSMs like Mamba on language modeling tasks, potentially offering more efficient alternatives for LLM training and inference."
    }
  },
  {
    "id": "88TC1AWV27",
    "title": "PICASO: Permutation-Invariant Context Composition with State Space Models",
    "abstract": "Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or 'contexts', retrieved from external knowledge bases to their input. However, processing additional contexts online incurs significant computation costs that scale with their length. State Space Models (SSMs) offer a promising solution by allowing a database of contexts to be mapped onto fixed-dimensional states from which to start the generation. A key challenge arises when attempting to leverage information present across multiple contexts, since there is no straightforward way to condition generation on multiple independent states in existing SSMs. To address this, we leverage a simple mathematical relation derived from SSM dynamics to compose multiple states into one that efficiently approximates the effect of concatenating raw context tokens. Since the temporal ordering of contexts can often be uninformative, we enforce permutation-invariance by efficiently averaging states obtained via our composition algorithm across all possible context orderings. We evaluate our resulting method on WikiText and MSMARCO in both zero-shot and fine-tuned settings, and show that we can match the strongest performing baseline while enjoying on average $5.4\\times$ speedup.",
    "authors": [
      "~Tian_Yu_Liu2",
      "~Alessandro_Achille1",
      "~Matthew_Trager2",
      "~Aditya_Golatkar1",
      "~Luca_Zancato1",
      "~Stefano_Soatto3"
    ],
    "pdf": "/pdf/76711c387c2784ca59f4181ffef4f95c5b5a0b04.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs by efficiently processing contextual knowledge at inference time. It focuses on reducing computation costs that scale with context length, which is directly related to GPU utilization and throughput. The use of State Space Models (SSMs) represents an emerging approach for more efficient sequence modeling compared to standard transformers. The method handles multiple contexts efficiently, addressing a common challenge in LLM deployments. Most importantly, it claims a 5.4x speedup while maintaining generation quality, which is directly relevant to achieving higher throughput and lower latency. The approach also addresses scalability concerns with increasing contextual information.",
      "Irrelevant Aspects": "The paper appears to focus more on algorithmic innovation rather than system-level implementation details. It doesn't provide extensive details about GPU utilization or specific hardware optimizations. The work is centered on inference optimization with limited discussion of training optimization aspects. There seems to be less emphasis on the practical deployment considerations that would be relevant for production systems.",
      "Summary": "PICASO introduces a method for efficiently composing multiple contextual states using State Space Models to reduce inference computation costs while maintaining generation quality. By leveraging mathematical properties of SSM dynamics and enforcing permutation invariance, the approach achieves significant speedups (5.4x on average) when processing multiple contexts. While primarily focused on algorithmic innovation rather than system implementation details, the work directly addresses key concerns in LLM inference optimization including computational efficiency, context handling, and throughput improvements."
    }
  },
  {
    "id": "dEypApI1MZ",
    "title": "How Feature Learning Can Improve Neural Scaling Laws",
    "abstract": "We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks.",
    "authors": [
      "~Blake_Bordelon1",
      "~Alexander_Atanasov1",
      "~Cengiz_Pehlevan2"
    ],
    "pdf": "/pdf/58bb9e99f179c666b77c64894649192536081ae6.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper investigates neural scaling laws which are fundamental to optimizing large language model training. It analyzes how performance scales with model size, training time, and data amount - all critical factors for GPU utilization and throughput optimization. The finding that feature learning can improve scaling exponents for hard tasks could lead to more efficient training approaches that better utilize compute resources. Understanding these scaling laws helps inform resource allocation decisions and training strategies for large models.",
      "Irrelevant Aspects": "The paper doesn't explicitly address inference optimization, latency reduction, or specific GPU utilization techniques. It focuses on theoretical analysis rather than implementation details or system-level optimizations. The experiments are on MLPs and CNNs rather than language models specifically. There's no discussion of practical engineering considerations for distributed training or memory optimization.",
      "Summary": "This paper develops a theoretical framework for understanding neural scaling laws beyond the kernel limit, with particular focus on how feature learning impacts performance scaling with model size, training time, and data. The authors identify three task regimes (hard, easy, and super-easy) and demonstrate that feature learning can significantly improve scaling exponents for hard tasks, approximately doubling the exponent for very hard tasks. While the paper doesn't directly address implementation aspects of training optimization, its insights into scaling laws provide valuable theoretical foundations for understanding how to efficiently scale large language models and optimize GPU utilization during training."
    }
  },
  {
    "id": "wA2RMD2AFq",
    "title": "Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training",
    "abstract": "Co-training can achieve parameter-efficient multi-task models but remains unexplored for quantization-aware training. Our investigation shows that directly introducing co-training into existing quantization-aware training (QAT) methods results in significant performance degradation. Our experimental study identifies that the primary issue with existing QAT methods stems from the inadequate activation quantization scales for the co-training framework. To address this issue, we propose Task-Specific Scales Quantization for Multi-Task Co-Training (TSQ-MTC) to tackle mismatched quantization scales. Specifically, a task-specific learnable multi-scale activation quantizer (TLMAQ) is incorporated to enrich the representational ability of shared features for different tasks. Additionally, we find that in the deeper layers of the Transformer model, the quantized network suffers from information distortion within the attention quantizer. A structure-based layer-by-layer distillation (SLLD) is then introduced to ensure that the quantized features effectively preserve the information from their full-precision counterparts. Our extensive experiments in two co-training scenarios demonstrate the effectiveness and versatility of TSQ-MTC. In particular, we successfully achieve a 4-bit quantized low-level visual foundation model based on IPT, which attains a PSNR comparable to the full-precision model while offering a $7.99\\times$ compression ratio in the $\\times4$ super-resolution task on the Set5 benchmark.",
    "authors": [
      "~Boyu_Liu3",
      "~Haoyu_Huang6",
      "~Linlin_Yang1",
      "~Yanjing_Li2",
      "~Guodong_Guo1",
      "~Xianbin_Cao2",
      "~Baochang_Zhang1"
    ],
    "pdf": "/pdf/76ab99be64d4b1a73df6926b76b01e55278fafb1.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Low-bit quantization techniques, quantization-aware training methods, transformer model optimization, attention mechanism quantization, maintaining performance while reducing model size, achieving significant compression ratios",
      "Irrelevant Aspects": "Focus on multi-task co-training rather than single model optimization, application to low-level visual tasks instead of language models, IPT (Image Processing Transformer) specific to vision tasks, PSNR metric for image quality instead of language model metrics",
      "Summary": "This paper presents a novel quantization method for multi-task co-training of models, with a focus on addressing quantization scale mismatches and information distortion in transformer models. While the application is to visual tasks (super-resolution), the techniques for low-bit quantization and transformer optimization could be relevant to large language model optimization. The paper introduces task-specific learnable activation quantizers and layer-by-layer distillation to maintain performance with 4-bit quantization, achieving significant compression ratios while preserving model quality."
    }
  },
  {
    "id": "Cs6MrbFuMq",
    "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment",
    "abstract": "Disaggregating the prefill and decoding phases represents an effective new paradigm for generative inference of large language models (LLM). This approach offers some significant system advantages, such as eliminating prefill-decoding interference and optimizing resource allocation. However, it is still an challenging open problem about how to deploy the disaggregated inference paradigm across a group of heterogeneous GPUs, which can be an economic alternative of the deployment over the homogeneous high performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for high throughput and cost-efficient LLM serving on heterogeneous GPUs following the disaggragated paradigm. Built on top of HexGen, the core component of HexGen-2 is a sophisticated scheduling algorithm that formalizes the allocation of disaggregated LLM inference computations and communications over heterogeneous GPUs and network connections as a constraint optimization problem. We leverage the graph partitioning and max-flow algorithm to co-optimize resource allocation, parallel strategies for distinct inference phases, and the efficiency of inter-phase key-value (KV) cache communications. We conduct extensive experiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models in various real-world settings, the results reveal that HexGen-2 delivers up to a 2.0$\\times$ and on average a 1.3$\\times$ improvement in serving throughput, reduces the average inference latency by 1.5$\\times$ compared with state-of-the-art systems given the same price budget, and achieves comparable inference performance with a 30% lower price budget.",
    "authors": [
      "~YOUHE_JIANG1",
      "~Ran_Yan5",
      "~Binhang_Yuan1"
    ],
    "pdf": "/pdf/04fd19a1bc4af0b4c71503e2e676b0008977a3ff.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on LLM inference optimization through disaggregation, addresses GPU utilization and scalability in heterogeneous environments, aims to improve throughput and reduce latency, optimizes resource allocation and parallel strategies, includes constraint optimization for computation and communication allocation, demonstrates significant performance improvements (up to 2.0x throughput, 1.5x latency reduction), and addresses cost-efficient deployment considerations.",
      "Irrelevant Aspects": "The paper doesn't address training optimization, doesn't cover specific model architecture optimizations, and doesn't mention quantization or other inference acceleration techniques beyond disaggregation.",
      "Summary": "HexGen-2 presents a distributed system for high-throughput, cost-efficient LLM serving on heterogeneous GPUs using disaggregated inference. It addresses key optimization challenges through scheduling algorithms that optimize resource allocation, parallel strategies, and KV cache communication. While focused solely on inference rather than training, it directly tackles GPU utilization, scalability, throughput, and latency concerns central to LLM optimization research."
    }
  },
  {
    "id": "0no1Wp2R2j",
    "title": "Going Beyond Feature Similarity: Effective Dataset distillation based on Class-aware Conditional Mutual Information",
    "abstract": "Dataset distillation (DD) aims to minimize the time and memory consumption needed for training deep neural networks on large datasets, by creating a smaller synthetic dataset that has similar performance to that of the full real dataset. However, current dataset distillation methods often result in synthetic datasets that are excessively difficult for networks to learn from, due to the compression of a substantial amount of information from the original data through metrics measuring feature similarity, e,g., distribution matching (DM). In this work, we introduce conditional mutual information (CMI) to assess the class-aware complexity of a dataset and propose a novel method by minimizing CMI. Specifically, we minimize the distillation loss while constraining the class-aware complexity of the synthetic dataset by minimizing its empirical CMI from the feature space of pre-trained networks, simultaneously. Conducting on a thorough set of experiments, we show that our method can serve as a general regularization method to existing DD methods and improve the performance and training efficiency.",
    "authors": [
      "~Xinhao_Zhong2",
      "~Bin_Chen4",
      "~Hao_Fang8",
      "~Xulin_Gu1",
      "~Shu-Tao_Xia1",
      "~EN-HUI_YANG1"
    ],
    "pdf": "/pdf/4d50e80a3e344361068f76c75ecd919c4421f967.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Dataset distillation's potential to reduce training time and memory consumption, which aligns with my interest in GPU utilization and scalability; The focus on improving training efficiency and performance; The proposed regularization method that could be applied to existing approaches for optimizing computational resources",
      "Irrelevant Aspects": "The paper doesn't specifically address large language models, which is my primary focus; No discussion of inference optimization, another key area of my expertise; The approach appears general rather than tailored to the unique characteristics and scale challenges of LLMs; No specific analysis of how this method scales to very large models",
      "Summary": "This paper introduces a novel dataset distillation method based on minimizing conditional mutual information (CMI) to create more efficient synthetic datasets. The approach addresses limitations in current DD methods that produce datasets difficult for networks to learn from. By constraining class-aware complexity through CMI minimization, the method serves as a regularization to existing DD techniques, improving both performance and training efficiency in experiments. While relevant to training optimization in general, the paper lacks specific application to large language models or inference optimization scenarios."
    }
  },
  {
    "id": "7zNYY1E2fq",
    "title": "Block-Attention for Efficient Prefilling",
    "abstract": "We introduce Block-attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. \nTraditional approaches often encode the entire context in an auto-regressive manner.\nInstead, Block-attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block.\nIn RAG scenarios, by defining each passage as a block, Block-attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference.\nThe implementation of Block-attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-attention mechanism. \nExperiments on 11 diverse benchmarks, including RAG, ICL, and general domains, demonstrate that after block fine-tuning, the Block-attention model not only achieves performance comparable to that of full-attention models, but can also seamlessly switch between the block and full attention modes without any performance loss.\nNotably, Block-attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the full-attention models, the TTFT and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively. Additionally, in Appendix A, we elaborate on how Block-attention is applied in Game AI scenario and the substantial potential benefits it entails. We strongly suggest researchers in the gaming field not to overlook this section.",
    "authors": [
      "~Dongyang_Ma4",
      "~Yan_Wang17",
      "~Tian_Lan7"
    ],
    "pdf": "/pdf/881244eb791dbac9fe9dfe409e000f2ac2a6e49f.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses inference optimization for LLMs with significant focus on reducing latency and improving efficiency. It introduces Block-attention, a novel attention mechanism that reduces time to first token (TTFT) by 98.7% and FLOPs by 99.8%, which are critical metrics for GPU utilization and inference throughput. The ability to handle 32K context length efficiently demonstrates scalability improvements. The approach enables reuse of KV states, which is a key optimization technique for attention mechanisms. The paper evaluates on diverse benchmarks and mentions applications in Game AI, showing practical relevance for real-world deployment scenarios.",
      "Irrelevant Aspects": "The paper has a specific focus on Retrieval-Augmented Generation (RAG) scenarios, which may limit the generalizability to other LLM use cases. While it mentions fine-tuning, the primary emphasis is on inference optimization rather than training optimization, which is a key part of my research interest.",
      "Summary": "This paper introduces Block-attention, a novel attention mechanism that divides input into blocks and computes KV states independently, enabling reuse of previously computed states. The approach demonstrates substantial efficiency gains with 98.7% reduction in TTFT and 99.8% reduction in FLOPs compared to full-attention models. It can handle 32K context length efficiently and maintains performance comparable to full-attention models while enabling seamless switching between block and full attention modes. The method shows promising applications in RAG scenarios and Game AI, addressing key challenges in LLM inference optimization for better GPU utilization and scalability."
    }
  },
  {
    "id": "PYmrUQmMEw",
    "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
    "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.",
    "authors": [
      "~Qingkai_Fang1",
      "~Shoutao_Guo1",
      "~Yan_Zhou9",
      "~Zhengrui_Ma1",
      "~Shaolei_Zhang1",
      "~Yang_Feng4"
    ],
    "pdf": "/pdf/25e8630f907c64822c1d82926d66b77340a6c148.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Novel model architecture designed for low-latency interaction, integration of components for computational efficiency, elimination of speech transcription step to reduce overhead, response latency of 226ms, efficient training timeline (3 days on 4 GPUs), implications for efficient development of speech-language models",
      "Irrelevant Aspects": "Primary focus on speech interaction quality rather than optimization techniques, dataset construction details, evaluation of content and style of responses rather than system performance metrics",
      "Summary": "LLaMA-Omni presents an architecture for speech interaction with LLMs that achieves low latency and efficient training. While the paper focuses on speech interaction capabilities, it contains valuable insights related to model architecture optimization and efficient training that are relevant to my research interests."
    }
  },
  {
    "id": "QOXrVMiHGK",
    "title": "PEARL: Parallel Speculative Decoding with Adaptive Draft Length",
    "abstract": "Speculative decoding (SD), where an extra draft model is employed to provide multiple **draft** tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration.\nHowever, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is *guessing* tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding.\nTo address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \n**P**arallel sp**E**culative decoding with **A**daptive d**R**aft **L**ength (PEARL). \nSpecifically, PEARL proposes *pre-verify* to verify the first draft token in advance during the drafting phase, and *post-verify* to generate more draft tokens during the verification phase.\nPEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem.\nExperiments on various text generation benchmarks demonstrate the effectiveness of our PEARL, leading to a superior speedup performance up to **4.43$\\times$** and **1.50$\\times$**, compared to auto-regressive decoding and vanilla speculative decoding, respectively.",
    "authors": [
      "~Tianyu_Liu6",
      "~Yun_Li3",
      "~Qitan_Lv1",
      "~Kai_Liu15",
      "~Jianchen_Zhu1",
      "~Winston_Hu1",
      "~Xiao_Sun8"
    ],
    "pdf": "/pdf/be0598358403ef8c14c48086b08f72088433a7d3.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization through speculative decoding, which is a core interest. It focuses on improving GPU utilization and throughput via parallel execution, specifically reducing the mutual waiting problem in existing methods. The adaptive draft length mechanism could provide better resource utilization. The claimed speedup of up to 4.43x compared to auto-regressive decoding demonstrates significant potential for improving throughput and reducing latency.",
      "Irrelevant Aspects": "The paper appears to focus exclusively on inference optimization without addressing training optimization aspects. It doesn't provide explicit details on GPU memory utilization or resource management beyond the parallel execution strategy. The adaptive draft length mechanism might have limitations in certain hardware configurations that aren't addressed.",
      "Summary": "PEARL introduces a parallel speculative decoding framework with adaptive draft length to alleviate the mutual waiting problem between draft and target models. By implementing pre-verify and post-verify strategies, it parallelizes the drafting and verification phases, achieving up to 4.43x speedup over auto-regressive decoding and 1.50x over vanilla speculative decoding."
    }
  },
  {
    "id": "JE9tCwe3lp",
    "title": "Autoregressive Video Generation without Vector Quantization",
    "abstract": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
    "authors": [
      "~Haoge_Deng1",
      "~Ting_Pan1",
      "~Haiwen_Diao2",
      "~Zhengxiong_Luo1",
      "~Yufeng_Cui1",
      "~Huchuan_Lu1",
      "~Shiguang_Shan2",
      "~Yonggang_Qi2",
      "~Xinlong_Wang2"
    ],
    "pdf": "/pdf/f9493043571f9ac8315899860b05fc1315b6d70c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on autoregressive modeling efficiency, improved inference speed, and better performance with smaller models (0.6B parameters). It eliminates vector quantization, which is a significant optimization approach. It addresses training efficiency with significantly lower costs and discusses scalability for extended durations. The architectural innovations in autoregressive modeling could be applicable to LLM optimization.",
      "Irrelevant Aspects": "The primary focus is on video generation rather than text-based language models. There's limited discussion of GPU utilization metrics in detail and no specific mention of distributed training systems or scaling across multiple GPUs. The applications domain is visual rather than linguistic.",
      "Summary": "NOVA presents an efficient autoregressive approach for video generation that avoids vector quantization, demonstrating how smaller models (0.6B parameters) can achieve superior performance with faster inference and lower training costs. While focused on video generation, the efficiency techniques and architectural innovations have potential applications for optimizing large language models through autoregressive modeling improvements, though the paper lacks direct discussion of language-specific optimizations or distributed training systems."
    }
  },
  {
    "id": "N5fVv6PZGz",
    "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
    "abstract": "Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.",
    "authors": [
      "~Keisuke_Kamahori1",
      "~Tian_Tang1",
      "~Yile_Gu1",
      "~Kan_Zhu1",
      "~Baris_Kasikci2"
    ],
    "pdf": "/pdf/b8cc03eb6ffb479c9b50c16f8825d02f667c2828.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on inference optimization for large language models with MoE architecture, addresses GPU utilization challenges, aims to reduce latency and increase throughput, provides solutions for resource-constrained environments, demonstrates performance improvements across multiple inference scenarios",
      "Irrelevant Aspects": "Only addresses inference optimization without covering training aspects, limited to MoE architecture rather than general LLM optimization approaches",
      "Summary": "Fiddler presents a CPU-GPU orchestration system for efficient inference of Mixture-of-Experts LLMs in resource-constrained environments. It optimizes execution strategies to avoid data movement overhead and leverages the distinct characteristics of both CPU and GPU resources. The system shows consistent performance improvements across different inference scenarios including single batch (1.26x speedup), long prefill (1.30x), and beam search (11.57x) compared to existing baselines."
    }
  },
  {
    "id": "ZyNEr7Xw5L",
    "title": "DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models",
    "abstract": "Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. \nTo mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. \nHowever, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($<$ 8bits) quantization.\nIn this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. \nAdditionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment.\nTo address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. \nOur method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters. Code is available at \\link{https://github.com/ugonfor/DGQ}.",
    "authors": [
      "~Hyogon_Ryu1",
      "~NaHyeon_Park2",
      "~Hyunjung_Shim1"
    ],
    "pdf": "/pdf/24e16630e735138aa00583e9d6d1f0f1adb6b60c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on quantization techniques for reducing memory usage and computational costs, which directly addresses my research interest in inference optimization for GPU utilization and scalability. The approach handles activation outliers and applies distribution-aware quantization without requiring fine-tuning, which relates to achieving higher throughput and lower latency in model deployment.",
      "Irrelevant Aspects": "The paper specifically targets text-to-image diffusion models rather than large language models. It emphasizes image quality and text-image alignment metrics rather than language-specific performance measures. The evaluation uses image-related datasets (MS-COCO and PartiPrompts) rather than language benchmarks.",
      "Summary": "DGQ proposes a distribution-aware group quantization method for text-to-image diffusion models that identifies and adaptively handles outliers while preserving image quality and text-image alignment. While focused on diffusion models rather than LLMs specifically, the quantization techniques and distribution-aware approach offer valuable insights for optimization of large-scale models with attention mechanisms."
    }
  },
  {
    "id": "ogO6DGE6FZ",
    "title": "SpinQuant: LLM Quantization with Learned Rotations",
    "abstract": "Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot. Code is available at https://github.com/facebookresearch/SpinQuant.",
    "authors": [
      "~Zechun_Liu1",
      "~Changsheng_Zhao2",
      "~Igor_Fedorov1",
      "~Bilge_Soran1",
      "~Dhruv_Choudhary1",
      "~Raghuraman_Krishnamoorthi1",
      "~Vikas_Chandra2",
      "~Yuandong_Tian1",
      "~Tijmen_Blankevoort1"
    ],
    "pdf": "/pdf/df22ed66a962124d235f9d8c773a13f8f43e109a.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on post-training quantization techniques for Large Language Models, which directly relates to inference optimization. It addresses reducing memory usage, latency, and power consumption through improved quantization methods. The approach of using learned rotation matrices to enhance quantization accuracy is novel and shows significant improvements over existing methods like LLM-QAT, SmoothQuant, and QuaRot. These aspects align well with the research interest in inference optimization for better GPU utilization and higher throughput with lower latency.",
      "Irrelevant Aspects": "The paper focuses primarily on post-training quantization rather than training optimization, which is only part of my research focus. There's limited discussion on how this specifically impacts GPU utilization at a low level or how it scales across distributed systems. The paper doesn't address the training phase optimization aspects of my research interest.",
      "Summary": "SpinQuant presents a novel approach to LLM quantization using learned rotation matrices to eliminate outliers and improve quantization accuracy. This is highly relevant to inference optimization, which forms half of my research focus. The technique demonstrates significant improvements in reducing the accuracy gap between quantized and full-precision models, with potential benefits for memory usage, latency, and power consumption. While it doesn't address training optimization, the paper provides valuable insights into improving inference efficiency through advanced quantization techniques."
    }
  },
  {
    "id": "pxclAomHat",
    "title": "On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models",
    "abstract": "Training Large Language Models (LLMs) poses significant memory challenges, making low-rank adaptation methods an attractive solution. Previously, Low-Rank Adaptation (LoRA) addressed this by adding a trainable low-rank matrix to the frozen pre-trained weights in each layer, reducing the number of trainable parameters and optimizer states. GaLore, which compresses the gradient matrix instead of the weight matrix, has demonstrated superior performance to LoRA with faster convergence and reduced memory consumption. Despite their empirical success, the performance of these methods has not been fully understood or explained theoretically. In this paper, we analyze the optimization landscapes of LoRA, GaLore, and full-rank methods, revealing that GaLore benefits from fewer spurious local minima and a larger region that satisfies the \\pl, a variant of Polyak-Łojasiewicz (PL) condition, leading to faster convergence. Our analysis leads to a novel method, GaRare, which further improves GaLore by using gradient random projection to reduce computational overhead. Practically, GaRare achieves strong performance in both pre-training and fine-tuning tasks, offering a more efficient approach to large-scale model adaptation.",
    "authors": [
      "~Xu-Hui_Liu1",
      "~Yali_Du1",
      "~Jun_Wang2",
      "~Yang_Yu5"
    ],
    "pdf": "/pdf/e7ca2f62cf6a7998769ccbd0c913bdae8f60832c.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Memory optimization techniques for LLM training, Parameter-efficient fine-tuning methods (LoRA, GaLore), Novel method (GaRare) that reduces computational overhead, Addressing scalability challenges in large model adaptation, Focus on both pre-training and fine-tuning tasks",
      "Irrelevant Aspects": "Primarily focuses on theoretical analysis of optimization landscapes rather than practical GPU utilization metrics, Limited discussion of inference optimization, Less emphasis on specific GPU memory management techniques",
      "Summary": "This paper analyzes optimization landscapes of low-rank adaptation methods (LoRA and GaLore) for LLMs, explaining why GaLore performs better theoretically. It introduces GaRare, a new method that improves GaLore using gradient random projection. The work focuses on reducing trainable parameters and memory consumption during LLM training, which directly relates to GPU utilization and scalability concerns in large model training. While not covering inference optimization, the paper offers valuable insights for training efficiency."
    }
  },
  {
    "id": "OvoCm1gGhN",
    "title": "Differential Transformer",
    "abstract": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models.",
    "authors": [
      "~Tianzhu_Ye1",
      "~Li_Dong1",
      "~Yuqing_Xia1",
      "~Yutao_Sun1",
      "~Yi_Zhu8",
      "~Gao_Huang1",
      "~Furu_Wei1"
    ],
    "pdf": "/pdf/6be641f0b250fd2d56f76cc0fa5dcdeec8bd67a2.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Differential attention mechanism could improve computational efficiency and GPU utilization by promoting sparse attention patterns, potential for better scalability with larger models and training tokens, reduction of activation outliers which impacts training stability and inference efficiency, improved long-context modeling which affects memory management during training and inference",
      "Irrelevant Aspects": "Focus on model behavior improvements like hallucination mitigation rather than system optimization metrics, robustness to order permutation is unrelated to performance optimization, lack of explicit discussion of GPU utilization, throughput measurements, or latency benchmarks",
      "Summary": "The Differential Transformer paper introduces a novel attention mechanism that could impact computational efficiency through sparse attention patterns and reduced activation outliers. While it doesn't directly address my primary interests in GPU utilization and optimization metrics, the architectural changes could indirectly benefit training and inference efficiency. The focus appears more on model quality improvements than system optimization, making it moderately relevant to my research interests."
    }
  },
  {
    "id": "1aF2D2CPHi",
    "title": "Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation",
    "abstract": "Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to copyrights and privacy concerns. For many users seeking open-vocabulary customization, Data-Free Knowledge Distillation (DFKD) emerges as a promising direction. Upon rethinking DFKD, we find that existing methods fail on CLIP due to their heavy reliance on BatchNorm layers, which are unexpectedly unusable in CLIP. Based on our findings, we adopt image-text matching to achieve DFKD for CLIP, enabling customization based on arbitrary class texts. This involves (i) inversing a surrogate dataset from CLIP based on text prompts; and (ii) distilling a student model from CLIP using the surrogate dataset. Specifically, we introduce style dictionary diversification to enhance the diversity of synthetic images. To prevent uncontrollable semantics introduced by diversification, we propose a class consistency maintaining strategy to ensure the consistency of synthetic images. Based on synthetic images with various styles, we further propose meta knowledge distillation to train the student model with good generalization ability. Moreover, we introduce a simple yet effective method to enable customization based on few example images. Comprehensive experiments showcase the superiority of our approach across twelve customized tasks, achieving a 9.33\\% improvement compared to existing DFKD methods.",
    "authors": [
      "~Yongxian_Wei1",
      "~Zixuan_Hu1",
      "~Li_Shen1",
      "~Zhenyi_Wang1",
      "~Chun_Yuan1",
      "~Dacheng_Tao1"
    ],
    "pdf": "/pdf/71b13cef3e274c5ae52fdee5ee93a77c280367f0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses inference optimization for large models through knowledge distillation, directly relating to GPU utilization and scalability concerns. It focuses on reducing model size and improving inference efficiency while maintaining performance, which aligns with goals of achieving higher throughput and lower latency. The approach to training student models more efficiently is relevant to training optimization. The method aims to make CLIP more deployable and customizable without sacrificing too much performance.",
      "Irrelevant Aspects": "The paper focuses specifically on vision-language models (CLIP) rather than pure text-based language models. A significant portion involves generating synthetic images, which doesn't translate directly to text-only models. The customization aspect is more about adapting to visual categories than computational efficiency per se.",
      "Summary": "This paper introduces a data-free knowledge distillation method for CLIP to address deployment challenges due to model size and inference inefficiency. The approach creates synthetic images from text prompts as a surrogate dataset for distillation, using style diversification and class consistency strategies. It includes meta knowledge distillation for better generalization and supports customization with few example images. The method achieves 9.33% improvement over existing DFKD methods across twelve tasks."
    }
  },
  {
    "id": "i2r7LDjba3",
    "title": "ECHOPulse: ECG Controlled Echocardio-gram Video Generation",
    "abstract": "Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic data and generating high-quality videos from routine health data. However, existing models often face high computational costs, slow inference, and rely on complex conditional prompts that require experts' annotations. To address these challenges, we propose ECHOPulse, an ECG-conditioned ECHO video generation model. ECHOPulse introduces two key advancements: (1) it accelerates ECHO video generation by leveraging VQ-VAE tokenization and masked visual token modeling for fast decoding, and (2) it conditions on readily accessible ECG signals, which are highly coherent with ECHO videos, bypassing complex conditional prompts. To the best of our knowledge, this is the first work to use time-series prompts like ECG signals for ECHO video generation. ECHOPulse not only enables controllable synthetic ECHO data generation but also provides updated cardiac function information for disease monitoring and prediction beyond ECG alone. Evaluations on three public and private datasets demonstrate state-of-the-art performance in ECHO video generation across both qualitative and quantitative measures. Additionally, ECHOPulse can be easily generalized to other modality generation tasks, such as cardiac MRI, fMRI, and 3D CT generation. We will make the synthetic ECHO dataset, along with the code and model, publicly available upon acceptance.",
    "authors": [
      "~Yiwei_Li2",
      "~Sekeun_Kim1",
      "~Zihao_Wu1",
      "~Hanqi_Jiang2",
      "~Yi_Pan6",
      "~Pengfei_Jin1",
      "~Sifan_Song1",
      "~Yucheng_Shi2",
      "~Xiaowei_Yu1",
      "~Tianze_Yang2",
      "~Tianming_Liu3",
      "~Quanzheng_Li1",
      "~Xiang_Li14"
    ],
    "pdf": "/pdf/bb51df8d53a0dc570603952a85c0a1572d2a85ee.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses high computational costs and slow inference issues in video generation, which directly relates to inference optimization. It introduces VQ-VAE tokenization and masked visual token modeling specifically for fast decoding, aligning with goals of higher throughput and lower latency. The approach demonstrates optimization techniques for better GPU utilization in neural network processing.",
      "Irrelevant Aspects": "The specific application domain is medical imaging rather than language processing, making the direct application to LLMs limited. The focus on video generation as opposed to text generation represents a different modality than language models. The conditioning on ECG signals is domain-specific and doesn't directly relate to language model optimization techniques.",
      "Summary": "ECHOPulse presents an inference optimization approach for medical video generation using tokenization and masked modeling for fast decoding. While the domain is different from language models, the techniques for accelerating inference and reducing computational overhead have relevance to broader ML optimization research. The paper's explicit focus on addressing slow inference and high computational costs makes it moderately relevant to optimization research."
    }
  },
  {
    "id": "yOOJwR15xg",
    "title": "MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models",
    "abstract": "The pretrain+fine-tune paradigm is foundational for deploying large language models (LLMs) across various downstream applications. Within this framework, Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning (PEFT), producing numerous reusable task-specific LoRA adapters. However, this approach requires explicit task intention selection, posing challenges for autonomous task sensing and switching during inference with multiple existing LoRA adapters embedded in a single LLM. In this work, we introduce MeteoRA (Multiple-Tasks embedded LoRA), a scalable and efficient framework that reuses multiple task-specific LoRA adapters into the base LLM via a full-mode Mixture-of-Experts (MoE) architecture. This framework also includes novel MoE forward acceleration strategies to address the efficiency challenges of traditional MoE implementations. Our evaluation, using the LlaMA2-13B and LlaMA3-8B base models equipped with 28 existing LoRA adapters through MeteoRA, demonstrates equivalent performance with the traditional PEFT method. Moreover, the LLM equipped with MeteoRA achieves superior performance in handling composite tasks, effectively solving ten sequential problems in a single inference pass, thereby demonstrating the framework's enhanced capability for timely adapter switching.",
    "authors": [
      "~Jingwei_Xu3",
      "~Junyu_Lai2",
      "~Yunpeng_Huang3"
    ],
    "pdf": "/pdf/26e0dfb80b33f6f11a71c29de41cc68f6d28d510.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses key aspects of LLM training and inference optimization, including parameter-efficient fine-tuning (LoRA), Mixture-of-Experts (MoE) architecture for scalability, novel forward acceleration strategies for efficiency, handling composite tasks in single inference passes for throughput improvement, and efficient adapter switching for latency reduction.",
      "Irrelevant Aspects": "Some aspects less directly related to core optimization include the specific task sensing and switching mechanisms, comparative performance analysis with traditional PEFT methods, specific model sizes used in experiments (LlaMA2-13B and LlaMA3-8B), and the specific number of LoRA adapters (28) used in the evaluation.",
      "Summary": "MeteoRA introduces an innovative approach to efficiently combine multiple task-specific LoRA adapters within a single LLM using MoE architecture. This directly addresses training and inference optimization challenges by enabling parameter-efficient fine-tuning, improving scalability, and enhancing throughput through composite task handling in single inference passes. The paper's focus on forward acceleration strategies and adapter switching aligns perfectly with research interests in GPU utilization, scalability, and latency reduction for large language models."
    }
  },
  {
    "id": "90Db4RUBc7",
    "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity",
    "abstract": "Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested.",
    "authors": [
      "~Mutian_He1",
      "~Philip_N._Garner1"
    ],
    "pdf": "/pdf/a9cc8edf71023451b13c38f586f7e6cc4ad8d7cf.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on converting transformer models to linear-time alternatives (Linformer and Mamba) which directly addresses inference optimization - a core aspect of my research. The Cross-Architecture Layerwise Distillation (CALD) approach combines fine-tuning with model conversion, which is relevant to model optimization techniques. The work aims to maintain model capabilities while improving computational efficiency, which aligns with goals of achieving higher throughput.",
      "Irrelevant Aspects": "The paper includes speech processing applications which fall outside my language model focus. It lacks specific discussion of GPU utilization, memory management, or hardware-specific optimizations that are central to my research. There's no explicit mention of scalability across multiple GPUs or distributed systems, which are critical aspects of my work.",
      "Summary": "This paper presents a method for converting transformer models to more efficient linear-time alternatives while fine-tuning them on target tasks. While highly relevant to model efficiency, it doesn't specifically address the hardware optimization aspects that are central to my research interest in GPU utilization and scalability."
    }
  },
  {
    "id": "Tb5PY5vwp6",
    "title": "HShare: Fast LLM Decoding by Hierarchical Key-Value Sharing",
    "abstract": "The frequent retrieval of Key-Value (KV) cache data has emerged as a significant factor contributing to the inefficiency of the inference process in large language models. Previous research has demonstrated that a small subset of critical KV cache tokens largely influences attention outcomes, leading to methods that either employ fixed sparsity patterns or dynamically select critical tokens based on the query. While dynamic sparse patterns have proven to be more effective, they introduce significant computational overhead, as critical tokens must be reselected for each self-attention computation. In this paper, we reveal substantial similarities in KV cache token criticality across neighboring queries, layers, and heads. Motivated by this insight, we propose HShare, a hierarchical KV sharing framework. HShare facilitates the sharing of critical KV cache token indices across layers, heads, and queries, which significantly reduces the computational overhead associated with query-aware dynamic token sparsity. In addition, we introduce a greedy algorithm that dynamically determines the optimal layer-level and head-level sharing configuration for the decoding phase. We evaluate the effectiveness and efficiency of HShare across various tasks using three models: LLaMA2-7b, LLaMA3-70b, and Mistral-7b. Experimental results demonstrate that HShare achieves competitive accuracy with different sharing ratios, while delivering up to an $8.6\\times$ speedup in self-attention operations and a $2.7\\times$ improvement in end-to-end throughput compared with FlashAttention2 and GPT-fast respectively. The source code is publicly available at ~\\url{https://github.com/wuhuaijin/HShare}.",
    "authors": [
      "~Huaijin_Wu1",
      "~Lianqiang_Li1",
      "~Hantao_Huang1",
      "~Tu_Yi1",
      "~Jihang_Zhang1",
      "~Minghui_Yu3",
      "~Junchi_Yan2"
    ],
    "pdf": "/pdf/fda8db959bbdef676e6fc30331fb96055a1f725f.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Directly addresses LLM inference optimization focusing on KV cache efficiency, which is critical for GPU utilization and throughput. Provides practical solutions (HShare framework) that achieve significant speedups in self-attention operations (8.6×) and end-to-end throughput (2.7×). The hierarchical sharing approach across layers, heads, and queries is a novel optimization strategy for reducing computational overhead in dynamic sparsity patterns. Evaluated on popular models (LLaMA2, LLaMA3, Mistral) with competitive accuracy retention.",
      "Irrelevant Aspects": "The paper primarily focuses on inference optimization rather than training optimization. While it addresses important aspects of my expertise, it doesn't cover the full scope of large language model systems that I'm interested in, such as training efficiency, model parallelism strategies, or quantization techniques for deployment.",
      "Summary": "HShare introduces a hierarchical KV sharing framework that leverages similarities in token criticality across queries, layers, and heads to optimize LLM inference. By sharing critical KV cache tokens hierarchically, it significantly reduces computational overhead associated with dynamic sparsity patterns while maintaining accuracy. The approach achieves substantial performance improvements in both self-attention operations and end-to-end throughput for popular LLM models."
    }
  },
  {
    "id": "LNYIUouhdt",
    "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
    "abstract": "The advancements in Large Language Models (LLMs) have been hindered by\ntheir substantial sizes, which necessitates LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression meth-\nods have two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the compressed weights after SVD\ntruncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening technique to ensure a direct map-\nping between singular values and compression loss. Moreover, SVD-LLM adopts\na parameter update with sequential low-rank approximation to compensate for\nthe accuracy degradation after SVD compression. We evaluate SVD-LLM on 10\ndatasets and seven models from three different LLM families at three different\nscales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts,\nespecially at high model compression ratios.",
    "authors": [
      "~Xin_Wang71",
      "~Yu_Zheng3",
      "~Zhongwei_Wan1",
      "~Mi_Zhang1"
    ],
    "pdf": "/pdf/1e69f75870a7fc9de5d820991191c2377690b0aa.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "LLM compression for inference optimization, model size reduction which can improve GPU utilization and potentially reduce latency, post-training optimization techniques, evaluation across multiple model families",
      "Irrelevant Aspects": "Limited focus on training optimization, no explicit mention of GPU utilization techniques, not directly addressing scalability in distributed systems, no explicit throughput or latency measurements",
      "Summary": "SVD-LLM presents a compression method for LLMs using a truncation-aware SVD technique and parameter updates after compression. This work is relevant to inference optimization through model compression, potentially improving memory usage and latency, though it doesn't explicitly address training optimization or scalability aspects."
    }
  },
  {
    "id": "i0zzO7Hslk",
    "title": "Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization",
    "abstract": "Pretraining large language models often requires significant computational resources and memory due to their vast parameter amount. An effective approach to enhance parameter efficiency in both training and inference is to parameterize each full-size weight as the product of two trainable low-rank factors. While low-rank fine-tuning has achieved great success, low-rank pretraining remains challenging as it requires learning extensive knowledge from scratch under the restrictive low-rank parameterization. During standard low-rank pretraining, separately optimizing the low-rank factors introduces redundant information from the full gradient, which hinders the learning process. To achieve efficient yet effective low-rank pretraining, we propose a **Lo**w-rank **R**iemannian **O**ptimizer (**LORO**). At each LORO update step, the low-rank factor pairs are jointly updated to ensure their full-size product moves along the steepest descent direction on the low-rank manifold, without the need to compute any memory-intensive full-size matrices or gradients. Hence, our LORO finds low-rank models that achieve high performance comparable to full-size pretrained models, while significantly reducing memory usage and accelerating both training and inference. A LLaMA 1B model pretrained with LORO achieves a perplexity score of 2\\% better than the full-size baseline, with a 54\\% reduction in model memory, a $\\times1.8$ speedup in training, and a $\\times2.2$ speedup in inference. The code is available on https://github.com/mzf666/LORO-main.",
    "authors": [
      "~Zhanfeng_Mo1",
      "~Long-Kai_Huang1",
      "~Sinno_Jialin_Pan1"
    ],
    "pdf": "/pdf/e1ef92e7d05f565a6ba64df2fe2571b74ad3a100.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses GPU memory efficiency during LLM pretraining and inference, claiming 54% memory reduction, 1.8× training speedup, and 2.2× inference speedup. It introduces low-rank parameterization that avoids computing memory-intensive full-size matrices or gradients, which directly improves GPU utilization. The method maintains comparable performance to full-size models while reducing parameters, addressing both training and inference optimization - core areas of my research interest.",
      "Irrelevant Aspects": "The mathematical focus on Riemannian optimization is quite specialized and may have limited applicability to general optimization strategies. The paper appears to focus primarily on pretraining rather than fine-tuning scenarios. There's limited discussion of how this approach scales in distributed training environments for extremely large models.",
      "Summary": "This paper introduces LORO (Low-rank Riemannian Optimizer) for parameter and memory efficient pretraining of large language models. The method parameterizes weights as products of low-rank factors and optimizes them using Riemannian optimization, avoiding full-size matrix computations. Results on a LLaMA 1B model show better perplexity than full-size baseline with 54% memory reduction, 1.8× training speedup, and 2.2× inference speedup. The approach is highly relevant to LLM training/inference optimization, addressing GPU utilization, memory efficiency, and throughput concerns."
    }
  },
  {
    "id": "KI45uDnmzv",
    "title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods",
    "abstract": "Mamba is an efficient sequence model that rivals Transformers and demonstrates significant potential as a foundational architecture for various tasks. Quantization is commonly used in neural networks to reduce model size and computational latency. However, applying quantization to Mamba remains underexplored, and existing quantization methods, which have been effective for CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot suffers a 21% accuracy drop on Vim-T$\\dagger$ even under W8A8). We have pioneered the exploration of this issue and identified several key challenges. First, significant outliers arepresent in gate projections, output projections, and matrix multiplications. Second, Mamba’s unique parallel scan further amplifies these outliers, leading to uneven and heavy-tailed data distributions. Third, even with the application of the Hadamard transform, the variance across channels in weights and activations still remains inconsistent. To these ends, we propose MambaQuant, a post-training quantization (PTQ) framework consisting of: 1) Karhunen-Lo`eve Transformation (KLT) enhanced rotation, rendering the rotation matrix adaptable to diverse channel distributions. 2) Smooth-Fused rotation, which equalizes channel variances and can merge additional parameters into model weights. Experiments show that MambaQuant can quantize both weights and activations into 8-bit with less than 1% accuracy loss for Mamba-based vision and language tasks. To our knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba family, paving the way for further advancements in its application.",
    "authors": [
      "~Zukang_Xu1",
      "~Yuxuan_Yue1",
      "~Xing_Hu6",
      "~Dawei_Yang3",
      "~Zhihang_Yuan1",
      "~Zixu_Jiang1",
      "~Zhixuan_Chen2",
      "~JiangyongYu1",
      "~XUCHEN1",
      "~Sifan_Zhou2"
    ],
    "pdf": "/pdf/3bab3f095ba979b4dc34eaf7053f1023c65652ac.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on quantization of Mamba models, which directly impacts inference optimization by reducing model size and computational requirements. Mamba models are positioned as alternatives to Transformers, which are core to LLM research. Quantization improves GPU utilization and reduces memory footprint. Lower precision computations increase throughput on modern GPUs. The paper addresses quantization challenges specific to Mamba architecture, which is important as these models gain traction. PTQ approaches are practical for deployment without expensive retraining. The focus on reducing computational latency aligns with goals of improving inference efficiency.",
      "Irrelevant Aspects": "The paper primarily addresses vision and language tasks without specific focus on large language models. Limited discussion of GPU utilization or scalability in distributed settings. Focuses on post-training quantization rather than training optimization. Doesn't address distributed training considerations or model parallelism strategies.",
      "Summary": "MambaQuant introduces a post-training quantization framework specifically designed for Mamba models, addressing unique challenges like outlier values and inconsistent variance across channels. By using Karhunen-Loève Transformation enhanced rotation and Smooth-Fused rotation, the method enables 8-bit quantization with minimal accuracy loss. This is directly relevant to inference optimization as it reduces computational requirements, improves GPU utilization, and lowers latency - all key concerns in LLM systems optimization."
    }
  },
  {
    "id": "EgJhwYR2tB",
    "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling",
    "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.",
    "authors": [
      "~Wenda_Xu1",
      "~Rujun_Han1",
      "~Zifeng_Wang1",
      "~Long_Le2",
      "~Dhruv_Madeka1",
      "~Lei_Li11",
      "~William_Yang_Wang2",
      "~Rishabh_Agarwal2",
      "~Chen-Yu_Lee2",
      "~Tomas_Pfister1"
    ],
    "pdf": "/pdf/dd7f731e798046f3b90897b085921520bbbbfec0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper introduces Speculative Knowledge Distillation (SKD), which addresses a core challenge in model optimization - efficiently transferring knowledge from large teacher models to smaller student models. This is directly relevant to training optimization for LLMs. The interleaved sampling approach that generates training data on-the-fly while aligning with the student's inference-time distribution has potential implications for inference optimization and resource utilization. The method could lead to better GPU utilization during both training and inference by creating more efficient student models.",
      "Irrelevant Aspects": "The paper doesn't explicitly focus on hardware-specific optimizations, GPU utilization metrics, or detailed throughput and latency measurements. The primary emphasis appears to be on improving model accuracy and bridging the teacher-student knowledge gap rather than computational efficiency. There's limited discussion of the specific implementation aspects that would directly impact GPU utilization at a low level.",
      "Summary": "Speculative Knowledge Distillation introduces a novel approach to knowledge distillation where student and teacher models cooperate to generate high-quality training data on-the-fly. The student proposes tokens while the teacher replaces poorly ranked ones based on its distribution, transferring knowledge adaptively. Evaluated on various text generation tasks, SKD consistently outperforms existing KD methods. While relevant to model optimization and potential efficiency gains, the paper focuses more on model performance than explicit computational efficiency metrics."
    }
  },
  {
    "id": "v0FzmPCd1e",
    "title": "Selective Attention Improves Transformer",
    "abstract": "Unneeded elements in the attention’s context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention’s context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
    "authors": [
      "~Yaniv_Leviathan1",
      "~Matan_Kalman1",
      "~Yossi_Matias2"
    ],
    "pdf": "/pdf/736a82addd28e9cf5eaa18ed9de4a08a7ae98a3d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper introduces Selective Attention, a parameter-free modification to standard attention mechanism that reduces attention to unneeded elements. This approach claims to significantly reduce memory requirements during inference (16X-47X less memory) while maintaining performance. It enables equivalent performance with fewer attention heads and parameters (approximately 2X reduction), which directly impacts GPU utilization and efficiency. The ability to decrease context buffer size while maintaining performance is particularly relevant for optimizing transformer inference.",
      "Irrelevant Aspects": "The abstract doesn't explicitly mention GPU utilization optimization techniques, throughput measurements, or latency benchmarks. There's no discussion of implementation complexity or how this approach scales across different hardware configurations. The paper also doesn't address training optimization specifically, focusing more on the inference phase benefits.",
      "Summary": "This paper introduces a promising optimization technique for transformer models that could significantly impact memory efficiency and computational requirements during inference. The claimed memory reductions (16X-47X) and parameter efficiency gains (2X) with maintained performance are directly relevant to LLM optimization goals. However, the abstract lacks specific details about GPU utilization, throughput, and latency metrics that would be of primary interest to someone focused on hardware optimization and scalability."
    }
  },
  {
    "id": "lo3nlFHOft",
    "title": "From Promise to Practice: Realizing High-performance Decentralized Training",
    "abstract": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability compared to synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when and how decentralization can shorten the per-iteration runtimes. To support the decentralized training of transformer-based models, we introduce a decentralized Adam algorithm that overlaps communications with computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy our solution in clusters with up to 64 GPUs, demonstrating its practical advantages in both runtime and generalization performance under a fixed iteration budget.\nThe experiment code is open-source at [https://github.com/WangZesen/Decentralized-Training-Exp](https://github.com/WangZesen/Decentralized-Training-Exp), and the extension code is open-source at [https://github.com/WangZesen/Decent-DP](https://github.com/WangZesen/Decent-DP).",
    "authors": [
      "~Zesen_Wang1",
      "~Jiaojiao_Zhang3",
      "~Xuyang_Wu1",
      "~Mikael_Johansson3"
    ],
    "pdf": "/pdf/1cf6984a81d5d495cc7e956a2783276ea569e5fd.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focuses on training optimization for deep neural networks, GPU utilization across multiple nodes, communication/computation overlap for better performance, implementation for transformer models, performance improvements in runtime (higher throughput, lower latency), and addresses scalability challenges in distributed training.",
      "Irrelevant Aspects": "Does not specifically address inference optimization, and may not cover all aspects of large language model training specifically, though transformers are relevant to LLMs.",
      "Summary": "This paper presents research on decentralized training of deep neural networks, focusing on improving performance over synchronous data-parallel methods like All-Reduce. It introduces a decentralized Adam algorithm for transformer models that overlaps communication with computation, demonstrating practical advantages in runtime and generalization performance. The paper addresses key aspects of training optimization, GPU utilization, and scalability, which are highly relevant to the research interests, though it doesn't cover inference optimization."
    }
  },
  {
    "id": "1EnpStvBU8",
    "title": "Feast Your Eyes:  Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
    "abstract": "In existing multimodal large language models (MLLMs), image resolution plays a significant role for  granular visual recognition.  However, directly increasing image resolution leads to expensive computational cost for MLLMs.  In this paper, we  reveal that a combination of low- and high-resolution visual features can efficiently mitigate this shortcoming.  Based on this principle, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for  images of different resolutions, where  high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also   greatly  reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive  experiments on 17 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 15 VL tasks, e.g., +5.2\\% on TextVQA.  More importantly,    both training and inference  of LLaVA-HR remain efficient with MRA, e.g.,  20 training hours and  faster inference speed  than LLaVA-NeXT.  Source codes are  released at: https://github.com/luogen1996/LLaVA-HR.",
    "authors": [
      "~Gen_Luo1",
      "~Yiyi_Zhou1",
      "~Yuxin_Zhang3",
      "~Xiawu_Zheng1",
      "~Xiaoshuai_Sun3",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/36573b501499899104b0312bf935a2ef40ae5df6.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on efficiency improvements for multimodal large language models, specifically addressing the computational cost of increasing image resolution. It presents MRA which reduces input sequence length of MLLMs, directly impacts GPU utilization, and claims faster inference speed than baseline models. The approach maintains efficient training (20 hours) and inference, which aligns with my research interests in optimization techniques for better GPU utilization and scalability.",
      "Irrelevant Aspects": "The paper primarily focuses on vision-language tasks and visual feature processing rather than pure language model optimization. The core contribution is about handling mixed resolutions rather than general optimization techniques for LLMs. Limited detailed information about GPU utilization metrics or scalability analysis specific to language processing.",
      "Summary": "This paper introduces Mixture-of-Resolution Adaptation (MRA), an efficient method for MLLMs that uses two visual pathways for different resolutions and embeds high-resolution information into low-resolution pathways via MR-Adapters. When applied to LLaVA to create LLaVA-HR, it outperforms existing MLLMs on 15 out of 17 vision-language tasks while maintaining efficient training and inference. The method reduces input sequence length and claims faster inference than LLaVA-NeXT, making it relevant to my research interests in optimization for better GPU utilization and lower latency."
    }
  },
  {
    "id": "dhAL5fy8wS",
    "title": "Data Selection via Optimal Control for Language Models",
    "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. \nWe formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.\nBased on these theoretical results, we introduce **P**MP-based **D**ata **S**election (**PDS**), a framework that approximates optimal data selection by solving the PMP conditions. \nIn our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.\nMoreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.\nPDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which helps mitigate the quick exhaustion of available web-crawled corpora. Our code, model, and data can be found at https://github.com/microsoft/LMOps/tree/main/data_selection.",
    "authors": [
      "~Yuxian_Gu1",
      "~Li_Dong1",
      "~Hongning_Wang1",
      "~Yaru_Hao1",
      "~Qingxiu_Dong1",
      "~Furu_Wei1",
      "~Minlie_Huang1"
    ],
    "pdf": "/pdf/109f2accbd5288e74e9df41e106a1a1c7660d919.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper directly addresses training optimization for LMs through intelligent data selection, which can significantly improve training efficiency and resource utilization. It presents a method (PDS) that accelerates learning and boosts performance across various model sizes, even scaling to ~400B models. The approach reduces data demand by 1.8 times, which helps with resource efficiency in pre-training. The scaling to very large models (~400B parameters) and datasets (~10T tokens) demonstrates relevance to scalability concerns in LM training.",
      "Irrelevant Aspects": "The paper doesn't directly address inference optimization techniques such as quantization, pruning, or efficient attention mechanisms. There's limited discussion of specific GPU utilization improvements or detailed throughput and latency measurements. The focus is primarily on data selection rather than the broader optimization of both training and inference pipelines.",
      "Summary": "This paper presents PMP-based Data Selection (PDS), a framework that formulates data selection as an Optimal Control problem solved using Pontryagin's Maximum Principle. The method selects high-quality pre-training data that accelerates LM learning and boosts performance across various tasks and model sizes. PDS demonstrates effectiveness even when extrapolated to ~400B models trained on ~10T tokens and reduces data demand by 1.8 times, improving resource efficiency in pre-training. While highly relevant to training optimization, it doesn't directly address inference optimization techniques."
    }
  },
  {
    "id": "wCXAlfvCy6",
    "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
    "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g., 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
    "authors": [
      "~Yukang_Chen1",
      "~Fuzhao_Xue1",
      "~Dacheng_Li1",
      "~Qinghao_Hu3",
      "~Ligeng_Zhu1",
      "~Xiuyu_Li1",
      "~Yunhao_Fang1",
      "~Haotian_Tang1",
      "~Shang_Yang1",
      "~Zhijian_Liu1",
      "~Yihui_He1",
      "~Hongxu_Yin2",
      "~Pavlo_Molchanov1",
      "~Jan_Kautz1",
      "~Linxi_Fan2",
      "~Yuke_Zhu1",
      "~Yao_Lu13",
      "~Song_Han5"
    ],
    "pdf": "/pdf/e17db66e0f52f4101f12f96ac2b23245c4eddb47.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Introduction of MM-SP (Multi-Modal Sequence Parallelism) for efficient parallelization of long video training and inference; ability to train 2M context length on 256 GPUs without gradient checkpointing; performance improvements of 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron; memory optimization techniques; integration with Hugging Face Transformers; scalability from 8 to 2048 video frames.",
      "Irrelevant Aspects": "Specific focus on visual-language models rather than general LLMs; evaluation on video benchmarks (VideoMME, etc.); needle-in-a-haystack evaluation metrics.",
      "Summary": "LongVILA presents a full-stack solution for training and inference optimization of long-context visual-language models, with particular emphasis on the MM-SP system for efficient parallelization. The work demonstrates significant improvements in GPU utilization, scalability, and throughput by enabling long-context training without gradient checkpointing and achieving substantial speedups over existing parallelism approaches. The paper directly addresses key challenges in large model training and inference optimization, making it highly relevant to research interests in GPU utilization, scalability, throughput, and latency optimization for large language models."
    }
  },
  {
    "id": "uWtLOy35WD",
    "title": "LLaVA-MoD: Making LLaVA Tiny via MoE-Knowledge Distillation",
    "abstract": "We introduce LLaVA-MoD, a novel framework designed to enable the efficient training of small-scale Multimodal Language Models ($s$-MLLM) distilling knowledge from large-scale MLLM ($l$-MLLM). Our approach tackles two fundamental challenges in MLLM distillation. First, we optimize the network structure of $s$-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the language model, striking a balance between computational efficiency and model expressiveness. Second, we propose a progressive knowledge transfer strategy for comprehensive knowledge transfer. This strategy begins with mimic distillation, where we minimize the Kullback-Leibler (KL) divergence between output distributions to enable $s$-MLLM to emulate $s$-MLLM's understanding. Following this, we introduce preference distillation via Preference Optimization (PO), where the key lies in treating $l$-MLLM as the reference model. During this phase, the $s$-MLLM's ability to discriminate between superior and inferior examples is significantly enhanced beyond $l$-MLLM, leading to a better $s$-MLLM that surpasses $l$-MLLM, particularly in hallucination benchmarks.\nExtensive experiments demonstrate that LLaVA-MoD surpasses existing works across various benchmarks while maintaining a minimal activated parameters and low computational costs. Remarkably, LLaVA-MoD-2B surpasses Qwen-VL-Chat-7B with an average gain of 8.8\\%, using merely $0.3\\%$ of the training data and 23\\% trainable parameters. The results underscore LLaVA-MoD's ability to effectively distill comprehensive knowledge from its teacher model, paving the way for developing efficient MLLMs.",
    "authors": [
      "~Fangxun_Shu1",
      "~Yue_Liao1",
      "~Lei_Zhang47",
      "~Le_Zhuo2",
      "~Chenning_Xu1",
      "~Guanghao_Zhang1",
      "~Haonan_Shi2",
      "~Long_Chan1",
      "~TaoZhong1",
      "~Zhelun_Yu1",
      "~Wanggui_He1",
      "~Siming_Fu1",
      "~Haoyuan_Li1",
      "~Si_Liu5",
      "~Hongsheng_Li3",
      "~Hao_Jiang13"
    ],
    "pdf": "/pdf/aec120da4d075b18ee14579b2e381a318767c929.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on training optimization through MoE architecture integration, which improves computational efficiency. The progressive knowledge transfer strategy using KL divergence minimization and preference optimization contributes to more efficient training processes. The approach achieves impressive parameter reduction (only 23% trainable parameters) and data efficiency (0.3% of training data) while maintaining performance, which directly relates to GPU utilization optimization. The creation of smaller models with comparable performance would likely lead to higher throughput and lower latency in inference scenarios.",
      "Irrelevant Aspects": "The paper specifically targets multimodal language models rather than general LLMs, which limits its direct applicability to my broader research interests. The evaluation focus includes hallucination benchmarks which are more about model capabilities than system optimization. The knowledge distillation emphasis is more on methodology than on system-level implications for GPU utilization and scalability.",
      "Summary": "LLaVA-MoD presents a framework for efficient training of small-scale Multimodal Language Models by integrating MoE architecture and implementing progressive knowledge transfer from larger models. The approach successfully reduces parameter count and data requirements while maintaining performance, which has direct relevance to training optimization and efficiency in ML systems. While the multimodal focus makes it somewhat specialized, the techniques for model compression and efficient training transfer well to general LLM optimization scenarios."
    }
  },
  {
    "id": "hheFYjOsWO",
    "title": "Mixture Compressor for Mixture-of-Experts LLMs Gains More",
    "abstract": "Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important-- only a small subset is critical. Building on these insights, we propose MC, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization (PMQ), which formulates the adaptive bit-width allocation as a Linear Programming (LP) problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning (ODP),  which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC compresses 76.6% of the model, with only a 3.8% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15%, with a performance drop of less than 0.6%. Remarkably, MC even surpasses floating-point 13b dense LLMs with significantly smaller parameter sizes, suggesting that mixture compression in MoE-LLMs has the potential to outperform both comparable and larger dense LLMs.  Our code is\navailable at https://github.com/Aaronhuang-778/MC-MoE",
    "authors": [
      "~Wei_Huang36",
      "~Yue_Liao1",
      "~Jianhui_Liu1",
      "~Ruifei_He1",
      "~Haoru_Tan1",
      "~Shiming_Zhang1",
      "~Hongsheng_Li3",
      "~Si_Liu5",
      "~XIAOJUAN_QI2"
    ],
    "pdf": "/pdf/e94575fa34948b81391cae62bccfc8ff9fed89f9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on optimizing Mixture-of-Experts LLMs by addressing memory consumption, loading latency, and expert redundancy. It proposes Pre-Loading Mixed-Precision Quantization (PMQ) to reduce storage overhead and Online Dynamic Pruning (ODP) to reduce activated parameters during inference. The approach is training-free and aims to improve GPU utilization and throughput while maintaining performance. The methods achieve extreme compression (76.6% at 2.54 bits with only 3.8% accuracy loss) and further reduce activated parameters by 15% during inference with minimal performance impact.",
      "Irrelevant Aspects": "The paper is specifically focused on MoE architectures without discussing integration with other optimization techniques like tensor/pipeline parallelism. There's limited discussion of hardware-specific optimizations or how different GPU architectures would benefit from these methods.",
      "Summary": "This paper presents a highly relevant approach to optimizing MoE-LLMs through mixed-precision quantization and dynamic pruning. The methods directly address key challenges in LLM deployment: memory usage, loading latency, and computational efficiency. By achieving extreme compression while maintaining performance, this work contributes significantly to improving GPU utilization and scalability of large language models, which aligns perfectly with research interests in training and inference optimization."
    }
  },
  {
    "id": "PTcMzQgKmn",
    "title": "A Training-Free Sub-quadratic Cost Transformer Model Serving Framework with Hierarchically Pruned Attention",
    "abstract": "In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. \nWhile many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities.\nAlthough recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \\log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. \nWe notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call \"attention locality\". Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation.\nHiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.",
    "authors": [
      "~Heejun_Lee1",
      "~Geon_Park1",
      "~Youngwan_Lee1",
      "~Jaduk_Suh1",
      "~Jina_Kim1",
      "~Wonyong_Jeong1",
      "~Bumsik_Kim1",
      "~Hyemin_Lee6",
      "~Myeongjae_Jeon3",
      "~Sung_Ju_Hwang1"
    ],
    "pdf": "/pdf/019a7ef17f3293253a5decefc0c2ab3dfce4f6a6.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Addresses quadratic complexity in transformers for better GPU utilization, proposes sub-quadratic attention mechanism O(T log T), implements KV cache offloading to optimize GPU memory, focuses on inference optimization with reduced latencies and maintained throughput, training-free approach applicable to existing models, enables million-token context processing on commodity GPUs",
      "Irrelevant Aspects": "Limited focus on training optimization strategies, doesn't appear to address distributed computing across multiple GPUs, minimal discussion on multi-modal processing optimizations",
      "Summary": "This paper presents HiP, a novel approach to optimize transformer inference by reducing attention mechanism complexity from quadratic to sub-quadratic O(T log T) through hierarchical pruning. The training-free method leverages observed attention locality patterns and implements KV cache offloading, significantly reducing memory usage while maintaining throughput. The approach enables efficient serving of pretrained LLMs with million-token contexts on commodity hardware, addressing key challenges in LLM deployment without requiring model retraining."
    }
  },
  {
    "id": "l0gZS0sAlf",
    "title": "Ensembles of Low-Rank Expert Adapters",
    "abstract": "The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization.  These challenges can undermine model generalization across tasks, resulting in reduced downstream performance.  Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset.  Building on these insights, we propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve the model's capability to handle diverse tasks.  ELREA clusters the training instructions based on their gradient directions, representing different areas of expertise and thereby reducing conflicts during optimization.  Expert adapters are then trained on these clusters, utilizing the low-rank adaptation (LoRA) technique to ensure training efficiency and model scalability.  During inference, ELREA combines predictions from the most relevant expert adapters based on the input data's gradient similarity to the training clusters, ensuring optimal adapter selection for each task.  Experiments show that our method outperforms baseline LoRA adapters trained on the full dataset and other ensemble approaches with similar training and inference complexity across a range of domain-specific tasks.",
    "authors": [
      "~Yinghao_Li3",
      "~Vianne_R._Gao1",
      "~Chao_Zhang15",
      "~MohamadAli_Torkamani1"
    ],
    "pdf": "/pdf/818f92de198c1fb592f1e2289646f6616caab66c.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization of large language models through parameter-efficient fine-tuning using LoRA. It discusses reducing optimization conflicts during training and improving model scalability. The inference approach combines predictions from expert adapters, which relates to inference optimization. The method aims to improve performance across diverse tasks while maintaining training efficiency.",
      "Irrelevant Aspects": "The paper focuses more on model effectiveness rather than computational efficiency metrics. It lacks detailed discussion of GPU utilization techniques, memory optimization strategies, or specific latency measurements. The emphasis is more on task performance than throughput or resource utilization optimization.",
      "Summary": "ELREA proposes a framework that clusters training instructions based on gradient directions to reduce optimization conflicts, then trains expert adapters using LoRA for efficiency. During inference, it combines predictions from relevant experts. While addressing training and inference optimization aspects of LLMs, the paper focuses more on model effectiveness across tasks rather than computational efficiency metrics like GPU utilization, throughput, or latency optimization."
    }
  },
  {
    "id": "OVxmpus9NA",
    "title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference",
    "abstract": "In spite of the great potential of large language models (LLMs) across various tasks, their deployment on resource-constrained devices remains challenging due to their excessive computational and memory demands. Quantization has emerged as an effective solution by storing weights in reduced precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially alleviate the memory-boundedness of LLM decoding, still suffers from prohibitive performance drop. In this work, \nwe argue that existing approaches fail to explore the diversity in computational patterns, redundancy, and sensitivity to approximations of the different phases of LLM inference, resorting to a uniform quantization policy throughout.\nInstead, we propose a novel phase-aware method that selectively allocates precision during different phases of LLM inference, achieving both strong context extraction during prefill and efficient memory bandwidth utilization during decoding. To further address the memory-boundedness of the decoding phase, we introduce Progressive Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering of precision deeper in the generated sequence, together with a spectrum of precision-switching schedulers that dynamically drive the precision-lowering decisions in either task-adaptive or prompt-adaptive manner. \nExtensive evaluation across diverse language tasks shows that when targeting Nvidia GPUs, PMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a throughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$ over uniform quantization approaches while preserving the output quality.",
    "authors": [
      "~Hao_Mark_Chen1",
      "~Fuwen_Tan1",
      "~Alexandros_Kouris1",
      "~Royson_Lee1",
      "~Hongxiang_Fan1",
      "~Stylianos_Venieris1"
    ],
    "pdf": "/pdf/94d87cc6f192f7890dbd1e8a9e31317938fb5ffb.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Mixed-precision optimization for LLM inference, phase-aware quantization strategy, progressive precision reduction, GPU-focused optimization, measured throughput improvements (1.4-12.2× speedup), performance evaluation on GPUs and NPUs, preservation of output quality despite optimization",
      "Irrelevant Aspects": "Limited focus on training optimization, less focus on theoretical aspects of quantization, doesn't address distributed inference scaling, limited discussion of potential trade-offs",
      "Summary": "This paper presents Progressive Mixed-Precision Decoding (PMPD), a technique that dynamically adjusts precision during different phases of LLM inference to improve efficiency. It addresses the memory-boundedness of LLM decoding by gradually lowering precision deeper in generated sequences. The approach shows significant performance improvements while maintaining output quality, making it highly relevant to my research on LLM inference optimization for improved GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "xtlMtbVfWu",
    "title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models",
    "abstract": "Distributed training methods are crucial for large language models (LLMs). However, existing distributed training methods often suffer from communication bottlenecks, stragglers, and limited elasticity, particularly in heterogeneous or large-scale environments. Local SGD methods have been proposed to address these issues, but their effectiveness remains limited to small-scale training due to additional memory overhead and lack of concerns on efficiency and stability. To tackle these issues, we propose EDiT, an innovative Efficient Distributed Training method that combines a tailored Local SGD approach with model sharding techniques to enhance large-scale training efficiency. EDiT performs layer-wise parameter synchronization during forward pass, reducing communication and memory overhead and enabling overlap. Besides, EDiT employs a pseudo gradient penalty strategy to suppress loss spikes, which ensures training stability and improves performance. Additionally, we introduce A-EDiT, a fully asynchronous variant of EDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we conduct a series of experiments to validate large-scale asynchronous training for LLMs, accompanied by comprehensive analyses. Experimental results demonstrate the superior performance of EDiT/A-EDiT, establishing them as robust solutions for distributed LLM training in diverse computational ecosystems. The code is available at Atorch codebase: https://github.com/intelligent-machine-learning/atorch/tree/main/atorch/local_sgd.",
    "authors": [
      "~Jialiang_Cheng1",
      "~Ning_Gao3",
      "~Yun_Yue3",
      "~Zhiling_Ye1",
      "~Jiadi_Jiang1",
      "~Jian_Sha1"
    ],
    "pdf": "/pdf/3f6d8046c1a9856022b0e94083d5bfbd5e844d65.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Distributed training optimization for large language models, addressing communication bottlenecks, Local SGD implementation, layer-wise parameter synchronization, memory overhead reduction, enabling computation-communication overlap, handling heterogeneous clusters, improving GPU utilization, increasing throughput, model sharding techniques, large-scale training scalability",
      "Irrelevant Aspects": "The paper doesn't address inference optimization, which is part of my research interest. It also doesn't specifically mention latency measurements or detailed GPU utilization metrics.",
      "Summary": "EDiT is a highly relevant paper that introduces an efficient distributed training method for LLMs using a tailored Local SGD approach combined with model sharding. It directly addresses key optimization challenges including communication bottlenecks, stragglers, and memory overhead. The layer-wise synchronization during forward pass and overlap capabilities are particularly valuable for improving GPU utilization and throughput. The paper also presents an asynchronous variant (A-EDiT) for heterogeneous clusters, enhancing scalability. While it covers training optimization thoroughly, it doesn't address inference optimization, which is a secondary focus of my research."
    }
  },
  {
    "id": "n0OtGl6VGb",
    "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
    "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. \nHowever, their increased computational and memory demands present significant challenges, especially when handling long sequences.\nThis paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. \nUnlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights.\nIn response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20\\% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve $2.8\\times$ peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4$\\times$ (with KIVI alone) to 5$\\times$ when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK.  Our code has been made available at https://github.com/SalesforceAIResearch/ThinK.",
    "authors": [
      "~Yuhui_Xu2",
      "~Zhanming_Jie2",
      "~Hanze_Dong1",
      "~Lei_Wang28",
      "~Xudong_Lu1",
      "~Aojun_Zhou2",
      "~Amrita_Saha2",
      "~Caiming_Xiong1",
      "~Doyen_Sahoo1"
    ],
    "pdf": "/pdf/7f5379da15a795ee6f5f3260d6b837fa1771b920.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses LLM inference optimization by reducing KV cache memory consumption, which is critical for GPU utilization and enabling larger batch sizes. It focuses on scalability for long sequences, achieves 2.8x peak memory reduction, increases batch size from 4x to 5x on a single GPU, and validates results on popular LLaMA and Mistral models. The method integrates with existing systems like KIVI and has publicly available code, enhancing practical adoption.",
      "Irrelevant Aspects": "The paper focuses exclusively on inference optimization without addressing training optimization. There is no specific discussion about latency measurements, which is another key aspect of my research interest. It also doesn't delve into specific GPU architectures or low-level hardware implementation details.",
      "Summary": "ThinK introduces a novel query-dependent pruning method for KV cache optimization during LLM inference, targeting the channel dimension rather than sequence length. By identifying and selectively pruning less significant channels based on attention weight distributions, it achieves over 20% reduction in KV cache memory costs while maintaining model accuracy. The approach shows particularly strong results when combined with other optimization methods like KIVI, enabling significant memory savings and allowing larger batch sizes on single GPUs. This work directly addresses key bottlenecks in LLM inference, especially for long-context scenarios, and offers practical improvements to GPU utilization and scalability."
    }
  },
  {
    "id": "E4Fk3YuG56",
    "title": "Cut Your Losses in Large-Vocabulary Language Models",
    "abstract": "As language models grow ever larger, so do their vocabularies.\nThis has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.\nCross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.\nWe propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.\nRather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.\nWe implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.\nTo improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.\nExperiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.",
    "authors": [
      "~Erik_Wijmans1",
      "~Brody_Huval2",
      "~Alexander_Hertzberg1",
      "~Vladlen_Koltun1",
      "~Philipp_Kraehenbuehl1"
    ],
    "pdf": "/pdf/2b5221aa12e28334c97f972675667f558215c7af.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Memory optimization during LLM training, GPU utilization efficiency, scalability with large vocabularies, custom kernel development for flash memory, throughput improvement through sparse computation, training acceleration without convergence sacrifice",
      "Irrelevant Aspects": "Limited focus on inference optimization, no direct discussion of latency reduction, narrow scope focusing only on cross-entropy computation rather than the entire training pipeline",
      "Summary": "The paper presents Cut Cross-Entropy (CCE), a method to drastically reduce memory consumption during LLM training by avoiding materializing the full logit matrix. CCE computes only the correct token's logit and performs log-sum-exp reduction in flash memory, reducing Gemma 2B's loss computation memory from 24GB to 1MB. The paper further optimizes throughput by leveraging softmax sparsity, demonstrating significant memory savings without compromising training speed or convergence."
    }
  },
  {
    "id": "AyC4uxx2HW",
    "title": "LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing",
    "abstract": "Large Language Model (LLM) providers typically train a family of models, each of a different size targeting a specific deployment scenario. Models in the family are all trained from scratch, making the process extremely resource intensive.\nRecent work has successfully reduced the cost of training model families through a combination of structured pruning and knowledge distillation; here, only the largest model in the family is trained from scratch, and smaller models are obtained via pruning. We observe that while effective, this strategy must still perform pruning and distillation with hundreds of billions of training tokens for every new model, keeping overall training costs high.\nIn this work, we introduce a novel nested weight-shared architecture named LLaMaFlex that can be pruned across both width and depth dimensions in a zero-shot manner to instantly yield a large number of highly accurate compressed models.\nLLaMaFlex starts from a pretrained model, and only requires a single continued training phase consisting of ~60B tokens, which trains the elastic network and an end-to-end Gumbel Softmax-based router; this router is able to interpolate smoothly across model sizes, enabling the \"train once, deploy many'' paradigm.\nWe train LLaMaFlex on Llama 3.1 8B and use it to zero-shot generate a family of compressed models that achieves accuracy on par with or better than state-of-the-art pruned, elastic/flexible, and trained-from-scratch models.",
    "authors": [
      "~Ruisi_Cai1",
      "~Saurav_Muralidharan1",
      "~Hongxu_Yin2",
      "~Zhangyang_Wang1",
      "~Jan_Kautz1",
      "~Pavlo_Molchanov1"
    ],
    "pdf": "/pdf/46b747dfa619971eeb62aafadf70f41f2aa9c8c9.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses LLM training optimization by introducing LLaMaFlex, a method to create multiple model sizes from a single training process. It reduces training resource requirements through generalized pruning and weight sharing, which relates to better GPU utilization. The approach enables creating compressed models through zero-shot pruning, potentially improving inference efficiency. It uses a single continued training phase of ~60B tokens rather than training each model separately, significantly reducing training costs and improving resource utilization.",
      "Irrelevant Aspects": "The paper doesn't provide detailed analysis of GPU utilization metrics or hardware-specific optimizations. There's limited focus on distributed systems scalability. Throughput and latency measurements during inference aren't emphasized. The work doesn't appear to address system-level optimizations that would be critical for production deployment.",
      "Summary": "LLaMaFlex introduces a nested weight-shared architecture that can be pruned across width and depth dimensions to instantly generate multiple model sizes from a single training process. By using a Gumbel Softmax-based router, it enables smooth interpolation across model sizes, implementing a 'train once, deploy many' paradigm that significantly reduces training resource requirements compared to traditional approaches that train each model from scratch."
    }
  },
  {
    "id": "q5sOv4xQe4",
    "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
    "abstract": "We introduce Hybrid Autoregressive Transformer (HART), the first autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components:  discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7$\\times$ higher throughput and 6.9-13.4$\\times$ lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.",
    "authors": [
      "~Haotian_Tang1",
      "~Yecheng_Wu1",
      "~Shang_Yang1",
      "~Enze_Xie1",
      "~Junsong_Chen1",
      "~Junyu_Chen4",
      "~Zhuoyang_Zhang1",
      "~Han_Cai1",
      "~Yao_Lu13",
      "~Song_Han5"
    ],
    "pdf": "/pdf/4422c0b812f02a0f6e2a62440407c01d2a937f8e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses training optimization through its hybrid tokenizer approach that reduces the prohibitive training costs for generating 1024px images. It demonstrates significant inference optimization with 4.5-7.7× higher throughput and 6.9-13.4× lower MACs compared to diffusion models. The architecture includes a lightweight residual diffusion module with only 37M parameters, indicating efficient resource utilization. The scalable-resolution discrete AR model suggests good scalability properties. These innovations represent meaningful advances in GPU utilization and computational efficiency for large generative models.",
      "Irrelevant Aspects": "The paper focuses on visual generation rather than language models specifically, which is the primary domain of my research. Much of the evaluation is based on image quality metrics (FID, CLIP scores) rather than system performance metrics. The comparison is primarily against diffusion models rather than other optimization approaches for transformers. The hybrid tokenizer approach is specialized for visual data and may not directly translate to text-based language models.",
      "Summary": "HART introduces a hybrid autoregressive transformer for efficient 1024x1024 image generation that combines discrete and continuous token representations. The approach addresses computational bottlenecks in existing AR models through architectural innovation, resulting in significantly improved throughput (4.5-7.7×) and reduced computational requirements (6.9-13.4× lower MACs). While focused on visual generation, the paper presents valuable insights into training and inference optimization for large autoregressive models that could inform work on language models. The hybrid approach demonstrates how combining discrete and continuous representations can improve efficiency while maintaining quality, a principle that could extend to other domains."
    }
  },
  {
    "id": "qTrEq31Shm",
    "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, LongPO-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales. Our code is available at https://github.com/DAMO-NLP-SG/LongPO.",
    "authors": [
      "~Guanzheng_Chen1",
      "~Xin_Li40",
      "~Michael_Shieh1",
      "~Lidong_Bing2"
    ],
    "pdf": "/pdf/28ff961a3610be5440eae19eb8c3b9d005cd6056.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization through LongPO method, self-generated preference data reduces need for human annotation, short-to-long capability transfer extends context window efficiently, enables smaller models (Mistral-7B) to achieve performance comparable to larger models (GPT-4), scalability demonstrated by extending context from 128K to 512K, retention of short-context performance while improving long-context capabilities",
      "Irrelevant Aspects": "Limited focus on GPU utilization optimization, minimal discussion of inference optimization techniques, absence of throughput and latency metrics, no specific attention to memory optimization strategies, doesn't address distributed training optimizations",
      "Summary": "LongPO introduces a self-evolution method that enables short-context LLMs to efficiently handle long-context tasks through preference optimization. The approach leverages self-generated short-to-long preference data to transfer capabilities while maintaining performance on both short and long contexts. While highly relevant for training optimization and model efficiency, it focuses more on capability extension than computational performance metrics like GPU utilization, throughput, and latency."
    }
  },
  {
    "id": "OfjIlbelrT",
    "title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
    "abstract": "Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold.\nFlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference.",
    "authors": [
      "~Xunhao_Lai1",
      "~Jianqiao_Lu1",
      "~Yao_Luo3",
      "~Yiyuan_Ma1",
      "~Xun_Zhou5"
    ],
    "pdf": "/pdf/edab632a2af3db247483aa2ac696e374a2894aa9.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Dynamic sparse attention mechanism for LLM inference optimization; focus on computational efficiency during the pre-filling phase; adaptive approach to varying input demands; potential for better GPU utilization, higher throughput, and lower latency; specific innovations in query-aware pattern determination and cumulative-attention based index selection.",
      "Irrelevant Aspects": "Limited focus on training optimization; specific implementation details that might be less relevant to general optimization principles; experimental evaluation on specific tasks that may not represent all inference scenarios.",
      "Summary": "FlexPrefill introduces a context-aware sparse attention mechanism that dynamically adjusts sparse patterns and computational budget during LLM inference pre-filling. It addresses the quadratic complexity challenge of attention with prompt length through query-aware pattern determination and cumulative-attention based index selection. The method shows promise for improving efficiency, speed, and scalability in LLM inference systems."
    }
  },
  {
    "id": "dGVZwyq5tV",
    "title": "Training-Free Activation Sparsity in Large Language Models",
    "abstract": "Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for  matrix multiplications during the forward pass. \nHowever, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. \nThis paper describes TEAL (**T**raining-Fre**e** **A**ctivation Sparsity in **L**LMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40\\% and 50\\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains.",
    "authors": [
      "~James_Liu3",
      "~Pragaash_Ponnusamy1",
      "~Tianle_Cai1",
      "~Han_Guo1",
      "~Yoon_Kim1",
      "~Ben_Athiwaratkun1"
    ],
    "pdf": "/pdf/90ba6edf1b1b298f53047104daefb8da911e5223.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Inference optimization through activation sparsity, improved GPU utilization by reducing compute and memory-movement in matrix multiplications, demonstrated throughput improvements with 1.53× and 1.8× speed-ups, scalability across models from 7B to 70B parameters, compatibility with weight quantization for compound optimizations, practical kernel implementations, and broad applicability across multiple model families (Llama-2, Llama-3, Mistral)",
      "Irrelevant Aspects": "Focus on inference rather than training optimization, limited discussion of ReLU-based sparsity approaches which are less relevant to modern transformer architectures, and minimal coverage of training pipeline optimizations",
      "Summary": "TEAL presents a training-free method to achieve 40-50% activation sparsity across LLMs, resulting in significant inference speedups without extensive training requirements. The approach directly addresses key optimization challenges in LLM deployment including GPU utilization, throughput, and scalability, making it highly relevant to research on efficient LLM systems."
    }
  },
  {
    "id": "NxyfSW6mLK",
    "title": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments",
    "abstract": "Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on relatively small datasets and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.",
    "authors": [
      "~Kaustubh_Sridhar1",
      "~Souradeep_Dutta2",
      "~Dinesh_Jayaraman2",
      "~Insup_Lee1"
    ],
    "pdf": "/pdf/7de864552f0638515d915497f208b6f5185bb64b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Efficiency improvements with smaller models and datasets, retrieval-augmented architecture, in-context learning mechanisms, semi-parametric approach combining neural networks with retrieval, transformer-based policy architecture, claims of outperforming state-of-the-art with fewer resources",
      "Irrelevant Aspects": "Focus on robotics and game-playing environments rather than language tasks, emphasis on agent behavior in environments rather than text processing, generalist agent concept rather than specialized language processing, focus on environment adaptation rather than model optimization for consistent environments, policy training rather than language model training",
      "Summary": "This paper proposes REGENT, a retrieval-augmented agent that uses smaller models with in-context learning to adapt to new environments. While focused on agent behavior in robotics and games rather than language processing, its approaches to efficiency through smaller models, retrieval augmentation, and semi-parametric architectures are directly relevant to LLM optimization. The paper's claims of 3x fewer parameters and order-of-magnitude fewer datapoints while outperforming state-of-the-art are particularly relevant to my research interests in GPU utilization and efficiency."
    }
  },
  {
    "id": "wJv4AIt4sK",
    "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
    "abstract": "The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.",
    "authors": [
      "~Simla_Burcu_Harma1",
      "~Ayan_Chakraborty1",
      "~Elizaveta_Kostenok1",
      "~Danila_Mishin1",
      "~Dongho_Ha1",
      "~Babak_Falsafi1",
      "~Martin_Jaggi1",
      "~Ming_Liu17",
      "~Yunho_Oh1",
      "~Suvinay_Subramanian1",
      "~Amir_Yazdanbakhsh1"
    ],
    "pdf": "/pdf/e53ec43cb50f78d023609f865a3301b8ba5ba74b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper directly addresses large language models (LLMs) including OPT and LLaMA models, which is central to my research interest. It focuses on sparsity and quantization, two critical compression techniques for improving GPU utilization and throughput during inference. The research examines how these techniques impact efficient deployment in resource-constrained environments, which relates to scalability and reducing serving costs. The paper provides insights into best practices for applying compression methods to maximize hardware resource efficiency, which aligns with my focus on inference optimization.",
      "Irrelevant Aspects": "The paper primarily focuses on model compression for deployment rather than training optimization, which is a key part of my research interest. It includes vision models (ViT and ResNet) which are not directly relevant to my LLM focus. The approach appears more theoretical and mathematical rather than providing specific implementation details for GPU optimization. There's no explicit discussion of latency measurements or throughput benchmarks, which are critical metrics for my research.",
      "Summary": "This paper investigates the interaction between sparsity and quantization in model compression, providing mathematical proof that these techniques are non-orthogonal. It examines large language models (OPT and LLaMA) and studies how the order of applying these techniques affects model accuracy. The research offers insights into best practices for model compression to maximize hardware efficiency during deployment. While it addresses important aspects of LLM inference optimization through compression techniques, it lacks focus on training optimization and specific GPU utilization strategies."
    }
  },
  {
    "id": "49v8meXjHS",
    "title": "kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
    "abstract": "Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy et al., 2017], enabling each token to attend to only its $k$ closest tokens. While $k$NN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for $k$NN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling [Mussmann et al., 2017] with $k$NN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.",
    "authors": [
      "~Themistoklis_Haris1"
    ],
    "pdf": "/pdf/b71467345cc6965e9802f1d40880d8777ea60f87.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Addresses quadratic complexity of self-attention, proposes sub-quadratic algorithms for both training and inference, focuses on improving scalability of Transformers, provides theoretical framework for efficient attention computation, demonstrates practical effectiveness through empirical experiments",
      "Irrelevant Aspects": "Primarily theoretical focus rather than implementation details, limited emphasis on specific hardware optimizations, narrow focus on attention mechanism rather than holistic system optimization",
      "Summary": "This paper is highly relevant to my expertise as it tackles the quadratic complexity bottleneck in Transformers through kNN attention methods. It proposes sub-quadratic algorithms that would directly improve GPU utilization, scalability, throughput, and latency for both training and inference of large language models. While more theoretical than implementation-focused, the insights could significantly impact Transformer system efficiency."
    }
  },
  {
    "id": "rAcgDBdKnP",
    "title": "OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting",
    "abstract": "Post-training quantization (PTQ) has emerged as a widely adopted technique for compressing and accelerating Large Language Models (LLMs).\nThe major challenge in LLM quantization is that uneven and heavy-tailed data distributions can expand the quantization range, thereby reducing bit precision for most values.\nRecent methods attempt to eliminate outliers and balance inter-channel differences by employing linear transformations; however, they remain  heuristic and are often overlook optimizing the data distribution across the entire quantization space.\nIn this paper, we introduce Quantization Space Utilization Rate (QSUR), a novel metric that effectively assesses the quantizability of transformed data by measuring the space utilization of the data in the quantization space. We complement QSUR with mathematical derivations that examine the effects and limitations of various transformations, guiding our development of Orthogonal and Scaling Transformation-based Quantization (OSTQuant). OSTQuant employs a learnable equivalent transformation, consisting of an orthogonal transformation and a scaling transformation, to optimize the distributions of weights and activations across the entire quantization space. Futhermore, we propose the KL-Top loss function, designed to mitigate noise during optimization while retaining richer semantic information within the limited calibration data imposed by PTQ.\nOSTQuant outperforms existing work on various LLMs and benchmarks. In the W4-only setting, it retains 99.5\\% of the floating-point accuracy. In the more challenging W4A4KV4 configuration, OSTQuant reduces the performance gap by 32\\% on the LLaMA-3-8B model compared to state-of-the-art methods. Code will be available.",
    "authors": [
      "~Xing_Hu6",
      "~Yuan_Cheng8",
      "~Dawei_Yang3",
      "~Zhixuan_Chen2",
      "~Zukang_Xu1",
      "~JiangyongYu1",
      "~XUCHEN1",
      "~Zhihang_Yuan1",
      "~Zhe_jiang5",
      "~Sifan_Zhou2"
    ],
    "pdf": "/pdf/14c21d8737fc02120423cc777efbc1a2ff3368ef.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on post-training quantization (PTQ) of Large Language Models, which is directly relevant to inference optimization. It addresses uneven and heavy-tailed data distributions that expand quantization ranges and reduce precision. The paper introduces OSTQuant, which employs orthogonal and scaling transformations to optimize distributions across the entire quantization space. It shows significant results in W4-only and W4A4KV4 configurations, maintaining 99.5% of floating-point accuracy in the former and reducing performance gaps by 32% in the latter. The method introduces Quantization Space Utilization Rate (QSUR) as a new metric and KL-Top loss function for better optimization with limited calibration data.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization or scalability metrics. There's no direct measurement of throughput or latency improvements. The focus is exclusively on post-training quantization rather than training optimization. The paper doesn't discuss implementation details specific to GPU architectures or optimization techniques beyond quantization.",
      "Summary": "OSTQuant presents a novel quantization approach for LLMs that uses orthogonal and scaling transformations to optimize data distributions across the quantization space. It introduces QSUR as a new metric to assess quantizability and KL-Top loss function to mitigate noise during optimization. The method achieves impressive results in maintaining accuracy while reducing model size, with 99.5% accuracy retention in W4-only setting and 32% performance gap reduction in challenging W4A4KV4 configuration. While not explicitly addressing GPU utilization metrics, the quantization improvements would naturally lead to better inference efficiency."
    }
  },
  {
    "id": "8g9fs6mdEG",
    "title": "Streaming Video Question-Answering with In-context Video KV-Cache Retrieval",
    "abstract": "We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos before responding to queries, and repeat this process for each new question. In contrast, our approach analyzes long videos in a streaming manner, allowing for prompt responses as soon as user queries are received. Building on a common Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring that input frames attend to a limited number of preceding frames, thereby reducing computational overhead. To prevent information loss, we store processed video key-value caches (KV-Caches) in RAM and disk, reloading them into GPU memory as needed. Additionally, we introduce a retrieval method that leverages an external retriever or the parameters within Video-LLMs to retrieve only query-relevant KV-Caches, ensuring both efficiency and accuracy in question answering. ReKV enables the separation of video analyzing and question-answering across different processes and GPUs, significantly enhancing the efficiency of StreamingVQA. Through comprehensive experimentation, we validate the efficacy and practicality of our approach, which significantly boosts efficiency and enhances applicability over existing VideoQA models.",
    "authors": [
      "~Shangzhe_Di1",
      "~Zhelun_Yu1",
      "~Guanghao_Zhang1",
      "~Haoyuan_Li1",
      "~TaoZhong1",
      "~Hao_Cheng9",
      "~Bolin_Li2",
      "~Wanggui_He1",
      "~Fangxun_Shu1",
      "~Hao_Jiang13"
    ],
    "pdf": "/pdf/e491e49e823ff5b84c75cc8daad0e1b80ebfd206.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on key aspects of my research interests including KV-cache optimization for better GPU memory management, inference optimization for improved throughput and reduced latency, and techniques for scaling LLM systems. The approach of managing KV-caches across different storage tiers (GPU, RAM, disk) and the separation of processing across different GPUs directly address core optimization challenges in large language model systems. The streaming approach for handling long inputs and the efficient retrieval mechanisms for query-relevant information are particularly relevant to my focus on performance optimization.",
      "Irrelevant Aspects": "The paper is specifically focused on video question-answering rather than general LLM applications, which limits its direct applicability to my broader research interests. As a training-free approach, it doesn't contribute to my interests in training optimization. The paper also doesn't discuss architectural modifications to the underlying models, focusing instead on system-level optimizations for a specific application domain.",
      "Summary": "ReKV presents a highly relevant approach to inference optimization for Video-LLMs by implementing efficient KV-cache management strategies, streaming processing techniques, and resource allocation across different GPUs. While its focus on video processing is domain-specific, the core techniques for memory management, cache retrieval, and computational efficiency directly align with my research interests in optimizing large language model systems for better GPU utilization, higher throughput, and lower latency."
    }
  },
  {
    "id": "cUN8lJB4rD",
    "title": "Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics",
    "abstract": "In distributed stochastic optimization, where parallel and asynchronous methods are employed, we establish optimal time complexities under virtually any computation behavior of workers/devices/CPUs/GPUs, capturing potential disconnections due to hardware and network delays, time-varying computation powers, and any possible fluctuations and trends of computation speeds. These real-world scenarios are formalized by our new universal computation model. Leveraging this model and new proof techniques, we discover tight lower bounds that apply to virtually all synchronous and asynchronous methods, including Minibatch SGD, Asynchronous SGD (Recht et al., 2011), and Picky SGD (Cohen et al., 2021). We show that these lower bounds, up to constant factors, are matched by the optimal Rennala SGD and Malenia SGD methods (Tyurin & Richtárik, 2023).",
    "authors": [
      "~Alexander_Tyurin1"
    ],
    "pdf": "/pdf/338a9850c70a9cce01d53d2d2bc6d26fc3e707da.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses parallel stochastic optimization which is directly relevant to distributed LLM training. It focuses on time complexities in parallel and asynchronous methods, considering GPU computation dynamics, hardware delays, and network issues - all crucial for optimizing LLM training systems. The analysis of various SGD methods (including synchronous and asynchronous variants) provides theoretical foundations for understanding scalability and throughput in large-scale training environments.",
      "Irrelevant Aspects": "The paper doesn't specifically address large language models or neural architectures. It lacks focus on inference optimization, which is half of my research interest. The theoretical approach to time complexity bounds doesn't provide concrete GPU utilization techniques. Memory optimization techniques, which are critical for LLM training, are not covered. The paper doesn't address model-specific optimizations like quantization, pruning, or attention mechanism optimizations.",
      "Summary": "This paper establishes optimal time complexity bounds for distributed stochastic optimization under various computation dynamics. While highly relevant to the theoretical foundations of parallel training optimization and GPU utilization, it lacks LLM-specific considerations and inference optimization content. The paper's analysis of asynchronous methods and computation variability provides valuable insights for training scalability but doesn't directly address practical implementation details for maximizing GPU throughput in LLM contexts."
    }
  },
  {
    "id": "nEDToD1R8M",
    "title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
    "abstract": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its \nkey components include: 1) using the linear interpolating diffusion form of flow-matching, 2) employing $\\boldsymbol v$-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. \nOur code is available at https://github.com/G-U-N/Rectified-Diffusion.",
    "authors": [
      "~Fu-Yun_Wang1",
      "~Ling_Yang1",
      "~Zhaoyang_Huang2",
      "~Mengdi_Wang1",
      "~Hongsheng_Li3"
    ],
    "pdf": "/pdf/d029e39a615ee12aaf8cb24323aec45997deff65.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Focus on improving generation speed (inference optimization), Training procedure simplification and cost reduction, Techniques for reducing computational intensity, Approaches to improve the efficiency of the model during inference",
      "Irrelevant Aspects": "The paper is focused on visual generation, not language models, The specific techniques are designed for diffusion models, which may not directly apply to LLMs, No explicit discussion of GPU utilization or scalability aspects",
      "Summary": "This paper presents \"Rectified Diffusion,\" a method that generalizes and improves upon the rectification process in flow-matching models for diffusion-based visual generation. The authors argue that the success of rectification lies in using a pretrained model to obtain matched noise-sample pairs, followed by retraining with these pairs, rather than focusing on straightening the ODE path. Their method simplifies training, reduces costs, and achieves superior performance on Stable Diffusion models. While focused on visual generation, the optimization principles could have broader applications in generative model training and inference."
    }
  },
  {
    "id": "oQ4igHyh3N",
    "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
    "abstract": "Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at {\\color{red}\\url{https://github.com/Haiyang-W/TokenFormer.git}}",
    "authors": [
      "~Haiyang_Wang2",
      "~Yue_Fan1",
      "~Muhammad_Ferjad_Naeem1",
      "~Yongqin_Xian1",
      "~Jan_Eric_Lenssen1",
      "~Liwei_Wang1",
      "~Federico_Tombari1",
      "~Bernt_Schiele1"
    ],
    "pdf": "/pdf/430572f27d6b814d52dfb4df61bbb5ccad24d5d0.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization and efficient scaling of large language models, progressive model scaling without retraining from scratch, reduction of training costs, architectural flexibility for better resource utilization, and novel approach to parameter management that enhances scalability",
      "Irrelevant Aspects": "Limited focus on inference optimization techniques such as latency reduction and throughput improvement, lack of hardware-specific optimizations for GPUs, absence of quantization or model compression techniques for efficient inference",
      "Summary": "TokenFormer introduces an innovative architecture that treats model parameters as tokens and replaces linear projections with token-parameter attention layers, enabling efficient scaling from 124M to 1.4B parameters without retraining from scratch. The approach significantly reduces training costs while maintaining performance comparable to traditional Transformers. While highly relevant to training optimization and scalability concerns, the paper doesn't explicitly address inference optimization aspects such as latency and throughput improvements."
    }
  },
  {
    "id": "lTrrnNdkOX",
    "title": "PT-T2I/V: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Image/Video-Task",
    "abstract": "The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to efficiently model global visual information. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the PT-T2I/V family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing computational complexity in image and video generation tasks (e.g., a reduction 59\\% compared to DiT and a reduction 34\\% compared to PixArt-$\\alpha$). The visual exhibition of and code are available at https://360cvgroup.github.io/Qihoo-T2X/.",
    "authors": [
      "~Jing_Wang28",
      "~Ao_Ma2",
      "~Jiasong_Feng4",
      "~Dawei_Leng1",
      "~Yuhui_Yin1",
      "~Xiaodan_Liang2"
    ],
    "pdf": "/pdf/128296bda44d9a59a49b57c8fe8964eb2c785f2a.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper addresses computational efficiency in transformer models through novel attention mechanisms that reduce redundant computation. It claims significant computational complexity reduction (59% compared to DiT and 34% compared to PixArt-$\\alpha$) while maintaining performance, which aligns with my research interests in optimization. The proxy-tokenized approach could potentially be applied to optimize other transformer architectures, including language models.",
      "Irrelevant Aspects": "The work focuses specifically on diffusion transformers for visual generation (text-to-image/video) rather than language models. It doesn't directly address GPU utilization metrics, distributed training/inference scalability, or latency measurements which are central to my research. The optimization techniques are specialized for visual data characteristics rather than general language processing.",
      "Summary": "PT-DiT introduces a sparse representative token attention mechanism that reduces computational redundancy in diffusion transformers by using proxy tokens to capture global semantics. While the paper addresses computational efficiency in transformer architectures, its focus on visual generation limits its direct applicability to language model optimization research, though the attention mechanism innovations could potentially inform optimization strategies for language models."
    }
  },
  {
    "id": "q44uq3tc2D",
    "title": "$\\gamma-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models",
    "abstract": "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue,  we propose an innovative MoD adaptation strategy for existing MLLMs called $\\gamma$-MoD.  In $\\gamma$-MoD,   a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer.  Based on ARank,  we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely  shared vision-language router and  masked routing learning.   With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets.  Experimental results not only validate the significant efficiency benefit of $\\gamma$-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs.  For example, with a minor performance drop,  i.e., -1.5%, $\\gamma$-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.",
    "authors": [
      "~Yaxin_Luo1",
      "~Gen_Luo1",
      "~Jiayi_Ji1",
      "~Yiyi_Zhou1",
      "~Xiaoshuai_Sun3",
      "~Zhiqiang_Shen1",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/69302eb76025af80c717cd2118664635d445723d.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper directly addresses computational cost reduction in MLLMs, aligning with better GPU utilization goals. It focuses on both training and inference optimization, reporting significant time reductions (31.0% training, 53.2% inference for LLaVA-HR). The approach improves throughput by skipping redundant computations through mixture of depths. It enhances scalability by converting over 90% of dense layers to MoD layers. The paper introduces specific optimization techniques (ARank, shared vision-language router, masked routing learning) to maintain performance while achieving computational sparsity.",
      "Irrelevant Aspects": "The focus is specifically on multimodal large language models rather than general text-based LLMs, though optimization techniques could be adapted. The paper doesn't explicitly discuss GPU utilization metrics, though reduced computation naturally leads to better utilization.",
      "Summary": "This paper presents highly relevant research on optimizing both training and inference of large language models, specifically multimodal ones. The proposed γ-MoD approach significantly reduces computational costs by identifying and skipping redundant token computations, achieving substantial time savings (over 50% for inference) with minimal performance degradation. The techniques and results directly address the core research interests of training/inference optimization, throughput improvement, and resource efficiency in large language model systems."
    }
  },
  {
    "id": "oXh0939Zzq",
    "title": "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
    "abstract": "Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduces dynamic $\\textbf{Lo}$w-rank $\\textbf{S}$parse $\\textbf{A}$daptation $\\textbf{(LoSA)}$, a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, to achieve the optimal sparse model architecture, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby dynamically determining the optimal layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by $\\textbf{68.73}$$\\downarrow$ and increased zero-shot accuracy by $\\textbf{16.32}$%$\\uparrow$, achieving a $\\textbf{2.60$\\times$}$ speedup on CPU and $\\textbf{2.23$\\times$}$ speedup on GPU, requiring only $\\textbf{45 minutes}$ of fine-tuning on $\\textbf{a single}$ NVIDIA A100 80GB GPU. Code is available at https://github.com/wzhuang-xmu/LoSA.",
    "authors": [
      "~Weizhong_Huang1",
      "~Yuxin_Zhang3",
      "~Xiawu_Zheng1",
      "~Liuyang5",
      "~Jing_Lin6",
      "~Yiwu_Yao1",
      "~Rongrong_Ji5"
    ],
    "pdf": "/pdf/2f1a6d2158fc9555f828a6136a65f6d140f29933.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": [
        "Sparsity optimization for LLMs to improve efficiency",
        "Low-Rank Adaptation (LoRA) integration with sparse models",
        "Inference latency reduction (no additional latency)",
        "GPU utilization improvement (2.23× speedup on GPU)",
        "Efficient training (45 minutes on single A100 GPU)",
        "Layer-wise optimization for resource utilization",
        "Model architecture optimization for scalability"
      ],
      "Irrelevant Aspects": [
        "Theoretical aspects of Representation Mutual Information",
        "Specific fine-tuning methodology details",
        "Reconstruction error metrics",
        "Language model performance metrics like perplexity"
      ],
      "Summary": "This paper introduces LoSA, a method for efficiently combining sparsity with low-rank adaptation in LLMs. It directly addresses key optimization concerns by reducing inference latency (2.23× GPU speedup) while maintaining performance, requiring minimal training resources (single A100 GPU, 45 minutes). The method optimizes layer-wise sparsity and adapts LoRA ranks based on reconstruction errors, demonstrating practical improvements in GPU utilization and throughput. While it doesn't cover all aspects of system optimization like distributed training, it provides significant contributions to inference optimization and resource efficiency."
    }
  },
  {
    "id": "RYrJqz44p4",
    "title": "Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning",
    "abstract": "Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. \nIn this paper, we delve into the concept of task-specific directions (TSDs)—critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Extensive experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and in-depth analyses further reveal the underlying mechanisms of LoRA-Dash.",
    "authors": [
      "~Chongjie_Si1",
      "~Zhiyi_Shi1",
      "~Shifan_Zhang1",
      "~Xiaokang_Yang1",
      "~Hanspeter_Pfister1",
      "~Wei_Shen2"
    ],
    "pdf": "/pdf/c33da87522b83dcc853eb1b2193bdbfa69c1af98.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on Parameter Efficient Fine-Tuning (PEFT) strategies like LoRA, which directly addresses training optimization for large language models. It introduces novel approaches to maximize the impact of task-specific directions during fine-tuning, which can improve GPU utilization and reduce resource consumption during training. The work on transitioning models efficiently from pretrained states to task-specific enhancements contributes to better scalability of large language model training.",
      "Irrelevant Aspects": "The paper does not explicitly address inference optimization techniques, latency reduction during inference, or throughput improvement in deployed systems. The abstract does not discuss specific implementation details for maximizing GPU memory utilization or distributed training strategies that would be directly relevant to infrastructure optimization.",
      "Summary": "This paper explores parameter-efficient fine-tuning methods, specifically focusing on task-specific directions and introducing a novel approach called LoRA-Dash. While highly relevant to training optimization and resource efficiency, the abstract does not provide sufficient detail on inference optimization aspects. The work appears valuable for reducing training resource requirements but may have limited direct applicability to inference-time performance improvements."
    }
  },
  {
    "id": "PpYy0dR3Qw",
    "title": "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression",
    "abstract": "In $D$istributed optimization and $L$earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of $Lo$cal training, which reduces the communication frequency, and $Co$mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.",
    "authors": [
      "~Laurent_Condat1",
      "~Arto_Maranjyan1",
      "~Peter_Richtárik1"
    ],
    "pdf": "/pdf/d98dc76318e299d274b10fc41812c88569561f4b.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Communication efficiency in distributed learning, local training to reduce communication frequency, compression techniques (sparsification and quantization), accelerated communication complexity, heterogeneous distributed systems",
      "Irrelevant Aspects": "Focus on strongly convex functions rather than non-convex optimization typical in LLMs, limited mention of inference optimization, focus on federated learning rather than large-scale data center training",
      "Summary": "LoCoDL combines local training and compression to improve communication efficiency in distributed learning. While it offers theoretical guarantees and practical benefits for communication reduction, its focus on convex optimization and federated learning makes it somewhat less directly applicable to LLM training optimization, though the techniques could provide valuable insights for improving GPU utilization and scalability in distributed systems."
    }
  },
  {
    "id": "zqzsZ5cXbB",
    "title": "Let the Code LLM Edit Itself When You Edit the Code",
    "abstract": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing $\\underline{\\textbf{P}\\text{ositional}\\  \\textbf{I}\\text{ntegrity}\\  \\textbf{E}\\text{ncoding}}$ (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85%  compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.",
    "authors": [
      "~Zhenyu_He3",
      "~Jun_Zhang27",
      "~Shengjie_Luo1",
      "~Jingjing_Xu1",
      "~Zhi_Zhang4",
      "~Di_He1"
    ],
    "pdf": "/pdf/029ac4c4e30379df5de6edfe4c104ac306fc2d10.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Focus on inference optimization for LLMs, addressing computational bottlenecks in real-time code editing, significant reduction in computational overhead (85%+), solution that works across different model sizes (1.3B to 33B), impact on latency and throughput in code editing scenarios, GPU utilization improvement through efficient computation",
      "Irrelevant Aspects": "The focus is specifically on code editing, which is a narrow application domain, the paper doesn't discuss training optimization, only inference, limited to models using rotary positional encoding (though this is common)",
      "Summary": "This paper presents Positional Integrity Encoding (PIE), a method to optimize inference for code LLMs during real-time code editing. It addresses the computational inefficiency of recomputing the entire KV cache when code is edited by modifying just the affected parts. The approach reduces computational overhead by over 85% while maintaining model accuracy across various model sizes and editing tasks, making it highly relevant to inference optimization for LLMs."
    }
  },
  {
    "id": "dDpB23VbVa",
    "title": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models",
    "abstract": "The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\\times$, without compromising the model performance compared to token-level training. Source code: \\url{https://github.com/shaochenze/PatchTrain}.",
    "authors": [
      "~Chenze_Shao1",
      "~Fandong_Meng3",
      "~Jie_Zhou8"
    ],
    "pdf": "/pdf/dc2cfc56513c9b5bee4590c32ce50b776f8c1d8e.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Training optimization, efficiency improvements, computational efficiency through token aggregation, training methodology innovation, training-inference alignment considerations",
      "Irrelevant Aspects": "Limited focus on inference optimization, no explicit discussion of GPU utilization improvements, no direct mention of throughput or latency during inference, absence of distributed training considerations",
      "Summary": "The paper introduces a novel patch-level training approach for LLMs that reduces training costs by aggregating tokens into higher-information-density units. While strongly relevant to training optimization aspects of my expertise, it has limited focus on inference optimization, which is also a key area of my research interest."
    }
  },
  {
    "id": "T9u56s7mbk",
    "title": "Learning Harmonized Representations for Speculative Sampling",
    "abstract": "Speculative sampling is a promising approach to accelerate the decoding stage for Large Language Models (LLMs). Recent advancements that leverage target LLM's contextual information, such as hidden states and KV cache, have shown significant practical improvements. However, these approaches suffer from inconsistent context between training and decoding. We also observe another discrepancy between the training and decoding objectives in existing speculative sampling methods. In this work, we propose a solution named HArmonized Speculative Sampling (HASS) that learns harmonized representations to address these issues. HASS accelerates the decoding stage without adding inference overhead through harmonized objective distillation and harmonized context alignment. Experiments on four LLaMA models demonstrate that HASS achieves 2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, surpassing EAGLE-2 by 8%-20%. The code is available at https://github.com/HArmonizedSS/HASS.",
    "authors": [
      "~Lefan_Zhang1",
      "~Xiaodan_Wang1",
      "~Yanhua_Huang1",
      "~Ruiwen_Xu1"
    ],
    "pdf": "/pdf/dc7ac5cfcbc6994621552d4b9e00b2d86e9467cc.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "The paper focuses on speculative sampling to accelerate LLM decoding, which directly relates to inference optimization. It addresses wall-clock time speedup ratio (2.81x-4.05x), improves throughput without adding inference overhead, and demonstrates concrete performance improvements over existing methods like EAGLE-2. The work deals with the crucial challenge of context inconsistency between training and decoding in LLM systems, and experiments on LLaMA models provide practical insights for real-world deployment.",
      "Irrelevant Aspects": "The paper doesn't explicitly address GPU utilization metrics or distributed scalability across multiple GPUs. It appears to focus more on algorithmic improvements rather than hardware-specific optimizations that would affect GPU efficiency. There's limited discussion on memory optimization strategies or detailed analysis of resource consumption during inference.",
      "Summary": "This paper presents HArmonized Speculative Sampling (HASS), a method to accelerate LLM decoding by addressing inconsistencies between training and decoding contexts. The technique achieves significant speedup (2.81x-4.05x) without adding inference overhead through harmonized objective distillation and context alignment. While it provides substantial improvements in inference speed and throughput, it primarily focuses on algorithmic aspects of speculative sampling rather than GPU utilization or scalability concerns."
    }
  },
  {
    "id": "W49UjcpGxx",
    "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
    "abstract": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \\textit{directly reusing adjacent-step features degrades video quality due to the loss of subtle variations}. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67$\\times$ speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality. \\textit{Our code will be made public upon publication.}",
    "authors": [
      "~Zhengyao_Lv1",
      "~Chenyang_Si2",
      "~Junhao_Song2",
      "~Zhenyu_Yang10",
      "~Yu_Qiao1",
      "~Ziwei_Liu1",
      "~Kwan-Yee_K._Wong1"
    ],
    "pdf": "/pdf/f193ec645c4a302ce9c2a19c5b8e5ee260774696.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Inference optimization techniques, feature caching strategies for improved GPU utilization, training-free acceleration methods, reducing redundant computations in ML models, achieving higher throughput with maintained quality, system-level optimizations for ML models, classifier-free guidance optimization for speed",
      "Irrelevant Aspects": "Focus on video diffusion models rather than large language models, temporal continuity preservation specific to video generation, video quality metrics not applicable to text generation, video-specific feature reuse strategies",
      "Summary": "This paper presents FasterCache, a training-free method to accelerate video diffusion model inference through dynamic feature reuse and optimization of classifier-free guidance. While focused on video generation rather than language models, the caching strategies and inference acceleration techniques have relevance to ML system optimization. The paper achieves 1.67× speedup while maintaining quality, demonstrating practical approaches to reducing computational redundancy and improving GPU utilization during inference."
    }
  },
  {
    "id": "b10lRabU9W",
    "title": "DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale",
    "abstract": "Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce \\textbf{DeepGate4}, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) an update strategy tailored for circuit graphs, which reduce memory complexity to sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse transformer with global and local structural encodings for AIGs; and (3) an inference acceleration CUDA kernel that fully exploit the unique sparsity patterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5\\% and 31.1\\% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1\\% and memory usage by 46.8\\%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.",
    "authors": [
      "~Ziyang_Zheng1",
      "~Shan_Huang7",
      "~Jianyuan_Zhong1",
      "~Zhengyuan_Shi1",
      "~Guohao_Dai4",
      "~Ningyi_Xu1",
      "~Qiang_Xu1"
    ],
    "pdf": "/pdf/73be0bed8ecf01fa8c143066b35552759a8323ee.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "Memory complexity reduction to sub-linear, addressing quadratic complexity of transformers, sparse attention mechanisms, CUDA kernel development for inference acceleration, GPU utilization optimization, scalability to large-scale models, significant runtime (35.1%) and memory usage (46.8%) improvements",
      "Irrelevant Aspects": "Domain-specific to circuit design and electronic design automation, graph-specific optimizations for AIGs rather than token sequences, application-specific tasks like testability analysis and logic reasoning, circuit-tailored update strategies",
      "Summary": "DeepGate4 introduces a graph transformer for efficient circuit representation learning with transferable optimization techniques for ML systems. While focused on EDA tasks rather than language processing, its innovations in reducing transformer quadratic complexity, implementing sparse operations, and developing CUDA kernels provide valuable insights for LLM optimization. The substantial memory and runtime improvements demonstrate effective scalability approaches applicable to large-scale models."
    }
  },
  {
    "id": "xXTkbTBmqq",
    "title": "OLMoE: Open Mixture-of-Experts Language Models",
    "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.",
    "authors": [
      "~Niklas_Muennighoff1",
      "~Luca_Soldaini1",
      "~Dirk_Groeneveld1",
      "~Kyle_Lo1",
      "~Jacob_Morrison2",
      "~Sewon_Min1",
      "~Weijia_Shi1",
      "~Evan_Pete_Walsh1",
      "~Oyvind_Tafjord2",
      "~Nathan_Lambert1",
      "~Yuling_Gu1",
      "~Shane_Arora1",
      "~Akshita_Bhagia1",
      "~Dustin_Schwenk1",
      "~David_Wadden1",
      "~Alexander_Wettig1",
      "~Binyuan_Hui1",
      "~Tim_Dettmers2",
      "~Douwe_Kiela1",
      "~Ali_Farhadi3",
      "~Noah_A._Smith2",
      "~Pang_Wei_Koh1",
      "~Amanpreet_Singh1",
      "~Hannaneh_Hajishirzi1"
    ],
    "pdf": "/pdf/4f02353f84957b0176b1d5326741499f768d49a4.pdf",
    "classify": {
      "score": 8,
      "Relevant Aspects": "Mixture-of-Experts (MoE) architecture which optimizes parameter usage; model efficiency with 7B parameters using only 1B per token; novel findings on MoE training optimization; open-source resources including weights, data, and code; performance improvements over similar and larger models",
      "Irrelevant Aspects": "Lack of explicit focus on GPU utilization and scalability metrics; no detailed discussion of throughput and latency measurements; primary emphasis on open model release rather than technical optimization details",
      "Summary": "OLMoE presents a highly relevant sparse MoE language model that balances parameter count and computational efficiency. The paper's focus on training optimization with MoE architecture and resource efficiency makes it valuable for research in LLM optimization, though it could provide more technical details on GPU utilization and performance metrics."
    }
  },
  {
    "id": "r8H7xhYPwz",
    "title": "Gated Delta Networks: Improving Mamba2 with Delta Rule",
    "abstract": "Linear Transformers have gained attention as efficient alternatives to standard Transformers, but their performance in retrieval and long-context tasks has been limited.  To address these limitations, recent work has explored two distinct mechanisms: gating for adaptive memory control and the delta update rule for precise memory modifications. We observe that these mechanisms are complementary—gating enables rapid memory erasure while the delta rule facilitates targeted updates. Building on this insight, we introduce the gated delta rule and develop a parallel training algorithm optimized for modern hardware. Our proposed architecture, Gated DeltaNet, consistently surpasses existing models like Mamba2 and DeltaNet across multiple benchmarks, including language modeling, common-sense reasoning, in-context retrieval, length extrapolation, and long-context understanding. We further enhance performance by developing hybrid architectures that combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, achieving both improved training efficiency and superior task performance.",
    "authors": [
      "~Songlin_Yang1",
      "~Jan_Kautz1",
      "~Ali_Hatamizadeh1"
    ],
    "pdf": "/pdf/3b2c0abfb1a7e49a0aec0aacedcce4ef8fbd620d.pdf",
    "classify": {
      "score": 7,
      "Relevant Aspects": "The paper focuses on Linear Transformers as efficient alternatives to standard Transformers, introduces a parallel training algorithm optimized for modern hardware, and claims improved training efficiency compared to existing models like Mamba2. The development of hybrid architectures could have implications for GPU utilization and scalability.",
      "Irrelevant Aspects": "The paper seems to focus more on the architecture itself rather than the optimization of training and inference processes. There's no explicit mention of GPU utilization, throughput, or latency metrics. The focus appears to be on model performance across benchmarks rather than the efficiency metrics relevant to the research interest.",
      "Summary": "This paper introduces Gated Delta Networks as an improvement to Mamba2, combining gating for adaptive memory control with delta update rule for precise memory modifications. It claims to surpass existing models across multiple benchmarks and mentions improved training efficiency. While it has some relevance to the research interest through its focus on efficient architectures and training optimization, it doesn't primarily focus on GPU utilization, throughput, and latency metrics that are central to the research interest."
    }
  },
  {
    "id": "6XUSDvBFkV",
    "title": "STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs",
    "abstract": "In this paper, we present the first structural binarization method for LLM compression to less than 1-bit precision. Although LLMs have achieved remarkable performance, their memory-bound nature during the inference stage hinders the adoption of resource-constrained devices. Reducing weights to 1-bit precision through binarization substantially enhances computational efficiency. We observe that randomly flipping some weights in binarized LLMs does not significantly degrade the model's performance, suggesting the potential for further compression. To exploit this, our STBLLM employs an N:M sparsity technique to achieve structural binarization of the weights. Specifically, we introduce a novel Standardized Importance (SI) metric, which considers weight magnitude and input feature norm to more accurately assess weight significance. Then, we propose a layer-wise approach, allowing different layers of the LLM to be sparsified with varying N:M ratios, thereby balancing compression and accuracy. Furthermore, we implement a fine-grained grouping strategy for less important weights, applying distinct quantization schemes to sparse, intermediate, and dense regions. Finally, we design a specialized CUDA kernel to support structural binarization. We conduct extensive experiments on LLaMA, OPT, and Mistral family. STBLLM achieves a perplexity of 11.07 at 0.55 bits per weight, outperforming the BiLLM by 3×. The results demonstrate that our approach performs better than other compressed binarization LLM methods while significantly reducing memory requirements. Code is released at https://github.com/pprp/STBLLM.",
    "authors": [
      "~Peijie_Dong1",
      "~Lujun_Li1",
      "~Yuedong_Zhong1",
      "~DaYou_Du1",
      "~Ruibo_FAN1",
      "~Yuhan_Chen6",
      "~Zhenheng_Tang2",
      "~Qiang_Wang14",
      "~Wei_Xue5",
      "~Yike_Guo1",
      "~Xiaowen_Chu2"
    ],
    "pdf": "/pdf/e637a398f57405c231dabb741fb48a1dd1a36144.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "Structural binarization for LLM compression to sub-1-bit precision, addressing memory-bound inference challenges, implementing specialized CUDA kernels for GPU utilization, reducing memory requirements while maintaining performance, layer-wise optimization approach with varying N:M ratios, fine-grained grouping strategy for different quantization schemes",
      "Irrelevant Aspects": "Limited focus on training optimization, insufficient details on throughput and latency measurements, minimal discussion of scalability across multiple GPUs",
      "Summary": "STBLLM presents a highly relevant approach for LLM inference optimization through structural binarization to sub-1-bit precision. The paper introduces innovative techniques like N:M sparsity, a Standardized Importance metric, and specialized CUDA kernels that directly address GPU utilization challenges. While it excellently covers memory reduction and compression techniques, it lacks extensive discussion on training optimization, throughput, latency measurements, and multi-GPU scalability, which are important aspects of my research interests."
    }
  },
  {
    "id": "ZU8OdDLTts",
    "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models",
    "abstract": "Large Language Models (LLMs) have greatly pushed forward advancements in natural language processing, yet their high memory and computational demands hinder practical deployment. Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reducing the high demands on computation and memory. However, current binarization methods struggle to narrow the distribution gap between binarized and full-precision weights, while also overlooking the column deviation in LLM weight distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit post-training quantization (PTQ) technique tailored for LLMs. To narrow the distribution shift between binarized and full-precision weights, we first design an alternating refined binarization (ARB) algorithm to progressively update the binarization parameters, which significantly reduces the quantization error. Moreover, considering the pivot role of calibration data and the column deviation in LLM weights, we further extend ARB to ARB-X and ARB-RC. In addition, we refine the weight partition strategy with column-group bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC with CGB, we obtain ARB-LLM$_{\\text{X}}$ and ARB-LLM$ _{\\text{RC}} $ respectively, which significantly outperform state-of-the-art (SOTA) binarization methods for LLMs.\nAs a binary PTQ method, our ARB-LLM$ _{\\text{RC}} $ is the first to surpass FP16 models of the same size. Code: https://github.com/ZHITENGLI/ARB-LLM.",
    "authors": [
      "~Zhiteng_Li2",
      "~Xianglong_Yan2",
      "~Tianao_Zhang1",
      "~Haotong_Qin1",
      "~Dong_Xie2",
      "~Jiang_Tian1",
      "~zhongchao_shi1",
      "~Linghe_Kong1",
      "~Yulun_Zhang1",
      "~Xiaokang_Yang1"
    ],
    "pdf": "/pdf/01ea15adbb9e354ab059bc5bf300c7d76bba4920.pdf",
    "classify": {
      "score": 9,
      "Relevant Aspects": "The paper focuses on 1-bit post-training quantization (PTQ) for LLMs, which directly addresses inference optimization. The binarization technique can significantly improve GPU utilization by enabling binary operations, reduce memory footprint by compressing weights to 1 bit, and potentially increase throughput while reducing latency. The method claims to be the first binary PTQ approach that surpasses FP16 models of the same size, indicating substantial performance improvements.",
      "Irrelevant Aspects": "The paper focuses on post-training quantization rather than training optimization techniques. It doesn't cover distributed training strategies or multi-GPU scaling approaches. The method is specifically for inference optimization rather than training-phase optimizations.",
      "Summary": "ARB-LLM presents a novel 1-bit post-training quantization technique for large language models that addresses key inference optimization challenges. The alternating refined binarization algorithm reduces quantization error while considering weight distribution characteristics in LLMs. By enabling 1-bit representations, the method promises significant improvements in GPU utilization, memory efficiency, throughput, and latency - all critical aspects of LLM deployment optimization."
    }
  }
]
